Files already downloaded and verified
Files already downloaded and verified
/home/hlf22/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/hlf22/anaconda3/envs/pytorch/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
Training Epoch: 1 [128/50000]	Loss: 4.7773	LR: 0.050000
Training Epoch: 1 [256/50000]	Loss: 4.7534	LR: 0.050000
Training Epoch: 1 [384/50000]	Loss: 4.7055	LR: 0.050000
Training Epoch: 1 [512/50000]	Loss: 4.6377	LR: 0.050000
Training Epoch: 1 [640/50000]	Loss: 4.7515	LR: 0.050000
Training Epoch: 1 [768/50000]	Loss: 4.6404	LR: 0.050000
Training Epoch: 1 [896/50000]	Loss: 4.8239	LR: 0.050000
Training Epoch: 1 [1024/50000]	Loss: 4.6798	LR: 0.050000
Training Epoch: 1 [1152/50000]	Loss: 4.7486	LR: 0.050000
Training Epoch: 1 [1280/50000]	Loss: 4.7981	LR: 0.050000
Training Epoch: 1 [1408/50000]	Loss: 4.6318	LR: 0.050000
Training Epoch: 1 [1536/50000]	Loss: 4.8459	LR: 0.050000
Training Epoch: 1 [1664/50000]	Loss: 4.6126	LR: 0.050000
Training Epoch: 1 [1792/50000]	Loss: 4.8307	LR: 0.050000
Training Epoch: 1 [1920/50000]	Loss: 4.7284	LR: 0.050000
Training Epoch: 1 [2048/50000]	Loss: 4.8477	LR: 0.050000
Training Epoch: 1 [2176/50000]	Loss: 4.6132	LR: 0.050000
Training Epoch: 1 [2304/50000]	Loss: 4.7006	LR: 0.050000
Training Epoch: 1 [2432/50000]	Loss: 4.5406	LR: 0.050000
Training Epoch: 1 [2560/50000]	Loss: 4.6992	LR: 0.050000
Training Epoch: 1 [2688/50000]	Loss: 4.4378	LR: 0.050000
Training Epoch: 1 [2816/50000]	Loss: 4.4165	LR: 0.050000
Training Epoch: 1 [2944/50000]	Loss: 4.6818	LR: 0.050000
Training Epoch: 1 [3072/50000]	Loss: 4.5207	LR: 0.050000
Training Epoch: 1 [3200/50000]	Loss: 4.7458	LR: 0.050000
Training Epoch: 1 [3328/50000]	Loss: 4.4134	LR: 0.050000
Training Epoch: 1 [3456/50000]	Loss: 4.4980	LR: 0.050000
Training Epoch: 1 [3584/50000]	Loss: 4.3726	LR: 0.050000
Training Epoch: 1 [3712/50000]	Loss: 4.4727	LR: 0.050000
Training Epoch: 1 [3840/50000]	Loss: 4.4191	LR: 0.050000
Training Epoch: 1 [3968/50000]	Loss: 4.1926	LR: 0.050000
Training Epoch: 1 [4096/50000]	Loss: 4.4477	LR: 0.050000
Training Epoch: 1 [4224/50000]	Loss: 4.5413	LR: 0.050000
Training Epoch: 1 [4352/50000]	Loss: 4.4700	LR: 0.050000
Training Epoch: 1 [4480/50000]	Loss: 4.5748	LR: 0.050000
Training Epoch: 1 [4608/50000]	Loss: 4.3896	LR: 0.050000
Training Epoch: 1 [4736/50000]	Loss: 4.3882	LR: 0.050000
Training Epoch: 1 [4864/50000]	Loss: 4.2914	LR: 0.050000
Training Epoch: 1 [4992/50000]	Loss: 4.5098	LR: 0.050000
Training Epoch: 1 [5120/50000]	Loss: 4.3335	LR: 0.050000
Training Epoch: 1 [5248/50000]	Loss: 4.4367	LR: 0.050000
Training Epoch: 1 [5376/50000]	Loss: 4.3503	LR: 0.050000
Training Epoch: 1 [5504/50000]	Loss: 4.3467	LR: 0.050000
Training Epoch: 1 [5632/50000]	Loss: 4.3560	LR: 0.050000
Training Epoch: 1 [5760/50000]	Loss: 4.3654	LR: 0.050000
Training Epoch: 1 [5888/50000]	Loss: 4.2456	LR: 0.050000
Training Epoch: 1 [6016/50000]	Loss: 4.1519	LR: 0.050000
Training Epoch: 1 [6144/50000]	Loss: 4.1059	LR: 0.050000
Training Epoch: 1 [6272/50000]	Loss: 4.4151	LR: 0.050000
Training Epoch: 1 [6400/50000]	Loss: 4.5087	LR: 0.050000
Training Epoch: 1 [6528/50000]	Loss: 4.1117	LR: 0.050000
Training Epoch: 1 [6656/50000]	Loss: 4.2402	LR: 0.050000
Training Epoch: 1 [6784/50000]	Loss: 4.1700	LR: 0.050000
Training Epoch: 1 [6912/50000]	Loss: 4.2383	LR: 0.050000
Training Epoch: 1 [7040/50000]	Loss: 4.3400	LR: 0.050000
Training Epoch: 1 [7168/50000]	Loss: 4.3365	LR: 0.050000
Training Epoch: 1 [7296/50000]	Loss: 4.1252	LR: 0.050000
Training Epoch: 1 [7424/50000]	Loss: 4.1468	LR: 0.050000
Training Epoch: 1 [7552/50000]	Loss: 4.2449	LR: 0.050000
Training Epoch: 1 [7680/50000]	Loss: 4.1644	LR: 0.050000
Training Epoch: 1 [7808/50000]	Loss: 4.1369	LR: 0.050000
Training Epoch: 1 [7936/50000]	Loss: 4.2714	LR: 0.050000
Training Epoch: 1 [8064/50000]	Loss: 4.2248	LR: 0.050000
Training Epoch: 1 [8192/50000]	Loss: 4.2597	LR: 0.050000
Training Epoch: 1 [8320/50000]	Loss: 4.1687	LR: 0.050000
Training Epoch: 1 [8448/50000]	Loss: 4.2938	LR: 0.050000
Training Epoch: 1 [8576/50000]	Loss: 4.3224	LR: 0.050000
Training Epoch: 1 [8704/50000]	Loss: 4.2645	LR: 0.050000
Training Epoch: 1 [8832/50000]	Loss: 4.1571	LR: 0.050000
Training Epoch: 1 [8960/50000]	Loss: 4.3014	LR: 0.050000
Training Epoch: 1 [9088/50000]	Loss: 4.3577	LR: 0.050000
Training Epoch: 1 [9216/50000]	Loss: 4.0257	LR: 0.050000
Training Epoch: 1 [9344/50000]	Loss: 4.2313	LR: 0.050000
Training Epoch: 1 [9472/50000]	Loss: 4.5780	LR: 0.050000
Training Epoch: 1 [9600/50000]	Loss: 4.2882	LR: 0.050000
Training Epoch: 1 [9728/50000]	Loss: 4.4099	LR: 0.050000
Training Epoch: 1 [9856/50000]	Loss: 4.1970	LR: 0.050000
Training Epoch: 1 [9984/50000]	Loss: 4.2446	LR: 0.050000
Training Epoch: 1 [10112/50000]	Loss: 4.0692	LR: 0.050000
Training Epoch: 1 [10240/50000]	Loss: 4.0850	LR: 0.050000
Training Epoch: 1 [10368/50000]	Loss: 4.2159	LR: 0.050000
Training Epoch: 1 [10496/50000]	Loss: 3.9105	LR: 0.050000
Training Epoch: 1 [10624/50000]	Loss: 4.0143	LR: 0.050000
Training Epoch: 1 [10752/50000]	Loss: 4.2583	LR: 0.050000
Training Epoch: 1 [10880/50000]	Loss: 4.2070	LR: 0.050000
Training Epoch: 1 [11008/50000]	Loss: 3.9735	LR: 0.050000
Training Epoch: 1 [11136/50000]	Loss: 4.2425	LR: 0.050000
Training Epoch: 1 [11264/50000]	Loss: 4.1983	LR: 0.050000
Training Epoch: 1 [11392/50000]	Loss: 4.1530	LR: 0.050000
Training Epoch: 1 [11520/50000]	Loss: 4.0454	LR: 0.050000
Training Epoch: 1 [11648/50000]	Loss: 4.0330	LR: 0.050000
Training Epoch: 1 [11776/50000]	Loss: 4.1309	LR: 0.050000
Training Epoch: 1 [11904/50000]	Loss: 3.9476	LR: 0.050000
Training Epoch: 1 [12032/50000]	Loss: 4.0008	LR: 0.050000
Training Epoch: 1 [12160/50000]	Loss: 4.0030	LR: 0.050000
Training Epoch: 1 [12288/50000]	Loss: 4.0335	LR: 0.050000
Training Epoch: 1 [12416/50000]	Loss: 3.8859	LR: 0.050000
Training Epoch: 1 [12544/50000]	Loss: 3.9918	LR: 0.050000
Training Epoch: 1 [12672/50000]	Loss: 3.9046	LR: 0.050000
Training Epoch: 1 [12800/50000]	Loss: 3.9675	LR: 0.050000
Training Epoch: 1 [12928/50000]	Loss: 4.1065	LR: 0.050000
Training Epoch: 1 [13056/50000]	Loss: 3.9641	LR: 0.050000
Training Epoch: 1 [13184/50000]	Loss: 4.0336	LR: 0.050000
Training Epoch: 1 [13312/50000]	Loss: 3.9899	LR: 0.050000
Training Epoch: 1 [13440/50000]	Loss: 4.0871	LR: 0.050000
Training Epoch: 1 [13568/50000]	Loss: 3.9273	LR: 0.050000
Training Epoch: 1 [13696/50000]	Loss: 4.1690	LR: 0.050000
Training Epoch: 1 [13824/50000]	Loss: 3.9143	LR: 0.050000
Training Epoch: 1 [13952/50000]	Loss: 4.1476	LR: 0.050000
Training Epoch: 1 [14080/50000]	Loss: 4.0038	LR: 0.050000
Training Epoch: 1 [14208/50000]	Loss: 4.0296	LR: 0.050000
Training Epoch: 1 [14336/50000]	Loss: 3.9925	LR: 0.050000
Training Epoch: 1 [14464/50000]	Loss: 4.0234	LR: 0.050000
Training Epoch: 1 [14592/50000]	Loss: 3.8691	LR: 0.050000
Training Epoch: 1 [14720/50000]	Loss: 3.8588	LR: 0.050000
Training Epoch: 1 [14848/50000]	Loss: 3.9609	LR: 0.050000
Training Epoch: 1 [14976/50000]	Loss: 4.0559	LR: 0.050000
Training Epoch: 1 [15104/50000]	Loss: 4.1205	LR: 0.050000
Training Epoch: 1 [15232/50000]	Loss: 3.9555	LR: 0.050000
Training Epoch: 1 [15360/50000]	Loss: 4.0635	LR: 0.050000
Training Epoch: 1 [15488/50000]	Loss: 3.8018	LR: 0.050000
Training Epoch: 1 [15616/50000]	Loss: 3.9985	LR: 0.050000
Training Epoch: 1 [15744/50000]	Loss: 3.8866	LR: 0.050000
Training Epoch: 1 [15872/50000]	Loss: 4.0118	LR: 0.050000
Training Epoch: 1 [16000/50000]	Loss: 3.7987	LR: 0.050000
Training Epoch: 1 [16128/50000]	Loss: 3.8575	LR: 0.050000
Training Epoch: 1 [16256/50000]	Loss: 3.8616	LR: 0.050000
Training Epoch: 1 [16384/50000]	Loss: 3.9372	LR: 0.050000
Training Epoch: 1 [16512/50000]	Loss: 4.0308	LR: 0.050000
Training Epoch: 1 [16640/50000]	Loss: 4.0066	LR: 0.050000
Training Epoch: 1 [16768/50000]	Loss: 4.0327	LR: 0.050000
Training Epoch: 1 [16896/50000]	Loss: 3.9550	LR: 0.050000
Training Epoch: 1 [17024/50000]	Loss: 3.7515	LR: 0.050000
Training Epoch: 1 [17152/50000]	Loss: 4.1312	LR: 0.050000
Training Epoch: 1 [17280/50000]	Loss: 3.7499	LR: 0.050000
Training Epoch: 1 [17408/50000]	Loss: 4.0434	LR: 0.050000
Training Epoch: 1 [17536/50000]	Loss: 4.0589	LR: 0.050000
Training Epoch: 1 [17664/50000]	Loss: 4.2054	LR: 0.050000
Training Epoch: 1 [17792/50000]	Loss: 3.8479	LR: 0.050000
Training Epoch: 1 [17920/50000]	Loss: 4.0469	LR: 0.050000
Training Epoch: 1 [18048/50000]	Loss: 3.9683	LR: 0.050000
Training Epoch: 1 [18176/50000]	Loss: 3.9916	LR: 0.050000
Training Epoch: 1 [18304/50000]	Loss: 4.1489	LR: 0.050000
Training Epoch: 1 [18432/50000]	Loss: 3.7959	LR: 0.050000
Training Epoch: 1 [18560/50000]	Loss: 3.9178	LR: 0.050000
Training Epoch: 1 [18688/50000]	Loss: 3.8937	LR: 0.050000
Training Epoch: 1 [18816/50000]	Loss: 3.8613	LR: 0.050000
Training Epoch: 1 [18944/50000]	Loss: 3.9168	LR: 0.050000
Training Epoch: 1 [19072/50000]	Loss: 3.9273	LR: 0.050000
Training Epoch: 1 [19200/50000]	Loss: 4.1009	LR: 0.050000
Training Epoch: 1 [19328/50000]	Loss: 3.9040	LR: 0.050000
Training Epoch: 1 [19456/50000]	Loss: 4.0095	LR: 0.050000
Training Epoch: 1 [19584/50000]	Loss: 3.9380	LR: 0.050000
Training Epoch: 1 [19712/50000]	Loss: 3.9362	LR: 0.050000
Training Epoch: 1 [19840/50000]	Loss: 4.0955	LR: 0.050000
Training Epoch: 1 [19968/50000]	Loss: 3.9825	LR: 0.050000
Training Epoch: 1 [20096/50000]	Loss: 3.8018	LR: 0.050000
Training Epoch: 1 [20224/50000]	Loss: 4.0845	LR: 0.050000
Training Epoch: 1 [20352/50000]	Loss: 3.9082	LR: 0.050000
Training Epoch: 1 [20480/50000]	Loss: 3.8394	LR: 0.050000
Training Epoch: 1 [20608/50000]	Loss: 3.7751	LR: 0.050000
Training Epoch: 1 [20736/50000]	Loss: 4.0890	LR: 0.050000
Training Epoch: 1 [20864/50000]	Loss: 3.8069	LR: 0.050000
Training Epoch: 1 [20992/50000]	Loss: 3.8563	LR: 0.050000
Training Epoch: 1 [21120/50000]	Loss: 3.9692	LR: 0.050000
Training Epoch: 1 [21248/50000]	Loss: 4.0420	LR: 0.050000
Training Epoch: 1 [21376/50000]	Loss: 3.9488	LR: 0.050000
Training Epoch: 1 [21504/50000]	Loss: 4.1200	LR: 0.050000
Training Epoch: 1 [21632/50000]	Loss: 3.9305	LR: 0.050000
Training Epoch: 1 [21760/50000]	Loss: 3.9391	LR: 0.050000
Training Epoch: 1 [21888/50000]	Loss: 3.6917	LR: 0.050000
Training Epoch: 1 [22016/50000]	Loss: 3.8567	LR: 0.050000
Training Epoch: 1 [22144/50000]	Loss: 3.7969	LR: 0.050000
Training Epoch: 1 [22272/50000]	Loss: 3.9877	LR: 0.050000
Training Epoch: 1 [22400/50000]	Loss: 3.9056	LR: 0.050000
Training Epoch: 1 [22528/50000]	Loss: 3.6279	LR: 0.050000
Training Epoch: 1 [22656/50000]	Loss: 3.8557	LR: 0.050000
Training Epoch: 1 [22784/50000]	Loss: 3.9076	LR: 0.050000
Training Epoch: 1 [22912/50000]	Loss: 3.9505	LR: 0.050000
Training Epoch: 1 [23040/50000]	Loss: 4.0489	LR: 0.050000
Training Epoch: 1 [23168/50000]	Loss: 3.9016	LR: 0.050000
Training Epoch: 1 [23296/50000]	Loss: 3.8614	LR: 0.050000
Training Epoch: 1 [23424/50000]	Loss: 3.9707	LR: 0.050000
Training Epoch: 1 [23552/50000]	Loss: 3.8925	LR: 0.050000
Training Epoch: 1 [23680/50000]	Loss: 3.8625	LR: 0.050000
Training Epoch: 1 [23808/50000]	Loss: 3.9860	LR: 0.050000
Training Epoch: 1 [23936/50000]	Loss: 3.8242	LR: 0.050000
Training Epoch: 1 [24064/50000]	Loss: 3.8952	LR: 0.050000
Training Epoch: 1 [24192/50000]	Loss: 3.8660	LR: 0.050000
Training Epoch: 1 [24320/50000]	Loss: 3.9641	LR: 0.050000
Training Epoch: 1 [24448/50000]	Loss: 3.9838	LR: 0.050000
Training Epoch: 1 [24576/50000]	Loss: 3.8204	LR: 0.050000
Training Epoch: 1 [24704/50000]	Loss: 3.9428	LR: 0.050000
Training Epoch: 1 [24832/50000]	Loss: 4.0285	LR: 0.050000
Training Epoch: 1 [24960/50000]	Loss: 3.6832	LR: 0.050000
Training Epoch: 1 [25088/50000]	Loss: 3.9136	LR: 0.050000
Training Epoch: 1 [25216/50000]	Loss: 3.9070	LR: 0.050000
Training Epoch: 1 [25344/50000]	Loss: 3.9015	LR: 0.050000
Training Epoch: 1 [25472/50000]	Loss: 4.0525	LR: 0.050000
Training Epoch: 1 [25600/50000]	Loss: 3.7204	LR: 0.050000
Training Epoch: 1 [25728/50000]	Loss: 3.9459	LR: 0.050000
Training Epoch: 1 [25856/50000]	Loss: 3.9362	LR: 0.050000
Training Epoch: 1 [25984/50000]	Loss: 3.8016	LR: 0.050000
Training Epoch: 1 [26112/50000]	Loss: 3.8650	LR: 0.050000
Training Epoch: 1 [26240/50000]	Loss: 3.7546	LR: 0.050000
Training Epoch: 1 [26368/50000]	Loss: 3.7981	LR: 0.050000
Training Epoch: 1 [26496/50000]	Loss: 3.9449	LR: 0.050000
Training Epoch: 1 [26624/50000]	Loss: 3.8939	LR: 0.050000
Training Epoch: 1 [26752/50000]	Loss: 3.8228	LR: 0.050000
Training Epoch: 1 [26880/50000]	Loss: 3.9313	LR: 0.050000
Training Epoch: 1 [27008/50000]	Loss: 3.8812	LR: 0.050000
Training Epoch: 1 [27136/50000]	Loss: 3.8866	LR: 0.050000
Training Epoch: 1 [27264/50000]	Loss: 3.8977	LR: 0.050000
Training Epoch: 1 [27392/50000]	Loss: 3.8133	LR: 0.050000
Training Epoch: 1 [27520/50000]	Loss: 3.8827	LR: 0.050000
Training Epoch: 1 [27648/50000]	Loss: 3.7482	LR: 0.050000
Training Epoch: 1 [27776/50000]	Loss: 3.7632	LR: 0.050000
Training Epoch: 1 [27904/50000]	Loss: 3.9760	LR: 0.050000
Training Epoch: 1 [28032/50000]	Loss: 3.6716	LR: 0.050000
Training Epoch: 1 [28160/50000]	Loss: 3.7710	LR: 0.050000
Training Epoch: 1 [28288/50000]	Loss: 3.8074	LR: 0.050000
Training Epoch: 1 [28416/50000]	Loss: 3.6862	LR: 0.050000
Training Epoch: 1 [28544/50000]	Loss: 3.9289	LR: 0.050000
Training Epoch: 1 [28672/50000]	Loss: 3.6612	LR: 0.050000
Training Epoch: 1 [28800/50000]	Loss: 3.8544	LR: 0.050000
Training Epoch: 1 [28928/50000]	Loss: 3.7345	LR: 0.050000
Training Epoch: 1 [29056/50000]	Loss: 4.0481	LR: 0.050000
Training Epoch: 1 [29184/50000]	Loss: 3.7109	LR: 0.050000
Training Epoch: 1 [29312/50000]	Loss: 3.8230	LR: 0.050000
Training Epoch: 1 [29440/50000]	Loss: 3.7600	LR: 0.050000
Training Epoch: 1 [29568/50000]	Loss: 3.7274	LR: 0.050000
Training Epoch: 1 [29696/50000]	Loss: 3.9156	LR: 0.050000
Training Epoch: 1 [29824/50000]	Loss: 3.6278	LR: 0.050000
Training Epoch: 1 [29952/50000]	Loss: 3.7962	LR: 0.050000
Training Epoch: 1 [30080/50000]	Loss: 3.8357	LR: 0.050000
Training Epoch: 1 [30208/50000]	Loss: 3.8390	LR: 0.050000
Training Epoch: 1 [30336/50000]	Loss: 3.6081	LR: 0.050000
Training Epoch: 1 [30464/50000]	Loss: 3.6570	LR: 0.050000
Training Epoch: 1 [30592/50000]	Loss: 3.7478	LR: 0.050000
Training Epoch: 1 [30720/50000]	Loss: 3.6804	LR: 0.050000
Training Epoch: 1 [30848/50000]	Loss: 3.6845	LR: 0.050000
Training Epoch: 1 [30976/50000]	Loss: 3.7985	LR: 0.050000
Training Epoch: 1 [31104/50000]	Loss: 3.9543	LR: 0.050000
Training Epoch: 1 [31232/50000]	Loss: 3.7012	LR: 0.050000
Training Epoch: 1 [31360/50000]	Loss: 3.8596	LR: 0.050000
Training Epoch: 1 [31488/50000]	Loss: 4.0206	LR: 0.050000
Training Epoch: 1 [31616/50000]	Loss: 3.8151	LR: 0.050000
Training Epoch: 1 [31744/50000]	Loss: 3.9301	LR: 0.050000
Training Epoch: 1 [31872/50000]	Loss: 3.6441	LR: 0.050000
Training Epoch: 1 [32000/50000]	Loss: 3.8333	LR: 0.050000
Training Epoch: 1 [32128/50000]	Loss: 3.8025	LR: 0.050000
Training Epoch: 1 [32256/50000]	Loss: 3.7547	LR: 0.050000
Training Epoch: 1 [32384/50000]	Loss: 3.9017	LR: 0.050000
Training Epoch: 1 [32512/50000]	Loss: 3.5247	LR: 0.050000
Training Epoch: 1 [32640/50000]	Loss: 3.7413	LR: 0.050000
Training Epoch: 1 [32768/50000]	Loss: 3.6533	LR: 0.050000
Training Epoch: 1 [32896/50000]	Loss: 3.7617	LR: 0.050000
Training Epoch: 1 [33024/50000]	Loss: 3.8483	LR: 0.050000
Training Epoch: 1 [33152/50000]	Loss: 4.0496	LR: 0.050000
Training Epoch: 1 [33280/50000]	Loss: 3.9300	LR: 0.050000
Training Epoch: 1 [33408/50000]	Loss: 3.8711	LR: 0.050000
Training Epoch: 1 [33536/50000]	Loss: 3.7437	LR: 0.050000
Training Epoch: 1 [33664/50000]	Loss: 3.8150	LR: 0.050000
Training Epoch: 1 [33792/50000]	Loss: 3.7086	LR: 0.050000
Training Epoch: 1 [33920/50000]	Loss: 3.6889	LR: 0.050000
Training Epoch: 1 [34048/50000]	Loss: 3.7137	LR: 0.050000
Training Epoch: 1 [34176/50000]	Loss: 3.7489	LR: 0.050000
Training Epoch: 1 [34304/50000]	Loss: 3.7712	LR: 0.050000
Training Epoch: 1 [34432/50000]	Loss: 3.8312	LR: 0.050000
Training Epoch: 1 [34560/50000]	Loss: 3.7516	LR: 0.050000
Training Epoch: 1 [34688/50000]	Loss: 3.8872	LR: 0.050000
Training Epoch: 1 [34816/50000]	Loss: 3.6533	LR: 0.050000
Training Epoch: 1 [34944/50000]	Loss: 3.4201	LR: 0.050000
Training Epoch: 1 [35072/50000]	Loss: 3.6821	LR: 0.050000
Training Epoch: 1 [35200/50000]	Loss: 3.7447	LR: 0.050000
Training Epoch: 1 [35328/50000]	Loss: 4.1090	LR: 0.050000
Training Epoch: 1 [35456/50000]	Loss: 3.9478	LR: 0.050000
Training Epoch: 1 [35584/50000]	Loss: 3.6978	LR: 0.050000
Training Epoch: 1 [35712/50000]	Loss: 3.7173	LR: 0.050000
Training Epoch: 1 [35840/50000]	Loss: 3.7808	LR: 0.050000
Training Epoch: 1 [35968/50000]	Loss: 3.9464	LR: 0.050000
Training Epoch: 1 [36096/50000]	Loss: 3.8841	LR: 0.050000
Training Epoch: 1 [36224/50000]	Loss: 3.8144	LR: 0.050000
Training Epoch: 1 [36352/50000]	Loss: 3.8558	LR: 0.050000
Training Epoch: 1 [36480/50000]	Loss: 3.8170	LR: 0.050000
Training Epoch: 1 [36608/50000]	Loss: 3.7802	LR: 0.050000
Training Epoch: 1 [36736/50000]	Loss: 3.8700	LR: 0.050000
Training Epoch: 1 [36864/50000]	Loss: 3.6675	LR: 0.050000
Training Epoch: 1 [36992/50000]	Loss: 3.6376	LR: 0.050000
Training Epoch: 1 [37120/50000]	Loss: 3.6889	LR: 0.050000
Training Epoch: 1 [37248/50000]	Loss: 3.8496	LR: 0.050000
Training Epoch: 1 [37376/50000]	Loss: 3.7430	LR: 0.050000
Training Epoch: 1 [37504/50000]	Loss: 3.7024	LR: 0.050000
Training Epoch: 1 [37632/50000]	Loss: 3.8469	LR: 0.050000
Training Epoch: 1 [37760/50000]	Loss: 3.6143	LR: 0.050000
Training Epoch: 1 [37888/50000]	Loss: 3.7190	LR: 0.050000
Training Epoch: 1 [38016/50000]	Loss: 3.7804	LR: 0.050000
Training Epoch: 1 [38144/50000]	Loss: 3.6982	LR: 0.050000
Training Epoch: 1 [38272/50000]	Loss: 3.8459	LR: 0.050000
Training Epoch: 1 [38400/50000]	Loss: 3.7077	LR: 0.050000
Training Epoch: 1 [38528/50000]	Loss: 3.6125	LR: 0.050000
Training Epoch: 1 [38656/50000]	Loss: 3.8959	LR: 0.050000
Training Epoch: 1 [38784/50000]	Loss: 3.7406	LR: 0.050000
Training Epoch: 1 [38912/50000]	Loss: 3.8438	LR: 0.050000
Training Epoch: 1 [39040/50000]	Loss: 3.6432	LR: 0.050000
Training Epoch: 1 [39168/50000]	Loss: 3.4640	LR: 0.050000
Training Epoch: 1 [39296/50000]	Loss: 3.7310	LR: 0.050000
Training Epoch: 1 [39424/50000]	Loss: 3.8685	LR: 0.050000
Training Epoch: 1 [39552/50000]	Loss: 3.5838	LR: 0.050000
Training Epoch: 1 [39680/50000]	Loss: 3.7566	LR: 0.050000
Training Epoch: 1 [39808/50000]	Loss: 3.7291	LR: 0.050000
Training Epoch: 1 [39936/50000]	Loss: 3.5979	LR: 0.050000
Training Epoch: 1 [40064/50000]	Loss: 3.4876	LR: 0.050000
Training Epoch: 1 [40192/50000]	Loss: 3.5739	LR: 0.050000
Training Epoch: 1 [40320/50000]	Loss: 3.7565	LR: 0.050000
Training Epoch: 1 [40448/50000]	Loss: 3.7506	LR: 0.050000
Training Epoch: 1 [40576/50000]	Loss: 3.7784	LR: 0.050000
Training Epoch: 1 [40704/50000]	Loss: 3.6453	LR: 0.050000
Training Epoch: 1 [40832/50000]	Loss: 3.7449	LR: 0.050000
Training Epoch: 1 [40960/50000]	Loss: 3.6258	LR: 0.050000
Training Epoch: 1 [41088/50000]	Loss: 3.4877	LR: 0.050000
Training Epoch: 1 [41216/50000]	Loss: 3.7221	LR: 0.050000
Training Epoch: 1 [41344/50000]	Loss: 3.5651	LR: 0.050000
Training Epoch: 1 [41472/50000]	Loss: 3.8441	LR: 0.050000
Training Epoch: 1 [41600/50000]	Loss: 3.6683	LR: 0.050000
Training Epoch: 1 [41728/50000]	Loss: 3.9035	LR: 0.050000
Training Epoch: 1 [41856/50000]	Loss: 3.8033	LR: 0.050000
Training Epoch: 1 [41984/50000]	Loss: 4.0345	LR: 0.050000
Training Epoch: 1 [42112/50000]	Loss: 3.7932	LR: 0.050000
Training Epoch: 1 [42240/50000]	Loss: 3.7573	LR: 0.050000
Training Epoch: 1 [42368/50000]	Loss: 3.7283	LR: 0.050000
Training Epoch: 1 [42496/50000]	Loss: 3.5865	LR: 0.050000
Training Epoch: 1 [42624/50000]	Loss: 3.8519	LR: 0.050000
Training Epoch: 1 [42752/50000]	Loss: 3.6138	LR: 0.050000
Training Epoch: 1 [42880/50000]	Loss: 3.6769	LR: 0.050000
Training Epoch: 1 [43008/50000]	Loss: 3.7271	LR: 0.050000
Training Epoch: 1 [43136/50000]	Loss: 3.7067	LR: 0.050000
Training Epoch: 1 [43264/50000]	Loss: 3.4513	LR: 0.050000
Training Epoch: 1 [43392/50000]	Loss: 3.8498	LR: 0.050000
Training Epoch: 1 [43520/50000]	Loss: 3.5449	LR: 0.050000
Training Epoch: 1 [43648/50000]	Loss: 3.6962	LR: 0.050000
Training Epoch: 1 [43776/50000]	Loss: 3.6921	LR: 0.050000
Training Epoch: 1 [43904/50000]	Loss: 3.5723	LR: 0.050000
Training Epoch: 1 [44032/50000]	Loss: 3.5707	LR: 0.050000
Training Epoch: 1 [44160/50000]	Loss: 3.4701	LR: 0.050000
Training Epoch: 1 [44288/50000]	Loss: 3.7439	LR: 0.050000
Training Epoch: 1 [44416/50000]	Loss: 3.5635	LR: 0.050000
Training Epoch: 1 [44544/50000]	Loss: 3.6256	LR: 0.050000
Training Epoch: 1 [44672/50000]	Loss: 3.7506	LR: 0.050000
Training Epoch: 1 [44800/50000]	Loss: 3.7514	LR: 0.050000
Training Epoch: 1 [44928/50000]	Loss: 3.5643	LR: 0.050000
Training Epoch: 1 [45056/50000]	Loss: 3.7681	LR: 0.050000
Training Epoch: 1 [45184/50000]	Loss: 3.5282	LR: 0.050000
Training Epoch: 1 [45312/50000]	Loss: 3.6523	LR: 0.050000
Training Epoch: 1 [45440/50000]	Loss: 3.6760	LR: 0.050000
Training Epoch: 1 [45568/50000]	Loss: 3.6814	LR: 0.050000
Training Epoch: 1 [45696/50000]	Loss: 3.7859	LR: 0.050000
Training Epoch: 1 [45824/50000]	Loss: 3.6828	LR: 0.050000
Training Epoch: 1 [45952/50000]	Loss: 3.6864	LR: 0.050000
Training Epoch: 1 [46080/50000]	Loss: 3.7142	LR: 0.050000
Training Epoch: 1 [46208/50000]	Loss: 3.7138	LR: 0.050000
Training Epoch: 1 [46336/50000]	Loss: 3.6047	LR: 0.050000
Training Epoch: 1 [46464/50000]	Loss: 3.5717	LR: 0.050000
Training Epoch: 1 [46592/50000]	Loss: 3.4520	LR: 0.050000
Training Epoch: 1 [46720/50000]	Loss: 3.6548	LR: 0.050000
Training Epoch: 1 [46848/50000]	Loss: 3.7642	LR: 0.050000
Training Epoch: 1 [46976/50000]	Loss: 3.7969	LR: 0.050000
Training Epoch: 1 [47104/50000]	Loss: 3.7239	LR: 0.050000
Training Epoch: 1 [47232/50000]	Loss: 3.7390	LR: 0.050000
Training Epoch: 1 [47360/50000]	Loss: 3.6187	LR: 0.050000
Training Epoch: 1 [47488/50000]	Loss: 3.7233	LR: 0.050000
Training Epoch: 1 [47616/50000]	Loss: 3.4674	LR: 0.050000
Training Epoch: 1 [47744/50000]	Loss: 3.5600	LR: 0.050000
Training Epoch: 1 [47872/50000]	Loss: 3.6343	LR: 0.050000
Training Epoch: 1 [48000/50000]	Loss: 3.7836	LR: 0.050000
Training Epoch: 1 [48128/50000]	Loss: 3.6828	LR: 0.050000
Training Epoch: 1 [48256/50000]	Loss: 3.6782	LR: 0.050000
Training Epoch: 1 [48384/50000]	Loss: 3.6615	LR: 0.050000
Training Epoch: 1 [48512/50000]	Loss: 3.2806	LR: 0.050000
Training Epoch: 1 [48640/50000]	Loss: 3.7539	LR: 0.050000
Training Epoch: 1 [48768/50000]	Loss: 3.3605	LR: 0.050000
Training Epoch: 1 [48896/50000]	Loss: 3.6141	LR: 0.050000
Training Epoch: 1 [49024/50000]	Loss: 3.4092	LR: 0.050000
Training Epoch: 1 [49152/50000]	Loss: 3.5762	LR: 0.050000
Training Epoch: 1 [49280/50000]	Loss: 3.7244	LR: 0.050000
Training Epoch: 1 [49408/50000]	Loss: 3.8227	LR: 0.050000
Training Epoch: 1 [49536/50000]	Loss: 3.6103	LR: 0.050000
Training Epoch: 1 [49664/50000]	Loss: 3.5941	LR: 0.050000
Training Epoch: 1 [49792/50000]	Loss: 3.6583	LR: 0.050000
Training Epoch: 1 [49920/50000]	Loss: 3.5858	LR: 0.050000
Training Epoch: 1 [50000/50000]	Loss: 3.6335	LR: 0.050000
epoch 1 training time consumed: 53.72s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134189 KB |    1045 MB |    3670 GB |    3670 GB |
|       from large pool |  123392 KB |    1034 MB |    3667 GB |    3666 GB |
|       from small pool |   10797 KB |      13 MB |       3 GB |       3 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134189 KB |    1045 MB |    3670 GB |    3670 GB |
|       from large pool |  123392 KB |    1034 MB |    3667 GB |    3666 GB |
|       from small pool |   10797 KB |      13 MB |       3 GB |       3 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |    1618 GB |    1618 GB |
|       from large pool |  155136 KB |  433088 KB |    1614 GB |    1614 GB |
|       from small pool |    1490 KB |    3494 KB |       4 GB |       4 GB |
|---------------------------------------------------------------------------|
| Allocations           |     254    |     334    |  142177    |  141923    |
|       from large pool |      24    |      65    |   73991    |   73967    |
|       from small pool |     230    |     273    |   68186    |   67956    |
|---------------------------------------------------------------------------|
| Active allocs         |     254    |     334    |  142177    |  141923    |
|       from large pool |      24    |      65    |   73991    |   73967    |
|       from small pool |     230    |     273    |   68186    |   67956    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |   74168    |   74147    |
|       from large pool |       9    |      14    |   35813    |   35804    |
|       from small pool |      12    |      16    |   38355    |   38343    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 1, Average loss: 0.0282, Accuracy: 0.1450, Time consumed:3.44s

Training Epoch: 2 [128/50000]	Loss: 3.5576	LR: 0.050000
Training Epoch: 2 [256/50000]	Loss: 3.7845	LR: 0.050000
Training Epoch: 2 [384/50000]	Loss: 3.5815	LR: 0.050000
Training Epoch: 2 [512/50000]	Loss: 3.6001	LR: 0.050000
Training Epoch: 2 [640/50000]	Loss: 3.5484	LR: 0.050000
Training Epoch: 2 [768/50000]	Loss: 3.3807	LR: 0.050000
Training Epoch: 2 [896/50000]	Loss: 3.5867	LR: 0.050000
Training Epoch: 2 [1024/50000]	Loss: 3.5944	LR: 0.050000
Training Epoch: 2 [1152/50000]	Loss: 3.5434	LR: 0.050000
Training Epoch: 2 [1280/50000]	Loss: 3.6078	LR: 0.050000
Training Epoch: 2 [1408/50000]	Loss: 3.6221	LR: 0.050000
Training Epoch: 2 [1536/50000]	Loss: 3.5764	LR: 0.050000
Training Epoch: 2 [1664/50000]	Loss: 3.6768	LR: 0.050000
Training Epoch: 2 [1792/50000]	Loss: 3.6219	LR: 0.050000
Training Epoch: 2 [1920/50000]	Loss: 3.4310	LR: 0.050000
Training Epoch: 2 [2048/50000]	Loss: 3.6488	LR: 0.050000
Training Epoch: 2 [2176/50000]	Loss: 3.4923	LR: 0.050000
Training Epoch: 2 [2304/50000]	Loss: 3.4386	LR: 0.050000
Training Epoch: 2 [2432/50000]	Loss: 3.5931	LR: 0.050000
Training Epoch: 2 [2560/50000]	Loss: 3.6696	LR: 0.050000
Training Epoch: 2 [2688/50000]	Loss: 3.6054	LR: 0.050000
Training Epoch: 2 [2816/50000]	Loss: 3.4213	LR: 0.050000
Training Epoch: 2 [2944/50000]	Loss: 3.4234	LR: 0.050000
Training Epoch: 2 [3072/50000]	Loss: 3.5554	LR: 0.050000
Training Epoch: 2 [3200/50000]	Loss: 3.4938	LR: 0.050000
Training Epoch: 2 [3328/50000]	Loss: 3.5012	LR: 0.050000
Training Epoch: 2 [3456/50000]	Loss: 3.5239	LR: 0.050000
Training Epoch: 2 [3584/50000]	Loss: 3.6095	LR: 0.050000
Training Epoch: 2 [3712/50000]	Loss: 3.5972	LR: 0.050000
Training Epoch: 2 [3840/50000]	Loss: 3.6843	LR: 0.050000
Training Epoch: 2 [3968/50000]	Loss: 3.5858	LR: 0.050000
Training Epoch: 2 [4096/50000]	Loss: 3.6667	LR: 0.050000
Training Epoch: 2 [4224/50000]	Loss: 3.5537	LR: 0.050000
Training Epoch: 2 [4352/50000]	Loss: 3.6499	LR: 0.050000
Training Epoch: 2 [4480/50000]	Loss: 3.4495	LR: 0.050000
Training Epoch: 2 [4608/50000]	Loss: 3.6863	LR: 0.050000
Training Epoch: 2 [4736/50000]	Loss: 3.5721	LR: 0.050000
Training Epoch: 2 [4864/50000]	Loss: 3.3615	LR: 0.050000
Training Epoch: 2 [4992/50000]	Loss: 3.4935	LR: 0.050000
Training Epoch: 2 [5120/50000]	Loss: 3.6461	LR: 0.050000
Training Epoch: 2 [5248/50000]	Loss: 3.6527	LR: 0.050000
Training Epoch: 2 [5376/50000]	Loss: 3.4499	LR: 0.050000
Training Epoch: 2 [5504/50000]	Loss: 3.4887	LR: 0.050000
Training Epoch: 2 [5632/50000]	Loss: 3.4817	LR: 0.050000
Training Epoch: 2 [5760/50000]	Loss: 3.5019	LR: 0.050000
Training Epoch: 2 [5888/50000]	Loss: 3.7089	LR: 0.050000
Training Epoch: 2 [6016/50000]	Loss: 3.4176	LR: 0.050000
Training Epoch: 2 [6144/50000]	Loss: 3.5227	LR: 0.050000
Training Epoch: 2 [6272/50000]	Loss: 3.3532	LR: 0.050000
Training Epoch: 2 [6400/50000]	Loss: 3.5686	LR: 0.050000
Training Epoch: 2 [6528/50000]	Loss: 3.5779	LR: 0.050000
Training Epoch: 2 [6656/50000]	Loss: 3.4877	LR: 0.050000
Training Epoch: 2 [6784/50000]	Loss: 3.5044	LR: 0.050000
Training Epoch: 2 [6912/50000]	Loss: 3.3146	LR: 0.050000
Training Epoch: 2 [7040/50000]	Loss: 3.6056	LR: 0.050000
Training Epoch: 2 [7168/50000]	Loss: 3.6765	LR: 0.050000
Training Epoch: 2 [7296/50000]	Loss: 3.1922	LR: 0.050000
Training Epoch: 2 [7424/50000]	Loss: 3.4209	LR: 0.050000
Training Epoch: 2 [7552/50000]	Loss: 3.3840	LR: 0.050000
Training Epoch: 2 [7680/50000]	Loss: 3.3886	LR: 0.050000
Training Epoch: 2 [7808/50000]	Loss: 3.4073	LR: 0.050000
Training Epoch: 2 [7936/50000]	Loss: 3.6561	LR: 0.050000
Training Epoch: 2 [8064/50000]	Loss: 3.7168	LR: 0.050000
Training Epoch: 2 [8192/50000]	Loss: 3.2881	LR: 0.050000
Training Epoch: 2 [8320/50000]	Loss: 3.6745	LR: 0.050000
Training Epoch: 2 [8448/50000]	Loss: 3.5980	LR: 0.050000
Training Epoch: 2 [8576/50000]	Loss: 3.3930	LR: 0.050000
Training Epoch: 2 [8704/50000]	Loss: 3.6056	LR: 0.050000
Training Epoch: 2 [8832/50000]	Loss: 3.3793	LR: 0.050000
Training Epoch: 2 [8960/50000]	Loss: 3.4271	LR: 0.050000
Training Epoch: 2 [9088/50000]	Loss: 3.3867	LR: 0.050000
Training Epoch: 2 [9216/50000]	Loss: 3.5139	LR: 0.050000
Training Epoch: 2 [9344/50000]	Loss: 3.3825	LR: 0.050000
Training Epoch: 2 [9472/50000]	Loss: 3.5127	LR: 0.050000
Training Epoch: 2 [9600/50000]	Loss: 3.5621	LR: 0.050000
Training Epoch: 2 [9728/50000]	Loss: 3.5453	LR: 0.050000
Training Epoch: 2 [9856/50000]	Loss: 3.3832	LR: 0.050000
Training Epoch: 2 [9984/50000]	Loss: 3.6768	LR: 0.050000
Training Epoch: 2 [10112/50000]	Loss: 3.3480	LR: 0.050000
Training Epoch: 2 [10240/50000]	Loss: 3.3313	LR: 0.050000
Training Epoch: 2 [10368/50000]	Loss: 3.4547	LR: 0.050000
Training Epoch: 2 [10496/50000]	Loss: 3.6285	LR: 0.050000
Training Epoch: 2 [10624/50000]	Loss: 3.5644	LR: 0.050000
Training Epoch: 2 [10752/50000]	Loss: 3.4216	LR: 0.050000
Training Epoch: 2 [10880/50000]	Loss: 3.3689	LR: 0.050000
Training Epoch: 2 [11008/50000]	Loss: 3.3292	LR: 0.050000
Training Epoch: 2 [11136/50000]	Loss: 3.4486	LR: 0.050000
Training Epoch: 2 [11264/50000]	Loss: 3.6243	LR: 0.050000
Training Epoch: 2 [11392/50000]	Loss: 3.8886	LR: 0.050000
Training Epoch: 2 [11520/50000]	Loss: 3.4843	LR: 0.050000
Training Epoch: 2 [11648/50000]	Loss: 3.6562	LR: 0.050000
Training Epoch: 2 [11776/50000]	Loss: 3.1224	LR: 0.050000
Training Epoch: 2 [11904/50000]	Loss: 3.6087	LR: 0.050000
Training Epoch: 2 [12032/50000]	Loss: 3.3198	LR: 0.050000
Training Epoch: 2 [12160/50000]	Loss: 3.4866	LR: 0.050000
Training Epoch: 2 [12288/50000]	Loss: 3.5484	LR: 0.050000
Training Epoch: 2 [12416/50000]	Loss: 3.5206	LR: 0.050000
Training Epoch: 2 [12544/50000]	Loss: 3.2529	LR: 0.050000
Training Epoch: 2 [12672/50000]	Loss: 3.4640	LR: 0.050000
Training Epoch: 2 [12800/50000]	Loss: 3.4950	LR: 0.050000
Training Epoch: 2 [12928/50000]	Loss: 3.3616	LR: 0.050000
Training Epoch: 2 [13056/50000]	Loss: 3.4394	LR: 0.050000
Training Epoch: 2 [13184/50000]	Loss: 3.4703	LR: 0.050000
Training Epoch: 2 [13312/50000]	Loss: 3.2440	LR: 0.050000
Training Epoch: 2 [13440/50000]	Loss: 3.4602	LR: 0.050000
Training Epoch: 2 [13568/50000]	Loss: 3.4804	LR: 0.050000
Training Epoch: 2 [13696/50000]	Loss: 3.3632	LR: 0.050000
Training Epoch: 2 [13824/50000]	Loss: 3.3748	LR: 0.050000
Training Epoch: 2 [13952/50000]	Loss: 3.3601	LR: 0.050000
Training Epoch: 2 [14080/50000]	Loss: 3.4627	LR: 0.050000
Training Epoch: 2 [14208/50000]	Loss: 3.5150	LR: 0.050000
Training Epoch: 2 [14336/50000]	Loss: 3.4654	LR: 0.050000
Training Epoch: 2 [14464/50000]	Loss: 3.4441	LR: 0.050000
Training Epoch: 2 [14592/50000]	Loss: 3.2708	LR: 0.050000
Training Epoch: 2 [14720/50000]	Loss: 3.6144	LR: 0.050000
Training Epoch: 2 [14848/50000]	Loss: 3.3878	LR: 0.050000
Training Epoch: 2 [14976/50000]	Loss: 3.6853	LR: 0.050000
Training Epoch: 2 [15104/50000]	Loss: 3.2696	LR: 0.050000
Training Epoch: 2 [15232/50000]	Loss: 3.3165	LR: 0.050000
Training Epoch: 2 [15360/50000]	Loss: 3.3270	LR: 0.050000
Training Epoch: 2 [15488/50000]	Loss: 3.5109	LR: 0.050000
Training Epoch: 2 [15616/50000]	Loss: 3.4604	LR: 0.050000
Training Epoch: 2 [15744/50000]	Loss: 3.6941	LR: 0.050000
Training Epoch: 2 [15872/50000]	Loss: 3.4049	LR: 0.050000
Training Epoch: 2 [16000/50000]	Loss: 3.4989	LR: 0.050000
Training Epoch: 2 [16128/50000]	Loss: 3.3788	LR: 0.050000
Training Epoch: 2 [16256/50000]	Loss: 3.6081	LR: 0.050000
Training Epoch: 2 [16384/50000]	Loss: 3.5749	LR: 0.050000
Training Epoch: 2 [16512/50000]	Loss: 3.5755	LR: 0.050000
Training Epoch: 2 [16640/50000]	Loss: 3.3359	LR: 0.050000
Training Epoch: 2 [16768/50000]	Loss: 3.3620	LR: 0.050000
Training Epoch: 2 [16896/50000]	Loss: 3.6267	LR: 0.050000
Training Epoch: 2 [17024/50000]	Loss: 3.2641	LR: 0.050000
Training Epoch: 2 [17152/50000]	Loss: 3.5402	LR: 0.050000
Training Epoch: 2 [17280/50000]	Loss: 3.3148	LR: 0.050000
Training Epoch: 2 [17408/50000]	Loss: 3.1731	LR: 0.050000
Training Epoch: 2 [17536/50000]	Loss: 3.3347	LR: 0.050000
Training Epoch: 2 [17664/50000]	Loss: 3.3786	LR: 0.050000
Training Epoch: 2 [17792/50000]	Loss: 3.5970	LR: 0.050000
Training Epoch: 2 [17920/50000]	Loss: 3.6563	LR: 0.050000
Training Epoch: 2 [18048/50000]	Loss: 3.5303	LR: 0.050000
Training Epoch: 2 [18176/50000]	Loss: 3.4126	LR: 0.050000
Training Epoch: 2 [18304/50000]	Loss: 3.7961	LR: 0.050000
Training Epoch: 2 [18432/50000]	Loss: 3.6881	LR: 0.050000
Training Epoch: 2 [18560/50000]	Loss: 3.4206	LR: 0.050000
Training Epoch: 2 [18688/50000]	Loss: 3.5683	LR: 0.050000
Training Epoch: 2 [18816/50000]	Loss: 3.2182	LR: 0.050000
Training Epoch: 2 [18944/50000]	Loss: 3.4895	LR: 0.050000
Training Epoch: 2 [19072/50000]	Loss: 3.3461	LR: 0.050000
Training Epoch: 2 [19200/50000]	Loss: 3.6027	LR: 0.050000
Training Epoch: 2 [19328/50000]	Loss: 3.4717	LR: 0.050000
Training Epoch: 2 [19456/50000]	Loss: 3.4317	LR: 0.050000
Training Epoch: 2 [19584/50000]	Loss: 3.3934	LR: 0.050000
Training Epoch: 2 [19712/50000]	Loss: 3.5518	LR: 0.050000
Training Epoch: 2 [19840/50000]	Loss: 3.3207	LR: 0.050000
Training Epoch: 2 [19968/50000]	Loss: 3.6060	LR: 0.050000
Training Epoch: 2 [20096/50000]	Loss: 3.3281	LR: 0.050000
Training Epoch: 2 [20224/50000]	Loss: 3.1382	LR: 0.050000
Training Epoch: 2 [20352/50000]	Loss: 3.2970	LR: 0.050000
Training Epoch: 2 [20480/50000]	Loss: 3.4243	LR: 0.050000
Training Epoch: 2 [20608/50000]	Loss: 3.2523	LR: 0.050000
Training Epoch: 2 [20736/50000]	Loss: 3.4951	LR: 0.050000
Training Epoch: 2 [20864/50000]	Loss: 3.3952	LR: 0.050000
Training Epoch: 2 [20992/50000]	Loss: 3.2430	LR: 0.050000
Training Epoch: 2 [21120/50000]	Loss: 3.6691	LR: 0.050000
Training Epoch: 2 [21248/50000]	Loss: 3.3769	LR: 0.050000
Training Epoch: 2 [21376/50000]	Loss: 3.2391	LR: 0.050000
Training Epoch: 2 [21504/50000]	Loss: 3.5387	LR: 0.050000
Training Epoch: 2 [21632/50000]	Loss: 3.4467	LR: 0.050000
Training Epoch: 2 [21760/50000]	Loss: 3.2365	LR: 0.050000
Training Epoch: 2 [21888/50000]	Loss: 3.2642	LR: 0.050000
Training Epoch: 2 [22016/50000]	Loss: 3.3267	LR: 0.050000
Training Epoch: 2 [22144/50000]	Loss: 3.2647	LR: 0.050000
Training Epoch: 2 [22272/50000]	Loss: 3.5292	LR: 0.050000
Training Epoch: 2 [22400/50000]	Loss: 3.4545	LR: 0.050000
Training Epoch: 2 [22528/50000]	Loss: 3.4168	LR: 0.050000
Training Epoch: 2 [22656/50000]	Loss: 3.5654	LR: 0.050000
Training Epoch: 2 [22784/50000]	Loss: 3.1694	LR: 0.050000
Training Epoch: 2 [22912/50000]	Loss: 3.3527	LR: 0.050000
Training Epoch: 2 [23040/50000]	Loss: 3.6478	LR: 0.050000
Training Epoch: 2 [23168/50000]	Loss: 3.3308	LR: 0.050000
Training Epoch: 2 [23296/50000]	Loss: 3.2918	LR: 0.050000
Training Epoch: 2 [23424/50000]	Loss: 3.3936	LR: 0.050000
Training Epoch: 2 [23552/50000]	Loss: 3.2589	LR: 0.050000
Training Epoch: 2 [23680/50000]	Loss: 3.2705	LR: 0.050000
Training Epoch: 2 [23808/50000]	Loss: 3.2299	LR: 0.050000
Training Epoch: 2 [23936/50000]	Loss: 3.4632	LR: 0.050000
Training Epoch: 2 [24064/50000]	Loss: 3.3780	LR: 0.050000
Training Epoch: 2 [24192/50000]	Loss: 3.1847	LR: 0.050000
Training Epoch: 2 [24320/50000]	Loss: 3.4057	LR: 0.050000
Training Epoch: 2 [24448/50000]	Loss: 3.2371	LR: 0.050000
Training Epoch: 2 [24576/50000]	Loss: 3.1378	LR: 0.050000
Training Epoch: 2 [24704/50000]	Loss: 3.2652	LR: 0.050000
Training Epoch: 2 [24832/50000]	Loss: 3.0495	LR: 0.050000
Training Epoch: 2 [24960/50000]	Loss: 3.2336	LR: 0.050000
Training Epoch: 2 [25088/50000]	Loss: 3.3768	LR: 0.050000
Training Epoch: 2 [25216/50000]	Loss: 3.1742	LR: 0.050000
Training Epoch: 2 [25344/50000]	Loss: 3.2898	LR: 0.050000
Training Epoch: 2 [25472/50000]	Loss: 3.3522	LR: 0.050000
Training Epoch: 2 [25600/50000]	Loss: 3.7045	LR: 0.050000
Training Epoch: 2 [25728/50000]	Loss: 3.4698	LR: 0.050000
Training Epoch: 2 [25856/50000]	Loss: 3.3999	LR: 0.050000
Training Epoch: 2 [25984/50000]	Loss: 3.1790	LR: 0.050000
Training Epoch: 2 [26112/50000]	Loss: 3.3657	LR: 0.050000
Training Epoch: 2 [26240/50000]	Loss: 3.1418	LR: 0.050000
Training Epoch: 2 [26368/50000]	Loss: 3.3399	LR: 0.050000
Training Epoch: 2 [26496/50000]	Loss: 3.5012	LR: 0.050000
Training Epoch: 2 [26624/50000]	Loss: 3.4689	LR: 0.050000
Training Epoch: 2 [26752/50000]	Loss: 3.5691	LR: 0.050000
Training Epoch: 2 [26880/50000]	Loss: 3.4185	LR: 0.050000
Training Epoch: 2 [27008/50000]	Loss: 3.3408	LR: 0.050000
Training Epoch: 2 [27136/50000]	Loss: 3.5230	LR: 0.050000
Training Epoch: 2 [27264/50000]	Loss: 3.3141	LR: 0.050000
Training Epoch: 2 [27392/50000]	Loss: 3.1020	LR: 0.050000
Training Epoch: 2 [27520/50000]	Loss: 3.1756	LR: 0.050000
Training Epoch: 2 [27648/50000]	Loss: 3.3139	LR: 0.050000
Training Epoch: 2 [27776/50000]	Loss: 3.4168	LR: 0.050000
Training Epoch: 2 [27904/50000]	Loss: 3.5094	LR: 0.050000
Training Epoch: 2 [28032/50000]	Loss: 3.5783	LR: 0.050000
Training Epoch: 2 [28160/50000]	Loss: 3.6566	LR: 0.050000
Training Epoch: 2 [28288/50000]	Loss: 3.3014	LR: 0.050000
Training Epoch: 2 [28416/50000]	Loss: 3.2699	LR: 0.050000
Training Epoch: 2 [28544/50000]	Loss: 3.3282	LR: 0.050000
Training Epoch: 2 [28672/50000]	Loss: 3.2108	LR: 0.050000
Training Epoch: 2 [28800/50000]	Loss: 3.4295	LR: 0.050000
Training Epoch: 2 [28928/50000]	Loss: 3.2061	LR: 0.050000
Training Epoch: 2 [29056/50000]	Loss: 3.2224	LR: 0.050000
Training Epoch: 2 [29184/50000]	Loss: 3.3487	LR: 0.050000
Training Epoch: 2 [29312/50000]	Loss: 3.3908	LR: 0.050000
Training Epoch: 2 [29440/50000]	Loss: 3.3811	LR: 0.050000
Training Epoch: 2 [29568/50000]	Loss: 3.1450	LR: 0.050000
Training Epoch: 2 [29696/50000]	Loss: 3.0759	LR: 0.050000
Training Epoch: 2 [29824/50000]	Loss: 3.2460	LR: 0.050000
Training Epoch: 2 [29952/50000]	Loss: 3.3010	LR: 0.050000
Training Epoch: 2 [30080/50000]	Loss: 3.3107	LR: 0.050000
Training Epoch: 2 [30208/50000]	Loss: 3.3943	LR: 0.050000
Training Epoch: 2 [30336/50000]	Loss: 3.4818	LR: 0.050000
Training Epoch: 2 [30464/50000]	Loss: 3.3158	LR: 0.050000
Training Epoch: 2 [30592/50000]	Loss: 3.5038	LR: 0.050000
Training Epoch: 2 [30720/50000]	Loss: 3.2577	LR: 0.050000
Training Epoch: 2 [30848/50000]	Loss: 3.4401	LR: 0.050000
Training Epoch: 2 [30976/50000]	Loss: 3.1643	LR: 0.050000
Training Epoch: 2 [31104/50000]	Loss: 3.1212	LR: 0.050000
Training Epoch: 2 [31232/50000]	Loss: 3.1208	LR: 0.050000
Training Epoch: 2 [31360/50000]	Loss: 3.1083	LR: 0.050000
Training Epoch: 2 [31488/50000]	Loss: 3.3949	LR: 0.050000
Training Epoch: 2 [31616/50000]	Loss: 3.2800	LR: 0.050000
Training Epoch: 2 [31744/50000]	Loss: 3.3637	LR: 0.050000
Training Epoch: 2 [31872/50000]	Loss: 3.2945	LR: 0.050000
Training Epoch: 2 [32000/50000]	Loss: 3.2198	LR: 0.050000
Training Epoch: 2 [32128/50000]	Loss: 3.2097	LR: 0.050000
Training Epoch: 2 [32256/50000]	Loss: 3.2865	LR: 0.050000
Training Epoch: 2 [32384/50000]	Loss: 3.2609	LR: 0.050000
Training Epoch: 2 [32512/50000]	Loss: 3.0858	LR: 0.050000
Training Epoch: 2 [32640/50000]	Loss: 3.1333	LR: 0.050000
Training Epoch: 2 [32768/50000]	Loss: 3.0128	LR: 0.050000
Training Epoch: 2 [32896/50000]	Loss: 3.2954	LR: 0.050000
Training Epoch: 2 [33024/50000]	Loss: 3.3752	LR: 0.050000
Training Epoch: 2 [33152/50000]	Loss: 3.0969	LR: 0.050000
Training Epoch: 2 [33280/50000]	Loss: 3.3703	LR: 0.050000
Training Epoch: 2 [33408/50000]	Loss: 3.0007	LR: 0.050000
Training Epoch: 2 [33536/50000]	Loss: 3.4078	LR: 0.050000
Training Epoch: 2 [33664/50000]	Loss: 3.0951	LR: 0.050000
Training Epoch: 2 [33792/50000]	Loss: 3.1949	LR: 0.050000
Training Epoch: 2 [33920/50000]	Loss: 3.3403	LR: 0.050000
Training Epoch: 2 [34048/50000]	Loss: 3.1059	LR: 0.050000
Training Epoch: 2 [34176/50000]	Loss: 3.1533	LR: 0.050000
Training Epoch: 2 [34304/50000]	Loss: 3.2695	LR: 0.050000
Training Epoch: 2 [34432/50000]	Loss: 3.2306	LR: 0.050000
Training Epoch: 2 [34560/50000]	Loss: 3.3163	LR: 0.050000
Training Epoch: 2 [34688/50000]	Loss: 3.1257	LR: 0.050000
Training Epoch: 2 [34816/50000]	Loss: 3.1089	LR: 0.050000
Training Epoch: 2 [34944/50000]	Loss: 3.0950	LR: 0.050000
Training Epoch: 2 [35072/50000]	Loss: 3.2226	LR: 0.050000
Training Epoch: 2 [35200/50000]	Loss: 3.2969	LR: 0.050000
Training Epoch: 2 [35328/50000]	Loss: 3.3394	LR: 0.050000
Training Epoch: 2 [35456/50000]	Loss: 3.2008	LR: 0.050000
Training Epoch: 2 [35584/50000]	Loss: 3.3065	LR: 0.050000
Training Epoch: 2 [35712/50000]	Loss: 3.1509	LR: 0.050000
Training Epoch: 2 [35840/50000]	Loss: 3.3333	LR: 0.050000
Training Epoch: 2 [35968/50000]	Loss: 3.3553	LR: 0.050000
Training Epoch: 2 [36096/50000]	Loss: 3.0550	LR: 0.050000
Training Epoch: 2 [36224/50000]	Loss: 3.4368	LR: 0.050000
Training Epoch: 2 [36352/50000]	Loss: 3.3252	LR: 0.050000
Training Epoch: 2 [36480/50000]	Loss: 3.2768	LR: 0.050000
Training Epoch: 2 [36608/50000]	Loss: 3.4137	LR: 0.050000
Training Epoch: 2 [36736/50000]	Loss: 3.2242	LR: 0.050000
Training Epoch: 2 [36864/50000]	Loss: 3.5878	LR: 0.050000
Training Epoch: 2 [36992/50000]	Loss: 3.1320	LR: 0.050000
Training Epoch: 2 [37120/50000]	Loss: 3.4106	LR: 0.050000
Training Epoch: 2 [37248/50000]	Loss: 2.9422	LR: 0.050000
Training Epoch: 2 [37376/50000]	Loss: 2.9200	LR: 0.050000
Training Epoch: 2 [37504/50000]	Loss: 3.1902	LR: 0.050000
Training Epoch: 2 [37632/50000]	Loss: 3.0772	LR: 0.050000
Training Epoch: 2 [37760/50000]	Loss: 3.2906	LR: 0.050000
Training Epoch: 2 [37888/50000]	Loss: 3.1535	LR: 0.050000
Training Epoch: 2 [38016/50000]	Loss: 3.2828	LR: 0.050000
Training Epoch: 2 [38144/50000]	Loss: 3.5392	LR: 0.050000
Training Epoch: 2 [38272/50000]	Loss: 3.1478	LR: 0.050000
Training Epoch: 2 [38400/50000]	Loss: 3.2116	LR: 0.050000
Training Epoch: 2 [38528/50000]	Loss: 3.3050	LR: 0.050000
Training Epoch: 2 [38656/50000]	Loss: 3.2725	LR: 0.050000
Training Epoch: 2 [38784/50000]	Loss: 3.2515	LR: 0.050000
Training Epoch: 2 [38912/50000]	Loss: 3.2376	LR: 0.050000
Training Epoch: 2 [39040/50000]	Loss: 3.3290	LR: 0.050000
Training Epoch: 2 [39168/50000]	Loss: 3.0788	LR: 0.050000
Training Epoch: 2 [39296/50000]	Loss: 3.2006	LR: 0.050000
Training Epoch: 2 [39424/50000]	Loss: 3.1145	LR: 0.050000
Training Epoch: 2 [39552/50000]	Loss: 3.0695	LR: 0.050000
Training Epoch: 2 [39680/50000]	Loss: 3.3324	LR: 0.050000
Training Epoch: 2 [39808/50000]	Loss: 3.2350	LR: 0.050000
Training Epoch: 2 [39936/50000]	Loss: 3.0702	LR: 0.050000
Training Epoch: 2 [40064/50000]	Loss: 3.4728	LR: 0.050000
Training Epoch: 2 [40192/50000]	Loss: 3.5177	LR: 0.050000
Training Epoch: 2 [40320/50000]	Loss: 3.4302	LR: 0.050000
Training Epoch: 2 [40448/50000]	Loss: 3.1808	LR: 0.050000
Training Epoch: 2 [40576/50000]	Loss: 3.3419	LR: 0.050000
Training Epoch: 2 [40704/50000]	Loss: 3.2207	LR: 0.050000
Training Epoch: 2 [40832/50000]	Loss: 3.3814	LR: 0.050000
Training Epoch: 2 [40960/50000]	Loss: 3.2177	LR: 0.050000
Training Epoch: 2 [41088/50000]	Loss: 3.2451	LR: 0.050000
Training Epoch: 2 [41216/50000]	Loss: 3.2651	LR: 0.050000
Training Epoch: 2 [41344/50000]	Loss: 3.2167	LR: 0.050000
Training Epoch: 2 [41472/50000]	Loss: 3.0888	LR: 0.050000
Training Epoch: 2 [41600/50000]	Loss: 3.1673	LR: 0.050000
Training Epoch: 2 [41728/50000]	Loss: 3.3362	LR: 0.050000
Training Epoch: 2 [41856/50000]	Loss: 3.4535	LR: 0.050000
Training Epoch: 2 [41984/50000]	Loss: 3.3587	LR: 0.050000
Training Epoch: 2 [42112/50000]	Loss: 3.3723	LR: 0.050000
Training Epoch: 2 [42240/50000]	Loss: 3.0715	LR: 0.050000
Training Epoch: 2 [42368/50000]	Loss: 3.1492	LR: 0.050000
Training Epoch: 2 [42496/50000]	Loss: 3.0545	LR: 0.050000
Training Epoch: 2 [42624/50000]	Loss: 3.2240	LR: 0.050000
Training Epoch: 2 [42752/50000]	Loss: 3.2543	LR: 0.050000
Training Epoch: 2 [42880/50000]	Loss: 3.4690	LR: 0.050000
Training Epoch: 2 [43008/50000]	Loss: 3.3300	LR: 0.050000
Training Epoch: 2 [43136/50000]	Loss: 3.1701	LR: 0.050000
Training Epoch: 2 [43264/50000]	Loss: 3.4118	LR: 0.050000
Training Epoch: 2 [43392/50000]	Loss: 3.4003	LR: 0.050000
Training Epoch: 2 [43520/50000]	Loss: 3.2539	LR: 0.050000
Training Epoch: 2 [43648/50000]	Loss: 3.2087	LR: 0.050000
Training Epoch: 2 [43776/50000]	Loss: 3.0611	LR: 0.050000
Training Epoch: 2 [43904/50000]	Loss: 3.2758	LR: 0.050000
Training Epoch: 2 [44032/50000]	Loss: 3.2700	LR: 0.050000
Training Epoch: 2 [44160/50000]	Loss: 3.4356	LR: 0.050000
Training Epoch: 2 [44288/50000]	Loss: 3.2183	LR: 0.050000
Training Epoch: 2 [44416/50000]	Loss: 3.1074	LR: 0.050000
Training Epoch: 2 [44544/50000]	Loss: 3.2680	LR: 0.050000
Training Epoch: 2 [44672/50000]	Loss: 3.2545	LR: 0.050000
Training Epoch: 2 [44800/50000]	Loss: 3.2874	LR: 0.050000
Training Epoch: 2 [44928/50000]	Loss: 3.3618	LR: 0.050000
Training Epoch: 2 [45056/50000]	Loss: 3.1969	LR: 0.050000
Training Epoch: 2 [45184/50000]	Loss: 3.2659	LR: 0.050000
Training Epoch: 2 [45312/50000]	Loss: 3.1927	LR: 0.050000
Training Epoch: 2 [45440/50000]	Loss: 3.0925	LR: 0.050000
Training Epoch: 2 [45568/50000]	Loss: 3.0052	LR: 0.050000
Training Epoch: 2 [45696/50000]	Loss: 3.1683	LR: 0.050000
Training Epoch: 2 [45824/50000]	Loss: 3.2289	LR: 0.050000
Training Epoch: 2 [45952/50000]	Loss: 3.1984	LR: 0.050000
Training Epoch: 2 [46080/50000]	Loss: 3.2489	LR: 0.050000
Training Epoch: 2 [46208/50000]	Loss: 3.1087	LR: 0.050000
Training Epoch: 2 [46336/50000]	Loss: 3.3194	LR: 0.050000
Training Epoch: 2 [46464/50000]	Loss: 3.2871	LR: 0.050000
Training Epoch: 2 [46592/50000]	Loss: 3.4848	LR: 0.050000
Training Epoch: 2 [46720/50000]	Loss: 3.2295	LR: 0.050000
Training Epoch: 2 [46848/50000]	Loss: 3.1760	LR: 0.050000
Training Epoch: 2 [46976/50000]	Loss: 3.2283	LR: 0.050000
Training Epoch: 2 [47104/50000]	Loss: 3.3556	LR: 0.050000
Training Epoch: 2 [47232/50000]	Loss: 3.1934	LR: 0.050000
Training Epoch: 2 [47360/50000]	Loss: 3.2781	LR: 0.050000
Training Epoch: 2 [47488/50000]	Loss: 3.1651	LR: 0.050000
Training Epoch: 2 [47616/50000]	Loss: 3.2040	LR: 0.050000
Training Epoch: 2 [47744/50000]	Loss: 3.2569	LR: 0.050000
Training Epoch: 2 [47872/50000]	Loss: 3.3524	LR: 0.050000
Training Epoch: 2 [48000/50000]	Loss: 3.3046	LR: 0.050000
Training Epoch: 2 [48128/50000]	Loss: 3.2083	LR: 0.050000
Training Epoch: 2 [48256/50000]	Loss: 3.3369	LR: 0.050000
Training Epoch: 2 [48384/50000]	Loss: 3.2136	LR: 0.050000
Training Epoch: 2 [48512/50000]	Loss: 3.1499	LR: 0.050000
Training Epoch: 2 [48640/50000]	Loss: 3.3511	LR: 0.050000
Training Epoch: 2 [48768/50000]	Loss: 3.3334	LR: 0.050000
Training Epoch: 2 [48896/50000]	Loss: 3.0913	LR: 0.050000
Training Epoch: 2 [49024/50000]	Loss: 3.1148	LR: 0.050000
Training Epoch: 2 [49152/50000]	Loss: 3.2415	LR: 0.050000
Training Epoch: 2 [49280/50000]	Loss: 3.2078	LR: 0.050000
Training Epoch: 2 [49408/50000]	Loss: 3.1226	LR: 0.050000
Training Epoch: 2 [49536/50000]	Loss: 3.3234	LR: 0.050000
Training Epoch: 2 [49664/50000]	Loss: 3.2753	LR: 0.050000
Training Epoch: 2 [49792/50000]	Loss: 3.1304	LR: 0.050000
Training Epoch: 2 [49920/50000]	Loss: 2.9058	LR: 0.050000
Training Epoch: 2 [50000/50000]	Loss: 3.0998	LR: 0.050000
epoch 2 training time consumed: 53.70s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |    7340 GB |    7340 GB |
|       from large pool |  123392 KB |    1034 MB |    7333 GB |    7333 GB |
|       from small pool |   10798 KB |      13 MB |       7 GB |       7 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |    7340 GB |    7340 GB |
|       from large pool |  123392 KB |    1034 MB |    7333 GB |    7333 GB |
|       from small pool |   10798 KB |      13 MB |       7 GB |       7 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |    3233 GB |    3233 GB |
|       from large pool |  155136 KB |  433088 KB |    3225 GB |    3225 GB |
|       from small pool |    1490 KB |    3494 KB |       8 GB |       8 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |  283786    |  283531    |
|       from large pool |      24    |      65    |  147911    |  147887    |
|       from small pool |     231    |     274    |  135875    |  135644    |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |  283786    |  283531    |
|       from large pool |      24    |      65    |  147911    |  147887    |
|       from small pool |     231    |     274    |  135875    |  135644    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |  144033    |  144012    |
|       from large pool |       9    |      14    |   71591    |   71582    |
|       from small pool |      12    |      16    |   72442    |   72430    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 2, Average loss: 0.0255, Accuracy: 0.2154, Time consumed:3.45s

Training Epoch: 3 [128/50000]	Loss: 3.0302	LR: 0.050000
Training Epoch: 3 [256/50000]	Loss: 3.1263	LR: 0.050000
Training Epoch: 3 [384/50000]	Loss: 2.9454	LR: 0.050000
Training Epoch: 3 [512/50000]	Loss: 3.1742	LR: 0.050000
Training Epoch: 3 [640/50000]	Loss: 3.1047	LR: 0.050000
Training Epoch: 3 [768/50000]	Loss: 3.2693	LR: 0.050000
Training Epoch: 3 [896/50000]	Loss: 3.2330	LR: 0.050000
Training Epoch: 3 [1024/50000]	Loss: 2.9949	LR: 0.050000
Training Epoch: 3 [1152/50000]	Loss: 2.9651	LR: 0.050000
Training Epoch: 3 [1280/50000]	Loss: 3.3998	LR: 0.050000
Training Epoch: 3 [1408/50000]	Loss: 3.2935	LR: 0.050000
Training Epoch: 3 [1536/50000]	Loss: 3.1113	LR: 0.050000
Training Epoch: 3 [1664/50000]	Loss: 3.3737	LR: 0.050000
Training Epoch: 3 [1792/50000]	Loss: 3.2044	LR: 0.050000
Training Epoch: 3 [1920/50000]	Loss: 2.9023	LR: 0.050000
Training Epoch: 3 [2048/50000]	Loss: 2.8241	LR: 0.050000
Training Epoch: 3 [2176/50000]	Loss: 3.1801	LR: 0.050000
Training Epoch: 3 [2304/50000]	Loss: 3.1204	LR: 0.050000
Training Epoch: 3 [2432/50000]	Loss: 3.1453	LR: 0.050000
Training Epoch: 3 [2560/50000]	Loss: 2.9646	LR: 0.050000
Training Epoch: 3 [2688/50000]	Loss: 2.8279	LR: 0.050000
Training Epoch: 3 [2816/50000]	Loss: 2.9203	LR: 0.050000
Training Epoch: 3 [2944/50000]	Loss: 3.0616	LR: 0.050000
Training Epoch: 3 [3072/50000]	Loss: 3.0003	LR: 0.050000
Training Epoch: 3 [3200/50000]	Loss: 3.1114	LR: 0.050000
Training Epoch: 3 [3328/50000]	Loss: 3.1061	LR: 0.050000
Training Epoch: 3 [3456/50000]	Loss: 2.9443	LR: 0.050000
Training Epoch: 3 [3584/50000]	Loss: 3.0898	LR: 0.050000
Training Epoch: 3 [3712/50000]	Loss: 3.1874	LR: 0.050000
Training Epoch: 3 [3840/50000]	Loss: 3.2662	LR: 0.050000
Training Epoch: 3 [3968/50000]	Loss: 2.8765	LR: 0.050000
Training Epoch: 3 [4096/50000]	Loss: 3.0090	LR: 0.050000
Training Epoch: 3 [4224/50000]	Loss: 2.9932	LR: 0.050000
Training Epoch: 3 [4352/50000]	Loss: 3.3119	LR: 0.050000
Training Epoch: 3 [4480/50000]	Loss: 3.1146	LR: 0.050000
Training Epoch: 3 [4608/50000]	Loss: 3.0299	LR: 0.050000
Training Epoch: 3 [4736/50000]	Loss: 2.9452	LR: 0.050000
Training Epoch: 3 [4864/50000]	Loss: 3.1284	LR: 0.050000
Training Epoch: 3 [4992/50000]	Loss: 3.2088	LR: 0.050000
Training Epoch: 3 [5120/50000]	Loss: 3.1367	LR: 0.050000
Training Epoch: 3 [5248/50000]	Loss: 3.0670	LR: 0.050000
Training Epoch: 3 [5376/50000]	Loss: 2.9624	LR: 0.050000
Training Epoch: 3 [5504/50000]	Loss: 3.0848	LR: 0.050000
Training Epoch: 3 [5632/50000]	Loss: 3.1514	LR: 0.050000
Training Epoch: 3 [5760/50000]	Loss: 3.5494	LR: 0.050000
Training Epoch: 3 [5888/50000]	Loss: 3.0474	LR: 0.050000
Training Epoch: 3 [6016/50000]	Loss: 3.3098	LR: 0.050000
Training Epoch: 3 [6144/50000]	Loss: 2.9532	LR: 0.050000
Training Epoch: 3 [6272/50000]	Loss: 3.1325	LR: 0.050000
Training Epoch: 3 [6400/50000]	Loss: 3.2508	LR: 0.050000
Training Epoch: 3 [6528/50000]	Loss: 3.2631	LR: 0.050000
Training Epoch: 3 [6656/50000]	Loss: 3.2034	LR: 0.050000
Training Epoch: 3 [6784/50000]	Loss: 3.0680	LR: 0.050000
Training Epoch: 3 [6912/50000]	Loss: 3.0331	LR: 0.050000
Training Epoch: 3 [7040/50000]	Loss: 3.1218	LR: 0.050000
Training Epoch: 3 [7168/50000]	Loss: 3.0845	LR: 0.050000
Training Epoch: 3 [7296/50000]	Loss: 3.1590	LR: 0.050000
Training Epoch: 3 [7424/50000]	Loss: 3.2619	LR: 0.050000
Training Epoch: 3 [7552/50000]	Loss: 3.1019	LR: 0.050000
Training Epoch: 3 [7680/50000]	Loss: 3.0894	LR: 0.050000
Training Epoch: 3 [7808/50000]	Loss: 3.0745	LR: 0.050000
Training Epoch: 3 [7936/50000]	Loss: 3.0719	LR: 0.050000
Training Epoch: 3 [8064/50000]	Loss: 3.1383	LR: 0.050000
Training Epoch: 3 [8192/50000]	Loss: 3.0420	LR: 0.050000
Training Epoch: 3 [8320/50000]	Loss: 2.9127	LR: 0.050000
Training Epoch: 3 [8448/50000]	Loss: 3.3728	LR: 0.050000
Training Epoch: 3 [8576/50000]	Loss: 3.1789	LR: 0.050000
Training Epoch: 3 [8704/50000]	Loss: 2.9186	LR: 0.050000
Training Epoch: 3 [8832/50000]	Loss: 2.9137	LR: 0.050000
Training Epoch: 3 [8960/50000]	Loss: 3.0944	LR: 0.050000
Training Epoch: 3 [9088/50000]	Loss: 3.1706	LR: 0.050000
Training Epoch: 3 [9216/50000]	Loss: 2.8035	LR: 0.050000
Training Epoch: 3 [9344/50000]	Loss: 2.6415	LR: 0.050000
Training Epoch: 3 [9472/50000]	Loss: 2.8130	LR: 0.050000
Training Epoch: 3 [9600/50000]	Loss: 3.0749	LR: 0.050000
Training Epoch: 3 [9728/50000]	Loss: 3.2444	LR: 0.050000
Training Epoch: 3 [9856/50000]	Loss: 3.2654	LR: 0.050000
Training Epoch: 3 [9984/50000]	Loss: 2.9753	LR: 0.050000
Training Epoch: 3 [10112/50000]	Loss: 2.8226	LR: 0.050000
Training Epoch: 3 [10240/50000]	Loss: 2.9480	LR: 0.050000
Training Epoch: 3 [10368/50000]	Loss: 3.1234	LR: 0.050000
Training Epoch: 3 [10496/50000]	Loss: 3.0446	LR: 0.050000
Training Epoch: 3 [10624/50000]	Loss: 3.0637	LR: 0.050000
Training Epoch: 3 [10752/50000]	Loss: 3.1393	LR: 0.050000
Training Epoch: 3 [10880/50000]	Loss: 2.9657	LR: 0.050000
Training Epoch: 3 [11008/50000]	Loss: 3.0271	LR: 0.050000
Training Epoch: 3 [11136/50000]	Loss: 3.1195	LR: 0.050000
Training Epoch: 3 [11264/50000]	Loss: 3.2138	LR: 0.050000
Training Epoch: 3 [11392/50000]	Loss: 3.0566	LR: 0.050000
Training Epoch: 3 [11520/50000]	Loss: 2.8966	LR: 0.050000
Training Epoch: 3 [11648/50000]	Loss: 3.0300	LR: 0.050000
Training Epoch: 3 [11776/50000]	Loss: 3.1734	LR: 0.050000
Training Epoch: 3 [11904/50000]	Loss: 3.0279	LR: 0.050000
Training Epoch: 3 [12032/50000]	Loss: 3.0173	LR: 0.050000
Training Epoch: 3 [12160/50000]	Loss: 3.1002	LR: 0.050000
Training Epoch: 3 [12288/50000]	Loss: 2.8854	LR: 0.050000
Training Epoch: 3 [12416/50000]	Loss: 2.8989	LR: 0.050000
Training Epoch: 3 [12544/50000]	Loss: 3.1823	LR: 0.050000
Training Epoch: 3 [12672/50000]	Loss: 3.1862	LR: 0.050000
Training Epoch: 3 [12800/50000]	Loss: 2.9651	LR: 0.050000
Training Epoch: 3 [12928/50000]	Loss: 3.1979	LR: 0.050000
Training Epoch: 3 [13056/50000]	Loss: 3.0255	LR: 0.050000
Training Epoch: 3 [13184/50000]	Loss: 2.9149	LR: 0.050000
Training Epoch: 3 [13312/50000]	Loss: 3.0513	LR: 0.050000
Training Epoch: 3 [13440/50000]	Loss: 2.9797	LR: 0.050000
Training Epoch: 3 [13568/50000]	Loss: 2.9584	LR: 0.050000
Training Epoch: 3 [13696/50000]	Loss: 3.0095	LR: 0.050000
Training Epoch: 3 [13824/50000]	Loss: 3.0451	LR: 0.050000
Training Epoch: 3 [13952/50000]	Loss: 3.0453	LR: 0.050000
Training Epoch: 3 [14080/50000]	Loss: 2.9737	LR: 0.050000
Training Epoch: 3 [14208/50000]	Loss: 2.9150	LR: 0.050000
Training Epoch: 3 [14336/50000]	Loss: 3.2193	LR: 0.050000
Training Epoch: 3 [14464/50000]	Loss: 3.1907	LR: 0.050000
Training Epoch: 3 [14592/50000]	Loss: 2.9714	LR: 0.050000
Training Epoch: 3 [14720/50000]	Loss: 2.9261	LR: 0.050000
Training Epoch: 3 [14848/50000]	Loss: 3.2661	LR: 0.050000
Training Epoch: 3 [14976/50000]	Loss: 3.0245	LR: 0.050000
Training Epoch: 3 [15104/50000]	Loss: 2.9978	LR: 0.050000
Training Epoch: 3 [15232/50000]	Loss: 3.2522	LR: 0.050000
Training Epoch: 3 [15360/50000]	Loss: 3.0922	LR: 0.050000
Training Epoch: 3 [15488/50000]	Loss: 3.0368	LR: 0.050000
Training Epoch: 3 [15616/50000]	Loss: 3.1878	LR: 0.050000
Training Epoch: 3 [15744/50000]	Loss: 3.0762	LR: 0.050000
Training Epoch: 3 [15872/50000]	Loss: 3.0155	LR: 0.050000
Training Epoch: 3 [16000/50000]	Loss: 3.0043	LR: 0.050000
Training Epoch: 3 [16128/50000]	Loss: 2.9015	LR: 0.050000
Training Epoch: 3 [16256/50000]	Loss: 3.1520	LR: 0.050000
Training Epoch: 3 [16384/50000]	Loss: 3.0641	LR: 0.050000
Training Epoch: 3 [16512/50000]	Loss: 2.8573	LR: 0.050000
Training Epoch: 3 [16640/50000]	Loss: 3.1788	LR: 0.050000
Training Epoch: 3 [16768/50000]	Loss: 2.8821	LR: 0.050000
Training Epoch: 3 [16896/50000]	Loss: 3.0294	LR: 0.050000
Training Epoch: 3 [17024/50000]	Loss: 3.0992	LR: 0.050000
Training Epoch: 3 [17152/50000]	Loss: 2.9837	LR: 0.050000
Training Epoch: 3 [17280/50000]	Loss: 3.2039	LR: 0.050000
Training Epoch: 3 [17408/50000]	Loss: 2.8584	LR: 0.050000
Training Epoch: 3 [17536/50000]	Loss: 2.8128	LR: 0.050000
Training Epoch: 3 [17664/50000]	Loss: 2.9724	LR: 0.050000
Training Epoch: 3 [17792/50000]	Loss: 2.9653	LR: 0.050000
Training Epoch: 3 [17920/50000]	Loss: 3.0847	LR: 0.050000
Training Epoch: 3 [18048/50000]	Loss: 3.2197	LR: 0.050000
Training Epoch: 3 [18176/50000]	Loss: 3.0956	LR: 0.050000
Training Epoch: 3 [18304/50000]	Loss: 2.8503	LR: 0.050000
Training Epoch: 3 [18432/50000]	Loss: 3.1663	LR: 0.050000
Training Epoch: 3 [18560/50000]	Loss: 3.2483	LR: 0.050000
Training Epoch: 3 [18688/50000]	Loss: 3.1271	LR: 0.050000
Training Epoch: 3 [18816/50000]	Loss: 3.1597	LR: 0.050000
Training Epoch: 3 [18944/50000]	Loss: 3.0709	LR: 0.050000
Training Epoch: 3 [19072/50000]	Loss: 3.1465	LR: 0.050000
Training Epoch: 3 [19200/50000]	Loss: 3.0053	LR: 0.050000
Training Epoch: 3 [19328/50000]	Loss: 2.9129	LR: 0.050000
Training Epoch: 3 [19456/50000]	Loss: 3.1648	LR: 0.050000
Training Epoch: 3 [19584/50000]	Loss: 2.9343	LR: 0.050000
Training Epoch: 3 [19712/50000]	Loss: 3.0859	LR: 0.050000
Training Epoch: 3 [19840/50000]	Loss: 2.8053	LR: 0.050000
Training Epoch: 3 [19968/50000]	Loss: 2.7860	LR: 0.050000
Training Epoch: 3 [20096/50000]	Loss: 3.0634	LR: 0.050000
Training Epoch: 3 [20224/50000]	Loss: 3.1784	LR: 0.050000
Training Epoch: 3 [20352/50000]	Loss: 2.8332	LR: 0.050000
Training Epoch: 3 [20480/50000]	Loss: 2.8991	LR: 0.050000
Training Epoch: 3 [20608/50000]	Loss: 2.8315	LR: 0.050000
Training Epoch: 3 [20736/50000]	Loss: 3.1245	LR: 0.050000
Training Epoch: 3 [20864/50000]	Loss: 3.0803	LR: 0.050000
Training Epoch: 3 [20992/50000]	Loss: 2.7412	LR: 0.050000
Training Epoch: 3 [21120/50000]	Loss: 3.1101	LR: 0.050000
Training Epoch: 3 [21248/50000]	Loss: 2.9578	LR: 0.050000
Training Epoch: 3 [21376/50000]	Loss: 3.1355	LR: 0.050000
Training Epoch: 3 [21504/50000]	Loss: 2.9553	LR: 0.050000
Training Epoch: 3 [21632/50000]	Loss: 3.1444	LR: 0.050000
Training Epoch: 3 [21760/50000]	Loss: 2.8959	LR: 0.050000
Training Epoch: 3 [21888/50000]	Loss: 2.7013	LR: 0.050000
Training Epoch: 3 [22016/50000]	Loss: 2.7477	LR: 0.050000
Training Epoch: 3 [22144/50000]	Loss: 3.0261	LR: 0.050000
Training Epoch: 3 [22272/50000]	Loss: 2.8847	LR: 0.050000
Training Epoch: 3 [22400/50000]	Loss: 2.8815	LR: 0.050000
Training Epoch: 3 [22528/50000]	Loss: 3.1083	LR: 0.050000
Training Epoch: 3 [22656/50000]	Loss: 3.0311	LR: 0.050000
Training Epoch: 3 [22784/50000]	Loss: 2.8016	LR: 0.050000
Training Epoch: 3 [22912/50000]	Loss: 2.9386	LR: 0.050000
Training Epoch: 3 [23040/50000]	Loss: 3.1409	LR: 0.050000
Training Epoch: 3 [23168/50000]	Loss: 2.8543	LR: 0.050000
Training Epoch: 3 [23296/50000]	Loss: 3.0317	LR: 0.050000
Training Epoch: 3 [23424/50000]	Loss: 3.2047	LR: 0.050000
Training Epoch: 3 [23552/50000]	Loss: 3.1570	LR: 0.050000
Training Epoch: 3 [23680/50000]	Loss: 3.0725	LR: 0.050000
Training Epoch: 3 [23808/50000]	Loss: 3.2543	LR: 0.050000
Training Epoch: 3 [23936/50000]	Loss: 3.1796	LR: 0.050000
Training Epoch: 3 [24064/50000]	Loss: 3.2400	LR: 0.050000
Training Epoch: 3 [24192/50000]	Loss: 3.0737	LR: 0.050000
Training Epoch: 3 [24320/50000]	Loss: 3.1508	LR: 0.050000
Training Epoch: 3 [24448/50000]	Loss: 2.7691	LR: 0.050000
Training Epoch: 3 [24576/50000]	Loss: 3.1474	LR: 0.050000
Training Epoch: 3 [24704/50000]	Loss: 3.1864	LR: 0.050000
Training Epoch: 3 [24832/50000]	Loss: 2.9039	LR: 0.050000
Training Epoch: 3 [24960/50000]	Loss: 2.9554	LR: 0.050000
Training Epoch: 3 [25088/50000]	Loss: 3.0292	LR: 0.050000
Training Epoch: 3 [25216/50000]	Loss: 3.0490	LR: 0.050000
Training Epoch: 3 [25344/50000]	Loss: 3.0139	LR: 0.050000
Training Epoch: 3 [25472/50000]	Loss: 2.9948	LR: 0.050000
Training Epoch: 3 [25600/50000]	Loss: 2.9574	LR: 0.050000
Training Epoch: 3 [25728/50000]	Loss: 3.0680	LR: 0.050000
Training Epoch: 3 [25856/50000]	Loss: 3.0002	LR: 0.050000
Training Epoch: 3 [25984/50000]	Loss: 2.7125	LR: 0.050000
Training Epoch: 3 [26112/50000]	Loss: 3.0209	LR: 0.050000
Training Epoch: 3 [26240/50000]	Loss: 2.9019	LR: 0.050000
Training Epoch: 3 [26368/50000]	Loss: 3.0560	LR: 0.050000
Training Epoch: 3 [26496/50000]	Loss: 2.9949	LR: 0.050000
Training Epoch: 3 [26624/50000]	Loss: 3.0701	LR: 0.050000
Training Epoch: 3 [26752/50000]	Loss: 3.0677	LR: 0.050000
Training Epoch: 3 [26880/50000]	Loss: 2.9121	LR: 0.050000
Training Epoch: 3 [27008/50000]	Loss: 3.0001	LR: 0.050000
Training Epoch: 3 [27136/50000]	Loss: 2.9117	LR: 0.050000
Training Epoch: 3 [27264/50000]	Loss: 3.1136	LR: 0.050000
Training Epoch: 3 [27392/50000]	Loss: 2.9101	LR: 0.050000
Training Epoch: 3 [27520/50000]	Loss: 2.8859	LR: 0.050000
Training Epoch: 3 [27648/50000]	Loss: 2.9513	LR: 0.050000
Training Epoch: 3 [27776/50000]	Loss: 3.0569	LR: 0.050000
Training Epoch: 3 [27904/50000]	Loss: 2.9901	LR: 0.050000
Training Epoch: 3 [28032/50000]	Loss: 2.8302	LR: 0.050000
Training Epoch: 3 [28160/50000]	Loss: 3.0607	LR: 0.050000
Training Epoch: 3 [28288/50000]	Loss: 2.9869	LR: 0.050000
Training Epoch: 3 [28416/50000]	Loss: 2.9610	LR: 0.050000
Training Epoch: 3 [28544/50000]	Loss: 3.1298	LR: 0.050000
Training Epoch: 3 [28672/50000]	Loss: 3.0298	LR: 0.050000
Training Epoch: 3 [28800/50000]	Loss: 2.8963	LR: 0.050000
Training Epoch: 3 [28928/50000]	Loss: 2.9296	LR: 0.050000
Training Epoch: 3 [29056/50000]	Loss: 2.7399	LR: 0.050000
Training Epoch: 3 [29184/50000]	Loss: 2.9762	LR: 0.050000
Training Epoch: 3 [29312/50000]	Loss: 2.8983	LR: 0.050000
Training Epoch: 3 [29440/50000]	Loss: 2.7947	LR: 0.050000
Training Epoch: 3 [29568/50000]	Loss: 3.1547	LR: 0.050000
Training Epoch: 3 [29696/50000]	Loss: 3.0887	LR: 0.050000
Training Epoch: 3 [29824/50000]	Loss: 3.0394	LR: 0.050000
Training Epoch: 3 [29952/50000]	Loss: 2.7878	LR: 0.050000
Training Epoch: 3 [30080/50000]	Loss: 2.9430	LR: 0.050000
Training Epoch: 3 [30208/50000]	Loss: 3.0227	LR: 0.050000
Training Epoch: 3 [30336/50000]	Loss: 2.7761	LR: 0.050000
Training Epoch: 3 [30464/50000]	Loss: 3.1029	LR: 0.050000
Training Epoch: 3 [30592/50000]	Loss: 3.0934	LR: 0.050000
Training Epoch: 3 [30720/50000]	Loss: 2.6975	LR: 0.050000
Training Epoch: 3 [30848/50000]	Loss: 3.0360	LR: 0.050000
Training Epoch: 3 [30976/50000]	Loss: 2.6536	LR: 0.050000
Training Epoch: 3 [31104/50000]	Loss: 2.7668	LR: 0.050000
Training Epoch: 3 [31232/50000]	Loss: 2.8042	LR: 0.050000
Training Epoch: 3 [31360/50000]	Loss: 2.6678	LR: 0.050000
Training Epoch: 3 [31488/50000]	Loss: 2.9219	LR: 0.050000
Training Epoch: 3 [31616/50000]	Loss: 2.8427	LR: 0.050000
Training Epoch: 3 [31744/50000]	Loss: 2.8836	LR: 0.050000
Training Epoch: 3 [31872/50000]	Loss: 2.9945	LR: 0.050000
Training Epoch: 3 [32000/50000]	Loss: 2.9901	LR: 0.050000
Training Epoch: 3 [32128/50000]	Loss: 2.9941	LR: 0.050000
Training Epoch: 3 [32256/50000]	Loss: 2.8478	LR: 0.050000
Training Epoch: 3 [32384/50000]	Loss: 2.6876	LR: 0.050000
Training Epoch: 3 [32512/50000]	Loss: 2.8369	LR: 0.050000
Training Epoch: 3 [32640/50000]	Loss: 3.0686	LR: 0.050000
Training Epoch: 3 [32768/50000]	Loss: 3.0529	LR: 0.050000
Training Epoch: 3 [32896/50000]	Loss: 2.5981	LR: 0.050000
Training Epoch: 3 [33024/50000]	Loss: 2.9216	LR: 0.050000
Training Epoch: 3 [33152/50000]	Loss: 2.8777	LR: 0.050000
Training Epoch: 3 [33280/50000]	Loss: 3.0855	LR: 0.050000
Training Epoch: 3 [33408/50000]	Loss: 2.7620	LR: 0.050000
Training Epoch: 3 [33536/50000]	Loss: 2.9386	LR: 0.050000
Training Epoch: 3 [33664/50000]	Loss: 3.1412	LR: 0.050000
Training Epoch: 3 [33792/50000]	Loss: 3.2815	LR: 0.050000
Training Epoch: 3 [33920/50000]	Loss: 3.0927	LR: 0.050000
Training Epoch: 3 [34048/50000]	Loss: 3.0431	LR: 0.050000
Training Epoch: 3 [34176/50000]	Loss: 3.0773	LR: 0.050000
Training Epoch: 3 [34304/50000]	Loss: 2.7779	LR: 0.050000
Training Epoch: 3 [34432/50000]	Loss: 2.8897	LR: 0.050000
Training Epoch: 3 [34560/50000]	Loss: 2.6983	LR: 0.050000
Training Epoch: 3 [34688/50000]	Loss: 3.0031	LR: 0.050000
Training Epoch: 3 [34816/50000]	Loss: 2.8452	LR: 0.050000
Training Epoch: 3 [34944/50000]	Loss: 2.7519	LR: 0.050000
Training Epoch: 3 [35072/50000]	Loss: 3.0429	LR: 0.050000
Training Epoch: 3 [35200/50000]	Loss: 3.1692	LR: 0.050000
Training Epoch: 3 [35328/50000]	Loss: 2.9708	LR: 0.050000
Training Epoch: 3 [35456/50000]	Loss: 2.9238	LR: 0.050000
Training Epoch: 3 [35584/50000]	Loss: 3.0981	LR: 0.050000
Training Epoch: 3 [35712/50000]	Loss: 3.1518	LR: 0.050000
Training Epoch: 3 [35840/50000]	Loss: 3.0505	LR: 0.050000
Training Epoch: 3 [35968/50000]	Loss: 2.6580	LR: 0.050000
Training Epoch: 3 [36096/50000]	Loss: 3.0890	LR: 0.050000
Training Epoch: 3 [36224/50000]	Loss: 2.8659	LR: 0.050000
Training Epoch: 3 [36352/50000]	Loss: 2.9226	LR: 0.050000
Training Epoch: 3 [36480/50000]	Loss: 2.8688	LR: 0.050000
Training Epoch: 3 [36608/50000]	Loss: 2.7117	LR: 0.050000
Training Epoch: 3 [36736/50000]	Loss: 3.0006	LR: 0.050000
Training Epoch: 3 [36864/50000]	Loss: 2.9411	LR: 0.050000
Training Epoch: 3 [36992/50000]	Loss: 3.1705	LR: 0.050000
Training Epoch: 3 [37120/50000]	Loss: 2.8858	LR: 0.050000
Training Epoch: 3 [37248/50000]	Loss: 2.8815	LR: 0.050000
Training Epoch: 3 [37376/50000]	Loss: 2.9466	LR: 0.050000
Training Epoch: 3 [37504/50000]	Loss: 2.8687	LR: 0.050000
Training Epoch: 3 [37632/50000]	Loss: 2.9405	LR: 0.050000
Training Epoch: 3 [37760/50000]	Loss: 2.9133	LR: 0.050000
Training Epoch: 3 [37888/50000]	Loss: 2.7842	LR: 0.050000
Training Epoch: 3 [38016/50000]	Loss: 2.8403	LR: 0.050000
Training Epoch: 3 [38144/50000]	Loss: 2.8412	LR: 0.050000
Training Epoch: 3 [38272/50000]	Loss: 2.8181	LR: 0.050000
Training Epoch: 3 [38400/50000]	Loss: 3.1824	LR: 0.050000
Training Epoch: 3 [38528/50000]	Loss: 3.0995	LR: 0.050000
Training Epoch: 3 [38656/50000]	Loss: 2.9573	LR: 0.050000
Training Epoch: 3 [38784/50000]	Loss: 2.9083	LR: 0.050000
Training Epoch: 3 [38912/50000]	Loss: 2.9877	LR: 0.050000
Training Epoch: 3 [39040/50000]	Loss: 2.9737	LR: 0.050000
Training Epoch: 3 [39168/50000]	Loss: 2.7871	LR: 0.050000
Training Epoch: 3 [39296/50000]	Loss: 3.0129	LR: 0.050000
Training Epoch: 3 [39424/50000]	Loss: 2.9973	LR: 0.050000
Training Epoch: 3 [39552/50000]	Loss: 2.9121	LR: 0.050000
Training Epoch: 3 [39680/50000]	Loss: 2.7625	LR: 0.050000
Training Epoch: 3 [39808/50000]	Loss: 2.9027	LR: 0.050000
Training Epoch: 3 [39936/50000]	Loss: 2.8887	LR: 0.050000
Training Epoch: 3 [40064/50000]	Loss: 2.9119	LR: 0.050000
Training Epoch: 3 [40192/50000]	Loss: 2.7252	LR: 0.050000
Training Epoch: 3 [40320/50000]	Loss: 2.7166	LR: 0.050000
Training Epoch: 3 [40448/50000]	Loss: 2.8048	LR: 0.050000
Training Epoch: 3 [40576/50000]	Loss: 2.8112	LR: 0.050000
Training Epoch: 3 [40704/50000]	Loss: 2.8766	LR: 0.050000
Training Epoch: 3 [40832/50000]	Loss: 2.8627	LR: 0.050000
Training Epoch: 3 [40960/50000]	Loss: 2.6368	LR: 0.050000
Training Epoch: 3 [41088/50000]	Loss: 2.9891	LR: 0.050000
Training Epoch: 3 [41216/50000]	Loss: 2.7051	LR: 0.050000
Training Epoch: 3 [41344/50000]	Loss: 2.8035	LR: 0.050000
Training Epoch: 3 [41472/50000]	Loss: 2.7779	LR: 0.050000
Training Epoch: 3 [41600/50000]	Loss: 2.9216	LR: 0.050000
Training Epoch: 3 [41728/50000]	Loss: 2.8282	LR: 0.050000
Training Epoch: 3 [41856/50000]	Loss: 2.8919	LR: 0.050000
Training Epoch: 3 [41984/50000]	Loss: 2.9345	LR: 0.050000
Training Epoch: 3 [42112/50000]	Loss: 2.7619	LR: 0.050000
Training Epoch: 3 [42240/50000]	Loss: 2.8007	LR: 0.050000
Training Epoch: 3 [42368/50000]	Loss: 2.9897	LR: 0.050000
Training Epoch: 3 [42496/50000]	Loss: 2.9980	LR: 0.050000
Training Epoch: 3 [42624/50000]	Loss: 2.9021	LR: 0.050000
Training Epoch: 3 [42752/50000]	Loss: 2.8764	LR: 0.050000
Training Epoch: 3 [42880/50000]	Loss: 2.7599	LR: 0.050000
Training Epoch: 3 [43008/50000]	Loss: 2.9965	LR: 0.050000
Training Epoch: 3 [43136/50000]	Loss: 2.9083	LR: 0.050000
Training Epoch: 3 [43264/50000]	Loss: 2.8509	LR: 0.050000
Training Epoch: 3 [43392/50000]	Loss: 2.8706	LR: 0.050000
Training Epoch: 3 [43520/50000]	Loss: 3.0578	LR: 0.050000
Training Epoch: 3 [43648/50000]	Loss: 2.8497	LR: 0.050000
Training Epoch: 3 [43776/50000]	Loss: 2.9716	LR: 0.050000
Training Epoch: 3 [43904/50000]	Loss: 2.8651	LR: 0.050000
Training Epoch: 3 [44032/50000]	Loss: 2.8816	LR: 0.050000
Training Epoch: 3 [44160/50000]	Loss: 2.6708	LR: 0.050000
Training Epoch: 3 [44288/50000]	Loss: 3.0545	LR: 0.050000
Training Epoch: 3 [44416/50000]	Loss: 2.9432	LR: 0.050000
Training Epoch: 3 [44544/50000]	Loss: 2.6873	LR: 0.050000
Training Epoch: 3 [44672/50000]	Loss: 3.0930	LR: 0.050000
Training Epoch: 3 [44800/50000]	Loss: 2.9512	LR: 0.050000
Training Epoch: 3 [44928/50000]	Loss: 3.1384	LR: 0.050000
Training Epoch: 3 [45056/50000]	Loss: 2.7313	LR: 0.050000
Training Epoch: 3 [45184/50000]	Loss: 3.0467	LR: 0.050000
Training Epoch: 3 [45312/50000]	Loss: 2.9370	LR: 0.050000
Training Epoch: 3 [45440/50000]	Loss: 2.8922	LR: 0.050000
Training Epoch: 3 [45568/50000]	Loss: 2.7824	LR: 0.050000
Training Epoch: 3 [45696/50000]	Loss: 2.9169	LR: 0.050000
Training Epoch: 3 [45824/50000]	Loss: 2.8433	LR: 0.050000
Training Epoch: 3 [45952/50000]	Loss: 2.6001	LR: 0.050000
Training Epoch: 3 [46080/50000]	Loss: 2.9696	LR: 0.050000
Training Epoch: 3 [46208/50000]	Loss: 2.8186	LR: 0.050000
Training Epoch: 3 [46336/50000]	Loss: 2.6615	LR: 0.050000
Training Epoch: 3 [46464/50000]	Loss: 2.6534	LR: 0.050000
Training Epoch: 3 [46592/50000]	Loss: 2.8988	LR: 0.050000
Training Epoch: 3 [46720/50000]	Loss: 3.0507	LR: 0.050000
Training Epoch: 3 [46848/50000]	Loss: 2.6666	LR: 0.050000
Training Epoch: 3 [46976/50000]	Loss: 2.9273	LR: 0.050000
Training Epoch: 3 [47104/50000]	Loss: 2.7184	LR: 0.050000
Training Epoch: 3 [47232/50000]	Loss: 2.9173	LR: 0.050000
Training Epoch: 3 [47360/50000]	Loss: 2.7626	LR: 0.050000
Training Epoch: 3 [47488/50000]	Loss: 3.0426	LR: 0.050000
Training Epoch: 3 [47616/50000]	Loss: 2.7466	LR: 0.050000
Training Epoch: 3 [47744/50000]	Loss: 2.9318	LR: 0.050000
Training Epoch: 3 [47872/50000]	Loss: 2.6591	LR: 0.050000
Training Epoch: 3 [48000/50000]	Loss: 2.8599	LR: 0.050000
Training Epoch: 3 [48128/50000]	Loss: 2.8113	LR: 0.050000
Training Epoch: 3 [48256/50000]	Loss: 3.0341	LR: 0.050000
Training Epoch: 3 [48384/50000]	Loss: 2.7572	LR: 0.050000
Training Epoch: 3 [48512/50000]	Loss: 2.9556	LR: 0.050000
Training Epoch: 3 [48640/50000]	Loss: 2.8746	LR: 0.050000
Training Epoch: 3 [48768/50000]	Loss: 2.6662	LR: 0.050000
Training Epoch: 3 [48896/50000]	Loss: 2.8814	LR: 0.050000
Training Epoch: 3 [49024/50000]	Loss: 2.6787	LR: 0.050000
Training Epoch: 3 [49152/50000]	Loss: 2.8886	LR: 0.050000
Training Epoch: 3 [49280/50000]	Loss: 2.7908	LR: 0.050000
Training Epoch: 3 [49408/50000]	Loss: 3.0206	LR: 0.050000
Training Epoch: 3 [49536/50000]	Loss: 2.8785	LR: 0.050000
Training Epoch: 3 [49664/50000]	Loss: 2.7687	LR: 0.050000
Training Epoch: 3 [49792/50000]	Loss: 2.8431	LR: 0.050000
Training Epoch: 3 [49920/50000]	Loss: 2.9643	LR: 0.050000
Training Epoch: 3 [50000/50000]	Loss: 2.9111	LR: 0.050000
epoch 3 training time consumed: 53.76s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   11010 GB |   11010 GB |
|       from large pool |  123392 KB |    1034 MB |   10999 GB |   10999 GB |
|       from small pool |   10798 KB |      13 MB |      10 GB |      10 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   11010 GB |   11010 GB |
|       from large pool |  123392 KB |    1034 MB |   10999 GB |   10999 GB |
|       from small pool |   10798 KB |      13 MB |      10 GB |      10 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |    4848 GB |    4848 GB |
|       from large pool |  155136 KB |  433088 KB |    4836 GB |    4836 GB |
|       from small pool |    1490 KB |    3494 KB |      12 GB |      12 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |  425395    |  425140    |
|       from large pool |      24    |      65    |  221831    |  221807    |
|       from small pool |     231    |     274    |  203564    |  203333    |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |  425395    |  425140    |
|       from large pool |      24    |      65    |  221831    |  221807    |
|       from small pool |     231    |     274    |  203564    |  203333    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |  213995    |  213974    |
|       from large pool |       9    |      14    |  107369    |  107360    |
|       from small pool |      12    |      16    |  106626    |  106614    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 3, Average loss: 0.0232, Accuracy: 0.2733, Time consumed:3.45s

Training Epoch: 4 [128/50000]	Loss: 2.8697	LR: 0.050000
Training Epoch: 4 [256/50000]	Loss: 2.7277	LR: 0.050000
Training Epoch: 4 [384/50000]	Loss: 2.9068	LR: 0.050000
Training Epoch: 4 [512/50000]	Loss: 2.8365	LR: 0.050000
Training Epoch: 4 [640/50000]	Loss: 2.8394	LR: 0.050000
Training Epoch: 4 [768/50000]	Loss: 2.5962	LR: 0.050000
Training Epoch: 4 [896/50000]	Loss: 2.8260	LR: 0.050000
Training Epoch: 4 [1024/50000]	Loss: 2.8743	LR: 0.050000
Training Epoch: 4 [1152/50000]	Loss: 2.6884	LR: 0.050000
Training Epoch: 4 [1280/50000]	Loss: 2.8050	LR: 0.050000
Training Epoch: 4 [1408/50000]	Loss: 2.7958	LR: 0.050000
Training Epoch: 4 [1536/50000]	Loss: 2.6629	LR: 0.050000
Training Epoch: 4 [1664/50000]	Loss: 2.7009	LR: 0.050000
Training Epoch: 4 [1792/50000]	Loss: 2.6351	LR: 0.050000
Training Epoch: 4 [1920/50000]	Loss: 2.8031	LR: 0.050000
Training Epoch: 4 [2048/50000]	Loss: 2.8127	LR: 0.050000
Training Epoch: 4 [2176/50000]	Loss: 2.7741	LR: 0.050000
Training Epoch: 4 [2304/50000]	Loss: 2.8086	LR: 0.050000
Training Epoch: 4 [2432/50000]	Loss: 2.8541	LR: 0.050000
Training Epoch: 4 [2560/50000]	Loss: 2.7094	LR: 0.050000
Training Epoch: 4 [2688/50000]	Loss: 2.6689	LR: 0.050000
Training Epoch: 4 [2816/50000]	Loss: 2.7513	LR: 0.050000
Training Epoch: 4 [2944/50000]	Loss: 2.8002	LR: 0.050000
Training Epoch: 4 [3072/50000]	Loss: 2.8472	LR: 0.050000
Training Epoch: 4 [3200/50000]	Loss: 2.8073	LR: 0.050000
Training Epoch: 4 [3328/50000]	Loss: 2.8832	LR: 0.050000
Training Epoch: 4 [3456/50000]	Loss: 2.8650	LR: 0.050000
Training Epoch: 4 [3584/50000]	Loss: 2.7502	LR: 0.050000
Training Epoch: 4 [3712/50000]	Loss: 2.6847	LR: 0.050000
Training Epoch: 4 [3840/50000]	Loss: 2.6356	LR: 0.050000
Training Epoch: 4 [3968/50000]	Loss: 2.9118	LR: 0.050000
Training Epoch: 4 [4096/50000]	Loss: 2.6545	LR: 0.050000
Training Epoch: 4 [4224/50000]	Loss: 2.6828	LR: 0.050000
Training Epoch: 4 [4352/50000]	Loss: 2.7990	LR: 0.050000
Training Epoch: 4 [4480/50000]	Loss: 2.5857	LR: 0.050000
Training Epoch: 4 [4608/50000]	Loss: 2.7750	LR: 0.050000
Training Epoch: 4 [4736/50000]	Loss: 2.5380	LR: 0.050000
Training Epoch: 4 [4864/50000]	Loss: 2.4392	LR: 0.050000
Training Epoch: 4 [4992/50000]	Loss: 2.6248	LR: 0.050000
Training Epoch: 4 [5120/50000]	Loss: 2.6128	LR: 0.050000
Training Epoch: 4 [5248/50000]	Loss: 2.7453	LR: 0.050000
Training Epoch: 4 [5376/50000]	Loss: 2.9975	LR: 0.050000
Training Epoch: 4 [5504/50000]	Loss: 2.7299	LR: 0.050000
Training Epoch: 4 [5632/50000]	Loss: 2.8113	LR: 0.050000
Training Epoch: 4 [5760/50000]	Loss: 2.6896	LR: 0.050000
Training Epoch: 4 [5888/50000]	Loss: 2.7239	LR: 0.050000
Training Epoch: 4 [6016/50000]	Loss: 2.6307	LR: 0.050000
Training Epoch: 4 [6144/50000]	Loss: 2.8337	LR: 0.050000
Training Epoch: 4 [6272/50000]	Loss: 2.7123	LR: 0.050000
Training Epoch: 4 [6400/50000]	Loss: 2.7822	LR: 0.050000
Training Epoch: 4 [6528/50000]	Loss: 2.7465	LR: 0.050000
Training Epoch: 4 [6656/50000]	Loss: 2.5908	LR: 0.050000
Training Epoch: 4 [6784/50000]	Loss: 2.9191	LR: 0.050000
Training Epoch: 4 [6912/50000]	Loss: 2.5147	LR: 0.050000
Training Epoch: 4 [7040/50000]	Loss: 2.8310	LR: 0.050000
Training Epoch: 4 [7168/50000]	Loss: 2.5766	LR: 0.050000
Training Epoch: 4 [7296/50000]	Loss: 2.8052	LR: 0.050000
Training Epoch: 4 [7424/50000]	Loss: 2.8142	LR: 0.050000
Training Epoch: 4 [7552/50000]	Loss: 2.5998	LR: 0.050000
Training Epoch: 4 [7680/50000]	Loss: 2.4963	LR: 0.050000
Training Epoch: 4 [7808/50000]	Loss: 3.0212	LR: 0.050000
Training Epoch: 4 [7936/50000]	Loss: 2.6145	LR: 0.050000
Training Epoch: 4 [8064/50000]	Loss: 2.7936	LR: 0.050000
Training Epoch: 4 [8192/50000]	Loss: 2.7410	LR: 0.050000
Training Epoch: 4 [8320/50000]	Loss: 2.6659	LR: 0.050000
Training Epoch: 4 [8448/50000]	Loss: 2.6743	LR: 0.050000
Training Epoch: 4 [8576/50000]	Loss: 2.9367	LR: 0.050000
Training Epoch: 4 [8704/50000]	Loss: 2.5009	LR: 0.050000
Training Epoch: 4 [8832/50000]	Loss: 2.5784	LR: 0.050000
Training Epoch: 4 [8960/50000]	Loss: 2.9279	LR: 0.050000
Training Epoch: 4 [9088/50000]	Loss: 2.6200	LR: 0.050000
Training Epoch: 4 [9216/50000]	Loss: 2.9079	LR: 0.050000
Training Epoch: 4 [9344/50000]	Loss: 2.6730	LR: 0.050000
Training Epoch: 4 [9472/50000]	Loss: 2.8910	LR: 0.050000
Training Epoch: 4 [9600/50000]	Loss: 2.4900	LR: 0.050000
Training Epoch: 4 [9728/50000]	Loss: 2.7184	LR: 0.050000
Training Epoch: 4 [9856/50000]	Loss: 2.7179	LR: 0.050000
Training Epoch: 4 [9984/50000]	Loss: 2.7256	LR: 0.050000
Training Epoch: 4 [10112/50000]	Loss: 2.6182	LR: 0.050000
Training Epoch: 4 [10240/50000]	Loss: 2.5798	LR: 0.050000
Training Epoch: 4 [10368/50000]	Loss: 2.8446	LR: 0.050000
Training Epoch: 4 [10496/50000]	Loss: 2.6331	LR: 0.050000
Training Epoch: 4 [10624/50000]	Loss: 2.5332	LR: 0.050000
Training Epoch: 4 [10752/50000]	Loss: 2.7062	LR: 0.050000
Training Epoch: 4 [10880/50000]	Loss: 2.9500	LR: 0.050000
Training Epoch: 4 [11008/50000]	Loss: 2.6613	LR: 0.050000
Training Epoch: 4 [11136/50000]	Loss: 2.7454	LR: 0.050000
Training Epoch: 4 [11264/50000]	Loss: 2.7053	LR: 0.050000
Training Epoch: 4 [11392/50000]	Loss: 2.3630	LR: 0.050000
Training Epoch: 4 [11520/50000]	Loss: 2.3217	LR: 0.050000
Training Epoch: 4 [11648/50000]	Loss: 2.7272	LR: 0.050000
Training Epoch: 4 [11776/50000]	Loss: 2.9099	LR: 0.050000
Training Epoch: 4 [11904/50000]	Loss: 2.9875	LR: 0.050000
Training Epoch: 4 [12032/50000]	Loss: 2.8295	LR: 0.050000
Training Epoch: 4 [12160/50000]	Loss: 2.8169	LR: 0.050000
Training Epoch: 4 [12288/50000]	Loss: 2.8216	LR: 0.050000
Training Epoch: 4 [12416/50000]	Loss: 2.8005	LR: 0.050000
Training Epoch: 4 [12544/50000]	Loss: 2.6151	LR: 0.050000
Training Epoch: 4 [12672/50000]	Loss: 2.7099	LR: 0.050000
Training Epoch: 4 [12800/50000]	Loss: 2.6395	LR: 0.050000
Training Epoch: 4 [12928/50000]	Loss: 2.9190	LR: 0.050000
Training Epoch: 4 [13056/50000]	Loss: 2.7220	LR: 0.050000
Training Epoch: 4 [13184/50000]	Loss: 2.6386	LR: 0.050000
Training Epoch: 4 [13312/50000]	Loss: 2.7542	LR: 0.050000
Training Epoch: 4 [13440/50000]	Loss: 2.7132	LR: 0.050000
Training Epoch: 4 [13568/50000]	Loss: 2.7261	LR: 0.050000
Training Epoch: 4 [13696/50000]	Loss: 2.7954	LR: 0.050000
Training Epoch: 4 [13824/50000]	Loss: 2.6332	LR: 0.050000
Training Epoch: 4 [13952/50000]	Loss: 2.5960	LR: 0.050000
Training Epoch: 4 [14080/50000]	Loss: 2.6927	LR: 0.050000
Training Epoch: 4 [14208/50000]	Loss: 2.8259	LR: 0.050000
Training Epoch: 4 [14336/50000]	Loss: 2.7643	LR: 0.050000
Training Epoch: 4 [14464/50000]	Loss: 2.8940	LR: 0.050000
Training Epoch: 4 [14592/50000]	Loss: 2.8289	LR: 0.050000
Training Epoch: 4 [14720/50000]	Loss: 2.7466	LR: 0.050000
Training Epoch: 4 [14848/50000]	Loss: 2.8034	LR: 0.050000
Training Epoch: 4 [14976/50000]	Loss: 2.7256	LR: 0.050000
Training Epoch: 4 [15104/50000]	Loss: 2.7057	LR: 0.050000
Training Epoch: 4 [15232/50000]	Loss: 2.6936	LR: 0.050000
Training Epoch: 4 [15360/50000]	Loss: 3.0023	LR: 0.050000
Training Epoch: 4 [15488/50000]	Loss: 2.9391	LR: 0.050000
Training Epoch: 4 [15616/50000]	Loss: 2.7510	LR: 0.050000
Training Epoch: 4 [15744/50000]	Loss: 2.8580	LR: 0.050000
Training Epoch: 4 [15872/50000]	Loss: 2.7280	LR: 0.050000
Training Epoch: 4 [16000/50000]	Loss: 2.7622	LR: 0.050000
Training Epoch: 4 [16128/50000]	Loss: 2.6932	LR: 0.050000
Training Epoch: 4 [16256/50000]	Loss: 2.6784	LR: 0.050000
Training Epoch: 4 [16384/50000]	Loss: 2.7933	LR: 0.050000
Training Epoch: 4 [16512/50000]	Loss: 2.7552	LR: 0.050000
Training Epoch: 4 [16640/50000]	Loss: 2.7818	LR: 0.050000
Training Epoch: 4 [16768/50000]	Loss: 2.6280	LR: 0.050000
Training Epoch: 4 [16896/50000]	Loss: 2.6261	LR: 0.050000
Training Epoch: 4 [17024/50000]	Loss: 2.6401	LR: 0.050000
Training Epoch: 4 [17152/50000]	Loss: 2.6335	LR: 0.050000
Training Epoch: 4 [17280/50000]	Loss: 2.6088	LR: 0.050000
Training Epoch: 4 [17408/50000]	Loss: 3.1746	LR: 0.050000
Training Epoch: 4 [17536/50000]	Loss: 2.8062	LR: 0.050000
Training Epoch: 4 [17664/50000]	Loss: 2.6908	LR: 0.050000
Training Epoch: 4 [17792/50000]	Loss: 2.5256	LR: 0.050000
Training Epoch: 4 [17920/50000]	Loss: 2.5553	LR: 0.050000
Training Epoch: 4 [18048/50000]	Loss: 2.7134	LR: 0.050000
Training Epoch: 4 [18176/50000]	Loss: 2.3832	LR: 0.050000
Training Epoch: 4 [18304/50000]	Loss: 2.6849	LR: 0.050000
Training Epoch: 4 [18432/50000]	Loss: 2.5414	LR: 0.050000
Training Epoch: 4 [18560/50000]	Loss: 2.7742	LR: 0.050000
Training Epoch: 4 [18688/50000]	Loss: 2.9881	LR: 0.050000
Training Epoch: 4 [18816/50000]	Loss: 2.7123	LR: 0.050000
Training Epoch: 4 [18944/50000]	Loss: 2.4613	LR: 0.050000
Training Epoch: 4 [19072/50000]	Loss: 2.4985	LR: 0.050000
Training Epoch: 4 [19200/50000]	Loss: 2.6864	LR: 0.050000
Training Epoch: 4 [19328/50000]	Loss: 2.4727	LR: 0.050000
Training Epoch: 4 [19456/50000]	Loss: 2.6673	LR: 0.050000
Training Epoch: 4 [19584/50000]	Loss: 2.6715	LR: 0.050000
Training Epoch: 4 [19712/50000]	Loss: 2.6263	LR: 0.050000
Training Epoch: 4 [19840/50000]	Loss: 2.5957	LR: 0.050000
Training Epoch: 4 [19968/50000]	Loss: 2.5171	LR: 0.050000
Training Epoch: 4 [20096/50000]	Loss: 2.5712	LR: 0.050000
Training Epoch: 4 [20224/50000]	Loss: 2.5062	LR: 0.050000
Training Epoch: 4 [20352/50000]	Loss: 2.5690	LR: 0.050000
Training Epoch: 4 [20480/50000]	Loss: 2.6130	LR: 0.050000
Training Epoch: 4 [20608/50000]	Loss: 2.4038	LR: 0.050000
Training Epoch: 4 [20736/50000]	Loss: 2.6657	LR: 0.050000
Training Epoch: 4 [20864/50000]	Loss: 2.4241	LR: 0.050000
Training Epoch: 4 [20992/50000]	Loss: 2.8979	LR: 0.050000
Training Epoch: 4 [21120/50000]	Loss: 2.7276	LR: 0.050000
Training Epoch: 4 [21248/50000]	Loss: 2.7751	LR: 0.050000
Training Epoch: 4 [21376/50000]	Loss: 2.6797	LR: 0.050000
Training Epoch: 4 [21504/50000]	Loss: 2.4224	LR: 0.050000
Training Epoch: 4 [21632/50000]	Loss: 2.6689	LR: 0.050000
Training Epoch: 4 [21760/50000]	Loss: 2.6207	LR: 0.050000
Training Epoch: 4 [21888/50000]	Loss: 2.4870	LR: 0.050000
Training Epoch: 4 [22016/50000]	Loss: 2.8470	LR: 0.050000
Training Epoch: 4 [22144/50000]	Loss: 2.5580	LR: 0.050000
Training Epoch: 4 [22272/50000]	Loss: 3.0663	LR: 0.050000
Training Epoch: 4 [22400/50000]	Loss: 2.7014	LR: 0.050000
Training Epoch: 4 [22528/50000]	Loss: 2.8168	LR: 0.050000
Training Epoch: 4 [22656/50000]	Loss: 2.8074	LR: 0.050000
Training Epoch: 4 [22784/50000]	Loss: 2.6516	LR: 0.050000
Training Epoch: 4 [22912/50000]	Loss: 2.5075	LR: 0.050000
Training Epoch: 4 [23040/50000]	Loss: 2.6793	LR: 0.050000
Training Epoch: 4 [23168/50000]	Loss: 2.5666	LR: 0.050000
Training Epoch: 4 [23296/50000]	Loss: 2.5717	LR: 0.050000
Training Epoch: 4 [23424/50000]	Loss: 2.7464	LR: 0.050000
Training Epoch: 4 [23552/50000]	Loss: 2.7446	LR: 0.050000
Training Epoch: 4 [23680/50000]	Loss: 2.8412	LR: 0.050000
Training Epoch: 4 [23808/50000]	Loss: 2.8191	LR: 0.050000
Training Epoch: 4 [23936/50000]	Loss: 2.8478	LR: 0.050000
Training Epoch: 4 [24064/50000]	Loss: 2.6554	LR: 0.050000
Training Epoch: 4 [24192/50000]	Loss: 2.6581	LR: 0.050000
Training Epoch: 4 [24320/50000]	Loss: 2.5622	LR: 0.050000
Training Epoch: 4 [24448/50000]	Loss: 2.6682	LR: 0.050000
Training Epoch: 4 [24576/50000]	Loss: 2.7199	LR: 0.050000
Training Epoch: 4 [24704/50000]	Loss: 2.4917	LR: 0.050000
Training Epoch: 4 [24832/50000]	Loss: 2.5420	LR: 0.050000
Training Epoch: 4 [24960/50000]	Loss: 2.4915	LR: 0.050000
Training Epoch: 4 [25088/50000]	Loss: 2.4922	LR: 0.050000
Training Epoch: 4 [25216/50000]	Loss: 2.8440	LR: 0.050000
Training Epoch: 4 [25344/50000]	Loss: 2.6119	LR: 0.050000
Training Epoch: 4 [25472/50000]	Loss: 2.7973	LR: 0.050000
Training Epoch: 4 [25600/50000]	Loss: 2.7641	LR: 0.050000
Training Epoch: 4 [25728/50000]	Loss: 2.5424	LR: 0.050000
Training Epoch: 4 [25856/50000]	Loss: 2.5567	LR: 0.050000
Training Epoch: 4 [25984/50000]	Loss: 2.8976	LR: 0.050000
Training Epoch: 4 [26112/50000]	Loss: 2.8549	LR: 0.050000
Training Epoch: 4 [26240/50000]	Loss: 2.6204	LR: 0.050000
Training Epoch: 4 [26368/50000]	Loss: 2.8873	LR: 0.050000
Training Epoch: 4 [26496/50000]	Loss: 2.7332	LR: 0.050000
Training Epoch: 4 [26624/50000]	Loss: 2.7271	LR: 0.050000
Training Epoch: 4 [26752/50000]	Loss: 2.7341	LR: 0.050000
Training Epoch: 4 [26880/50000]	Loss: 2.5537	LR: 0.050000
Training Epoch: 4 [27008/50000]	Loss: 2.7474	LR: 0.050000
Training Epoch: 4 [27136/50000]	Loss: 2.5969	LR: 0.050000
Training Epoch: 4 [27264/50000]	Loss: 2.5697	LR: 0.050000
Training Epoch: 4 [27392/50000]	Loss: 2.7663	LR: 0.050000
Training Epoch: 4 [27520/50000]	Loss: 2.8074	LR: 0.050000
Training Epoch: 4 [27648/50000]	Loss: 2.5596	LR: 0.050000
Training Epoch: 4 [27776/50000]	Loss: 2.6544	LR: 0.050000
Training Epoch: 4 [27904/50000]	Loss: 2.5872	LR: 0.050000
Training Epoch: 4 [28032/50000]	Loss: 2.9629	LR: 0.050000
Training Epoch: 4 [28160/50000]	Loss: 2.8658	LR: 0.050000
Training Epoch: 4 [28288/50000]	Loss: 2.7527	LR: 0.050000
Training Epoch: 4 [28416/50000]	Loss: 2.5691	LR: 0.050000
Training Epoch: 4 [28544/50000]	Loss: 2.5399	LR: 0.050000
Training Epoch: 4 [28672/50000]	Loss: 2.5768	LR: 0.050000
Training Epoch: 4 [28800/50000]	Loss: 2.9358	LR: 0.050000
Training Epoch: 4 [28928/50000]	Loss: 2.6459	LR: 0.050000
Training Epoch: 4 [29056/50000]	Loss: 2.5996	LR: 0.050000
Training Epoch: 4 [29184/50000]	Loss: 2.5456	LR: 0.050000
Training Epoch: 4 [29312/50000]	Loss: 2.7244	LR: 0.050000
Training Epoch: 4 [29440/50000]	Loss: 2.4995	LR: 0.050000
Training Epoch: 4 [29568/50000]	Loss: 2.8199	LR: 0.050000
Training Epoch: 4 [29696/50000]	Loss: 2.3904	LR: 0.050000
Training Epoch: 4 [29824/50000]	Loss: 2.4606	LR: 0.050000
Training Epoch: 4 [29952/50000]	Loss: 2.8146	LR: 0.050000
Training Epoch: 4 [30080/50000]	Loss: 2.4528	LR: 0.050000
Training Epoch: 4 [30208/50000]	Loss: 2.7039	LR: 0.050000
Training Epoch: 4 [30336/50000]	Loss: 2.8212	LR: 0.050000
Training Epoch: 4 [30464/50000]	Loss: 2.3840	LR: 0.050000
Training Epoch: 4 [30592/50000]	Loss: 2.6057	LR: 0.050000
Training Epoch: 4 [30720/50000]	Loss: 2.4707	LR: 0.050000
Training Epoch: 4 [30848/50000]	Loss: 2.6078	LR: 0.050000
Training Epoch: 4 [30976/50000]	Loss: 2.7196	LR: 0.050000
Training Epoch: 4 [31104/50000]	Loss: 2.4832	LR: 0.050000
Training Epoch: 4 [31232/50000]	Loss: 2.4961	LR: 0.050000
Training Epoch: 4 [31360/50000]	Loss: 2.8250	LR: 0.050000
Training Epoch: 4 [31488/50000]	Loss: 2.2756	LR: 0.050000
Training Epoch: 4 [31616/50000]	Loss: 2.3961	LR: 0.050000
Training Epoch: 4 [31744/50000]	Loss: 2.7980	LR: 0.050000
Training Epoch: 4 [31872/50000]	Loss: 2.6205	LR: 0.050000
Training Epoch: 4 [32000/50000]	Loss: 2.6856	LR: 0.050000
Training Epoch: 4 [32128/50000]	Loss: 2.6844	LR: 0.050000
Training Epoch: 4 [32256/50000]	Loss: 2.3526	LR: 0.050000
Training Epoch: 4 [32384/50000]	Loss: 2.5622	LR: 0.050000
Training Epoch: 4 [32512/50000]	Loss: 2.3819	LR: 0.050000
Training Epoch: 4 [32640/50000]	Loss: 2.8177	LR: 0.050000
Training Epoch: 4 [32768/50000]	Loss: 2.5975	LR: 0.050000
Training Epoch: 4 [32896/50000]	Loss: 2.4753	LR: 0.050000
Training Epoch: 4 [33024/50000]	Loss: 2.5227	LR: 0.050000
Training Epoch: 4 [33152/50000]	Loss: 2.7328	LR: 0.050000
Training Epoch: 4 [33280/50000]	Loss: 2.7904	LR: 0.050000
Training Epoch: 4 [33408/50000]	Loss: 2.6427	LR: 0.050000
Training Epoch: 4 [33536/50000]	Loss: 2.7069	LR: 0.050000
Training Epoch: 4 [33664/50000]	Loss: 2.8260	LR: 0.050000
Training Epoch: 4 [33792/50000]	Loss: 2.3851	LR: 0.050000
Training Epoch: 4 [33920/50000]	Loss: 2.6032	LR: 0.050000
Training Epoch: 4 [34048/50000]	Loss: 2.7994	LR: 0.050000
Training Epoch: 4 [34176/50000]	Loss: 2.6560	LR: 0.050000
Training Epoch: 4 [34304/50000]	Loss: 2.4471	LR: 0.050000
Training Epoch: 4 [34432/50000]	Loss: 2.6684	LR: 0.050000
Training Epoch: 4 [34560/50000]	Loss: 2.5522	LR: 0.050000
Training Epoch: 4 [34688/50000]	Loss: 2.6602	LR: 0.050000
Training Epoch: 4 [34816/50000]	Loss: 2.5921	LR: 0.050000
Training Epoch: 4 [34944/50000]	Loss: 2.4768	LR: 0.050000
Training Epoch: 4 [35072/50000]	Loss: 2.7190	LR: 0.050000
Training Epoch: 4 [35200/50000]	Loss: 2.5535	LR: 0.050000
Training Epoch: 4 [35328/50000]	Loss: 2.6051	LR: 0.050000
Training Epoch: 4 [35456/50000]	Loss: 2.5649	LR: 0.050000
Training Epoch: 4 [35584/50000]	Loss: 2.7412	LR: 0.050000
Training Epoch: 4 [35712/50000]	Loss: 2.6705	LR: 0.050000
Training Epoch: 4 [35840/50000]	Loss: 2.3505	LR: 0.050000
Training Epoch: 4 [35968/50000]	Loss: 2.6876	LR: 0.050000
Training Epoch: 4 [36096/50000]	Loss: 2.3514	LR: 0.050000
Training Epoch: 4 [36224/50000]	Loss: 2.6189	LR: 0.050000
Training Epoch: 4 [36352/50000]	Loss: 2.5652	LR: 0.050000
Training Epoch: 4 [36480/50000]	Loss: 2.5671	LR: 0.050000
Training Epoch: 4 [36608/50000]	Loss: 2.6490	LR: 0.050000
Training Epoch: 4 [36736/50000]	Loss: 2.3822	LR: 0.050000
Training Epoch: 4 [36864/50000]	Loss: 2.7441	LR: 0.050000
Training Epoch: 4 [36992/50000]	Loss: 2.5486	LR: 0.050000
Training Epoch: 4 [37120/50000]	Loss: 2.4745	LR: 0.050000
Training Epoch: 4 [37248/50000]	Loss: 2.4482	LR: 0.050000
Training Epoch: 4 [37376/50000]	Loss: 2.7591	LR: 0.050000
Training Epoch: 4 [37504/50000]	Loss: 2.5672	LR: 0.050000
Training Epoch: 4 [37632/50000]	Loss: 2.5548	LR: 0.050000
Training Epoch: 4 [37760/50000]	Loss: 2.5666	LR: 0.050000
Training Epoch: 4 [37888/50000]	Loss: 2.5283	LR: 0.050000
Training Epoch: 4 [38016/50000]	Loss: 2.5533	LR: 0.050000
Training Epoch: 4 [38144/50000]	Loss: 2.4015	LR: 0.050000
Training Epoch: 4 [38272/50000]	Loss: 2.5057	LR: 0.050000
Training Epoch: 4 [38400/50000]	Loss: 2.5886	LR: 0.050000
Training Epoch: 4 [38528/50000]	Loss: 2.6866	LR: 0.050000
Training Epoch: 4 [38656/50000]	Loss: 2.4401	LR: 0.050000
Training Epoch: 4 [38784/50000]	Loss: 2.4769	LR: 0.050000
Training Epoch: 4 [38912/50000]	Loss: 2.4581	LR: 0.050000
Training Epoch: 4 [39040/50000]	Loss: 2.5502	LR: 0.050000
Training Epoch: 4 [39168/50000]	Loss: 2.2807	LR: 0.050000
Training Epoch: 4 [39296/50000]	Loss: 2.4840	LR: 0.050000
Training Epoch: 4 [39424/50000]	Loss: 2.5864	LR: 0.050000
Training Epoch: 4 [39552/50000]	Loss: 2.8618	LR: 0.050000
Training Epoch: 4 [39680/50000]	Loss: 2.5216	LR: 0.050000
Training Epoch: 4 [39808/50000]	Loss: 2.3121	LR: 0.050000
Training Epoch: 4 [39936/50000]	Loss: 2.7232	LR: 0.050000
Training Epoch: 4 [40064/50000]	Loss: 2.7405	LR: 0.050000
Training Epoch: 4 [40192/50000]	Loss: 2.5084	LR: 0.050000
Training Epoch: 4 [40320/50000]	Loss: 2.6724	LR: 0.050000
Training Epoch: 4 [40448/50000]	Loss: 2.7500	LR: 0.050000
Training Epoch: 4 [40576/50000]	Loss: 2.5788	LR: 0.050000
Training Epoch: 4 [40704/50000]	Loss: 2.7689	LR: 0.050000
Training Epoch: 4 [40832/50000]	Loss: 2.5876	LR: 0.050000
Training Epoch: 4 [40960/50000]	Loss: 2.4295	LR: 0.050000
Training Epoch: 4 [41088/50000]	Loss: 2.5311	LR: 0.050000
Training Epoch: 4 [41216/50000]	Loss: 2.5562	LR: 0.050000
Training Epoch: 4 [41344/50000]	Loss: 2.8262	LR: 0.050000
Training Epoch: 4 [41472/50000]	Loss: 2.5711	LR: 0.050000
Training Epoch: 4 [41600/50000]	Loss: 2.7129	LR: 0.050000
Training Epoch: 4 [41728/50000]	Loss: 2.4393	LR: 0.050000
Training Epoch: 4 [41856/50000]	Loss: 2.3326	LR: 0.050000
Training Epoch: 4 [41984/50000]	Loss: 2.8025	LR: 0.050000
Training Epoch: 4 [42112/50000]	Loss: 2.4152	LR: 0.050000
Training Epoch: 4 [42240/50000]	Loss: 2.4737	LR: 0.050000
Training Epoch: 4 [42368/50000]	Loss: 2.6072	LR: 0.050000
Training Epoch: 4 [42496/50000]	Loss: 2.5913	LR: 0.050000
Training Epoch: 4 [42624/50000]	Loss: 2.3880	LR: 0.050000
Training Epoch: 4 [42752/50000]	Loss: 2.7080	LR: 0.050000
Training Epoch: 4 [42880/50000]	Loss: 2.4828	LR: 0.050000
Training Epoch: 4 [43008/50000]	Loss: 2.7799	LR: 0.050000
Training Epoch: 4 [43136/50000]	Loss: 2.6563	LR: 0.050000
Training Epoch: 4 [43264/50000]	Loss: 2.5428	LR: 0.050000
Training Epoch: 4 [43392/50000]	Loss: 2.6279	LR: 0.050000
Training Epoch: 4 [43520/50000]	Loss: 2.4084	LR: 0.050000
Training Epoch: 4 [43648/50000]	Loss: 2.4832	LR: 0.050000
Training Epoch: 4 [43776/50000]	Loss: 2.2513	LR: 0.050000
Training Epoch: 4 [43904/50000]	Loss: 2.6715	LR: 0.050000
Training Epoch: 4 [44032/50000]	Loss: 2.6607	LR: 0.050000
Training Epoch: 4 [44160/50000]	Loss: 2.4798	LR: 0.050000
Training Epoch: 4 [44288/50000]	Loss: 2.5479	LR: 0.050000
Training Epoch: 4 [44416/50000]	Loss: 2.5110	LR: 0.050000
Training Epoch: 4 [44544/50000]	Loss: 2.6859	LR: 0.050000
Training Epoch: 4 [44672/50000]	Loss: 2.4730	LR: 0.050000
Training Epoch: 4 [44800/50000]	Loss: 2.6487	LR: 0.050000
Training Epoch: 4 [44928/50000]	Loss: 2.5242	LR: 0.050000
Training Epoch: 4 [45056/50000]	Loss: 2.4666	LR: 0.050000
Training Epoch: 4 [45184/50000]	Loss: 2.5114	LR: 0.050000
Training Epoch: 4 [45312/50000]	Loss: 2.4691	LR: 0.050000
Training Epoch: 4 [45440/50000]	Loss: 2.6974	LR: 0.050000
Training Epoch: 4 [45568/50000]	Loss: 2.6804	LR: 0.050000
Training Epoch: 4 [45696/50000]	Loss: 2.6868	LR: 0.050000
Training Epoch: 4 [45824/50000]	Loss: 2.4823	LR: 0.050000
Training Epoch: 4 [45952/50000]	Loss: 2.5340	LR: 0.050000
Training Epoch: 4 [46080/50000]	Loss: 2.7190	LR: 0.050000
Training Epoch: 4 [46208/50000]	Loss: 2.6090	LR: 0.050000
Training Epoch: 4 [46336/50000]	Loss: 2.2251	LR: 0.050000
Training Epoch: 4 [46464/50000]	Loss: 2.7429	LR: 0.050000
Training Epoch: 4 [46592/50000]	Loss: 2.4715	LR: 0.050000
Training Epoch: 4 [46720/50000]	Loss: 2.7756	LR: 0.050000
Training Epoch: 4 [46848/50000]	Loss: 2.5542	LR: 0.050000
Training Epoch: 4 [46976/50000]	Loss: 2.6911	LR: 0.050000
Training Epoch: 4 [47104/50000]	Loss: 2.5747	LR: 0.050000
Training Epoch: 4 [47232/50000]	Loss: 2.8799	LR: 0.050000
Training Epoch: 4 [47360/50000]	Loss: 2.5473	LR: 0.050000
Training Epoch: 4 [47488/50000]	Loss: 2.4781	LR: 0.050000
Training Epoch: 4 [47616/50000]	Loss: 2.5415	LR: 0.050000
Training Epoch: 4 [47744/50000]	Loss: 2.7823	LR: 0.050000
Training Epoch: 4 [47872/50000]	Loss: 2.7576	LR: 0.050000
Training Epoch: 4 [48000/50000]	Loss: 2.8177	LR: 0.050000
Training Epoch: 4 [48128/50000]	Loss: 2.4695	LR: 0.050000
Training Epoch: 4 [48256/50000]	Loss: 2.8377	LR: 0.050000
Training Epoch: 4 [48384/50000]	Loss: 2.4997	LR: 0.050000
Training Epoch: 4 [48512/50000]	Loss: 2.5474	LR: 0.050000
Training Epoch: 4 [48640/50000]	Loss: 2.4543	LR: 0.050000
Training Epoch: 4 [48768/50000]	Loss: 2.7298	LR: 0.050000
Training Epoch: 4 [48896/50000]	Loss: 2.5332	LR: 0.050000
Training Epoch: 4 [49024/50000]	Loss: 2.6033	LR: 0.050000
Training Epoch: 4 [49152/50000]	Loss: 2.5025	LR: 0.050000
Training Epoch: 4 [49280/50000]	Loss: 2.5679	LR: 0.050000
Training Epoch: 4 [49408/50000]	Loss: 2.7029	LR: 0.050000
Training Epoch: 4 [49536/50000]	Loss: 2.4066	LR: 0.050000
Training Epoch: 4 [49664/50000]	Loss: 2.6563	LR: 0.050000
Training Epoch: 4 [49792/50000]	Loss: 2.2568	LR: 0.050000
Training Epoch: 4 [49920/50000]	Loss: 2.5687	LR: 0.050000
Training Epoch: 4 [50000/50000]	Loss: 2.6295	LR: 0.050000
epoch 4 training time consumed: 53.88s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   14680 GB |   14680 GB |
|       from large pool |  123392 KB |    1034 MB |   14666 GB |   14666 GB |
|       from small pool |   10798 KB |      13 MB |      14 GB |      14 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   14680 GB |   14680 GB |
|       from large pool |  123392 KB |    1034 MB |   14666 GB |   14666 GB |
|       from small pool |   10798 KB |      13 MB |      14 GB |      14 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |    6463 GB |    6463 GB |
|       from large pool |  155136 KB |  433088 KB |    6447 GB |    6447 GB |
|       from small pool |    1490 KB |    3494 KB |      16 GB |      16 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |  567004    |  566749    |
|       from large pool |      24    |      65    |  295751    |  295727    |
|       from small pool |     231    |     274    |  271253    |  271022    |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |  567004    |  566749    |
|       from large pool |      24    |      65    |  295751    |  295727    |
|       from small pool |     231    |     274    |  271253    |  271022    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |  283861    |  283840    |
|       from large pool |       9    |      14    |  143147    |  143138    |
|       from small pool |      12    |      16    |  140714    |  140702    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 4, Average loss: 0.0199, Accuracy: 0.3557, Time consumed:3.47s

Training Epoch: 5 [128/50000]	Loss: 2.2538	LR: 0.050000
Training Epoch: 5 [256/50000]	Loss: 2.4267	LR: 0.050000
Training Epoch: 5 [384/50000]	Loss: 2.4674	LR: 0.050000
Training Epoch: 5 [512/50000]	Loss: 2.4755	LR: 0.050000
Training Epoch: 5 [640/50000]	Loss: 2.3912	LR: 0.050000
Training Epoch: 5 [768/50000]	Loss: 2.4453	LR: 0.050000
Training Epoch: 5 [896/50000]	Loss: 2.3982	LR: 0.050000
Training Epoch: 5 [1024/50000]	Loss: 2.4560	LR: 0.050000
Training Epoch: 5 [1152/50000]	Loss: 2.4406	LR: 0.050000
Training Epoch: 5 [1280/50000]	Loss: 2.3568	LR: 0.050000
Training Epoch: 5 [1408/50000]	Loss: 2.4518	LR: 0.050000
Training Epoch: 5 [1536/50000]	Loss: 2.5766	LR: 0.050000
Training Epoch: 5 [1664/50000]	Loss: 2.3614	LR: 0.050000
Training Epoch: 5 [1792/50000]	Loss: 2.2991	LR: 0.050000
Training Epoch: 5 [1920/50000]	Loss: 2.5107	LR: 0.050000
Training Epoch: 5 [2048/50000]	Loss: 2.5467	LR: 0.050000
Training Epoch: 5 [2176/50000]	Loss: 2.4446	LR: 0.050000
Training Epoch: 5 [2304/50000]	Loss: 2.5443	LR: 0.050000
Training Epoch: 5 [2432/50000]	Loss: 2.3253	LR: 0.050000
Training Epoch: 5 [2560/50000]	Loss: 2.3305	LR: 0.050000
Training Epoch: 5 [2688/50000]	Loss: 2.4646	LR: 0.050000
Training Epoch: 5 [2816/50000]	Loss: 2.4448	LR: 0.050000
Training Epoch: 5 [2944/50000]	Loss: 2.4944	LR: 0.050000
Training Epoch: 5 [3072/50000]	Loss: 2.4032	LR: 0.050000
Training Epoch: 5 [3200/50000]	Loss: 2.6561	LR: 0.050000
Training Epoch: 5 [3328/50000]	Loss: 2.5382	LR: 0.050000
Training Epoch: 5 [3456/50000]	Loss: 2.3641	LR: 0.050000
Training Epoch: 5 [3584/50000]	Loss: 2.4838	LR: 0.050000
Training Epoch: 5 [3712/50000]	Loss: 2.5387	LR: 0.050000
Training Epoch: 5 [3840/50000]	Loss: 2.3059	LR: 0.050000
Training Epoch: 5 [3968/50000]	Loss: 2.2570	LR: 0.050000
Training Epoch: 5 [4096/50000]	Loss: 2.4760	LR: 0.050000
Training Epoch: 5 [4224/50000]	Loss: 2.1136	LR: 0.050000
Training Epoch: 5 [4352/50000]	Loss: 2.6736	LR: 0.050000
Training Epoch: 5 [4480/50000]	Loss: 2.3749	LR: 0.050000
Training Epoch: 5 [4608/50000]	Loss: 2.3662	LR: 0.050000
Training Epoch: 5 [4736/50000]	Loss: 2.5226	LR: 0.050000
Training Epoch: 5 [4864/50000]	Loss: 2.4523	LR: 0.050000
Training Epoch: 5 [4992/50000]	Loss: 2.5947	LR: 0.050000
Training Epoch: 5 [5120/50000]	Loss: 2.6774	LR: 0.050000
Training Epoch: 5 [5248/50000]	Loss: 2.5658	LR: 0.050000
Training Epoch: 5 [5376/50000]	Loss: 2.3258	LR: 0.050000
Training Epoch: 5 [5504/50000]	Loss: 2.5845	LR: 0.050000
Training Epoch: 5 [5632/50000]	Loss: 2.4422	LR: 0.050000
Training Epoch: 5 [5760/50000]	Loss: 2.3734	LR: 0.050000
Training Epoch: 5 [5888/50000]	Loss: 2.5516	LR: 0.050000
Training Epoch: 5 [6016/50000]	Loss: 2.4264	LR: 0.050000
Training Epoch: 5 [6144/50000]	Loss: 2.5681	LR: 0.050000
Training Epoch: 5 [6272/50000]	Loss: 2.4419	LR: 0.050000
Training Epoch: 5 [6400/50000]	Loss: 2.4532	LR: 0.050000
Training Epoch: 5 [6528/50000]	Loss: 2.5966	LR: 0.050000
Training Epoch: 5 [6656/50000]	Loss: 2.3944	LR: 0.050000
Training Epoch: 5 [6784/50000]	Loss: 2.4902	LR: 0.050000
Training Epoch: 5 [6912/50000]	Loss: 2.3911	LR: 0.050000
Training Epoch: 5 [7040/50000]	Loss: 2.3624	LR: 0.050000
Training Epoch: 5 [7168/50000]	Loss: 2.4755	LR: 0.050000
Training Epoch: 5 [7296/50000]	Loss: 2.4365	LR: 0.050000
Training Epoch: 5 [7424/50000]	Loss: 2.4987	LR: 0.050000
Training Epoch: 5 [7552/50000]	Loss: 2.3759	LR: 0.050000
Training Epoch: 5 [7680/50000]	Loss: 2.2370	LR: 0.050000
Training Epoch: 5 [7808/50000]	Loss: 2.6033	LR: 0.050000
Training Epoch: 5 [7936/50000]	Loss: 2.6003	LR: 0.050000
Training Epoch: 5 [8064/50000]	Loss: 2.3566	LR: 0.050000
Training Epoch: 5 [8192/50000]	Loss: 2.3490	LR: 0.050000
Training Epoch: 5 [8320/50000]	Loss: 2.4691	LR: 0.050000
Training Epoch: 5 [8448/50000]	Loss: 2.3161	LR: 0.050000
Training Epoch: 5 [8576/50000]	Loss: 2.5380	LR: 0.050000
Training Epoch: 5 [8704/50000]	Loss: 2.5637	LR: 0.050000
Training Epoch: 5 [8832/50000]	Loss: 2.4168	LR: 0.050000
Training Epoch: 5 [8960/50000]	Loss: 2.6095	LR: 0.050000
Training Epoch: 5 [9088/50000]	Loss: 2.5977	LR: 0.050000
Training Epoch: 5 [9216/50000]	Loss: 2.4719	LR: 0.050000
Training Epoch: 5 [9344/50000]	Loss: 2.4888	LR: 0.050000
Training Epoch: 5 [9472/50000]	Loss: 2.6116	LR: 0.050000
Training Epoch: 5 [9600/50000]	Loss: 2.4163	LR: 0.050000
Training Epoch: 5 [9728/50000]	Loss: 2.7645	LR: 0.050000
Training Epoch: 5 [9856/50000]	Loss: 2.6299	LR: 0.050000
Training Epoch: 5 [9984/50000]	Loss: 2.5664	LR: 0.050000
Training Epoch: 5 [10112/50000]	Loss: 2.6269	LR: 0.050000
Training Epoch: 5 [10240/50000]	Loss: 2.3868	LR: 0.050000
Training Epoch: 5 [10368/50000]	Loss: 2.3524	LR: 0.050000
Training Epoch: 5 [10496/50000]	Loss: 2.7313	LR: 0.050000
Training Epoch: 5 [10624/50000]	Loss: 2.3654	LR: 0.050000
Training Epoch: 5 [10752/50000]	Loss: 2.4210	LR: 0.050000
Training Epoch: 5 [10880/50000]	Loss: 2.1061	LR: 0.050000
Training Epoch: 5 [11008/50000]	Loss: 2.6824	LR: 0.050000
Training Epoch: 5 [11136/50000]	Loss: 2.8468	LR: 0.050000
Training Epoch: 5 [11264/50000]	Loss: 2.4390	LR: 0.050000
Training Epoch: 5 [11392/50000]	Loss: 2.3783	LR: 0.050000
Training Epoch: 5 [11520/50000]	Loss: 2.3888	LR: 0.050000
Training Epoch: 5 [11648/50000]	Loss: 2.4037	LR: 0.050000
Training Epoch: 5 [11776/50000]	Loss: 2.1459	LR: 0.050000
Training Epoch: 5 [11904/50000]	Loss: 2.0929	LR: 0.050000
Training Epoch: 5 [12032/50000]	Loss: 2.4042	LR: 0.050000
Training Epoch: 5 [12160/50000]	Loss: 2.5232	LR: 0.050000
Training Epoch: 5 [12288/50000]	Loss: 2.1817	LR: 0.050000
Training Epoch: 5 [12416/50000]	Loss: 2.6677	LR: 0.050000
Training Epoch: 5 [12544/50000]	Loss: 2.2689	LR: 0.050000
Training Epoch: 5 [12672/50000]	Loss: 2.5593	LR: 0.050000
Training Epoch: 5 [12800/50000]	Loss: 2.3573	LR: 0.050000
Training Epoch: 5 [12928/50000]	Loss: 2.5034	LR: 0.050000
Training Epoch: 5 [13056/50000]	Loss: 2.5143	LR: 0.050000
Training Epoch: 5 [13184/50000]	Loss: 2.4960	LR: 0.050000
Training Epoch: 5 [13312/50000]	Loss: 2.4004	LR: 0.050000
Training Epoch: 5 [13440/50000]	Loss: 2.2620	LR: 0.050000
Training Epoch: 5 [13568/50000]	Loss: 2.3118	LR: 0.050000
Training Epoch: 5 [13696/50000]	Loss: 2.3980	LR: 0.050000
Training Epoch: 5 [13824/50000]	Loss: 2.4666	LR: 0.050000
Training Epoch: 5 [13952/50000]	Loss: 2.3592	LR: 0.050000
Training Epoch: 5 [14080/50000]	Loss: 2.4293	LR: 0.050000
Training Epoch: 5 [14208/50000]	Loss: 2.3933	LR: 0.050000
Training Epoch: 5 [14336/50000]	Loss: 2.3639	LR: 0.050000
Training Epoch: 5 [14464/50000]	Loss: 2.3929	LR: 0.050000
Training Epoch: 5 [14592/50000]	Loss: 2.5922	LR: 0.050000
Training Epoch: 5 [14720/50000]	Loss: 2.2797	LR: 0.050000
Training Epoch: 5 [14848/50000]	Loss: 2.6169	LR: 0.050000
Training Epoch: 5 [14976/50000]	Loss: 2.3063	LR: 0.050000
Training Epoch: 5 [15104/50000]	Loss: 2.4191	LR: 0.050000
Training Epoch: 5 [15232/50000]	Loss: 2.3862	LR: 0.050000
Training Epoch: 5 [15360/50000]	Loss: 2.1930	LR: 0.050000
Training Epoch: 5 [15488/50000]	Loss: 2.1767	LR: 0.050000
Training Epoch: 5 [15616/50000]	Loss: 2.4310	LR: 0.050000
Training Epoch: 5 [15744/50000]	Loss: 2.1913	LR: 0.050000
Training Epoch: 5 [15872/50000]	Loss: 2.4513	LR: 0.050000
Training Epoch: 5 [16000/50000]	Loss: 2.5545	LR: 0.050000
Training Epoch: 5 [16128/50000]	Loss: 2.3973	LR: 0.050000
Training Epoch: 5 [16256/50000]	Loss: 2.7220	LR: 0.050000
Training Epoch: 5 [16384/50000]	Loss: 2.4846	LR: 0.050000
Training Epoch: 5 [16512/50000]	Loss: 2.2077	LR: 0.050000
Training Epoch: 5 [16640/50000]	Loss: 2.3772	LR: 0.050000
Training Epoch: 5 [16768/50000]	Loss: 2.3282	LR: 0.050000
Training Epoch: 5 [16896/50000]	Loss: 2.5819	LR: 0.050000
Training Epoch: 5 [17024/50000]	Loss: 2.6857	LR: 0.050000
Training Epoch: 5 [17152/50000]	Loss: 2.3299	LR: 0.050000
Training Epoch: 5 [17280/50000]	Loss: 2.5902	LR: 0.050000
Training Epoch: 5 [17408/50000]	Loss: 2.4368	LR: 0.050000
Training Epoch: 5 [17536/50000]	Loss: 2.2908	LR: 0.050000
Training Epoch: 5 [17664/50000]	Loss: 2.4471	LR: 0.050000
Training Epoch: 5 [17792/50000]	Loss: 2.3659	LR: 0.050000
Training Epoch: 5 [17920/50000]	Loss: 2.4823	LR: 0.050000
Training Epoch: 5 [18048/50000]	Loss: 2.4651	LR: 0.050000
Training Epoch: 5 [18176/50000]	Loss: 2.2980	LR: 0.050000
Training Epoch: 5 [18304/50000]	Loss: 2.7204	LR: 0.050000
Training Epoch: 5 [18432/50000]	Loss: 2.3614	LR: 0.050000
Training Epoch: 5 [18560/50000]	Loss: 2.3289	LR: 0.050000
Training Epoch: 5 [18688/50000]	Loss: 2.5788	LR: 0.050000
Training Epoch: 5 [18816/50000]	Loss: 2.2113	LR: 0.050000
Training Epoch: 5 [18944/50000]	Loss: 2.4264	LR: 0.050000
Training Epoch: 5 [19072/50000]	Loss: 2.5447	LR: 0.050000
Training Epoch: 5 [19200/50000]	Loss: 2.5709	LR: 0.050000
Training Epoch: 5 [19328/50000]	Loss: 2.3054	LR: 0.050000
Training Epoch: 5 [19456/50000]	Loss: 2.3482	LR: 0.050000
Training Epoch: 5 [19584/50000]	Loss: 2.2681	LR: 0.050000
Training Epoch: 5 [19712/50000]	Loss: 2.5890	LR: 0.050000
Training Epoch: 5 [19840/50000]	Loss: 2.5508	LR: 0.050000
Training Epoch: 5 [19968/50000]	Loss: 2.5298	LR: 0.050000
Training Epoch: 5 [20096/50000]	Loss: 2.2717	LR: 0.050000
Training Epoch: 5 [20224/50000]	Loss: 2.2095	LR: 0.050000
Training Epoch: 5 [20352/50000]	Loss: 2.4898	LR: 0.050000
Training Epoch: 5 [20480/50000]	Loss: 2.0788	LR: 0.050000
Training Epoch: 5 [20608/50000]	Loss: 2.3455	LR: 0.050000
Training Epoch: 5 [20736/50000]	Loss: 2.2778	LR: 0.050000
Training Epoch: 5 [20864/50000]	Loss: 2.5192	LR: 0.050000
Training Epoch: 5 [20992/50000]	Loss: 2.3183	LR: 0.050000
Training Epoch: 5 [21120/50000]	Loss: 2.4786	LR: 0.050000
Training Epoch: 5 [21248/50000]	Loss: 2.5600	LR: 0.050000
Training Epoch: 5 [21376/50000]	Loss: 2.1634	LR: 0.050000
Training Epoch: 5 [21504/50000]	Loss: 2.5106	LR: 0.050000
Training Epoch: 5 [21632/50000]	Loss: 2.6011	LR: 0.050000
Training Epoch: 5 [21760/50000]	Loss: 2.0122	LR: 0.050000
Training Epoch: 5 [21888/50000]	Loss: 2.3020	LR: 0.050000
Training Epoch: 5 [22016/50000]	Loss: 2.2576	LR: 0.050000
Training Epoch: 5 [22144/50000]	Loss: 2.4547	LR: 0.050000
Training Epoch: 5 [22272/50000]	Loss: 2.4235	LR: 0.050000
Training Epoch: 5 [22400/50000]	Loss: 2.4596	LR: 0.050000
Training Epoch: 5 [22528/50000]	Loss: 2.3696	LR: 0.050000
Training Epoch: 5 [22656/50000]	Loss: 2.4125	LR: 0.050000
Training Epoch: 5 [22784/50000]	Loss: 2.3439	LR: 0.050000
Training Epoch: 5 [22912/50000]	Loss: 2.6022	LR: 0.050000
Training Epoch: 5 [23040/50000]	Loss: 2.5999	LR: 0.050000
Training Epoch: 5 [23168/50000]	Loss: 2.1448	LR: 0.050000
Training Epoch: 5 [23296/50000]	Loss: 2.4093	LR: 0.050000
Training Epoch: 5 [23424/50000]	Loss: 2.3317	LR: 0.050000
Training Epoch: 5 [23552/50000]	Loss: 2.4719	LR: 0.050000
Training Epoch: 5 [23680/50000]	Loss: 2.2188	LR: 0.050000
Training Epoch: 5 [23808/50000]	Loss: 2.4289	LR: 0.050000
Training Epoch: 5 [23936/50000]	Loss: 2.4589	LR: 0.050000
Training Epoch: 5 [24064/50000]	Loss: 2.3678	LR: 0.050000
Training Epoch: 5 [24192/50000]	Loss: 2.2243	LR: 0.050000
Training Epoch: 5 [24320/50000]	Loss: 2.3372	LR: 0.050000
Training Epoch: 5 [24448/50000]	Loss: 2.3877	LR: 0.050000
Training Epoch: 5 [24576/50000]	Loss: 2.4423	LR: 0.050000
Training Epoch: 5 [24704/50000]	Loss: 2.5865	LR: 0.050000
Training Epoch: 5 [24832/50000]	Loss: 2.1025	LR: 0.050000
Training Epoch: 5 [24960/50000]	Loss: 2.4586	LR: 0.050000
Training Epoch: 5 [25088/50000]	Loss: 2.2899	LR: 0.050000
Training Epoch: 5 [25216/50000]	Loss: 2.5174	LR: 0.050000
Training Epoch: 5 [25344/50000]	Loss: 2.4474	LR: 0.050000
Training Epoch: 5 [25472/50000]	Loss: 2.2288	LR: 0.050000
Training Epoch: 5 [25600/50000]	Loss: 2.3707	LR: 0.050000
Training Epoch: 5 [25728/50000]	Loss: 2.1342	LR: 0.050000
Training Epoch: 5 [25856/50000]	Loss: 2.2644	LR: 0.050000
Training Epoch: 5 [25984/50000]	Loss: 2.1885	LR: 0.050000
Training Epoch: 5 [26112/50000]	Loss: 2.0299	LR: 0.050000
Training Epoch: 5 [26240/50000]	Loss: 2.3048	LR: 0.050000
Training Epoch: 5 [26368/50000]	Loss: 2.4036	LR: 0.050000
Training Epoch: 5 [26496/50000]	Loss: 2.5546	LR: 0.050000
Training Epoch: 5 [26624/50000]	Loss: 2.0765	LR: 0.050000
Training Epoch: 5 [26752/50000]	Loss: 2.3676	LR: 0.050000
Training Epoch: 5 [26880/50000]	Loss: 2.4971	LR: 0.050000
Training Epoch: 5 [27008/50000]	Loss: 2.3224	LR: 0.050000
Training Epoch: 5 [27136/50000]	Loss: 2.2807	LR: 0.050000
Training Epoch: 5 [27264/50000]	Loss: 2.1920	LR: 0.050000
Training Epoch: 5 [27392/50000]	Loss: 1.9748	LR: 0.050000
Training Epoch: 5 [27520/50000]	Loss: 2.3977	LR: 0.050000
Training Epoch: 5 [27648/50000]	Loss: 2.1277	LR: 0.050000
Training Epoch: 5 [27776/50000]	Loss: 2.6228	LR: 0.050000
Training Epoch: 5 [27904/50000]	Loss: 2.2593	LR: 0.050000
Training Epoch: 5 [28032/50000]	Loss: 2.1813	LR: 0.050000
Training Epoch: 5 [28160/50000]	Loss: 2.2687	LR: 0.050000
Training Epoch: 5 [28288/50000]	Loss: 2.3484	LR: 0.050000
Training Epoch: 5 [28416/50000]	Loss: 2.5554	LR: 0.050000
Training Epoch: 5 [28544/50000]	Loss: 2.1777	LR: 0.050000
Training Epoch: 5 [28672/50000]	Loss: 2.6348	LR: 0.050000
Training Epoch: 5 [28800/50000]	Loss: 2.2112	LR: 0.050000
Training Epoch: 5 [28928/50000]	Loss: 2.0721	LR: 0.050000
Training Epoch: 5 [29056/50000]	Loss: 2.3389	LR: 0.050000
Training Epoch: 5 [29184/50000]	Loss: 2.2162	LR: 0.050000
Training Epoch: 5 [29312/50000]	Loss: 2.5079	LR: 0.050000
Training Epoch: 5 [29440/50000]	Loss: 2.3219	LR: 0.050000
Training Epoch: 5 [29568/50000]	Loss: 2.4014	LR: 0.050000
Training Epoch: 5 [29696/50000]	Loss: 2.6033	LR: 0.050000
Training Epoch: 5 [29824/50000]	Loss: 2.5487	LR: 0.050000
Training Epoch: 5 [29952/50000]	Loss: 1.9867	LR: 0.050000
Training Epoch: 5 [30080/50000]	Loss: 2.4906	LR: 0.050000
Training Epoch: 5 [30208/50000]	Loss: 2.4088	LR: 0.050000
Training Epoch: 5 [30336/50000]	Loss: 2.4698	LR: 0.050000
Training Epoch: 5 [30464/50000]	Loss: 2.5567	LR: 0.050000
Training Epoch: 5 [30592/50000]	Loss: 2.5270	LR: 0.050000
Training Epoch: 5 [30720/50000]	Loss: 2.1630	LR: 0.050000
Training Epoch: 5 [30848/50000]	Loss: 2.1873	LR: 0.050000
Training Epoch: 5 [30976/50000]	Loss: 2.3316	LR: 0.050000
Training Epoch: 5 [31104/50000]	Loss: 2.4231	LR: 0.050000
Training Epoch: 5 [31232/50000]	Loss: 2.4253	LR: 0.050000
Training Epoch: 5 [31360/50000]	Loss: 2.2556	LR: 0.050000
Training Epoch: 5 [31488/50000]	Loss: 2.2528	LR: 0.050000
Training Epoch: 5 [31616/50000]	Loss: 2.3085	LR: 0.050000
Training Epoch: 5 [31744/50000]	Loss: 2.2397	LR: 0.050000
Training Epoch: 5 [31872/50000]	Loss: 2.2442	LR: 0.050000
Training Epoch: 5 [32000/50000]	Loss: 2.3301	LR: 0.050000
Training Epoch: 5 [32128/50000]	Loss: 2.3435	LR: 0.050000
Training Epoch: 5 [32256/50000]	Loss: 2.1798	LR: 0.050000
Training Epoch: 5 [32384/50000]	Loss: 2.1648	LR: 0.050000
Training Epoch: 5 [32512/50000]	Loss: 2.3449	LR: 0.050000
Training Epoch: 5 [32640/50000]	Loss: 2.3554	LR: 0.050000
Training Epoch: 5 [32768/50000]	Loss: 2.3140	LR: 0.050000
Training Epoch: 5 [32896/50000]	Loss: 2.2247	LR: 0.050000
Training Epoch: 5 [33024/50000]	Loss: 2.4072	LR: 0.050000
Training Epoch: 5 [33152/50000]	Loss: 2.4728	LR: 0.050000
Training Epoch: 5 [33280/50000]	Loss: 2.2846	LR: 0.050000
Training Epoch: 5 [33408/50000]	Loss: 2.2390	LR: 0.050000
Training Epoch: 5 [33536/50000]	Loss: 2.4342	LR: 0.050000
Training Epoch: 5 [33664/50000]	Loss: 2.5534	LR: 0.050000
Training Epoch: 5 [33792/50000]	Loss: 2.1773	LR: 0.050000
Training Epoch: 5 [33920/50000]	Loss: 2.3103	LR: 0.050000
Training Epoch: 5 [34048/50000]	Loss: 2.1824	LR: 0.050000
Training Epoch: 5 [34176/50000]	Loss: 2.3742	LR: 0.050000
Training Epoch: 5 [34304/50000]	Loss: 2.3107	LR: 0.050000
Training Epoch: 5 [34432/50000]	Loss: 2.3281	LR: 0.050000
Training Epoch: 5 [34560/50000]	Loss: 2.2364	LR: 0.050000
Training Epoch: 5 [34688/50000]	Loss: 2.3177	LR: 0.050000
Training Epoch: 5 [34816/50000]	Loss: 2.3667	LR: 0.050000
Training Epoch: 5 [34944/50000]	Loss: 2.1972	LR: 0.050000
Training Epoch: 5 [35072/50000]	Loss: 2.0891	LR: 0.050000
Training Epoch: 5 [35200/50000]	Loss: 2.3216	LR: 0.050000
Training Epoch: 5 [35328/50000]	Loss: 2.2578	LR: 0.050000
Training Epoch: 5 [35456/50000]	Loss: 2.4984	LR: 0.050000
Training Epoch: 5 [35584/50000]	Loss: 2.2490	LR: 0.050000
Training Epoch: 5 [35712/50000]	Loss: 2.3256	LR: 0.050000
Training Epoch: 5 [35840/50000]	Loss: 2.3965	LR: 0.050000
Training Epoch: 5 [35968/50000]	Loss: 2.1982	LR: 0.050000
Training Epoch: 5 [36096/50000]	Loss: 2.0825	LR: 0.050000
Training Epoch: 5 [36224/50000]	Loss: 2.2924	LR: 0.050000
Training Epoch: 5 [36352/50000]	Loss: 2.2732	LR: 0.050000
Training Epoch: 5 [36480/50000]	Loss: 2.5862	LR: 0.050000
Training Epoch: 5 [36608/50000]	Loss: 2.2913	LR: 0.050000
Training Epoch: 5 [36736/50000]	Loss: 2.4005	LR: 0.050000
Training Epoch: 5 [36864/50000]	Loss: 2.2826	LR: 0.050000
Training Epoch: 5 [36992/50000]	Loss: 2.2723	LR: 0.050000
Training Epoch: 5 [37120/50000]	Loss: 2.5478	LR: 0.050000
Training Epoch: 5 [37248/50000]	Loss: 2.2814	LR: 0.050000
Training Epoch: 5 [37376/50000]	Loss: 2.4168	LR: 0.050000
Training Epoch: 5 [37504/50000]	Loss: 2.2849	LR: 0.050000
Training Epoch: 5 [37632/50000]	Loss: 2.3508	LR: 0.050000
Training Epoch: 5 [37760/50000]	Loss: 2.6317	LR: 0.050000
Training Epoch: 5 [37888/50000]	Loss: 2.2553	LR: 0.050000
Training Epoch: 5 [38016/50000]	Loss: 2.3393	LR: 0.050000
Training Epoch: 5 [38144/50000]	Loss: 2.1454	LR: 0.050000
Training Epoch: 5 [38272/50000]	Loss: 2.4205	LR: 0.050000
Training Epoch: 5 [38400/50000]	Loss: 2.3964	LR: 0.050000
Training Epoch: 5 [38528/50000]	Loss: 2.2271	LR: 0.050000
Training Epoch: 5 [38656/50000]	Loss: 2.1117	LR: 0.050000
Training Epoch: 5 [38784/50000]	Loss: 2.2558	LR: 0.050000
Training Epoch: 5 [38912/50000]	Loss: 2.2157	LR: 0.050000
Training Epoch: 5 [39040/50000]	Loss: 2.4191	LR: 0.050000
Training Epoch: 5 [39168/50000]	Loss: 2.1407	LR: 0.050000
Training Epoch: 5 [39296/50000]	Loss: 2.4339	LR: 0.050000
Training Epoch: 5 [39424/50000]	Loss: 2.1661	LR: 0.050000
Training Epoch: 5 [39552/50000]	Loss: 2.2908	LR: 0.050000
Training Epoch: 5 [39680/50000]	Loss: 2.1957	LR: 0.050000
Training Epoch: 5 [39808/50000]	Loss: 2.2789	LR: 0.050000
Training Epoch: 5 [39936/50000]	Loss: 2.2504	LR: 0.050000
Training Epoch: 5 [40064/50000]	Loss: 2.1935	LR: 0.050000
Training Epoch: 5 [40192/50000]	Loss: 2.0809	LR: 0.050000
Training Epoch: 5 [40320/50000]	Loss: 2.2118	LR: 0.050000
Training Epoch: 5 [40448/50000]	Loss: 2.3865	LR: 0.050000
Training Epoch: 5 [40576/50000]	Loss: 2.2572	LR: 0.050000
Training Epoch: 5 [40704/50000]	Loss: 2.3463	LR: 0.050000
Training Epoch: 5 [40832/50000]	Loss: 2.2050	LR: 0.050000
Training Epoch: 5 [40960/50000]	Loss: 2.1112	LR: 0.050000
Training Epoch: 5 [41088/50000]	Loss: 2.4699	LR: 0.050000
Training Epoch: 5 [41216/50000]	Loss: 2.0853	LR: 0.050000
Training Epoch: 5 [41344/50000]	Loss: 2.2360	LR: 0.050000
Training Epoch: 5 [41472/50000]	Loss: 2.3163	LR: 0.050000
Training Epoch: 5 [41600/50000]	Loss: 2.4175	LR: 0.050000
Training Epoch: 5 [41728/50000]	Loss: 2.4060	LR: 0.050000
Training Epoch: 5 [41856/50000]	Loss: 2.2009	LR: 0.050000
Training Epoch: 5 [41984/50000]	Loss: 2.2077	LR: 0.050000
Training Epoch: 5 [42112/50000]	Loss: 2.5793	LR: 0.050000
Training Epoch: 5 [42240/50000]	Loss: 2.5636	LR: 0.050000
Training Epoch: 5 [42368/50000]	Loss: 2.4359	LR: 0.050000
Training Epoch: 5 [42496/50000]	Loss: 2.3468	LR: 0.050000
Training Epoch: 5 [42624/50000]	Loss: 2.5690	LR: 0.050000
Training Epoch: 5 [42752/50000]	Loss: 2.2425	LR: 0.050000
Training Epoch: 5 [42880/50000]	Loss: 2.2531	LR: 0.050000
Training Epoch: 5 [43008/50000]	Loss: 2.3204	LR: 0.050000
Training Epoch: 5 [43136/50000]	Loss: 2.1540	LR: 0.050000
Training Epoch: 5 [43264/50000]	Loss: 2.2395	LR: 0.050000
Training Epoch: 5 [43392/50000]	Loss: 2.0815	LR: 0.050000
Training Epoch: 5 [43520/50000]	Loss: 2.2228	LR: 0.050000
Training Epoch: 5 [43648/50000]	Loss: 2.3779	LR: 0.050000
Training Epoch: 5 [43776/50000]	Loss: 2.2630	LR: 0.050000
Training Epoch: 5 [43904/50000]	Loss: 2.2325	LR: 0.050000
Training Epoch: 5 [44032/50000]	Loss: 2.1509	LR: 0.050000
Training Epoch: 5 [44160/50000]	Loss: 2.1812	LR: 0.050000
Training Epoch: 5 [44288/50000]	Loss: 2.5011	LR: 0.050000
Training Epoch: 5 [44416/50000]	Loss: 2.4167	LR: 0.050000
Training Epoch: 5 [44544/50000]	Loss: 2.3256	LR: 0.050000
Training Epoch: 5 [44672/50000]	Loss: 2.2789	LR: 0.050000
Training Epoch: 5 [44800/50000]	Loss: 2.3619	LR: 0.050000
Training Epoch: 5 [44928/50000]	Loss: 2.4600	LR: 0.050000
Training Epoch: 5 [45056/50000]	Loss: 2.3272	LR: 0.050000
Training Epoch: 5 [45184/50000]	Loss: 2.3425	LR: 0.050000
Training Epoch: 5 [45312/50000]	Loss: 2.3787	LR: 0.050000
Training Epoch: 5 [45440/50000]	Loss: 2.0868	LR: 0.050000
Training Epoch: 5 [45568/50000]	Loss: 2.2794	LR: 0.050000
Training Epoch: 5 [45696/50000]	Loss: 2.3202	LR: 0.050000
Training Epoch: 5 [45824/50000]	Loss: 2.3077	LR: 0.050000
Training Epoch: 5 [45952/50000]	Loss: 2.1163	LR: 0.050000
Training Epoch: 5 [46080/50000]	Loss: 2.2288	LR: 0.050000
Training Epoch: 5 [46208/50000]	Loss: 2.2801	LR: 0.050000
Training Epoch: 5 [46336/50000]	Loss: 2.2271	LR: 0.050000
Training Epoch: 5 [46464/50000]	Loss: 2.2363	LR: 0.050000
Training Epoch: 5 [46592/50000]	Loss: 2.2121	LR: 0.050000
Training Epoch: 5 [46720/50000]	Loss: 2.3633	LR: 0.050000
Training Epoch: 5 [46848/50000]	Loss: 2.2489	LR: 0.050000
Training Epoch: 5 [46976/50000]	Loss: 2.4714	LR: 0.050000
Training Epoch: 5 [47104/50000]	Loss: 2.1078	LR: 0.050000
Training Epoch: 5 [47232/50000]	Loss: 2.2572	LR: 0.050000
Training Epoch: 5 [47360/50000]	Loss: 2.1411	LR: 0.050000
Training Epoch: 5 [47488/50000]	Loss: 2.2248	LR: 0.050000
Training Epoch: 5 [47616/50000]	Loss: 2.2722	LR: 0.050000
Training Epoch: 5 [47744/50000]	Loss: 2.2007	LR: 0.050000
Training Epoch: 5 [47872/50000]	Loss: 2.3078	LR: 0.050000
Training Epoch: 5 [48000/50000]	Loss: 2.4969	LR: 0.050000
Training Epoch: 5 [48128/50000]	Loss: 2.3293	LR: 0.050000
Training Epoch: 5 [48256/50000]	Loss: 2.4492	LR: 0.050000
Training Epoch: 5 [48384/50000]	Loss: 2.4260	LR: 0.050000
Training Epoch: 5 [48512/50000]	Loss: 2.4787	LR: 0.050000
Training Epoch: 5 [48640/50000]	Loss: 2.3908	LR: 0.050000
Training Epoch: 5 [48768/50000]	Loss: 2.0454	LR: 0.050000
Training Epoch: 5 [48896/50000]	Loss: 2.2481	LR: 0.050000
Training Epoch: 5 [49024/50000]	Loss: 2.3392	LR: 0.050000
Training Epoch: 5 [49152/50000]	Loss: 2.2528	LR: 0.050000
Training Epoch: 5 [49280/50000]	Loss: 2.1861	LR: 0.050000
Training Epoch: 5 [49408/50000]	Loss: 2.2577	LR: 0.050000
Training Epoch: 5 [49536/50000]	Loss: 2.1952	LR: 0.050000
Training Epoch: 5 [49664/50000]	Loss: 2.2685	LR: 0.050000
Training Epoch: 5 [49792/50000]	Loss: 2.2047	LR: 0.050000
Training Epoch: 5 [49920/50000]	Loss: 2.3867	LR: 0.050000
Training Epoch: 5 [50000/50000]	Loss: 1.9734	LR: 0.050000
epoch 5 training time consumed: 53.90s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   18350 GB |   18350 GB |
|       from large pool |  123392 KB |    1034 MB |   18332 GB |   18332 GB |
|       from small pool |   10798 KB |      13 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   18350 GB |   18350 GB |
|       from large pool |  123392 KB |    1034 MB |   18332 GB |   18332 GB |
|       from small pool |   10798 KB |      13 MB |      18 GB |      18 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |    8078 GB |    8078 GB |
|       from large pool |  155136 KB |  433088 KB |    8058 GB |    8058 GB |
|       from small pool |    1490 KB |    3494 KB |      20 GB |      20 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |  708613    |  708358    |
|       from large pool |      24    |      65    |  369671    |  369647    |
|       from small pool |     231    |     274    |  338942    |  338711    |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |  708613    |  708358    |
|       from large pool |      24    |      65    |  369671    |  369647    |
|       from small pool |     231    |     274    |  338942    |  338711    |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |  353823    |  353802    |
|       from large pool |       9    |      14    |  178925    |  178916    |
|       from small pool |      12    |      16    |  174898    |  174886    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 5, Average loss: 0.0195, Accuracy: 0.3720, Time consumed:3.45s

Training Epoch: 6 [128/50000]	Loss: 2.1436	LR: 0.050000
Training Epoch: 6 [256/50000]	Loss: 2.4875	LR: 0.050000
Training Epoch: 6 [384/50000]	Loss: 2.3410	LR: 0.050000
Training Epoch: 6 [512/50000]	Loss: 2.1907	LR: 0.050000
Training Epoch: 6 [640/50000]	Loss: 2.1017	LR: 0.050000
Training Epoch: 6 [768/50000]	Loss: 1.9872	LR: 0.050000
Training Epoch: 6 [896/50000]	Loss: 2.0326	LR: 0.050000
Training Epoch: 6 [1024/50000]	Loss: 1.9068	LR: 0.050000
Training Epoch: 6 [1152/50000]	Loss: 2.1340	LR: 0.050000
Training Epoch: 6 [1280/50000]	Loss: 2.1293	LR: 0.050000
Training Epoch: 6 [1408/50000]	Loss: 2.0413	LR: 0.050000
Training Epoch: 6 [1536/50000]	Loss: 2.1268	LR: 0.050000
Training Epoch: 6 [1664/50000]	Loss: 1.9742	LR: 0.050000
Training Epoch: 6 [1792/50000]	Loss: 2.0115	LR: 0.050000
Training Epoch: 6 [1920/50000]	Loss: 2.4514	LR: 0.050000
Training Epoch: 6 [2048/50000]	Loss: 1.9679	LR: 0.050000
Training Epoch: 6 [2176/50000]	Loss: 2.0369	LR: 0.050000
Training Epoch: 6 [2304/50000]	Loss: 2.1897	LR: 0.050000
Training Epoch: 6 [2432/50000]	Loss: 2.1636	LR: 0.050000
Training Epoch: 6 [2560/50000]	Loss: 2.3438	LR: 0.050000
Training Epoch: 6 [2688/50000]	Loss: 2.0885	LR: 0.050000
Training Epoch: 6 [2816/50000]	Loss: 2.0380	LR: 0.050000
Training Epoch: 6 [2944/50000]	Loss: 2.2403	LR: 0.050000
Training Epoch: 6 [3072/50000]	Loss: 2.2360	LR: 0.050000
Training Epoch: 6 [3200/50000]	Loss: 2.0778	LR: 0.050000
Training Epoch: 6 [3328/50000]	Loss: 2.2611	LR: 0.050000
Training Epoch: 6 [3456/50000]	Loss: 2.1543	LR: 0.050000
Training Epoch: 6 [3584/50000]	Loss: 2.3419	LR: 0.050000
Training Epoch: 6 [3712/50000]	Loss: 2.0184	LR: 0.050000
Training Epoch: 6 [3840/50000]	Loss: 1.9699	LR: 0.050000
Training Epoch: 6 [3968/50000]	Loss: 2.2529	LR: 0.050000
Training Epoch: 6 [4096/50000]	Loss: 2.3256	LR: 0.050000
Training Epoch: 6 [4224/50000]	Loss: 2.4788	LR: 0.050000
Training Epoch: 6 [4352/50000]	Loss: 2.0318	LR: 0.050000
Training Epoch: 6 [4480/50000]	Loss: 2.4267	LR: 0.050000
Training Epoch: 6 [4608/50000]	Loss: 2.1599	LR: 0.050000
Training Epoch: 6 [4736/50000]	Loss: 2.2986	LR: 0.050000
Training Epoch: 6 [4864/50000]	Loss: 2.1510	LR: 0.050000
Training Epoch: 6 [4992/50000]	Loss: 2.4160	LR: 0.050000
Training Epoch: 6 [5120/50000]	Loss: 2.4501	LR: 0.050000
Training Epoch: 6 [5248/50000]	Loss: 2.0303	LR: 0.050000
Training Epoch: 6 [5376/50000]	Loss: 2.2335	LR: 0.050000
Training Epoch: 6 [5504/50000]	Loss: 2.2186	LR: 0.050000
Training Epoch: 6 [5632/50000]	Loss: 1.8822	LR: 0.050000
Training Epoch: 6 [5760/50000]	Loss: 2.3044	LR: 0.050000
Training Epoch: 6 [5888/50000]	Loss: 2.2591	LR: 0.050000
Training Epoch: 6 [6016/50000]	Loss: 2.2184	LR: 0.050000
Training Epoch: 6 [6144/50000]	Loss: 2.1970	LR: 0.050000
Training Epoch: 6 [6272/50000]	Loss: 2.0419	LR: 0.050000
Training Epoch: 6 [6400/50000]	Loss: 2.2164	LR: 0.050000
Training Epoch: 6 [6528/50000]	Loss: 2.3479	LR: 0.050000
Training Epoch: 6 [6656/50000]	Loss: 1.9886	LR: 0.050000
Training Epoch: 6 [6784/50000]	Loss: 2.2854	LR: 0.050000
Training Epoch: 6 [6912/50000]	Loss: 2.2082	LR: 0.050000
Training Epoch: 6 [7040/50000]	Loss: 2.2934	LR: 0.050000
Training Epoch: 6 [7168/50000]	Loss: 2.1356	LR: 0.050000
Training Epoch: 6 [7296/50000]	Loss: 2.3988	LR: 0.050000
Training Epoch: 6 [7424/50000]	Loss: 2.1451	LR: 0.050000
Training Epoch: 6 [7552/50000]	Loss: 2.1659	LR: 0.050000
Training Epoch: 6 [7680/50000]	Loss: 2.0842	LR: 0.050000
Training Epoch: 6 [7808/50000]	Loss: 2.1159	LR: 0.050000
Training Epoch: 6 [7936/50000]	Loss: 2.3256	LR: 0.050000
Training Epoch: 6 [8064/50000]	Loss: 2.3108	LR: 0.050000
Training Epoch: 6 [8192/50000]	Loss: 2.2287	LR: 0.050000
Training Epoch: 6 [8320/50000]	Loss: 2.3318	LR: 0.050000
Training Epoch: 6 [8448/50000]	Loss: 1.9649	LR: 0.050000
Training Epoch: 6 [8576/50000]	Loss: 2.1486	LR: 0.050000
Training Epoch: 6 [8704/50000]	Loss: 2.1024	LR: 0.050000
Training Epoch: 6 [8832/50000]	Loss: 2.1160	LR: 0.050000
Training Epoch: 6 [8960/50000]	Loss: 1.9862	LR: 0.050000
Training Epoch: 6 [9088/50000]	Loss: 2.3672	LR: 0.050000
Training Epoch: 6 [9216/50000]	Loss: 2.1001	LR: 0.050000
Training Epoch: 6 [9344/50000]	Loss: 2.0809	LR: 0.050000
Training Epoch: 6 [9472/50000]	Loss: 2.5763	LR: 0.050000
Training Epoch: 6 [9600/50000]	Loss: 2.1771	LR: 0.050000
Training Epoch: 6 [9728/50000]	Loss: 2.4661	LR: 0.050000
Training Epoch: 6 [9856/50000]	Loss: 1.9045	LR: 0.050000
Training Epoch: 6 [9984/50000]	Loss: 2.2730	LR: 0.050000
Training Epoch: 6 [10112/50000]	Loss: 2.0815	LR: 0.050000
Training Epoch: 6 [10240/50000]	Loss: 2.1763	LR: 0.050000
Training Epoch: 6 [10368/50000]	Loss: 2.3195	LR: 0.050000
Training Epoch: 6 [10496/50000]	Loss: 2.0361	LR: 0.050000
Training Epoch: 6 [10624/50000]	Loss: 2.0803	LR: 0.050000
Training Epoch: 6 [10752/50000]	Loss: 2.5479	LR: 0.050000
Training Epoch: 6 [10880/50000]	Loss: 2.2330	LR: 0.050000
Training Epoch: 6 [11008/50000]	Loss: 2.3705	LR: 0.050000
Training Epoch: 6 [11136/50000]	Loss: 2.1085	LR: 0.050000
Training Epoch: 6 [11264/50000]	Loss: 1.9882	LR: 0.050000
Training Epoch: 6 [11392/50000]	Loss: 2.0922	LR: 0.050000
Training Epoch: 6 [11520/50000]	Loss: 2.2098	LR: 0.050000
Training Epoch: 6 [11648/50000]	Loss: 2.0345	LR: 0.050000
Training Epoch: 6 [11776/50000]	Loss: 2.2041	LR: 0.050000
Training Epoch: 6 [11904/50000]	Loss: 2.3517	LR: 0.050000
Training Epoch: 6 [12032/50000]	Loss: 1.9443	LR: 0.050000
Training Epoch: 6 [12160/50000]	Loss: 2.1312	LR: 0.050000
Training Epoch: 6 [12288/50000]	Loss: 2.3355	LR: 0.050000
Training Epoch: 6 [12416/50000]	Loss: 2.4838	LR: 0.050000
Training Epoch: 6 [12544/50000]	Loss: 2.2588	LR: 0.050000
Training Epoch: 6 [12672/50000]	Loss: 2.2547	LR: 0.050000
Training Epoch: 6 [12800/50000]	Loss: 2.2220	LR: 0.050000
Training Epoch: 6 [12928/50000]	Loss: 2.0258	LR: 0.050000
Training Epoch: 6 [13056/50000]	Loss: 2.1864	LR: 0.050000
Training Epoch: 6 [13184/50000]	Loss: 2.3491	LR: 0.050000
Training Epoch: 6 [13312/50000]	Loss: 2.2826	LR: 0.050000
Training Epoch: 6 [13440/50000]	Loss: 2.3225	LR: 0.050000
Training Epoch: 6 [13568/50000]	Loss: 2.4009	LR: 0.050000
Training Epoch: 6 [13696/50000]	Loss: 2.1607	LR: 0.050000
Training Epoch: 6 [13824/50000]	Loss: 2.1530	LR: 0.050000
Training Epoch: 6 [13952/50000]	Loss: 1.9046	LR: 0.050000
Training Epoch: 6 [14080/50000]	Loss: 2.2583	LR: 0.050000
Training Epoch: 6 [14208/50000]	Loss: 2.2015	LR: 0.050000
Training Epoch: 6 [14336/50000]	Loss: 2.3288	LR: 0.050000
Training Epoch: 6 [14464/50000]	Loss: 2.2330	LR: 0.050000
Training Epoch: 6 [14592/50000]	Loss: 2.4974	LR: 0.050000
Training Epoch: 6 [14720/50000]	Loss: 2.2275	LR: 0.050000
Training Epoch: 6 [14848/50000]	Loss: 2.0238	LR: 0.050000
Training Epoch: 6 [14976/50000]	Loss: 2.0015	LR: 0.050000
Training Epoch: 6 [15104/50000]	Loss: 2.4304	LR: 0.050000
Training Epoch: 6 [15232/50000]	Loss: 2.1793	LR: 0.050000
Training Epoch: 6 [15360/50000]	Loss: 1.9824	LR: 0.050000
Training Epoch: 6 [15488/50000]	Loss: 2.0715	LR: 0.050000
Training Epoch: 6 [15616/50000]	Loss: 2.0426	LR: 0.050000
Training Epoch: 6 [15744/50000]	Loss: 1.8849	LR: 0.050000
Training Epoch: 6 [15872/50000]	Loss: 2.5280	LR: 0.050000
Training Epoch: 6 [16000/50000]	Loss: 2.0948	LR: 0.050000
Training Epoch: 6 [16128/50000]	Loss: 2.1027	LR: 0.050000
Training Epoch: 6 [16256/50000]	Loss: 2.1046	LR: 0.050000
Training Epoch: 6 [16384/50000]	Loss: 2.2165	LR: 0.050000
Training Epoch: 6 [16512/50000]	Loss: 2.0888	LR: 0.050000
Training Epoch: 6 [16640/50000]	Loss: 2.2757	LR: 0.050000
Training Epoch: 6 [16768/50000]	Loss: 2.0682	LR: 0.050000
Training Epoch: 6 [16896/50000]	Loss: 2.3757	LR: 0.050000
Training Epoch: 6 [17024/50000]	Loss: 2.0661	LR: 0.050000
Training Epoch: 6 [17152/50000]	Loss: 2.1529	LR: 0.050000
Training Epoch: 6 [17280/50000]	Loss: 2.2104	LR: 0.050000
Training Epoch: 6 [17408/50000]	Loss: 2.0567	LR: 0.050000
Training Epoch: 6 [17536/50000]	Loss: 2.1897	LR: 0.050000
Training Epoch: 6 [17664/50000]	Loss: 1.9458	LR: 0.050000
Training Epoch: 6 [17792/50000]	Loss: 2.0457	LR: 0.050000
Training Epoch: 6 [17920/50000]	Loss: 2.3315	LR: 0.050000
Training Epoch: 6 [18048/50000]	Loss: 2.0240	LR: 0.050000
Training Epoch: 6 [18176/50000]	Loss: 1.9296	LR: 0.050000
Training Epoch: 6 [18304/50000]	Loss: 2.2751	LR: 0.050000
Training Epoch: 6 [18432/50000]	Loss: 2.1024	LR: 0.050000
Training Epoch: 6 [18560/50000]	Loss: 2.2661	LR: 0.050000
Training Epoch: 6 [18688/50000]	Loss: 2.0137	LR: 0.050000
Training Epoch: 6 [18816/50000]	Loss: 2.0814	LR: 0.050000
Training Epoch: 6 [18944/50000]	Loss: 2.2718	LR: 0.050000
Training Epoch: 6 [19072/50000]	Loss: 2.1160	LR: 0.050000
Training Epoch: 6 [19200/50000]	Loss: 2.0866	LR: 0.050000
Training Epoch: 6 [19328/50000]	Loss: 2.3103	LR: 0.050000
Training Epoch: 6 [19456/50000]	Loss: 2.0352	LR: 0.050000
Training Epoch: 6 [19584/50000]	Loss: 2.2482	LR: 0.050000
Training Epoch: 6 [19712/50000]	Loss: 2.2216	LR: 0.050000
Training Epoch: 6 [19840/50000]	Loss: 2.3206	LR: 0.050000
Training Epoch: 6 [19968/50000]	Loss: 1.8862	LR: 0.050000
Training Epoch: 6 [20096/50000]	Loss: 2.1496	LR: 0.050000
Training Epoch: 6 [20224/50000]	Loss: 2.0303	LR: 0.050000
Training Epoch: 6 [20352/50000]	Loss: 2.0429	LR: 0.050000
Training Epoch: 6 [20480/50000]	Loss: 2.3269	LR: 0.050000
Training Epoch: 6 [20608/50000]	Loss: 2.0369	LR: 0.050000
Training Epoch: 6 [20736/50000]	Loss: 2.1846	LR: 0.050000
Training Epoch: 6 [20864/50000]	Loss: 2.2000	LR: 0.050000
Training Epoch: 6 [20992/50000]	Loss: 1.9339	LR: 0.050000
Training Epoch: 6 [21120/50000]	Loss: 2.1800	LR: 0.050000
Training Epoch: 6 [21248/50000]	Loss: 2.0805	LR: 0.050000
Training Epoch: 6 [21376/50000]	Loss: 1.9026	LR: 0.050000
Training Epoch: 6 [21504/50000]	Loss: 2.1510	LR: 0.050000
Training Epoch: 6 [21632/50000]	Loss: 2.1648	LR: 0.050000
Training Epoch: 6 [21760/50000]	Loss: 1.8431	LR: 0.050000
Training Epoch: 6 [21888/50000]	Loss: 2.1328	LR: 0.050000
Training Epoch: 6 [22016/50000]	Loss: 2.2707	LR: 0.050000
Training Epoch: 6 [22144/50000]	Loss: 2.0106	LR: 0.050000
Training Epoch: 6 [22272/50000]	Loss: 2.0903	LR: 0.050000
Training Epoch: 6 [22400/50000]	Loss: 1.9419	LR: 0.050000
Training Epoch: 6 [22528/50000]	Loss: 1.9464	LR: 0.050000
Training Epoch: 6 [22656/50000]	Loss: 2.1231	LR: 0.050000
Training Epoch: 6 [22784/50000]	Loss: 1.8233	LR: 0.050000
Training Epoch: 6 [22912/50000]	Loss: 2.2421	LR: 0.050000
Training Epoch: 6 [23040/50000]	Loss: 2.1104	LR: 0.050000
Training Epoch: 6 [23168/50000]	Loss: 2.2882	LR: 0.050000
Training Epoch: 6 [23296/50000]	Loss: 2.4045	LR: 0.050000
Training Epoch: 6 [23424/50000]	Loss: 2.0394	LR: 0.050000
Training Epoch: 6 [23552/50000]	Loss: 2.1545	LR: 0.050000
Training Epoch: 6 [23680/50000]	Loss: 2.2290	LR: 0.050000
Training Epoch: 6 [23808/50000]	Loss: 2.1926	LR: 0.050000
Training Epoch: 6 [23936/50000]	Loss: 2.0824	LR: 0.050000
Training Epoch: 6 [24064/50000]	Loss: 2.1107	LR: 0.050000
Training Epoch: 6 [24192/50000]	Loss: 2.3155	LR: 0.050000
Training Epoch: 6 [24320/50000]	Loss: 2.0455	LR: 0.050000
Training Epoch: 6 [24448/50000]	Loss: 2.2352	LR: 0.050000
Training Epoch: 6 [24576/50000]	Loss: 1.9291	LR: 0.050000
Training Epoch: 6 [24704/50000]	Loss: 2.0642	LR: 0.050000
Training Epoch: 6 [24832/50000]	Loss: 2.1900	LR: 0.050000
Training Epoch: 6 [24960/50000]	Loss: 2.3053	LR: 0.050000
Training Epoch: 6 [25088/50000]	Loss: 2.3876	LR: 0.050000
Training Epoch: 6 [25216/50000]	Loss: 2.2286	LR: 0.050000
Training Epoch: 6 [25344/50000]	Loss: 2.1540	LR: 0.050000
Training Epoch: 6 [25472/50000]	Loss: 1.8105	LR: 0.050000
Training Epoch: 6 [25600/50000]	Loss: 1.9894	LR: 0.050000
Training Epoch: 6 [25728/50000]	Loss: 2.0842	LR: 0.050000
Training Epoch: 6 [25856/50000]	Loss: 2.0065	LR: 0.050000
Training Epoch: 6 [25984/50000]	Loss: 2.3435	LR: 0.050000
Training Epoch: 6 [26112/50000]	Loss: 2.5087	LR: 0.050000
Training Epoch: 6 [26240/50000]	Loss: 1.9597	LR: 0.050000
Training Epoch: 6 [26368/50000]	Loss: 2.2426	LR: 0.050000
Training Epoch: 6 [26496/50000]	Loss: 2.0309	LR: 0.050000
Training Epoch: 6 [26624/50000]	Loss: 2.2583	LR: 0.050000
Training Epoch: 6 [26752/50000]	Loss: 1.7930	LR: 0.050000
Training Epoch: 6 [26880/50000]	Loss: 2.0573	LR: 0.050000
Training Epoch: 6 [27008/50000]	Loss: 1.9164	LR: 0.050000
Training Epoch: 6 [27136/50000]	Loss: 2.3411	LR: 0.050000
Training Epoch: 6 [27264/50000]	Loss: 2.1778	LR: 0.050000
Training Epoch: 6 [27392/50000]	Loss: 2.1651	LR: 0.050000
Training Epoch: 6 [27520/50000]	Loss: 2.2357	LR: 0.050000
Training Epoch: 6 [27648/50000]	Loss: 2.1858	LR: 0.050000
Training Epoch: 6 [27776/50000]	Loss: 2.0303	LR: 0.050000
Training Epoch: 6 [27904/50000]	Loss: 2.1138	LR: 0.050000
Training Epoch: 6 [28032/50000]	Loss: 2.2986	LR: 0.050000
Training Epoch: 6 [28160/50000]	Loss: 2.2291	LR: 0.050000
Training Epoch: 6 [28288/50000]	Loss: 2.1434	LR: 0.050000
Training Epoch: 6 [28416/50000]	Loss: 2.1198	LR: 0.050000
Training Epoch: 6 [28544/50000]	Loss: 2.2809	LR: 0.050000
Training Epoch: 6 [28672/50000]	Loss: 1.9325	LR: 0.050000
Training Epoch: 6 [28800/50000]	Loss: 1.9686	LR: 0.050000
Training Epoch: 6 [28928/50000]	Loss: 2.1090	LR: 0.050000
Training Epoch: 6 [29056/50000]	Loss: 2.3390	LR: 0.050000
Training Epoch: 6 [29184/50000]	Loss: 2.1931	LR: 0.050000
Training Epoch: 6 [29312/50000]	Loss: 2.1934	LR: 0.050000
Training Epoch: 6 [29440/50000]	Loss: 2.2104	LR: 0.050000
Training Epoch: 6 [29568/50000]	Loss: 2.2162	LR: 0.050000
Training Epoch: 6 [29696/50000]	Loss: 2.1427	LR: 0.050000
Training Epoch: 6 [29824/50000]	Loss: 2.1478	LR: 0.050000
Training Epoch: 6 [29952/50000]	Loss: 2.2828	LR: 0.050000
Training Epoch: 6 [30080/50000]	Loss: 2.0395	LR: 0.050000
Training Epoch: 6 [30208/50000]	Loss: 2.0385	LR: 0.050000
Training Epoch: 6 [30336/50000]	Loss: 1.9444	LR: 0.050000
Training Epoch: 6 [30464/50000]	Loss: 2.0783	LR: 0.050000
Training Epoch: 6 [30592/50000]	Loss: 2.1882	LR: 0.050000
Training Epoch: 6 [30720/50000]	Loss: 1.8312	LR: 0.050000
Training Epoch: 6 [30848/50000]	Loss: 2.2670	LR: 0.050000
Training Epoch: 6 [30976/50000]	Loss: 2.1826	LR: 0.050000
Training Epoch: 6 [31104/50000]	Loss: 1.9617	LR: 0.050000
Training Epoch: 6 [31232/50000]	Loss: 1.9328	LR: 0.050000
Training Epoch: 6 [31360/50000]	Loss: 2.1428	LR: 0.050000
Training Epoch: 6 [31488/50000]	Loss: 2.0748	LR: 0.050000
Training Epoch: 6 [31616/50000]	Loss: 2.0485	LR: 0.050000
Training Epoch: 6 [31744/50000]	Loss: 2.2714	LR: 0.050000
Training Epoch: 6 [31872/50000]	Loss: 2.0670	LR: 0.050000
Training Epoch: 6 [32000/50000]	Loss: 2.4103	LR: 0.050000
Training Epoch: 6 [32128/50000]	Loss: 2.4060	LR: 0.050000
Training Epoch: 6 [32256/50000]	Loss: 2.2745	LR: 0.050000
Training Epoch: 6 [32384/50000]	Loss: 2.2836	LR: 0.050000
Training Epoch: 6 [32512/50000]	Loss: 2.2311	LR: 0.050000
Training Epoch: 6 [32640/50000]	Loss: 2.0707	LR: 0.050000
Training Epoch: 6 [32768/50000]	Loss: 2.0824	LR: 0.050000
Training Epoch: 6 [32896/50000]	Loss: 2.1406	LR: 0.050000
Training Epoch: 6 [33024/50000]	Loss: 2.1790	LR: 0.050000
Training Epoch: 6 [33152/50000]	Loss: 2.1665	LR: 0.050000
Training Epoch: 6 [33280/50000]	Loss: 2.0063	LR: 0.050000
Training Epoch: 6 [33408/50000]	Loss: 2.6023	LR: 0.050000
Training Epoch: 6 [33536/50000]	Loss: 1.8394	LR: 0.050000
Training Epoch: 6 [33664/50000]	Loss: 2.0382	LR: 0.050000
Training Epoch: 6 [33792/50000]	Loss: 2.0193	LR: 0.050000
Training Epoch: 6 [33920/50000]	Loss: 2.2277	LR: 0.050000
Training Epoch: 6 [34048/50000]	Loss: 2.3571	LR: 0.050000
Training Epoch: 6 [34176/50000]	Loss: 2.0946	LR: 0.050000
Training Epoch: 6 [34304/50000]	Loss: 2.0848	LR: 0.050000
Training Epoch: 6 [34432/50000]	Loss: 1.9548	LR: 0.050000
Training Epoch: 6 [34560/50000]	Loss: 1.9134	LR: 0.050000
Training Epoch: 6 [34688/50000]	Loss: 2.1511	LR: 0.050000
Training Epoch: 6 [34816/50000]	Loss: 1.9935	LR: 0.050000
Training Epoch: 6 [34944/50000]	Loss: 1.7527	LR: 0.050000
Training Epoch: 6 [35072/50000]	Loss: 2.2705	LR: 0.050000
Training Epoch: 6 [35200/50000]	Loss: 2.2693	LR: 0.050000
Training Epoch: 6 [35328/50000]	Loss: 2.0460	LR: 0.050000
Training Epoch: 6 [35456/50000]	Loss: 2.1576	LR: 0.050000
Training Epoch: 6 [35584/50000]	Loss: 2.1350	LR: 0.050000
Training Epoch: 6 [35712/50000]	Loss: 2.1771	LR: 0.050000
Training Epoch: 6 [35840/50000]	Loss: 2.2436	LR: 0.050000
Training Epoch: 6 [35968/50000]	Loss: 2.2095	LR: 0.050000
Training Epoch: 6 [36096/50000]	Loss: 2.1633	LR: 0.050000
Training Epoch: 6 [36224/50000]	Loss: 2.1368	LR: 0.050000
Training Epoch: 6 [36352/50000]	Loss: 2.2729	LR: 0.050000
Training Epoch: 6 [36480/50000]	Loss: 2.2473	LR: 0.050000
Training Epoch: 6 [36608/50000]	Loss: 1.8584	LR: 0.050000
Training Epoch: 6 [36736/50000]	Loss: 2.0782	LR: 0.050000
Training Epoch: 6 [36864/50000]	Loss: 1.9378	LR: 0.050000
Training Epoch: 6 [36992/50000]	Loss: 1.9504	LR: 0.050000
Training Epoch: 6 [37120/50000]	Loss: 2.0906	LR: 0.050000
Training Epoch: 6 [37248/50000]	Loss: 2.1630	LR: 0.050000
Training Epoch: 6 [37376/50000]	Loss: 2.2200	LR: 0.050000
Training Epoch: 6 [37504/50000]	Loss: 1.8924	LR: 0.050000
Training Epoch: 6 [37632/50000]	Loss: 2.0068	LR: 0.050000
Training Epoch: 6 [37760/50000]	Loss: 2.3592	LR: 0.050000
Training Epoch: 6 [37888/50000]	Loss: 2.0230	LR: 0.050000
Training Epoch: 6 [38016/50000]	Loss: 2.1609	LR: 0.050000
Training Epoch: 6 [38144/50000]	Loss: 2.1904	LR: 0.050000
Training Epoch: 6 [38272/50000]	Loss: 1.9801	LR: 0.050000
Training Epoch: 6 [38400/50000]	Loss: 2.1242	LR: 0.050000
Training Epoch: 6 [38528/50000]	Loss: 1.9437	LR: 0.050000
Training Epoch: 6 [38656/50000]	Loss: 2.0889	LR: 0.050000
Training Epoch: 6 [38784/50000]	Loss: 2.1652	LR: 0.050000
Training Epoch: 6 [38912/50000]	Loss: 2.3334	LR: 0.050000
Training Epoch: 6 [39040/50000]	Loss: 2.2609	LR: 0.050000
Training Epoch: 6 [39168/50000]	Loss: 2.4474	LR: 0.050000
Training Epoch: 6 [39296/50000]	Loss: 2.2449	LR: 0.050000
Training Epoch: 6 [39424/50000]	Loss: 1.9075	LR: 0.050000
Training Epoch: 6 [39552/50000]	Loss: 2.2881	LR: 0.050000
Training Epoch: 6 [39680/50000]	Loss: 2.2916	LR: 0.050000
Training Epoch: 6 [39808/50000]	Loss: 2.1355	LR: 0.050000
Training Epoch: 6 [39936/50000]	Loss: 2.3123	LR: 0.050000
Training Epoch: 6 [40064/50000]	Loss: 2.0142	LR: 0.050000
Training Epoch: 6 [40192/50000]	Loss: 1.8367	LR: 0.050000
Training Epoch: 6 [40320/50000]	Loss: 2.0322	LR: 0.050000
Training Epoch: 6 [40448/50000]	Loss: 2.0335	LR: 0.050000
Training Epoch: 6 [40576/50000]	Loss: 1.8729	LR: 0.050000
Training Epoch: 6 [40704/50000]	Loss: 2.0424	LR: 0.050000
Training Epoch: 6 [40832/50000]	Loss: 1.9189	LR: 0.050000
Training Epoch: 6 [40960/50000]	Loss: 2.1857	LR: 0.050000
Training Epoch: 6 [41088/50000]	Loss: 2.0222	LR: 0.050000
Training Epoch: 6 [41216/50000]	Loss: 2.1549	LR: 0.050000
Training Epoch: 6 [41344/50000]	Loss: 1.8689	LR: 0.050000
Training Epoch: 6 [41472/50000]	Loss: 1.9878	LR: 0.050000
Training Epoch: 6 [41600/50000]	Loss: 2.2592	LR: 0.050000
Training Epoch: 6 [41728/50000]	Loss: 1.9623	LR: 0.050000
Training Epoch: 6 [41856/50000]	Loss: 1.8359	LR: 0.050000
Training Epoch: 6 [41984/50000]	Loss: 2.1989	LR: 0.050000
Training Epoch: 6 [42112/50000]	Loss: 2.4266	LR: 0.050000
Training Epoch: 6 [42240/50000]	Loss: 2.0692	LR: 0.050000
Training Epoch: 6 [42368/50000]	Loss: 2.1549	LR: 0.050000
Training Epoch: 6 [42496/50000]	Loss: 1.9974	LR: 0.050000
Training Epoch: 6 [42624/50000]	Loss: 2.1796	LR: 0.050000
Training Epoch: 6 [42752/50000]	Loss: 1.8891	LR: 0.050000
Training Epoch: 6 [42880/50000]	Loss: 2.2461	LR: 0.050000
Training Epoch: 6 [43008/50000]	Loss: 2.0271	LR: 0.050000
Training Epoch: 6 [43136/50000]	Loss: 2.1320	LR: 0.050000
Training Epoch: 6 [43264/50000]	Loss: 2.0991	LR: 0.050000
Training Epoch: 6 [43392/50000]	Loss: 2.0749	LR: 0.050000
Training Epoch: 6 [43520/50000]	Loss: 2.2057	LR: 0.050000
Training Epoch: 6 [43648/50000]	Loss: 2.0387	LR: 0.050000
Training Epoch: 6 [43776/50000]	Loss: 1.9193	LR: 0.050000
Training Epoch: 6 [43904/50000]	Loss: 2.0252	LR: 0.050000
Training Epoch: 6 [44032/50000]	Loss: 2.1771	LR: 0.050000
Training Epoch: 6 [44160/50000]	Loss: 2.0882	LR: 0.050000
Training Epoch: 6 [44288/50000]	Loss: 2.3006	LR: 0.050000
Training Epoch: 6 [44416/50000]	Loss: 1.9827	LR: 0.050000
Training Epoch: 6 [44544/50000]	Loss: 1.9186	LR: 0.050000
Training Epoch: 6 [44672/50000]	Loss: 1.8961	LR: 0.050000
Training Epoch: 6 [44800/50000]	Loss: 1.8170	LR: 0.050000
Training Epoch: 6 [44928/50000]	Loss: 2.0931	LR: 0.050000
Training Epoch: 6 [45056/50000]	Loss: 2.3057	LR: 0.050000
Training Epoch: 6 [45184/50000]	Loss: 1.9196	LR: 0.050000
Training Epoch: 6 [45312/50000]	Loss: 2.0840	LR: 0.050000
Training Epoch: 6 [45440/50000]	Loss: 1.9319	LR: 0.050000
Training Epoch: 6 [45568/50000]	Loss: 2.0737	LR: 0.050000
Training Epoch: 6 [45696/50000]	Loss: 1.8708	LR: 0.050000
Training Epoch: 6 [45824/50000]	Loss: 1.9642	LR: 0.050000
Training Epoch: 6 [45952/50000]	Loss: 2.3332	LR: 0.050000
Training Epoch: 6 [46080/50000]	Loss: 2.1061	LR: 0.050000
Training Epoch: 6 [46208/50000]	Loss: 2.0081	LR: 0.050000
Training Epoch: 6 [46336/50000]	Loss: 1.8588	LR: 0.050000
Training Epoch: 6 [46464/50000]	Loss: 2.0604	LR: 0.050000
Training Epoch: 6 [46592/50000]	Loss: 2.1130	LR: 0.050000
Training Epoch: 6 [46720/50000]	Loss: 2.2116	LR: 0.050000
Training Epoch: 6 [46848/50000]	Loss: 2.0214	LR: 0.050000
Training Epoch: 6 [46976/50000]	Loss: 2.0659	LR: 0.050000
Training Epoch: 6 [47104/50000]	Loss: 2.1374	LR: 0.050000
Training Epoch: 6 [47232/50000]	Loss: 2.1231	LR: 0.050000
Training Epoch: 6 [47360/50000]	Loss: 2.2805	LR: 0.050000
Training Epoch: 6 [47488/50000]	Loss: 2.0582	LR: 0.050000
Training Epoch: 6 [47616/50000]	Loss: 2.1762	LR: 0.050000
Training Epoch: 6 [47744/50000]	Loss: 2.2646	LR: 0.050000
Training Epoch: 6 [47872/50000]	Loss: 2.2164	LR: 0.050000
Training Epoch: 6 [48000/50000]	Loss: 2.0145	LR: 0.050000
Training Epoch: 6 [48128/50000]	Loss: 1.9415	LR: 0.050000
Training Epoch: 6 [48256/50000]	Loss: 1.8608	LR: 0.050000
Training Epoch: 6 [48384/50000]	Loss: 2.1479	LR: 0.050000
Training Epoch: 6 [48512/50000]	Loss: 2.3111	LR: 0.050000
Training Epoch: 6 [48640/50000]	Loss: 1.8920	LR: 0.050000
Training Epoch: 6 [48768/50000]	Loss: 1.9065	LR: 0.050000
Training Epoch: 6 [48896/50000]	Loss: 2.0691	LR: 0.050000
Training Epoch: 6 [49024/50000]	Loss: 2.2349	LR: 0.050000
Training Epoch: 6 [49152/50000]	Loss: 2.2056	LR: 0.050000
Training Epoch: 6 [49280/50000]	Loss: 1.9782	LR: 0.050000
Training Epoch: 6 [49408/50000]	Loss: 2.1208	LR: 0.050000
Training Epoch: 6 [49536/50000]	Loss: 2.2364	LR: 0.050000
Training Epoch: 6 [49664/50000]	Loss: 2.1960	LR: 0.050000
Training Epoch: 6 [49792/50000]	Loss: 1.9808	LR: 0.050000
Training Epoch: 6 [49920/50000]	Loss: 1.9463	LR: 0.050000
Training Epoch: 6 [50000/50000]	Loss: 1.9865	LR: 0.050000
epoch 6 training time consumed: 53.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   22020 GB |   22020 GB |
|       from large pool |  123392 KB |    1034 MB |   21999 GB |   21999 GB |
|       from small pool |   10798 KB |      13 MB |      21 GB |      21 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   22020 GB |   22020 GB |
|       from large pool |  123392 KB |    1034 MB |   21999 GB |   21999 GB |
|       from small pool |   10798 KB |      13 MB |      21 GB |      21 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |    9693 GB |    9693 GB |
|       from large pool |  155136 KB |  433088 KB |    9669 GB |    9669 GB |
|       from small pool |    1490 KB |    3494 KB |      24 GB |      24 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |     850 K  |     849 K  |
|       from large pool |      24    |      65    |     443 K  |     443 K  |
|       from small pool |     231    |     274    |     406 K  |     406 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |     850 K  |     849 K  |
|       from large pool |      24    |      65    |     443 K  |     443 K  |
|       from small pool |     231    |     274    |     406 K  |     406 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |  423689    |  423668    |
|       from large pool |       9    |      14    |  214703    |  214694    |
|       from small pool |      12    |      16    |  208986    |  208974    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 6, Average loss: 0.0165, Accuracy: 0.4386, Time consumed:3.46s

Training Epoch: 7 [128/50000]	Loss: 1.9864	LR: 0.050000
Training Epoch: 7 [256/50000]	Loss: 1.7720	LR: 0.050000
Training Epoch: 7 [384/50000]	Loss: 1.6885	LR: 0.050000
Training Epoch: 7 [512/50000]	Loss: 1.9416	LR: 0.050000
Training Epoch: 7 [640/50000]	Loss: 1.9845	LR: 0.050000
Training Epoch: 7 [768/50000]	Loss: 1.9186	LR: 0.050000
Training Epoch: 7 [896/50000]	Loss: 1.7423	LR: 0.050000
Training Epoch: 7 [1024/50000]	Loss: 1.8510	LR: 0.050000
Training Epoch: 7 [1152/50000]	Loss: 2.0633	LR: 0.050000
Training Epoch: 7 [1280/50000]	Loss: 1.8642	LR: 0.050000
Training Epoch: 7 [1408/50000]	Loss: 2.2647	LR: 0.050000
Training Epoch: 7 [1536/50000]	Loss: 1.8688	LR: 0.050000
Training Epoch: 7 [1664/50000]	Loss: 1.9993	LR: 0.050000
Training Epoch: 7 [1792/50000]	Loss: 1.8554	LR: 0.050000
Training Epoch: 7 [1920/50000]	Loss: 1.8105	LR: 0.050000
Training Epoch: 7 [2048/50000]	Loss: 2.0758	LR: 0.050000
Training Epoch: 7 [2176/50000]	Loss: 1.9462	LR: 0.050000
Training Epoch: 7 [2304/50000]	Loss: 1.9535	LR: 0.050000
Training Epoch: 7 [2432/50000]	Loss: 2.0576	LR: 0.050000
Training Epoch: 7 [2560/50000]	Loss: 1.9697	LR: 0.050000
Training Epoch: 7 [2688/50000]	Loss: 2.0083	LR: 0.050000
Training Epoch: 7 [2816/50000]	Loss: 1.9874	LR: 0.050000
Training Epoch: 7 [2944/50000]	Loss: 2.1192	LR: 0.050000
Training Epoch: 7 [3072/50000]	Loss: 2.0976	LR: 0.050000
Training Epoch: 7 [3200/50000]	Loss: 1.8638	LR: 0.050000
Training Epoch: 7 [3328/50000]	Loss: 2.0003	LR: 0.050000
Training Epoch: 7 [3456/50000]	Loss: 2.1210	LR: 0.050000
Training Epoch: 7 [3584/50000]	Loss: 1.8502	LR: 0.050000
Training Epoch: 7 [3712/50000]	Loss: 1.9603	LR: 0.050000
Training Epoch: 7 [3840/50000]	Loss: 2.0129	LR: 0.050000
Training Epoch: 7 [3968/50000]	Loss: 2.0280	LR: 0.050000
Training Epoch: 7 [4096/50000]	Loss: 1.7985	LR: 0.050000
Training Epoch: 7 [4224/50000]	Loss: 2.0190	LR: 0.050000
Training Epoch: 7 [4352/50000]	Loss: 2.1040	LR: 0.050000
Training Epoch: 7 [4480/50000]	Loss: 1.8908	LR: 0.050000
Training Epoch: 7 [4608/50000]	Loss: 1.8790	LR: 0.050000
Training Epoch: 7 [4736/50000]	Loss: 1.9200	LR: 0.050000
Training Epoch: 7 [4864/50000]	Loss: 1.9137	LR: 0.050000
Training Epoch: 7 [4992/50000]	Loss: 1.9569	LR: 0.050000
Training Epoch: 7 [5120/50000]	Loss: 1.7627	LR: 0.050000
Training Epoch: 7 [5248/50000]	Loss: 1.7721	LR: 0.050000
Training Epoch: 7 [5376/50000]	Loss: 1.7501	LR: 0.050000
Training Epoch: 7 [5504/50000]	Loss: 1.9761	LR: 0.050000
Training Epoch: 7 [5632/50000]	Loss: 1.7973	LR: 0.050000
Training Epoch: 7 [5760/50000]	Loss: 1.9834	LR: 0.050000
Training Epoch: 7 [5888/50000]	Loss: 1.8790	LR: 0.050000
Training Epoch: 7 [6016/50000]	Loss: 1.8541	LR: 0.050000
Training Epoch: 7 [6144/50000]	Loss: 1.9518	LR: 0.050000
Training Epoch: 7 [6272/50000]	Loss: 2.0318	LR: 0.050000
Training Epoch: 7 [6400/50000]	Loss: 2.2656	LR: 0.050000
Training Epoch: 7 [6528/50000]	Loss: 1.7675	LR: 0.050000
Training Epoch: 7 [6656/50000]	Loss: 2.0573	LR: 0.050000
Training Epoch: 7 [6784/50000]	Loss: 1.9811	LR: 0.050000
Training Epoch: 7 [6912/50000]	Loss: 1.9150	LR: 0.050000
Training Epoch: 7 [7040/50000]	Loss: 1.8561	LR: 0.050000
Training Epoch: 7 [7168/50000]	Loss: 1.8899	LR: 0.050000
Training Epoch: 7 [7296/50000]	Loss: 2.0962	LR: 0.050000
Training Epoch: 7 [7424/50000]	Loss: 1.8147	LR: 0.050000
Training Epoch: 7 [7552/50000]	Loss: 2.0426	LR: 0.050000
Training Epoch: 7 [7680/50000]	Loss: 1.8232	LR: 0.050000
Training Epoch: 7 [7808/50000]	Loss: 1.7986	LR: 0.050000
Training Epoch: 7 [7936/50000]	Loss: 2.1122	LR: 0.050000
Training Epoch: 7 [8064/50000]	Loss: 1.9544	LR: 0.050000
Training Epoch: 7 [8192/50000]	Loss: 2.0146	LR: 0.050000
Training Epoch: 7 [8320/50000]	Loss: 1.9214	LR: 0.050000
Training Epoch: 7 [8448/50000]	Loss: 1.9013	LR: 0.050000
Training Epoch: 7 [8576/50000]	Loss: 1.7418	LR: 0.050000
Training Epoch: 7 [8704/50000]	Loss: 2.1608	LR: 0.050000
Training Epoch: 7 [8832/50000]	Loss: 1.8805	LR: 0.050000
Training Epoch: 7 [8960/50000]	Loss: 1.9845	LR: 0.050000
Training Epoch: 7 [9088/50000]	Loss: 1.8612	LR: 0.050000
Training Epoch: 7 [9216/50000]	Loss: 2.0375	LR: 0.050000
Training Epoch: 7 [9344/50000]	Loss: 1.9833	LR: 0.050000
Training Epoch: 7 [9472/50000]	Loss: 2.3365	LR: 0.050000
Training Epoch: 7 [9600/50000]	Loss: 2.0609	LR: 0.050000
Training Epoch: 7 [9728/50000]	Loss: 2.0393	LR: 0.050000
Training Epoch: 7 [9856/50000]	Loss: 2.0781	LR: 0.050000
Training Epoch: 7 [9984/50000]	Loss: 1.7563	LR: 0.050000
Training Epoch: 7 [10112/50000]	Loss: 2.2333	LR: 0.050000
Training Epoch: 7 [10240/50000]	Loss: 2.0858	LR: 0.050000
Training Epoch: 7 [10368/50000]	Loss: 2.1229	LR: 0.050000
Training Epoch: 7 [10496/50000]	Loss: 1.7898	LR: 0.050000
Training Epoch: 7 [10624/50000]	Loss: 1.9810	LR: 0.050000
Training Epoch: 7 [10752/50000]	Loss: 1.8188	LR: 0.050000
Training Epoch: 7 [10880/50000]	Loss: 1.5474	LR: 0.050000
Training Epoch: 7 [11008/50000]	Loss: 1.6092	LR: 0.050000
Training Epoch: 7 [11136/50000]	Loss: 2.1913	LR: 0.050000
Training Epoch: 7 [11264/50000]	Loss: 2.1445	LR: 0.050000
Training Epoch: 7 [11392/50000]	Loss: 1.8976	LR: 0.050000
Training Epoch: 7 [11520/50000]	Loss: 2.0409	LR: 0.050000
Training Epoch: 7 [11648/50000]	Loss: 2.0674	LR: 0.050000
Training Epoch: 7 [11776/50000]	Loss: 2.1201	LR: 0.050000
Training Epoch: 7 [11904/50000]	Loss: 2.0459	LR: 0.050000
Training Epoch: 7 [12032/50000]	Loss: 1.7548	LR: 0.050000
Training Epoch: 7 [12160/50000]	Loss: 1.9935	LR: 0.050000
Training Epoch: 7 [12288/50000]	Loss: 1.9953	LR: 0.050000
Training Epoch: 7 [12416/50000]	Loss: 2.1278	LR: 0.050000
Training Epoch: 7 [12544/50000]	Loss: 1.8333	LR: 0.050000
Training Epoch: 7 [12672/50000]	Loss: 1.9173	LR: 0.050000
Training Epoch: 7 [12800/50000]	Loss: 2.0826	LR: 0.050000
Training Epoch: 7 [12928/50000]	Loss: 2.2405	LR: 0.050000
Training Epoch: 7 [13056/50000]	Loss: 1.8847	LR: 0.050000
Training Epoch: 7 [13184/50000]	Loss: 1.8060	LR: 0.050000
Training Epoch: 7 [13312/50000]	Loss: 1.8644	LR: 0.050000
Training Epoch: 7 [13440/50000]	Loss: 2.0468	LR: 0.050000
Training Epoch: 7 [13568/50000]	Loss: 2.0860	LR: 0.050000
Training Epoch: 7 [13696/50000]	Loss: 1.8105	LR: 0.050000
Training Epoch: 7 [13824/50000]	Loss: 2.0880	LR: 0.050000
Training Epoch: 7 [13952/50000]	Loss: 2.0570	LR: 0.050000
Training Epoch: 7 [14080/50000]	Loss: 2.2006	LR: 0.050000
Training Epoch: 7 [14208/50000]	Loss: 2.0118	LR: 0.050000
Training Epoch: 7 [14336/50000]	Loss: 2.0153	LR: 0.050000
Training Epoch: 7 [14464/50000]	Loss: 1.8711	LR: 0.050000
Training Epoch: 7 [14592/50000]	Loss: 2.1640	LR: 0.050000
Training Epoch: 7 [14720/50000]	Loss: 2.1635	LR: 0.050000
Training Epoch: 7 [14848/50000]	Loss: 2.0155	LR: 0.050000
Training Epoch: 7 [14976/50000]	Loss: 1.8174	LR: 0.050000
Training Epoch: 7 [15104/50000]	Loss: 1.8978	LR: 0.050000
Training Epoch: 7 [15232/50000]	Loss: 2.3001	LR: 0.050000
Training Epoch: 7 [15360/50000]	Loss: 1.7363	LR: 0.050000
Training Epoch: 7 [15488/50000]	Loss: 2.0844	LR: 0.050000
Training Epoch: 7 [15616/50000]	Loss: 2.0578	LR: 0.050000
Training Epoch: 7 [15744/50000]	Loss: 2.0225	LR: 0.050000
Training Epoch: 7 [15872/50000]	Loss: 2.2489	LR: 0.050000
Training Epoch: 7 [16000/50000]	Loss: 2.2979	LR: 0.050000
Training Epoch: 7 [16128/50000]	Loss: 2.0409	LR: 0.050000
Training Epoch: 7 [16256/50000]	Loss: 2.0820	LR: 0.050000
Training Epoch: 7 [16384/50000]	Loss: 1.9939	LR: 0.050000
Training Epoch: 7 [16512/50000]	Loss: 2.1479	LR: 0.050000
Training Epoch: 7 [16640/50000]	Loss: 2.0033	LR: 0.050000
Training Epoch: 7 [16768/50000]	Loss: 1.9524	LR: 0.050000
Training Epoch: 7 [16896/50000]	Loss: 1.9984	LR: 0.050000
Training Epoch: 7 [17024/50000]	Loss: 2.0799	LR: 0.050000
Training Epoch: 7 [17152/50000]	Loss: 2.0139	LR: 0.050000
Training Epoch: 7 [17280/50000]	Loss: 2.0235	LR: 0.050000
Training Epoch: 7 [17408/50000]	Loss: 1.8856	LR: 0.050000
Training Epoch: 7 [17536/50000]	Loss: 2.0241	LR: 0.050000
Training Epoch: 7 [17664/50000]	Loss: 1.8422	LR: 0.050000
Training Epoch: 7 [17792/50000]	Loss: 2.0030	LR: 0.050000
Training Epoch: 7 [17920/50000]	Loss: 1.8660	LR: 0.050000
Training Epoch: 7 [18048/50000]	Loss: 1.9618	LR: 0.050000
Training Epoch: 7 [18176/50000]	Loss: 2.3138	LR: 0.050000
Training Epoch: 7 [18304/50000]	Loss: 2.1132	LR: 0.050000
Training Epoch: 7 [18432/50000]	Loss: 1.6781	LR: 0.050000
Training Epoch: 7 [18560/50000]	Loss: 2.1745	LR: 0.050000
Training Epoch: 7 [18688/50000]	Loss: 1.6920	LR: 0.050000
Training Epoch: 7 [18816/50000]	Loss: 1.6182	LR: 0.050000
Training Epoch: 7 [18944/50000]	Loss: 1.9215	LR: 0.050000
Training Epoch: 7 [19072/50000]	Loss: 1.8718	LR: 0.050000
Training Epoch: 7 [19200/50000]	Loss: 1.5985	LR: 0.050000
Training Epoch: 7 [19328/50000]	Loss: 1.8252	LR: 0.050000
Training Epoch: 7 [19456/50000]	Loss: 2.0903	LR: 0.050000
Training Epoch: 7 [19584/50000]	Loss: 1.9906	LR: 0.050000
Training Epoch: 7 [19712/50000]	Loss: 2.1111	LR: 0.050000
Training Epoch: 7 [19840/50000]	Loss: 2.0330	LR: 0.050000
Training Epoch: 7 [19968/50000]	Loss: 1.8827	LR: 0.050000
Training Epoch: 7 [20096/50000]	Loss: 2.0388	LR: 0.050000
Training Epoch: 7 [20224/50000]	Loss: 1.9585	LR: 0.050000
Training Epoch: 7 [20352/50000]	Loss: 1.8426	LR: 0.050000
Training Epoch: 7 [20480/50000]	Loss: 2.0482	LR: 0.050000
Training Epoch: 7 [20608/50000]	Loss: 1.8189	LR: 0.050000
Training Epoch: 7 [20736/50000]	Loss: 2.1421	LR: 0.050000
Training Epoch: 7 [20864/50000]	Loss: 1.8983	LR: 0.050000
Training Epoch: 7 [20992/50000]	Loss: 1.9928	LR: 0.050000
Training Epoch: 7 [21120/50000]	Loss: 2.1181	LR: 0.050000
Training Epoch: 7 [21248/50000]	Loss: 1.8629	LR: 0.050000
Training Epoch: 7 [21376/50000]	Loss: 1.9251	LR: 0.050000
Training Epoch: 7 [21504/50000]	Loss: 2.2568	LR: 0.050000
Training Epoch: 7 [21632/50000]	Loss: 1.7555	LR: 0.050000
Training Epoch: 7 [21760/50000]	Loss: 1.9046	LR: 0.050000
Training Epoch: 7 [21888/50000]	Loss: 2.0852	LR: 0.050000
Training Epoch: 7 [22016/50000]	Loss: 1.8736	LR: 0.050000
Training Epoch: 7 [22144/50000]	Loss: 2.3436	LR: 0.050000
Training Epoch: 7 [22272/50000]	Loss: 1.8960	LR: 0.050000
Training Epoch: 7 [22400/50000]	Loss: 1.8759	LR: 0.050000
Training Epoch: 7 [22528/50000]	Loss: 2.1050	LR: 0.050000
Training Epoch: 7 [22656/50000]	Loss: 1.8856	LR: 0.050000
Training Epoch: 7 [22784/50000]	Loss: 1.8860	LR: 0.050000
Training Epoch: 7 [22912/50000]	Loss: 2.0705	LR: 0.050000
Training Epoch: 7 [23040/50000]	Loss: 1.9322	LR: 0.050000
Training Epoch: 7 [23168/50000]	Loss: 2.0314	LR: 0.050000
Training Epoch: 7 [23296/50000]	Loss: 1.8248	LR: 0.050000
Training Epoch: 7 [23424/50000]	Loss: 1.9493	LR: 0.050000
Training Epoch: 7 [23552/50000]	Loss: 2.0412	LR: 0.050000
Training Epoch: 7 [23680/50000]	Loss: 2.1811	LR: 0.050000
Training Epoch: 7 [23808/50000]	Loss: 1.7175	LR: 0.050000
Training Epoch: 7 [23936/50000]	Loss: 1.9458	LR: 0.050000
Training Epoch: 7 [24064/50000]	Loss: 2.0092	LR: 0.050000
Training Epoch: 7 [24192/50000]	Loss: 2.0422	LR: 0.050000
Training Epoch: 7 [24320/50000]	Loss: 1.6253	LR: 0.050000
Training Epoch: 7 [24448/50000]	Loss: 2.0206	LR: 0.050000
Training Epoch: 7 [24576/50000]	Loss: 1.9915	LR: 0.050000
Training Epoch: 7 [24704/50000]	Loss: 1.6170	LR: 0.050000
Training Epoch: 7 [24832/50000]	Loss: 2.0128	LR: 0.050000
Training Epoch: 7 [24960/50000]	Loss: 1.9492	LR: 0.050000
Training Epoch: 7 [25088/50000]	Loss: 2.2215	LR: 0.050000
Training Epoch: 7 [25216/50000]	Loss: 1.8363	LR: 0.050000
Training Epoch: 7 [25344/50000]	Loss: 1.7374	LR: 0.050000
Training Epoch: 7 [25472/50000]	Loss: 2.1965	LR: 0.050000
Training Epoch: 7 [25600/50000]	Loss: 1.7900	LR: 0.050000
Training Epoch: 7 [25728/50000]	Loss: 1.8203	LR: 0.050000
Training Epoch: 7 [25856/50000]	Loss: 1.9109	LR: 0.050000
Training Epoch: 7 [25984/50000]	Loss: 1.9263	LR: 0.050000
Training Epoch: 7 [26112/50000]	Loss: 2.0082	LR: 0.050000
Training Epoch: 7 [26240/50000]	Loss: 1.8140	LR: 0.050000
Training Epoch: 7 [26368/50000]	Loss: 2.1477	LR: 0.050000
Training Epoch: 7 [26496/50000]	Loss: 1.9311	LR: 0.050000
Training Epoch: 7 [26624/50000]	Loss: 1.8819	LR: 0.050000
Training Epoch: 7 [26752/50000]	Loss: 1.9815	LR: 0.050000
Training Epoch: 7 [26880/50000]	Loss: 2.0304	LR: 0.050000
Training Epoch: 7 [27008/50000]	Loss: 1.8517	LR: 0.050000
Training Epoch: 7 [27136/50000]	Loss: 1.9883	LR: 0.050000
Training Epoch: 7 [27264/50000]	Loss: 1.8039	LR: 0.050000
Training Epoch: 7 [27392/50000]	Loss: 2.0861	LR: 0.050000
Training Epoch: 7 [27520/50000]	Loss: 1.9218	LR: 0.050000
Training Epoch: 7 [27648/50000]	Loss: 1.8091	LR: 0.050000
Training Epoch: 7 [27776/50000]	Loss: 1.9831	LR: 0.050000
Training Epoch: 7 [27904/50000]	Loss: 1.6571	LR: 0.050000
Training Epoch: 7 [28032/50000]	Loss: 2.2119	LR: 0.050000
Training Epoch: 7 [28160/50000]	Loss: 2.2947	LR: 0.050000
Training Epoch: 7 [28288/50000]	Loss: 1.9206	LR: 0.050000
Training Epoch: 7 [28416/50000]	Loss: 1.8786	LR: 0.050000
Training Epoch: 7 [28544/50000]	Loss: 1.8159	LR: 0.050000
Training Epoch: 7 [28672/50000]	Loss: 1.8584	LR: 0.050000
Training Epoch: 7 [28800/50000]	Loss: 1.9798	LR: 0.050000
Training Epoch: 7 [28928/50000]	Loss: 2.1263	LR: 0.050000
Training Epoch: 7 [29056/50000]	Loss: 1.9352	LR: 0.050000
Training Epoch: 7 [29184/50000]	Loss: 1.8423	LR: 0.050000
Training Epoch: 7 [29312/50000]	Loss: 2.2104	LR: 0.050000
Training Epoch: 7 [29440/50000]	Loss: 2.1024	LR: 0.050000
Training Epoch: 7 [29568/50000]	Loss: 1.8484	LR: 0.050000
Training Epoch: 7 [29696/50000]	Loss: 1.9997	LR: 0.050000
Training Epoch: 7 [29824/50000]	Loss: 2.0222	LR: 0.050000
Training Epoch: 7 [29952/50000]	Loss: 1.6391	LR: 0.050000
Training Epoch: 7 [30080/50000]	Loss: 1.8624	LR: 0.050000
Training Epoch: 7 [30208/50000]	Loss: 2.1070	LR: 0.050000
Training Epoch: 7 [30336/50000]	Loss: 2.1219	LR: 0.050000
Training Epoch: 7 [30464/50000]	Loss: 2.0685	LR: 0.050000
Training Epoch: 7 [30592/50000]	Loss: 1.8914	LR: 0.050000
Training Epoch: 7 [30720/50000]	Loss: 1.8490	LR: 0.050000
Training Epoch: 7 [30848/50000]	Loss: 1.9446	LR: 0.050000
Training Epoch: 7 [30976/50000]	Loss: 1.9218	LR: 0.050000
Training Epoch: 7 [31104/50000]	Loss: 2.0288	LR: 0.050000
Training Epoch: 7 [31232/50000]	Loss: 1.8711	LR: 0.050000
Training Epoch: 7 [31360/50000]	Loss: 1.8932	LR: 0.050000
Training Epoch: 7 [31488/50000]	Loss: 1.9639	LR: 0.050000
Training Epoch: 7 [31616/50000]	Loss: 2.0632	LR: 0.050000
Training Epoch: 7 [31744/50000]	Loss: 1.9299	LR: 0.050000
Training Epoch: 7 [31872/50000]	Loss: 1.9752	LR: 0.050000
Training Epoch: 7 [32000/50000]	Loss: 2.0009	LR: 0.050000
Training Epoch: 7 [32128/50000]	Loss: 1.9532	LR: 0.050000
Training Epoch: 7 [32256/50000]	Loss: 1.7090	LR: 0.050000
Training Epoch: 7 [32384/50000]	Loss: 2.0633	LR: 0.050000
Training Epoch: 7 [32512/50000]	Loss: 1.9509	LR: 0.050000
Training Epoch: 7 [32640/50000]	Loss: 1.9095	LR: 0.050000
Training Epoch: 7 [32768/50000]	Loss: 2.0756	LR: 0.050000
Training Epoch: 7 [32896/50000]	Loss: 1.7655	LR: 0.050000
Training Epoch: 7 [33024/50000]	Loss: 1.7707	LR: 0.050000
Training Epoch: 7 [33152/50000]	Loss: 1.8373	LR: 0.050000
Training Epoch: 7 [33280/50000]	Loss: 2.1745	LR: 0.050000
Training Epoch: 7 [33408/50000]	Loss: 1.8669	LR: 0.050000
Training Epoch: 7 [33536/50000]	Loss: 1.8253	LR: 0.050000
Training Epoch: 7 [33664/50000]	Loss: 2.1977	LR: 0.050000
Training Epoch: 7 [33792/50000]	Loss: 2.0599	LR: 0.050000
Training Epoch: 7 [33920/50000]	Loss: 1.7267	LR: 0.050000
Training Epoch: 7 [34048/50000]	Loss: 1.9679	LR: 0.050000
Training Epoch: 7 [34176/50000]	Loss: 1.9197	LR: 0.050000
Training Epoch: 7 [34304/50000]	Loss: 2.1855	LR: 0.050000
Training Epoch: 7 [34432/50000]	Loss: 1.7064	LR: 0.050000
Training Epoch: 7 [34560/50000]	Loss: 1.9354	LR: 0.050000
Training Epoch: 7 [34688/50000]	Loss: 1.6877	LR: 0.050000
Training Epoch: 7 [34816/50000]	Loss: 1.8135	LR: 0.050000
Training Epoch: 7 [34944/50000]	Loss: 1.9735	LR: 0.050000
Training Epoch: 7 [35072/50000]	Loss: 1.7729	LR: 0.050000
Training Epoch: 7 [35200/50000]	Loss: 2.0967	LR: 0.050000
Training Epoch: 7 [35328/50000]	Loss: 1.9715	LR: 0.050000
Training Epoch: 7 [35456/50000]	Loss: 1.7701	LR: 0.050000
Training Epoch: 7 [35584/50000]	Loss: 2.1509	LR: 0.050000
Training Epoch: 7 [35712/50000]	Loss: 2.0108	LR: 0.050000
Training Epoch: 7 [35840/50000]	Loss: 1.9429	LR: 0.050000
Training Epoch: 7 [35968/50000]	Loss: 1.9155	LR: 0.050000
Training Epoch: 7 [36096/50000]	Loss: 2.0978	LR: 0.050000
Training Epoch: 7 [36224/50000]	Loss: 1.9170	LR: 0.050000
Training Epoch: 7 [36352/50000]	Loss: 1.9207	LR: 0.050000
Training Epoch: 7 [36480/50000]	Loss: 1.9235	LR: 0.050000
Training Epoch: 7 [36608/50000]	Loss: 1.9846	LR: 0.050000
Training Epoch: 7 [36736/50000]	Loss: 1.9112	LR: 0.050000
Training Epoch: 7 [36864/50000]	Loss: 1.9234	LR: 0.050000
Training Epoch: 7 [36992/50000]	Loss: 2.0275	LR: 0.050000
Training Epoch: 7 [37120/50000]	Loss: 2.0768	LR: 0.050000
Training Epoch: 7 [37248/50000]	Loss: 1.9127	LR: 0.050000
Training Epoch: 7 [37376/50000]	Loss: 2.1354	LR: 0.050000
Training Epoch: 7 [37504/50000]	Loss: 2.0010	LR: 0.050000
Training Epoch: 7 [37632/50000]	Loss: 1.8630	LR: 0.050000
Training Epoch: 7 [37760/50000]	Loss: 1.9447	LR: 0.050000
Training Epoch: 7 [37888/50000]	Loss: 1.7695	LR: 0.050000
Training Epoch: 7 [38016/50000]	Loss: 2.0979	LR: 0.050000
Training Epoch: 7 [38144/50000]	Loss: 1.6890	LR: 0.050000
Training Epoch: 7 [38272/50000]	Loss: 1.9426	LR: 0.050000
Training Epoch: 7 [38400/50000]	Loss: 1.8001	LR: 0.050000
Training Epoch: 7 [38528/50000]	Loss: 1.7970	LR: 0.050000
Training Epoch: 7 [38656/50000]	Loss: 1.8360	LR: 0.050000
Training Epoch: 7 [38784/50000]	Loss: 2.0968	LR: 0.050000
Training Epoch: 7 [38912/50000]	Loss: 1.8399	LR: 0.050000
Training Epoch: 7 [39040/50000]	Loss: 1.8817	LR: 0.050000
Training Epoch: 7 [39168/50000]	Loss: 1.9430	LR: 0.050000
Training Epoch: 7 [39296/50000]	Loss: 1.7263	LR: 0.050000
Training Epoch: 7 [39424/50000]	Loss: 1.8234	LR: 0.050000
Training Epoch: 7 [39552/50000]	Loss: 1.9048	LR: 0.050000
Training Epoch: 7 [39680/50000]	Loss: 2.0582	LR: 0.050000
Training Epoch: 7 [39808/50000]	Loss: 2.3238	LR: 0.050000
Training Epoch: 7 [39936/50000]	Loss: 1.9344	LR: 0.050000
Training Epoch: 7 [40064/50000]	Loss: 2.1346	LR: 0.050000
Training Epoch: 7 [40192/50000]	Loss: 1.9060	LR: 0.050000
Training Epoch: 7 [40320/50000]	Loss: 1.8196	LR: 0.050000
Training Epoch: 7 [40448/50000]	Loss: 1.9421	LR: 0.050000
Training Epoch: 7 [40576/50000]	Loss: 2.2095	LR: 0.050000
Training Epoch: 7 [40704/50000]	Loss: 2.1574	LR: 0.050000
Training Epoch: 7 [40832/50000]	Loss: 1.9705	LR: 0.050000
Training Epoch: 7 [40960/50000]	Loss: 1.7568	LR: 0.050000
Training Epoch: 7 [41088/50000]	Loss: 1.8278	LR: 0.050000
Training Epoch: 7 [41216/50000]	Loss: 1.8495	LR: 0.050000
Training Epoch: 7 [41344/50000]	Loss: 1.8685	LR: 0.050000
Training Epoch: 7 [41472/50000]	Loss: 1.9383	LR: 0.050000
Training Epoch: 7 [41600/50000]	Loss: 1.9501	LR: 0.050000
Training Epoch: 7 [41728/50000]	Loss: 1.7269	LR: 0.050000
Training Epoch: 7 [41856/50000]	Loss: 1.9635	LR: 0.050000
Training Epoch: 7 [41984/50000]	Loss: 1.9521	LR: 0.050000
Training Epoch: 7 [42112/50000]	Loss: 2.2119	LR: 0.050000
Training Epoch: 7 [42240/50000]	Loss: 1.7983	LR: 0.050000
Training Epoch: 7 [42368/50000]	Loss: 1.9675	LR: 0.050000
Training Epoch: 7 [42496/50000]	Loss: 1.7079	LR: 0.050000
Training Epoch: 7 [42624/50000]	Loss: 1.7604	LR: 0.050000
Training Epoch: 7 [42752/50000]	Loss: 2.0530	LR: 0.050000
Training Epoch: 7 [42880/50000]	Loss: 1.9193	LR: 0.050000
Training Epoch: 7 [43008/50000]	Loss: 1.9198	LR: 0.050000
Training Epoch: 7 [43136/50000]	Loss: 1.9247	LR: 0.050000
Training Epoch: 7 [43264/50000]	Loss: 1.7641	LR: 0.050000
Training Epoch: 7 [43392/50000]	Loss: 1.9441	LR: 0.050000
Training Epoch: 7 [43520/50000]	Loss: 1.9328	LR: 0.050000
Training Epoch: 7 [43648/50000]	Loss: 1.8423	LR: 0.050000
Training Epoch: 7 [43776/50000]	Loss: 2.2426	LR: 0.050000
Training Epoch: 7 [43904/50000]	Loss: 2.0815	LR: 0.050000
Training Epoch: 7 [44032/50000]	Loss: 1.9741	LR: 0.050000
Training Epoch: 7 [44160/50000]	Loss: 1.8468	LR: 0.050000
Training Epoch: 7 [44288/50000]	Loss: 1.8821	LR: 0.050000
Training Epoch: 7 [44416/50000]	Loss: 1.8878	LR: 0.050000
Training Epoch: 7 [44544/50000]	Loss: 1.8937	LR: 0.050000
Training Epoch: 7 [44672/50000]	Loss: 1.7922	LR: 0.050000
Training Epoch: 7 [44800/50000]	Loss: 2.0475	LR: 0.050000
Training Epoch: 7 [44928/50000]	Loss: 1.9585	LR: 0.050000
Training Epoch: 7 [45056/50000]	Loss: 1.7136	LR: 0.050000
Training Epoch: 7 [45184/50000]	Loss: 1.7811	LR: 0.050000
Training Epoch: 7 [45312/50000]	Loss: 1.9663	LR: 0.050000
Training Epoch: 7 [45440/50000]	Loss: 1.8902	LR: 0.050000
Training Epoch: 7 [45568/50000]	Loss: 1.9710	LR: 0.050000
Training Epoch: 7 [45696/50000]	Loss: 1.9792	LR: 0.050000
Training Epoch: 7 [45824/50000]	Loss: 2.1705	LR: 0.050000
Training Epoch: 7 [45952/50000]	Loss: 2.1126	LR: 0.050000
Training Epoch: 7 [46080/50000]	Loss: 2.2458	LR: 0.050000
Training Epoch: 7 [46208/50000]	Loss: 1.8567	LR: 0.050000
Training Epoch: 7 [46336/50000]	Loss: 2.0533	LR: 0.050000
Training Epoch: 7 [46464/50000]	Loss: 2.0577	LR: 0.050000
Training Epoch: 7 [46592/50000]	Loss: 2.1983	LR: 0.050000
Training Epoch: 7 [46720/50000]	Loss: 1.8423	LR: 0.050000
Training Epoch: 7 [46848/50000]	Loss: 2.0292	LR: 0.050000
Training Epoch: 7 [46976/50000]	Loss: 1.7585	LR: 0.050000
Training Epoch: 7 [47104/50000]	Loss: 1.8759	LR: 0.050000
Training Epoch: 7 [47232/50000]	Loss: 1.7313	LR: 0.050000
Training Epoch: 7 [47360/50000]	Loss: 1.9413	LR: 0.050000
Training Epoch: 7 [47488/50000]	Loss: 1.7721	LR: 0.050000
Training Epoch: 7 [47616/50000]	Loss: 1.8399	LR: 0.050000
Training Epoch: 7 [47744/50000]	Loss: 1.8352	LR: 0.050000
Training Epoch: 7 [47872/50000]	Loss: 1.8903	LR: 0.050000
Training Epoch: 7 [48000/50000]	Loss: 1.8386	LR: 0.050000
Training Epoch: 7 [48128/50000]	Loss: 1.7193	LR: 0.050000
Training Epoch: 7 [48256/50000]	Loss: 1.7666	LR: 0.050000
Training Epoch: 7 [48384/50000]	Loss: 2.1431	LR: 0.050000
Training Epoch: 7 [48512/50000]	Loss: 1.8385	LR: 0.050000
Training Epoch: 7 [48640/50000]	Loss: 1.9455	LR: 0.050000
Training Epoch: 7 [48768/50000]	Loss: 2.0269	LR: 0.050000
Training Epoch: 7 [48896/50000]	Loss: 1.8646	LR: 0.050000
Training Epoch: 7 [49024/50000]	Loss: 1.6804	LR: 0.050000
Training Epoch: 7 [49152/50000]	Loss: 1.9651	LR: 0.050000
Training Epoch: 7 [49280/50000]	Loss: 1.9394	LR: 0.050000
Training Epoch: 7 [49408/50000]	Loss: 2.0145	LR: 0.050000
Training Epoch: 7 [49536/50000]	Loss: 1.9878	LR: 0.050000
Training Epoch: 7 [49664/50000]	Loss: 1.8828	LR: 0.050000
Training Epoch: 7 [49792/50000]	Loss: 1.7097	LR: 0.050000
Training Epoch: 7 [49920/50000]	Loss: 1.8807	LR: 0.050000
Training Epoch: 7 [50000/50000]	Loss: 1.9983	LR: 0.050000
epoch 7 training time consumed: 53.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   25691 GB |   25690 GB |
|       from large pool |  123392 KB |    1034 MB |   25665 GB |   25665 GB |
|       from small pool |   10798 KB |      13 MB |      25 GB |      25 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   25691 GB |   25690 GB |
|       from large pool |  123392 KB |    1034 MB |   25665 GB |   25665 GB |
|       from small pool |   10798 KB |      13 MB |      25 GB |      25 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   11308 GB |   11308 GB |
|       from large pool |  155136 KB |  433088 KB |   11280 GB |   11280 GB |
|       from small pool |    1490 KB |    3494 KB |      28 GB |      28 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |     991 K  |     991 K  |
|       from large pool |      24    |      65    |     517 K  |     517 K  |
|       from small pool |     231    |     274    |     474 K  |     474 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |     991 K  |     991 K  |
|       from large pool |      24    |      65    |     517 K  |     517 K  |
|       from small pool |     231    |     274    |     474 K  |     474 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |  493651    |  493630    |
|       from large pool |       9    |      14    |  250481    |  250472    |
|       from small pool |      12    |      16    |  243170    |  243158    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 7, Average loss: 0.0163, Accuracy: 0.4503, Time consumed:3.45s

Training Epoch: 8 [128/50000]	Loss: 1.6868	LR: 0.050000
Training Epoch: 8 [256/50000]	Loss: 1.6046	LR: 0.050000
Training Epoch: 8 [384/50000]	Loss: 1.8132	LR: 0.050000
Training Epoch: 8 [512/50000]	Loss: 1.7429	LR: 0.050000
Training Epoch: 8 [640/50000]	Loss: 1.9495	LR: 0.050000
Training Epoch: 8 [768/50000]	Loss: 2.1394	LR: 0.050000
Training Epoch: 8 [896/50000]	Loss: 1.8659	LR: 0.050000
Training Epoch: 8 [1024/50000]	Loss: 1.7930	LR: 0.050000
Training Epoch: 8 [1152/50000]	Loss: 1.9304	LR: 0.050000
Training Epoch: 8 [1280/50000]	Loss: 1.8521	LR: 0.050000
Training Epoch: 8 [1408/50000]	Loss: 1.8988	LR: 0.050000
Training Epoch: 8 [1536/50000]	Loss: 2.0576	LR: 0.050000
Training Epoch: 8 [1664/50000]	Loss: 1.7830	LR: 0.050000
Training Epoch: 8 [1792/50000]	Loss: 1.7865	LR: 0.050000
Training Epoch: 8 [1920/50000]	Loss: 2.0899	LR: 0.050000
Training Epoch: 8 [2048/50000]	Loss: 1.8413	LR: 0.050000
Training Epoch: 8 [2176/50000]	Loss: 1.9454	LR: 0.050000
Training Epoch: 8 [2304/50000]	Loss: 1.6660	LR: 0.050000
Training Epoch: 8 [2432/50000]	Loss: 1.8735	LR: 0.050000
Training Epoch: 8 [2560/50000]	Loss: 1.6906	LR: 0.050000
Training Epoch: 8 [2688/50000]	Loss: 2.0046	LR: 0.050000
Training Epoch: 8 [2816/50000]	Loss: 1.8770	LR: 0.050000
Training Epoch: 8 [2944/50000]	Loss: 1.7937	LR: 0.050000
Training Epoch: 8 [3072/50000]	Loss: 1.8783	LR: 0.050000
Training Epoch: 8 [3200/50000]	Loss: 1.9133	LR: 0.050000
Training Epoch: 8 [3328/50000]	Loss: 1.8478	LR: 0.050000
Training Epoch: 8 [3456/50000]	Loss: 1.8007	LR: 0.050000
Training Epoch: 8 [3584/50000]	Loss: 1.8514	LR: 0.050000
Training Epoch: 8 [3712/50000]	Loss: 1.7912	LR: 0.050000
Training Epoch: 8 [3840/50000]	Loss: 2.0029	LR: 0.050000
Training Epoch: 8 [3968/50000]	Loss: 2.0473	LR: 0.050000
Training Epoch: 8 [4096/50000]	Loss: 1.9585	LR: 0.050000
Training Epoch: 8 [4224/50000]	Loss: 1.6671	LR: 0.050000
Training Epoch: 8 [4352/50000]	Loss: 1.8319	LR: 0.050000
Training Epoch: 8 [4480/50000]	Loss: 1.8424	LR: 0.050000
Training Epoch: 8 [4608/50000]	Loss: 1.6699	LR: 0.050000
Training Epoch: 8 [4736/50000]	Loss: 1.9486	LR: 0.050000
Training Epoch: 8 [4864/50000]	Loss: 1.7798	LR: 0.050000
Training Epoch: 8 [4992/50000]	Loss: 1.9198	LR: 0.050000
Training Epoch: 8 [5120/50000]	Loss: 1.6479	LR: 0.050000
Training Epoch: 8 [5248/50000]	Loss: 1.7907	LR: 0.050000
Training Epoch: 8 [5376/50000]	Loss: 1.8497	LR: 0.050000
Training Epoch: 8 [5504/50000]	Loss: 1.7174	LR: 0.050000
Training Epoch: 8 [5632/50000]	Loss: 1.8527	LR: 0.050000
Training Epoch: 8 [5760/50000]	Loss: 1.8327	LR: 0.050000
Training Epoch: 8 [5888/50000]	Loss: 1.7416	LR: 0.050000
Training Epoch: 8 [6016/50000]	Loss: 1.6786	LR: 0.050000
Training Epoch: 8 [6144/50000]	Loss: 1.8221	LR: 0.050000
Training Epoch: 8 [6272/50000]	Loss: 2.3373	LR: 0.050000
Training Epoch: 8 [6400/50000]	Loss: 2.0506	LR: 0.050000
Training Epoch: 8 [6528/50000]	Loss: 2.1077	LR: 0.050000
Training Epoch: 8 [6656/50000]	Loss: 2.1899	LR: 0.050000
Training Epoch: 8 [6784/50000]	Loss: 2.0208	LR: 0.050000
Training Epoch: 8 [6912/50000]	Loss: 1.7153	LR: 0.050000
Training Epoch: 8 [7040/50000]	Loss: 1.7969	LR: 0.050000
Training Epoch: 8 [7168/50000]	Loss: 1.7113	LR: 0.050000
Training Epoch: 8 [7296/50000]	Loss: 1.7585	LR: 0.050000
Training Epoch: 8 [7424/50000]	Loss: 1.8041	LR: 0.050000
Training Epoch: 8 [7552/50000]	Loss: 1.9562	LR: 0.050000
Training Epoch: 8 [7680/50000]	Loss: 1.9183	LR: 0.050000
Training Epoch: 8 [7808/50000]	Loss: 1.7424	LR: 0.050000
Training Epoch: 8 [7936/50000]	Loss: 1.7611	LR: 0.050000
Training Epoch: 8 [8064/50000]	Loss: 1.6567	LR: 0.050000
Training Epoch: 8 [8192/50000]	Loss: 1.9682	LR: 0.050000
Training Epoch: 8 [8320/50000]	Loss: 1.8274	LR: 0.050000
Training Epoch: 8 [8448/50000]	Loss: 2.1497	LR: 0.050000
Training Epoch: 8 [8576/50000]	Loss: 1.7232	LR: 0.050000
Training Epoch: 8 [8704/50000]	Loss: 1.8835	LR: 0.050000
Training Epoch: 8 [8832/50000]	Loss: 1.7455	LR: 0.050000
Training Epoch: 8 [8960/50000]	Loss: 1.4796	LR: 0.050000
Training Epoch: 8 [9088/50000]	Loss: 1.6966	LR: 0.050000
Training Epoch: 8 [9216/50000]	Loss: 1.6536	LR: 0.050000
Training Epoch: 8 [9344/50000]	Loss: 1.6919	LR: 0.050000
Training Epoch: 8 [9472/50000]	Loss: 1.9878	LR: 0.050000
Training Epoch: 8 [9600/50000]	Loss: 1.7206	LR: 0.050000
Training Epoch: 8 [9728/50000]	Loss: 1.7762	LR: 0.050000
Training Epoch: 8 [9856/50000]	Loss: 1.6927	LR: 0.050000
Training Epoch: 8 [9984/50000]	Loss: 2.0067	LR: 0.050000
Training Epoch: 8 [10112/50000]	Loss: 1.9789	LR: 0.050000
Training Epoch: 8 [10240/50000]	Loss: 1.9529	LR: 0.050000
Training Epoch: 8 [10368/50000]	Loss: 1.6619	LR: 0.050000
Training Epoch: 8 [10496/50000]	Loss: 1.8107	LR: 0.050000
Training Epoch: 8 [10624/50000]	Loss: 1.5758	LR: 0.050000
Training Epoch: 8 [10752/50000]	Loss: 2.2080	LR: 0.050000
Training Epoch: 8 [10880/50000]	Loss: 1.8761	LR: 0.050000
Training Epoch: 8 [11008/50000]	Loss: 1.7265	LR: 0.050000
Training Epoch: 8 [11136/50000]	Loss: 2.1260	LR: 0.050000
Training Epoch: 8 [11264/50000]	Loss: 1.8271	LR: 0.050000
Training Epoch: 8 [11392/50000]	Loss: 1.7685	LR: 0.050000
Training Epoch: 8 [11520/50000]	Loss: 1.8912	LR: 0.050000
Training Epoch: 8 [11648/50000]	Loss: 2.0489	LR: 0.050000
Training Epoch: 8 [11776/50000]	Loss: 1.6825	LR: 0.050000
Training Epoch: 8 [11904/50000]	Loss: 2.0613	LR: 0.050000
Training Epoch: 8 [12032/50000]	Loss: 1.7913	LR: 0.050000
Training Epoch: 8 [12160/50000]	Loss: 1.8565	LR: 0.050000
Training Epoch: 8 [12288/50000]	Loss: 1.7873	LR: 0.050000
Training Epoch: 8 [12416/50000]	Loss: 1.8361	LR: 0.050000
Training Epoch: 8 [12544/50000]	Loss: 1.9639	LR: 0.050000
Training Epoch: 8 [12672/50000]	Loss: 1.7605	LR: 0.050000
Training Epoch: 8 [12800/50000]	Loss: 1.7745	LR: 0.050000
Training Epoch: 8 [12928/50000]	Loss: 1.8414	LR: 0.050000
Training Epoch: 8 [13056/50000]	Loss: 1.5872	LR: 0.050000
Training Epoch: 8 [13184/50000]	Loss: 1.6191	LR: 0.050000
Training Epoch: 8 [13312/50000]	Loss: 1.4591	LR: 0.050000
Training Epoch: 8 [13440/50000]	Loss: 1.9385	LR: 0.050000
Training Epoch: 8 [13568/50000]	Loss: 1.6809	LR: 0.050000
Training Epoch: 8 [13696/50000]	Loss: 1.9138	LR: 0.050000
Training Epoch: 8 [13824/50000]	Loss: 1.9427	LR: 0.050000
Training Epoch: 8 [13952/50000]	Loss: 1.6600	LR: 0.050000
Training Epoch: 8 [14080/50000]	Loss: 2.0783	LR: 0.050000
Training Epoch: 8 [14208/50000]	Loss: 1.6768	LR: 0.050000
Training Epoch: 8 [14336/50000]	Loss: 2.0105	LR: 0.050000
Training Epoch: 8 [14464/50000]	Loss: 1.8804	LR: 0.050000
Training Epoch: 8 [14592/50000]	Loss: 1.9135	LR: 0.050000
Training Epoch: 8 [14720/50000]	Loss: 1.9374	LR: 0.050000
Training Epoch: 8 [14848/50000]	Loss: 1.9058	LR: 0.050000
Training Epoch: 8 [14976/50000]	Loss: 1.7961	LR: 0.050000
Training Epoch: 8 [15104/50000]	Loss: 1.9547	LR: 0.050000
Training Epoch: 8 [15232/50000]	Loss: 1.8863	LR: 0.050000
Training Epoch: 8 [15360/50000]	Loss: 1.8492	LR: 0.050000
Training Epoch: 8 [15488/50000]	Loss: 1.6500	LR: 0.050000
Training Epoch: 8 [15616/50000]	Loss: 1.8445	LR: 0.050000
Training Epoch: 8 [15744/50000]	Loss: 1.8266	LR: 0.050000
Training Epoch: 8 [15872/50000]	Loss: 1.7491	LR: 0.050000
Training Epoch: 8 [16000/50000]	Loss: 2.0065	LR: 0.050000
Training Epoch: 8 [16128/50000]	Loss: 1.8294	LR: 0.050000
Training Epoch: 8 [16256/50000]	Loss: 1.6681	LR: 0.050000
Training Epoch: 8 [16384/50000]	Loss: 1.8753	LR: 0.050000
Training Epoch: 8 [16512/50000]	Loss: 1.9159	LR: 0.050000
Training Epoch: 8 [16640/50000]	Loss: 1.9900	LR: 0.050000
Training Epoch: 8 [16768/50000]	Loss: 1.8181	LR: 0.050000
Training Epoch: 8 [16896/50000]	Loss: 1.6559	LR: 0.050000
Training Epoch: 8 [17024/50000]	Loss: 1.9935	LR: 0.050000
Training Epoch: 8 [17152/50000]	Loss: 1.9639	LR: 0.050000
Training Epoch: 8 [17280/50000]	Loss: 1.8193	LR: 0.050000
Training Epoch: 8 [17408/50000]	Loss: 1.6406	LR: 0.050000
Training Epoch: 8 [17536/50000]	Loss: 2.0895	LR: 0.050000
Training Epoch: 8 [17664/50000]	Loss: 1.8457	LR: 0.050000
Training Epoch: 8 [17792/50000]	Loss: 1.8863	LR: 0.050000
Training Epoch: 8 [17920/50000]	Loss: 1.9603	LR: 0.050000
Training Epoch: 8 [18048/50000]	Loss: 1.8613	LR: 0.050000
Training Epoch: 8 [18176/50000]	Loss: 2.1684	LR: 0.050000
Training Epoch: 8 [18304/50000]	Loss: 1.9881	LR: 0.050000
Training Epoch: 8 [18432/50000]	Loss: 1.6914	LR: 0.050000
Training Epoch: 8 [18560/50000]	Loss: 1.9787	LR: 0.050000
Training Epoch: 8 [18688/50000]	Loss: 1.8047	LR: 0.050000
Training Epoch: 8 [18816/50000]	Loss: 1.9161	LR: 0.050000
Training Epoch: 8 [18944/50000]	Loss: 2.0480	LR: 0.050000
Training Epoch: 8 [19072/50000]	Loss: 1.7297	LR: 0.050000
Training Epoch: 8 [19200/50000]	Loss: 1.7824	LR: 0.050000
Training Epoch: 8 [19328/50000]	Loss: 1.8626	LR: 0.050000
Training Epoch: 8 [19456/50000]	Loss: 1.8400	LR: 0.050000
Training Epoch: 8 [19584/50000]	Loss: 1.7233	LR: 0.050000
Training Epoch: 8 [19712/50000]	Loss: 1.8447	LR: 0.050000
Training Epoch: 8 [19840/50000]	Loss: 1.9563	LR: 0.050000
Training Epoch: 8 [19968/50000]	Loss: 1.6713	LR: 0.050000
Training Epoch: 8 [20096/50000]	Loss: 1.8867	LR: 0.050000
Training Epoch: 8 [20224/50000]	Loss: 2.2191	LR: 0.050000
Training Epoch: 8 [20352/50000]	Loss: 1.7046	LR: 0.050000
Training Epoch: 8 [20480/50000]	Loss: 1.8514	LR: 0.050000
Training Epoch: 8 [20608/50000]	Loss: 1.6361	LR: 0.050000
Training Epoch: 8 [20736/50000]	Loss: 1.8241	LR: 0.050000
Training Epoch: 8 [20864/50000]	Loss: 1.8361	LR: 0.050000
Training Epoch: 8 [20992/50000]	Loss: 1.8495	LR: 0.050000
Training Epoch: 8 [21120/50000]	Loss: 1.7702	LR: 0.050000
Training Epoch: 8 [21248/50000]	Loss: 1.9948	LR: 0.050000
Training Epoch: 8 [21376/50000]	Loss: 1.9357	LR: 0.050000
Training Epoch: 8 [21504/50000]	Loss: 1.5194	LR: 0.050000
Training Epoch: 8 [21632/50000]	Loss: 1.7366	LR: 0.050000
Training Epoch: 8 [21760/50000]	Loss: 1.9018	LR: 0.050000
Training Epoch: 8 [21888/50000]	Loss: 2.0489	LR: 0.050000
Training Epoch: 8 [22016/50000]	Loss: 1.8265	LR: 0.050000
Training Epoch: 8 [22144/50000]	Loss: 1.9143	LR: 0.050000
Training Epoch: 8 [22272/50000]	Loss: 1.8357	LR: 0.050000
Training Epoch: 8 [22400/50000]	Loss: 1.7329	LR: 0.050000
Training Epoch: 8 [22528/50000]	Loss: 2.0147	LR: 0.050000
Training Epoch: 8 [22656/50000]	Loss: 1.7779	LR: 0.050000
Training Epoch: 8 [22784/50000]	Loss: 1.8808	LR: 0.050000
Training Epoch: 8 [22912/50000]	Loss: 1.7685	LR: 0.050000
Training Epoch: 8 [23040/50000]	Loss: 1.5550	LR: 0.050000
Training Epoch: 8 [23168/50000]	Loss: 1.6841	LR: 0.050000
Training Epoch: 8 [23296/50000]	Loss: 1.5578	LR: 0.050000
Training Epoch: 8 [23424/50000]	Loss: 1.7828	LR: 0.050000
Training Epoch: 8 [23552/50000]	Loss: 1.6198	LR: 0.050000
Training Epoch: 8 [23680/50000]	Loss: 1.6848	LR: 0.050000
Training Epoch: 8 [23808/50000]	Loss: 1.8996	LR: 0.050000
Training Epoch: 8 [23936/50000]	Loss: 1.7954	LR: 0.050000
Training Epoch: 8 [24064/50000]	Loss: 1.8061	LR: 0.050000
Training Epoch: 8 [24192/50000]	Loss: 2.0024	LR: 0.050000
Training Epoch: 8 [24320/50000]	Loss: 1.9805	LR: 0.050000
Training Epoch: 8 [24448/50000]	Loss: 1.6676	LR: 0.050000
Training Epoch: 8 [24576/50000]	Loss: 2.0131	LR: 0.050000
Training Epoch: 8 [24704/50000]	Loss: 1.8287	LR: 0.050000
Training Epoch: 8 [24832/50000]	Loss: 1.7827	LR: 0.050000
Training Epoch: 8 [24960/50000]	Loss: 2.0908	LR: 0.050000
Training Epoch: 8 [25088/50000]	Loss: 1.9403	LR: 0.050000
Training Epoch: 8 [25216/50000]	Loss: 2.0465	LR: 0.050000
Training Epoch: 8 [25344/50000]	Loss: 1.8967	LR: 0.050000
Training Epoch: 8 [25472/50000]	Loss: 2.0240	LR: 0.050000
Training Epoch: 8 [25600/50000]	Loss: 1.6059	LR: 0.050000
Training Epoch: 8 [25728/50000]	Loss: 1.8809	LR: 0.050000
Training Epoch: 8 [25856/50000]	Loss: 1.9923	LR: 0.050000
Training Epoch: 8 [25984/50000]	Loss: 1.7978	LR: 0.050000
Training Epoch: 8 [26112/50000]	Loss: 1.7478	LR: 0.050000
Training Epoch: 8 [26240/50000]	Loss: 1.7950	LR: 0.050000
Training Epoch: 8 [26368/50000]	Loss: 1.9252	LR: 0.050000
Training Epoch: 8 [26496/50000]	Loss: 1.9603	LR: 0.050000
Training Epoch: 8 [26624/50000]	Loss: 1.7840	LR: 0.050000
Training Epoch: 8 [26752/50000]	Loss: 1.8665	LR: 0.050000
Training Epoch: 8 [26880/50000]	Loss: 1.9314	LR: 0.050000
Training Epoch: 8 [27008/50000]	Loss: 1.8557	LR: 0.050000
Training Epoch: 8 [27136/50000]	Loss: 1.9788	LR: 0.050000
Training Epoch: 8 [27264/50000]	Loss: 1.9003	LR: 0.050000
Training Epoch: 8 [27392/50000]	Loss: 1.7323	LR: 0.050000
Training Epoch: 8 [27520/50000]	Loss: 1.9221	LR: 0.050000
Training Epoch: 8 [27648/50000]	Loss: 1.7167	LR: 0.050000
Training Epoch: 8 [27776/50000]	Loss: 1.8780	LR: 0.050000
Training Epoch: 8 [27904/50000]	Loss: 1.7591	LR: 0.050000
Training Epoch: 8 [28032/50000]	Loss: 1.7854	LR: 0.050000
Training Epoch: 8 [28160/50000]	Loss: 2.1081	LR: 0.050000
Training Epoch: 8 [28288/50000]	Loss: 1.6873	LR: 0.050000
Training Epoch: 8 [28416/50000]	Loss: 1.8333	LR: 0.050000
Training Epoch: 8 [28544/50000]	Loss: 1.9416	LR: 0.050000
Training Epoch: 8 [28672/50000]	Loss: 1.8436	LR: 0.050000
Training Epoch: 8 [28800/50000]	Loss: 1.8403	LR: 0.050000
Training Epoch: 8 [28928/50000]	Loss: 1.5479	LR: 0.050000
Training Epoch: 8 [29056/50000]	Loss: 1.9809	LR: 0.050000
Training Epoch: 8 [29184/50000]	Loss: 1.6870	LR: 0.050000
Training Epoch: 8 [29312/50000]	Loss: 1.7279	LR: 0.050000
Training Epoch: 8 [29440/50000]	Loss: 1.8998	LR: 0.050000
Training Epoch: 8 [29568/50000]	Loss: 1.8016	LR: 0.050000
Training Epoch: 8 [29696/50000]	Loss: 1.8307	LR: 0.050000
Training Epoch: 8 [29824/50000]	Loss: 1.7191	LR: 0.050000
Training Epoch: 8 [29952/50000]	Loss: 2.0892	LR: 0.050000
Training Epoch: 8 [30080/50000]	Loss: 1.7356	LR: 0.050000
Training Epoch: 8 [30208/50000]	Loss: 1.7333	LR: 0.050000
Training Epoch: 8 [30336/50000]	Loss: 1.7898	LR: 0.050000
Training Epoch: 8 [30464/50000]	Loss: 1.6180	LR: 0.050000
Training Epoch: 8 [30592/50000]	Loss: 2.0442	LR: 0.050000
Training Epoch: 8 [30720/50000]	Loss: 1.6420	LR: 0.050000
Training Epoch: 8 [30848/50000]	Loss: 1.7522	LR: 0.050000
Training Epoch: 8 [30976/50000]	Loss: 1.9548	LR: 0.050000
Training Epoch: 8 [31104/50000]	Loss: 1.9794	LR: 0.050000
Training Epoch: 8 [31232/50000]	Loss: 1.7770	LR: 0.050000
Training Epoch: 8 [31360/50000]	Loss: 1.7882	LR: 0.050000
Training Epoch: 8 [31488/50000]	Loss: 1.7232	LR: 0.050000
Training Epoch: 8 [31616/50000]	Loss: 1.8224	LR: 0.050000
Training Epoch: 8 [31744/50000]	Loss: 1.7894	LR: 0.050000
Training Epoch: 8 [31872/50000]	Loss: 1.8381	LR: 0.050000
Training Epoch: 8 [32000/50000]	Loss: 1.8530	LR: 0.050000
Training Epoch: 8 [32128/50000]	Loss: 2.0741	LR: 0.050000
Training Epoch: 8 [32256/50000]	Loss: 1.8920	LR: 0.050000
Training Epoch: 8 [32384/50000]	Loss: 1.8167	LR: 0.050000
Training Epoch: 8 [32512/50000]	Loss: 1.6145	LR: 0.050000
Training Epoch: 8 [32640/50000]	Loss: 1.8954	LR: 0.050000
Training Epoch: 8 [32768/50000]	Loss: 1.8349	LR: 0.050000
Training Epoch: 8 [32896/50000]	Loss: 1.7216	LR: 0.050000
Training Epoch: 8 [33024/50000]	Loss: 1.6820	LR: 0.050000
Training Epoch: 8 [33152/50000]	Loss: 1.8976	LR: 0.050000
Training Epoch: 8 [33280/50000]	Loss: 1.7748	LR: 0.050000
Training Epoch: 8 [33408/50000]	Loss: 1.7415	LR: 0.050000
Training Epoch: 8 [33536/50000]	Loss: 1.7585	LR: 0.050000
Training Epoch: 8 [33664/50000]	Loss: 1.8714	LR: 0.050000
Training Epoch: 8 [33792/50000]	Loss: 1.7263	LR: 0.050000
Training Epoch: 8 [33920/50000]	Loss: 1.6526	LR: 0.050000
Training Epoch: 8 [34048/50000]	Loss: 1.8917	LR: 0.050000
Training Epoch: 8 [34176/50000]	Loss: 1.8800	LR: 0.050000
Training Epoch: 8 [34304/50000]	Loss: 1.9530	LR: 0.050000
Training Epoch: 8 [34432/50000]	Loss: 1.6434	LR: 0.050000
Training Epoch: 8 [34560/50000]	Loss: 2.0696	LR: 0.050000
Training Epoch: 8 [34688/50000]	Loss: 1.5748	LR: 0.050000
Training Epoch: 8 [34816/50000]	Loss: 1.8833	LR: 0.050000
Training Epoch: 8 [34944/50000]	Loss: 1.7287	LR: 0.050000
Training Epoch: 8 [35072/50000]	Loss: 1.7079	LR: 0.050000
Training Epoch: 8 [35200/50000]	Loss: 1.8883	LR: 0.050000
Training Epoch: 8 [35328/50000]	Loss: 1.8009	LR: 0.050000
Training Epoch: 8 [35456/50000]	Loss: 1.8066	LR: 0.050000
Training Epoch: 8 [35584/50000]	Loss: 1.8583	LR: 0.050000
Training Epoch: 8 [35712/50000]	Loss: 1.7068	LR: 0.050000
Training Epoch: 8 [35840/50000]	Loss: 1.9275	LR: 0.050000
Training Epoch: 8 [35968/50000]	Loss: 1.8921	LR: 0.050000
Training Epoch: 8 [36096/50000]	Loss: 1.7428	LR: 0.050000
Training Epoch: 8 [36224/50000]	Loss: 1.7946	LR: 0.050000
Training Epoch: 8 [36352/50000]	Loss: 1.7738	LR: 0.050000
Training Epoch: 8 [36480/50000]	Loss: 1.5108	LR: 0.050000
Training Epoch: 8 [36608/50000]	Loss: 1.7902	LR: 0.050000
Training Epoch: 8 [36736/50000]	Loss: 1.9356	LR: 0.050000
Training Epoch: 8 [36864/50000]	Loss: 1.8617	LR: 0.050000
Training Epoch: 8 [36992/50000]	Loss: 1.8971	LR: 0.050000
Training Epoch: 8 [37120/50000]	Loss: 1.8577	LR: 0.050000
Training Epoch: 8 [37248/50000]	Loss: 1.9004	LR: 0.050000
Training Epoch: 8 [37376/50000]	Loss: 1.6954	LR: 0.050000
Training Epoch: 8 [37504/50000]	Loss: 1.5757	LR: 0.050000
Training Epoch: 8 [37632/50000]	Loss: 1.8414	LR: 0.050000
Training Epoch: 8 [37760/50000]	Loss: 1.7190	LR: 0.050000
Training Epoch: 8 [37888/50000]	Loss: 1.9369	LR: 0.050000
Training Epoch: 8 [38016/50000]	Loss: 1.8709	LR: 0.050000
Training Epoch: 8 [38144/50000]	Loss: 1.9531	LR: 0.050000
Training Epoch: 8 [38272/50000]	Loss: 2.3525	LR: 0.050000
Training Epoch: 8 [38400/50000]	Loss: 1.9727	LR: 0.050000
Training Epoch: 8 [38528/50000]	Loss: 1.8185	LR: 0.050000
Training Epoch: 8 [38656/50000]	Loss: 1.7653	LR: 0.050000
Training Epoch: 8 [38784/50000]	Loss: 1.8985	LR: 0.050000
Training Epoch: 8 [38912/50000]	Loss: 1.7963	LR: 0.050000
Training Epoch: 8 [39040/50000]	Loss: 2.1254	LR: 0.050000
Training Epoch: 8 [39168/50000]	Loss: 1.9372	LR: 0.050000
Training Epoch: 8 [39296/50000]	Loss: 2.0103	LR: 0.050000
Training Epoch: 8 [39424/50000]	Loss: 2.0566	LR: 0.050000
Training Epoch: 8 [39552/50000]	Loss: 1.6580	LR: 0.050000
Training Epoch: 8 [39680/50000]	Loss: 1.7856	LR: 0.050000
Training Epoch: 8 [39808/50000]	Loss: 1.8421	LR: 0.050000
Training Epoch: 8 [39936/50000]	Loss: 1.8907	LR: 0.050000
Training Epoch: 8 [40064/50000]	Loss: 1.8689	LR: 0.050000
Training Epoch: 8 [40192/50000]	Loss: 1.6325	LR: 0.050000
Training Epoch: 8 [40320/50000]	Loss: 1.7837	LR: 0.050000
Training Epoch: 8 [40448/50000]	Loss: 1.8747	LR: 0.050000
Training Epoch: 8 [40576/50000]	Loss: 1.6644	LR: 0.050000
Training Epoch: 8 [40704/50000]	Loss: 1.8203	LR: 0.050000
Training Epoch: 8 [40832/50000]	Loss: 1.8480	LR: 0.050000
Training Epoch: 8 [40960/50000]	Loss: 1.6803	LR: 0.050000
Training Epoch: 8 [41088/50000]	Loss: 1.9603	LR: 0.050000
Training Epoch: 8 [41216/50000]	Loss: 1.7905	LR: 0.050000
Training Epoch: 8 [41344/50000]	Loss: 1.6828	LR: 0.050000
Training Epoch: 8 [41472/50000]	Loss: 1.9685	LR: 0.050000
Training Epoch: 8 [41600/50000]	Loss: 1.6200	LR: 0.050000
Training Epoch: 8 [41728/50000]	Loss: 1.6744	LR: 0.050000
Training Epoch: 8 [41856/50000]	Loss: 1.6639	LR: 0.050000
Training Epoch: 8 [41984/50000]	Loss: 1.6297	LR: 0.050000
Training Epoch: 8 [42112/50000]	Loss: 1.9717	LR: 0.050000
Training Epoch: 8 [42240/50000]	Loss: 1.7946	LR: 0.050000
Training Epoch: 8 [42368/50000]	Loss: 2.0169	LR: 0.050000
Training Epoch: 8 [42496/50000]	Loss: 1.6987	LR: 0.050000
Training Epoch: 8 [42624/50000]	Loss: 1.8092	LR: 0.050000
Training Epoch: 8 [42752/50000]	Loss: 1.6559	LR: 0.050000
Training Epoch: 8 [42880/50000]	Loss: 1.7745	LR: 0.050000
Training Epoch: 8 [43008/50000]	Loss: 1.8397	LR: 0.050000
Training Epoch: 8 [43136/50000]	Loss: 1.9421	LR: 0.050000
Training Epoch: 8 [43264/50000]	Loss: 1.4223	LR: 0.050000
Training Epoch: 8 [43392/50000]	Loss: 1.8077	LR: 0.050000
Training Epoch: 8 [43520/50000]	Loss: 1.7637	LR: 0.050000
Training Epoch: 8 [43648/50000]	Loss: 1.9523	LR: 0.050000
Training Epoch: 8 [43776/50000]	Loss: 1.5678	LR: 0.050000
Training Epoch: 8 [43904/50000]	Loss: 1.6127	LR: 0.050000
Training Epoch: 8 [44032/50000]	Loss: 1.8308	LR: 0.050000
Training Epoch: 8 [44160/50000]	Loss: 1.7141	LR: 0.050000
Training Epoch: 8 [44288/50000]	Loss: 1.5910	LR: 0.050000
Training Epoch: 8 [44416/50000]	Loss: 2.0220	LR: 0.050000
Training Epoch: 8 [44544/50000]	Loss: 1.6903	LR: 0.050000
Training Epoch: 8 [44672/50000]	Loss: 1.8264	LR: 0.050000
Training Epoch: 8 [44800/50000]	Loss: 1.8002	LR: 0.050000
Training Epoch: 8 [44928/50000]	Loss: 1.6656	LR: 0.050000
Training Epoch: 8 [45056/50000]	Loss: 1.9050	LR: 0.050000
Training Epoch: 8 [45184/50000]	Loss: 1.8202	LR: 0.050000
Training Epoch: 8 [45312/50000]	Loss: 1.7309	LR: 0.050000
Training Epoch: 8 [45440/50000]	Loss: 1.8278	LR: 0.050000
Training Epoch: 8 [45568/50000]	Loss: 1.8871	LR: 0.050000
Training Epoch: 8 [45696/50000]	Loss: 2.0620	LR: 0.050000
Training Epoch: 8 [45824/50000]	Loss: 1.7995	LR: 0.050000
Training Epoch: 8 [45952/50000]	Loss: 1.6744	LR: 0.050000
Training Epoch: 8 [46080/50000]	Loss: 1.7174	LR: 0.050000
Training Epoch: 8 [46208/50000]	Loss: 2.0094	LR: 0.050000
Training Epoch: 8 [46336/50000]	Loss: 1.8533	LR: 0.050000
Training Epoch: 8 [46464/50000]	Loss: 1.6873	LR: 0.050000
Training Epoch: 8 [46592/50000]	Loss: 1.7212	LR: 0.050000
Training Epoch: 8 [46720/50000]	Loss: 1.8714	LR: 0.050000
Training Epoch: 8 [46848/50000]	Loss: 1.6792	LR: 0.050000
Training Epoch: 8 [46976/50000]	Loss: 1.6524	LR: 0.050000
Training Epoch: 8 [47104/50000]	Loss: 1.6745	LR: 0.050000
Training Epoch: 8 [47232/50000]	Loss: 1.7374	LR: 0.050000
Training Epoch: 8 [47360/50000]	Loss: 1.8596	LR: 0.050000
Training Epoch: 8 [47488/50000]	Loss: 1.9562	LR: 0.050000
Training Epoch: 8 [47616/50000]	Loss: 1.7910	LR: 0.050000
Training Epoch: 8 [47744/50000]	Loss: 1.5921	LR: 0.050000
Training Epoch: 8 [47872/50000]	Loss: 1.7512	LR: 0.050000
Training Epoch: 8 [48000/50000]	Loss: 1.8250	LR: 0.050000
Training Epoch: 8 [48128/50000]	Loss: 1.9085	LR: 0.050000
Training Epoch: 8 [48256/50000]	Loss: 1.9712	LR: 0.050000
Training Epoch: 8 [48384/50000]	Loss: 1.9275	LR: 0.050000
Training Epoch: 8 [48512/50000]	Loss: 1.9713	LR: 0.050000
Training Epoch: 8 [48640/50000]	Loss: 1.6701	LR: 0.050000
Training Epoch: 8 [48768/50000]	Loss: 1.8650	LR: 0.050000
Training Epoch: 8 [48896/50000]	Loss: 1.7915	LR: 0.050000
Training Epoch: 8 [49024/50000]	Loss: 1.8830	LR: 0.050000
Training Epoch: 8 [49152/50000]	Loss: 1.6658	LR: 0.050000
Training Epoch: 8 [49280/50000]	Loss: 1.7534	LR: 0.050000
Training Epoch: 8 [49408/50000]	Loss: 2.0999	LR: 0.050000
Training Epoch: 8 [49536/50000]	Loss: 1.5191	LR: 0.050000
Training Epoch: 8 [49664/50000]	Loss: 1.5535	LR: 0.050000
Training Epoch: 8 [49792/50000]	Loss: 2.0923	LR: 0.050000
Training Epoch: 8 [49920/50000]	Loss: 1.8304	LR: 0.050000
Training Epoch: 8 [50000/50000]	Loss: 1.5657	LR: 0.050000
epoch 8 training time consumed: 53.95s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   29361 GB |   29360 GB |
|       from large pool |  123392 KB |    1034 MB |   29332 GB |   29331 GB |
|       from small pool |   10798 KB |      13 MB |      28 GB |      28 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   29361 GB |   29360 GB |
|       from large pool |  123392 KB |    1034 MB |   29332 GB |   29331 GB |
|       from small pool |   10798 KB |      13 MB |      28 GB |      28 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   12923 GB |   12923 GB |
|       from large pool |  155136 KB |  433088 KB |   12891 GB |   12891 GB |
|       from small pool |    1490 KB |    3494 KB |      32 GB |      32 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    1133 K  |    1133 K  |
|       from large pool |      24    |      65    |     591 K  |     591 K  |
|       from small pool |     231    |     274    |     542 K  |     541 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    1133 K  |    1133 K  |
|       from large pool |      24    |      65    |     591 K  |     591 K  |
|       from small pool |     231    |     274    |     542 K  |     541 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |  563517    |  563496    |
|       from large pool |       9    |      14    |  286259    |  286250    |
|       from small pool |      12    |      16    |  277258    |  277246    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 8, Average loss: 0.0157, Accuracy: 0.4586, Time consumed:3.46s

Training Epoch: 9 [128/50000]	Loss: 1.7671	LR: 0.050000
Training Epoch: 9 [256/50000]	Loss: 1.6716	LR: 0.050000
Training Epoch: 9 [384/50000]	Loss: 1.6590	LR: 0.050000
Training Epoch: 9 [512/50000]	Loss: 1.6632	LR: 0.050000
Training Epoch: 9 [640/50000]	Loss: 1.7239	LR: 0.050000
Training Epoch: 9 [768/50000]	Loss: 1.7763	LR: 0.050000
Training Epoch: 9 [896/50000]	Loss: 2.0038	LR: 0.050000
Training Epoch: 9 [1024/50000]	Loss: 1.9237	LR: 0.050000
Training Epoch: 9 [1152/50000]	Loss: 1.6667	LR: 0.050000
Training Epoch: 9 [1280/50000]	Loss: 1.6584	LR: 0.050000
Training Epoch: 9 [1408/50000]	Loss: 1.6142	LR: 0.050000
Training Epoch: 9 [1536/50000]	Loss: 1.8009	LR: 0.050000
Training Epoch: 9 [1664/50000]	Loss: 1.5951	LR: 0.050000
Training Epoch: 9 [1792/50000]	Loss: 1.6932	LR: 0.050000
Training Epoch: 9 [1920/50000]	Loss: 1.7710	LR: 0.050000
Training Epoch: 9 [2048/50000]	Loss: 1.6948	LR: 0.050000
Training Epoch: 9 [2176/50000]	Loss: 1.8123	LR: 0.050000
Training Epoch: 9 [2304/50000]	Loss: 1.8896	LR: 0.050000
Training Epoch: 9 [2432/50000]	Loss: 1.6396	LR: 0.050000
Training Epoch: 9 [2560/50000]	Loss: 1.7719	LR: 0.050000
Training Epoch: 9 [2688/50000]	Loss: 1.5320	LR: 0.050000
Training Epoch: 9 [2816/50000]	Loss: 1.6615	LR: 0.050000
Training Epoch: 9 [2944/50000]	Loss: 1.9281	LR: 0.050000
Training Epoch: 9 [3072/50000]	Loss: 1.5532	LR: 0.050000
Training Epoch: 9 [3200/50000]	Loss: 1.8636	LR: 0.050000
Training Epoch: 9 [3328/50000]	Loss: 1.8967	LR: 0.050000
Training Epoch: 9 [3456/50000]	Loss: 1.7192	LR: 0.050000
Training Epoch: 9 [3584/50000]	Loss: 1.8350	LR: 0.050000
Training Epoch: 9 [3712/50000]	Loss: 1.6361	LR: 0.050000
Training Epoch: 9 [3840/50000]	Loss: 1.6822	LR: 0.050000
Training Epoch: 9 [3968/50000]	Loss: 1.7083	LR: 0.050000
Training Epoch: 9 [4096/50000]	Loss: 1.5567	LR: 0.050000
Training Epoch: 9 [4224/50000]	Loss: 1.7182	LR: 0.050000
Training Epoch: 9 [4352/50000]	Loss: 1.9894	LR: 0.050000
Training Epoch: 9 [4480/50000]	Loss: 1.6619	LR: 0.050000
Training Epoch: 9 [4608/50000]	Loss: 1.5518	LR: 0.050000
Training Epoch: 9 [4736/50000]	Loss: 1.5563	LR: 0.050000
Training Epoch: 9 [4864/50000]	Loss: 1.7866	LR: 0.050000
Training Epoch: 9 [4992/50000]	Loss: 1.7104	LR: 0.050000
Training Epoch: 9 [5120/50000]	Loss: 1.7133	LR: 0.050000
Training Epoch: 9 [5248/50000]	Loss: 1.8775	LR: 0.050000
Training Epoch: 9 [5376/50000]	Loss: 1.9531	LR: 0.050000
Training Epoch: 9 [5504/50000]	Loss: 1.6745	LR: 0.050000
Training Epoch: 9 [5632/50000]	Loss: 1.5415	LR: 0.050000
Training Epoch: 9 [5760/50000]	Loss: 1.6366	LR: 0.050000
Training Epoch: 9 [5888/50000]	Loss: 1.5676	LR: 0.050000
Training Epoch: 9 [6016/50000]	Loss: 1.5612	LR: 0.050000
Training Epoch: 9 [6144/50000]	Loss: 2.0909	LR: 0.050000
Training Epoch: 9 [6272/50000]	Loss: 1.7571	LR: 0.050000
Training Epoch: 9 [6400/50000]	Loss: 1.6781	LR: 0.050000
Training Epoch: 9 [6528/50000]	Loss: 1.8526	LR: 0.050000
Training Epoch: 9 [6656/50000]	Loss: 1.6886	LR: 0.050000
Training Epoch: 9 [6784/50000]	Loss: 1.8692	LR: 0.050000
Training Epoch: 9 [6912/50000]	Loss: 1.3817	LR: 0.050000
Training Epoch: 9 [7040/50000]	Loss: 1.6640	LR: 0.050000
Training Epoch: 9 [7168/50000]	Loss: 1.7834	LR: 0.050000
Training Epoch: 9 [7296/50000]	Loss: 1.7188	LR: 0.050000
Training Epoch: 9 [7424/50000]	Loss: 1.7526	LR: 0.050000
Training Epoch: 9 [7552/50000]	Loss: 1.6660	LR: 0.050000
Training Epoch: 9 [7680/50000]	Loss: 1.5693	LR: 0.050000
Training Epoch: 9 [7808/50000]	Loss: 1.7057	LR: 0.050000
Training Epoch: 9 [7936/50000]	Loss: 1.7857	LR: 0.050000
Training Epoch: 9 [8064/50000]	Loss: 1.9584	LR: 0.050000
Training Epoch: 9 [8192/50000]	Loss: 1.6985	LR: 0.050000
Training Epoch: 9 [8320/50000]	Loss: 1.8326	LR: 0.050000
Training Epoch: 9 [8448/50000]	Loss: 1.8086	LR: 0.050000
Training Epoch: 9 [8576/50000]	Loss: 1.4626	LR: 0.050000
Training Epoch: 9 [8704/50000]	Loss: 1.5730	LR: 0.050000
Training Epoch: 9 [8832/50000]	Loss: 1.8619	LR: 0.050000
Training Epoch: 9 [8960/50000]	Loss: 1.6497	LR: 0.050000
Training Epoch: 9 [9088/50000]	Loss: 1.8215	LR: 0.050000
Training Epoch: 9 [9216/50000]	Loss: 1.7879	LR: 0.050000
Training Epoch: 9 [9344/50000]	Loss: 1.6489	LR: 0.050000
Training Epoch: 9 [9472/50000]	Loss: 1.6834	LR: 0.050000
Training Epoch: 9 [9600/50000]	Loss: 1.6111	LR: 0.050000
Training Epoch: 9 [9728/50000]	Loss: 1.7657	LR: 0.050000
Training Epoch: 9 [9856/50000]	Loss: 1.6721	LR: 0.050000
Training Epoch: 9 [9984/50000]	Loss: 1.7334	LR: 0.050000
Training Epoch: 9 [10112/50000]	Loss: 1.5665	LR: 0.050000
Training Epoch: 9 [10240/50000]	Loss: 1.8752	LR: 0.050000
Training Epoch: 9 [10368/50000]	Loss: 1.8770	LR: 0.050000
Training Epoch: 9 [10496/50000]	Loss: 1.6496	LR: 0.050000
Training Epoch: 9 [10624/50000]	Loss: 1.5819	LR: 0.050000
Training Epoch: 9 [10752/50000]	Loss: 1.6391	LR: 0.050000
Training Epoch: 9 [10880/50000]	Loss: 1.6191	LR: 0.050000
Training Epoch: 9 [11008/50000]	Loss: 1.6488	LR: 0.050000
Training Epoch: 9 [11136/50000]	Loss: 1.7875	LR: 0.050000
Training Epoch: 9 [11264/50000]	Loss: 1.8892	LR: 0.050000
Training Epoch: 9 [11392/50000]	Loss: 1.5771	LR: 0.050000
Training Epoch: 9 [11520/50000]	Loss: 1.7652	LR: 0.050000
Training Epoch: 9 [11648/50000]	Loss: 1.7975	LR: 0.050000
Training Epoch: 9 [11776/50000]	Loss: 1.8377	LR: 0.050000
Training Epoch: 9 [11904/50000]	Loss: 1.8485	LR: 0.050000
Training Epoch: 9 [12032/50000]	Loss: 1.6056	LR: 0.050000
Training Epoch: 9 [12160/50000]	Loss: 1.4954	LR: 0.050000
Training Epoch: 9 [12288/50000]	Loss: 1.3651	LR: 0.050000
Training Epoch: 9 [12416/50000]	Loss: 1.8496	LR: 0.050000
Training Epoch: 9 [12544/50000]	Loss: 1.7522	LR: 0.050000
Training Epoch: 9 [12672/50000]	Loss: 1.6609	LR: 0.050000
Training Epoch: 9 [12800/50000]	Loss: 1.5671	LR: 0.050000
Training Epoch: 9 [12928/50000]	Loss: 1.9507	LR: 0.050000
Training Epoch: 9 [13056/50000]	Loss: 1.7257	LR: 0.050000
Training Epoch: 9 [13184/50000]	Loss: 1.4512	LR: 0.050000
Training Epoch: 9 [13312/50000]	Loss: 1.8743	LR: 0.050000
Training Epoch: 9 [13440/50000]	Loss: 1.8621	LR: 0.050000
Training Epoch: 9 [13568/50000]	Loss: 1.8577	LR: 0.050000
Training Epoch: 9 [13696/50000]	Loss: 1.8537	LR: 0.050000
Training Epoch: 9 [13824/50000]	Loss: 1.7378	LR: 0.050000
Training Epoch: 9 [13952/50000]	Loss: 1.5850	LR: 0.050000
Training Epoch: 9 [14080/50000]	Loss: 1.6758	LR: 0.050000
Training Epoch: 9 [14208/50000]	Loss: 1.6574	LR: 0.050000
Training Epoch: 9 [14336/50000]	Loss: 1.8845	LR: 0.050000
Training Epoch: 9 [14464/50000]	Loss: 1.8384	LR: 0.050000
Training Epoch: 9 [14592/50000]	Loss: 1.6892	LR: 0.050000
Training Epoch: 9 [14720/50000]	Loss: 1.9626	LR: 0.050000
Training Epoch: 9 [14848/50000]	Loss: 2.0014	LR: 0.050000
Training Epoch: 9 [14976/50000]	Loss: 1.7069	LR: 0.050000
Training Epoch: 9 [15104/50000]	Loss: 1.8079	LR: 0.050000
Training Epoch: 9 [15232/50000]	Loss: 1.7411	LR: 0.050000
Training Epoch: 9 [15360/50000]	Loss: 1.7316	LR: 0.050000
Training Epoch: 9 [15488/50000]	Loss: 1.4494	LR: 0.050000
Training Epoch: 9 [15616/50000]	Loss: 1.9438	LR: 0.050000
Training Epoch: 9 [15744/50000]	Loss: 1.5966	LR: 0.050000
Training Epoch: 9 [15872/50000]	Loss: 1.7562	LR: 0.050000
Training Epoch: 9 [16000/50000]	Loss: 1.6803	LR: 0.050000
Training Epoch: 9 [16128/50000]	Loss: 1.6194	LR: 0.050000
Training Epoch: 9 [16256/50000]	Loss: 1.9269	LR: 0.050000
Training Epoch: 9 [16384/50000]	Loss: 2.0063	LR: 0.050000
Training Epoch: 9 [16512/50000]	Loss: 1.7489	LR: 0.050000
Training Epoch: 9 [16640/50000]	Loss: 1.7265	LR: 0.050000
Training Epoch: 9 [16768/50000]	Loss: 1.6768	LR: 0.050000
Training Epoch: 9 [16896/50000]	Loss: 1.6064	LR: 0.050000
Training Epoch: 9 [17024/50000]	Loss: 1.6169	LR: 0.050000
Training Epoch: 9 [17152/50000]	Loss: 1.6370	LR: 0.050000
Training Epoch: 9 [17280/50000]	Loss: 2.0789	LR: 0.050000
Training Epoch: 9 [17408/50000]	Loss: 1.6509	LR: 0.050000
Training Epoch: 9 [17536/50000]	Loss: 1.8632	LR: 0.050000
Training Epoch: 9 [17664/50000]	Loss: 2.0387	LR: 0.050000
Training Epoch: 9 [17792/50000]	Loss: 1.8079	LR: 0.050000
Training Epoch: 9 [17920/50000]	Loss: 1.7468	LR: 0.050000
Training Epoch: 9 [18048/50000]	Loss: 1.7670	LR: 0.050000
Training Epoch: 9 [18176/50000]	Loss: 2.0746	LR: 0.050000
Training Epoch: 9 [18304/50000]	Loss: 1.5974	LR: 0.050000
Training Epoch: 9 [18432/50000]	Loss: 1.7246	LR: 0.050000
Training Epoch: 9 [18560/50000]	Loss: 1.6587	LR: 0.050000
Training Epoch: 9 [18688/50000]	Loss: 1.7465	LR: 0.050000
Training Epoch: 9 [18816/50000]	Loss: 1.8620	LR: 0.050000
Training Epoch: 9 [18944/50000]	Loss: 1.6313	LR: 0.050000
Training Epoch: 9 [19072/50000]	Loss: 1.5132	LR: 0.050000
Training Epoch: 9 [19200/50000]	Loss: 1.5530	LR: 0.050000
Training Epoch: 9 [19328/50000]	Loss: 1.6615	LR: 0.050000
Training Epoch: 9 [19456/50000]	Loss: 1.7824	LR: 0.050000
Training Epoch: 9 [19584/50000]	Loss: 1.5611	LR: 0.050000
Training Epoch: 9 [19712/50000]	Loss: 1.6897	LR: 0.050000
Training Epoch: 9 [19840/50000]	Loss: 1.6703	LR: 0.050000
Training Epoch: 9 [19968/50000]	Loss: 1.8260	LR: 0.050000
Training Epoch: 9 [20096/50000]	Loss: 1.9044	LR: 0.050000
Training Epoch: 9 [20224/50000]	Loss: 1.8670	LR: 0.050000
Training Epoch: 9 [20352/50000]	Loss: 1.6994	LR: 0.050000
Training Epoch: 9 [20480/50000]	Loss: 1.9076	LR: 0.050000
Training Epoch: 9 [20608/50000]	Loss: 1.8887	LR: 0.050000
Training Epoch: 9 [20736/50000]	Loss: 1.6626	LR: 0.050000
Training Epoch: 9 [20864/50000]	Loss: 1.8910	LR: 0.050000
Training Epoch: 9 [20992/50000]	Loss: 1.4876	LR: 0.050000
Training Epoch: 9 [21120/50000]	Loss: 1.7273	LR: 0.050000
Training Epoch: 9 [21248/50000]	Loss: 1.7168	LR: 0.050000
Training Epoch: 9 [21376/50000]	Loss: 1.9586	LR: 0.050000
Training Epoch: 9 [21504/50000]	Loss: 1.7055	LR: 0.050000
Training Epoch: 9 [21632/50000]	Loss: 1.6105	LR: 0.050000
Training Epoch: 9 [21760/50000]	Loss: 1.8599	LR: 0.050000
Training Epoch: 9 [21888/50000]	Loss: 1.5180	LR: 0.050000
Training Epoch: 9 [22016/50000]	Loss: 1.7097	LR: 0.050000
Training Epoch: 9 [22144/50000]	Loss: 1.6987	LR: 0.050000
Training Epoch: 9 [22272/50000]	Loss: 1.6432	LR: 0.050000
Training Epoch: 9 [22400/50000]	Loss: 1.8635	LR: 0.050000
Training Epoch: 9 [22528/50000]	Loss: 1.5205	LR: 0.050000
Training Epoch: 9 [22656/50000]	Loss: 1.7749	LR: 0.050000
Training Epoch: 9 [22784/50000]	Loss: 1.3211	LR: 0.050000
Training Epoch: 9 [22912/50000]	Loss: 1.9837	LR: 0.050000
Training Epoch: 9 [23040/50000]	Loss: 1.6154	LR: 0.050000
Training Epoch: 9 [23168/50000]	Loss: 1.6774	LR: 0.050000
Training Epoch: 9 [23296/50000]	Loss: 1.6761	LR: 0.050000
Training Epoch: 9 [23424/50000]	Loss: 1.4886	LR: 0.050000
Training Epoch: 9 [23552/50000]	Loss: 1.7409	LR: 0.050000
Training Epoch: 9 [23680/50000]	Loss: 1.7713	LR: 0.050000
Training Epoch: 9 [23808/50000]	Loss: 1.6680	LR: 0.050000
Training Epoch: 9 [23936/50000]	Loss: 1.7447	LR: 0.050000
Training Epoch: 9 [24064/50000]	Loss: 1.5966	LR: 0.050000
Training Epoch: 9 [24192/50000]	Loss: 1.7674	LR: 0.050000
Training Epoch: 9 [24320/50000]	Loss: 1.7088	LR: 0.050000
Training Epoch: 9 [24448/50000]	Loss: 1.7838	LR: 0.050000
Training Epoch: 9 [24576/50000]	Loss: 1.6250	LR: 0.050000
Training Epoch: 9 [24704/50000]	Loss: 1.5119	LR: 0.050000
Training Epoch: 9 [24832/50000]	Loss: 1.5300	LR: 0.050000
Training Epoch: 9 [24960/50000]	Loss: 1.6340	LR: 0.050000
Training Epoch: 9 [25088/50000]	Loss: 1.5783	LR: 0.050000
Training Epoch: 9 [25216/50000]	Loss: 1.7599	LR: 0.050000
Training Epoch: 9 [25344/50000]	Loss: 1.6389	LR: 0.050000
Training Epoch: 9 [25472/50000]	Loss: 1.5836	LR: 0.050000
Training Epoch: 9 [25600/50000]	Loss: 1.5339	LR: 0.050000
Training Epoch: 9 [25728/50000]	Loss: 1.5122	LR: 0.050000
Training Epoch: 9 [25856/50000]	Loss: 1.6284	LR: 0.050000
Training Epoch: 9 [25984/50000]	Loss: 1.8740	LR: 0.050000
Training Epoch: 9 [26112/50000]	Loss: 1.7929	LR: 0.050000
Training Epoch: 9 [26240/50000]	Loss: 1.5805	LR: 0.050000
Training Epoch: 9 [26368/50000]	Loss: 1.8354	LR: 0.050000
Training Epoch: 9 [26496/50000]	Loss: 1.6746	LR: 0.050000
Training Epoch: 9 [26624/50000]	Loss: 1.7035	LR: 0.050000
Training Epoch: 9 [26752/50000]	Loss: 1.4724	LR: 0.050000
Training Epoch: 9 [26880/50000]	Loss: 1.8354	LR: 0.050000
Training Epoch: 9 [27008/50000]	Loss: 1.9660	LR: 0.050000
Training Epoch: 9 [27136/50000]	Loss: 1.6317	LR: 0.050000
Training Epoch: 9 [27264/50000]	Loss: 1.4761	LR: 0.050000
Training Epoch: 9 [27392/50000]	Loss: 1.6658	LR: 0.050000
Training Epoch: 9 [27520/50000]	Loss: 1.5739	LR: 0.050000
Training Epoch: 9 [27648/50000]	Loss: 1.6642	LR: 0.050000
Training Epoch: 9 [27776/50000]	Loss: 1.7258	LR: 0.050000
Training Epoch: 9 [27904/50000]	Loss: 1.6871	LR: 0.050000
Training Epoch: 9 [28032/50000]	Loss: 1.7380	LR: 0.050000
Training Epoch: 9 [28160/50000]	Loss: 1.5963	LR: 0.050000
Training Epoch: 9 [28288/50000]	Loss: 1.6118	LR: 0.050000
Training Epoch: 9 [28416/50000]	Loss: 1.9558	LR: 0.050000
Training Epoch: 9 [28544/50000]	Loss: 1.7238	LR: 0.050000
Training Epoch: 9 [28672/50000]	Loss: 1.6738	LR: 0.050000
Training Epoch: 9 [28800/50000]	Loss: 1.9999	LR: 0.050000
Training Epoch: 9 [28928/50000]	Loss: 1.4783	LR: 0.050000
Training Epoch: 9 [29056/50000]	Loss: 1.7822	LR: 0.050000
Training Epoch: 9 [29184/50000]	Loss: 1.9053	LR: 0.050000
Training Epoch: 9 [29312/50000]	Loss: 1.7447	LR: 0.050000
Training Epoch: 9 [29440/50000]	Loss: 1.4182	LR: 0.050000
Training Epoch: 9 [29568/50000]	Loss: 1.7922	LR: 0.050000
Training Epoch: 9 [29696/50000]	Loss: 1.6446	LR: 0.050000
Training Epoch: 9 [29824/50000]	Loss: 1.8174	LR: 0.050000
Training Epoch: 9 [29952/50000]	Loss: 1.5506	LR: 0.050000
Training Epoch: 9 [30080/50000]	Loss: 1.8177	LR: 0.050000
Training Epoch: 9 [30208/50000]	Loss: 1.8397	LR: 0.050000
Training Epoch: 9 [30336/50000]	Loss: 1.8926	LR: 0.050000
Training Epoch: 9 [30464/50000]	Loss: 1.5969	LR: 0.050000
Training Epoch: 9 [30592/50000]	Loss: 1.5586	LR: 0.050000
Training Epoch: 9 [30720/50000]	Loss: 1.7371	LR: 0.050000
Training Epoch: 9 [30848/50000]	Loss: 1.8911	LR: 0.050000
Training Epoch: 9 [30976/50000]	Loss: 1.8055	LR: 0.050000
Training Epoch: 9 [31104/50000]	Loss: 1.5722	LR: 0.050000
Training Epoch: 9 [31232/50000]	Loss: 1.8408	LR: 0.050000
Training Epoch: 9 [31360/50000]	Loss: 1.7847	LR: 0.050000
Training Epoch: 9 [31488/50000]	Loss: 1.8075	LR: 0.050000
Training Epoch: 9 [31616/50000]	Loss: 1.4944	LR: 0.050000
Training Epoch: 9 [31744/50000]	Loss: 1.4435	LR: 0.050000
Training Epoch: 9 [31872/50000]	Loss: 1.6942	LR: 0.050000
Training Epoch: 9 [32000/50000]	Loss: 1.6047	LR: 0.050000
Training Epoch: 9 [32128/50000]	Loss: 1.9418	LR: 0.050000
Training Epoch: 9 [32256/50000]	Loss: 1.6749	LR: 0.050000
Training Epoch: 9 [32384/50000]	Loss: 1.6847	LR: 0.050000
Training Epoch: 9 [32512/50000]	Loss: 1.6701	LR: 0.050000
Training Epoch: 9 [32640/50000]	Loss: 1.6771	LR: 0.050000
Training Epoch: 9 [32768/50000]	Loss: 1.7985	LR: 0.050000
Training Epoch: 9 [32896/50000]	Loss: 1.5311	LR: 0.050000
Training Epoch: 9 [33024/50000]	Loss: 1.7549	LR: 0.050000
Training Epoch: 9 [33152/50000]	Loss: 1.5825	LR: 0.050000
Training Epoch: 9 [33280/50000]	Loss: 2.0898	LR: 0.050000
Training Epoch: 9 [33408/50000]	Loss: 1.7016	LR: 0.050000
Training Epoch: 9 [33536/50000]	Loss: 1.5405	LR: 0.050000
Training Epoch: 9 [33664/50000]	Loss: 1.5815	LR: 0.050000
Training Epoch: 9 [33792/50000]	Loss: 1.7609	LR: 0.050000
Training Epoch: 9 [33920/50000]	Loss: 1.9391	LR: 0.050000
Training Epoch: 9 [34048/50000]	Loss: 1.7538	LR: 0.050000
Training Epoch: 9 [34176/50000]	Loss: 1.5134	LR: 0.050000
Training Epoch: 9 [34304/50000]	Loss: 1.7092	LR: 0.050000
Training Epoch: 9 [34432/50000]	Loss: 1.7915	LR: 0.050000
Training Epoch: 9 [34560/50000]	Loss: 1.9841	LR: 0.050000
Training Epoch: 9 [34688/50000]	Loss: 1.8375	LR: 0.050000
Training Epoch: 9 [34816/50000]	Loss: 1.7265	LR: 0.050000
Training Epoch: 9 [34944/50000]	Loss: 1.7671	LR: 0.050000
Training Epoch: 9 [35072/50000]	Loss: 1.5507	LR: 0.050000
Training Epoch: 9 [35200/50000]	Loss: 1.6742	LR: 0.050000
Training Epoch: 9 [35328/50000]	Loss: 1.8326	LR: 0.050000
Training Epoch: 9 [35456/50000]	Loss: 1.7732	LR: 0.050000
Training Epoch: 9 [35584/50000]	Loss: 1.8708	LR: 0.050000
Training Epoch: 9 [35712/50000]	Loss: 1.5165	LR: 0.050000
Training Epoch: 9 [35840/50000]	Loss: 1.8923	LR: 0.050000
Training Epoch: 9 [35968/50000]	Loss: 1.6882	LR: 0.050000
Training Epoch: 9 [36096/50000]	Loss: 1.8870	LR: 0.050000
Training Epoch: 9 [36224/50000]	Loss: 1.9002	LR: 0.050000
Training Epoch: 9 [36352/50000]	Loss: 1.7233	LR: 0.050000
Training Epoch: 9 [36480/50000]	Loss: 1.6933	LR: 0.050000
Training Epoch: 9 [36608/50000]	Loss: 1.5794	LR: 0.050000
Training Epoch: 9 [36736/50000]	Loss: 1.6039	LR: 0.050000
Training Epoch: 9 [36864/50000]	Loss: 1.8772	LR: 0.050000
Training Epoch: 9 [36992/50000]	Loss: 1.7046	LR: 0.050000
Training Epoch: 9 [37120/50000]	Loss: 1.5724	LR: 0.050000
Training Epoch: 9 [37248/50000]	Loss: 1.5708	LR: 0.050000
Training Epoch: 9 [37376/50000]	Loss: 1.6936	LR: 0.050000
Training Epoch: 9 [37504/50000]	Loss: 1.7159	LR: 0.050000
Training Epoch: 9 [37632/50000]	Loss: 1.5885	LR: 0.050000
Training Epoch: 9 [37760/50000]	Loss: 1.5339	LR: 0.050000
Training Epoch: 9 [37888/50000]	Loss: 1.7689	LR: 0.050000
Training Epoch: 9 [38016/50000]	Loss: 1.7568	LR: 0.050000
Training Epoch: 9 [38144/50000]	Loss: 1.5764	LR: 0.050000
Training Epoch: 9 [38272/50000]	Loss: 2.0464	LR: 0.050000
Training Epoch: 9 [38400/50000]	Loss: 1.6386	LR: 0.050000
Training Epoch: 9 [38528/50000]	Loss: 1.5700	LR: 0.050000
Training Epoch: 9 [38656/50000]	Loss: 1.8016	LR: 0.050000
Training Epoch: 9 [38784/50000]	Loss: 1.6777	LR: 0.050000
Training Epoch: 9 [38912/50000]	Loss: 1.7370	LR: 0.050000
Training Epoch: 9 [39040/50000]	Loss: 1.6469	LR: 0.050000
Training Epoch: 9 [39168/50000]	Loss: 1.7095	LR: 0.050000
Training Epoch: 9 [39296/50000]	Loss: 1.6549	LR: 0.050000
Training Epoch: 9 [39424/50000]	Loss: 1.5860	LR: 0.050000
Training Epoch: 9 [39552/50000]	Loss: 1.7262	LR: 0.050000
Training Epoch: 9 [39680/50000]	Loss: 1.8634	LR: 0.050000
Training Epoch: 9 [39808/50000]	Loss: 1.7702	LR: 0.050000
Training Epoch: 9 [39936/50000]	Loss: 1.5324	LR: 0.050000
Training Epoch: 9 [40064/50000]	Loss: 1.6620	LR: 0.050000
Training Epoch: 9 [40192/50000]	Loss: 1.7405	LR: 0.050000
Training Epoch: 9 [40320/50000]	Loss: 1.6861	LR: 0.050000
Training Epoch: 9 [40448/50000]	Loss: 1.6427	LR: 0.050000
Training Epoch: 9 [40576/50000]	Loss: 1.6727	LR: 0.050000
Training Epoch: 9 [40704/50000]	Loss: 1.8176	LR: 0.050000
Training Epoch: 9 [40832/50000]	Loss: 1.7344	LR: 0.050000
Training Epoch: 9 [40960/50000]	Loss: 1.6981	LR: 0.050000
Training Epoch: 9 [41088/50000]	Loss: 1.8802	LR: 0.050000
Training Epoch: 9 [41216/50000]	Loss: 1.8968	LR: 0.050000
Training Epoch: 9 [41344/50000]	Loss: 1.4638	LR: 0.050000
Training Epoch: 9 [41472/50000]	Loss: 1.7446	LR: 0.050000
Training Epoch: 9 [41600/50000]	Loss: 1.8029	LR: 0.050000
Training Epoch: 9 [41728/50000]	Loss: 1.9135	LR: 0.050000
Training Epoch: 9 [41856/50000]	Loss: 1.6791	LR: 0.050000
Training Epoch: 9 [41984/50000]	Loss: 1.6187	LR: 0.050000
Training Epoch: 9 [42112/50000]	Loss: 1.6010	LR: 0.050000
Training Epoch: 9 [42240/50000]	Loss: 1.8003	LR: 0.050000
Training Epoch: 9 [42368/50000]	Loss: 1.6098	LR: 0.050000
Training Epoch: 9 [42496/50000]	Loss: 1.6422	LR: 0.050000
Training Epoch: 9 [42624/50000]	Loss: 1.7298	LR: 0.050000
Training Epoch: 9 [42752/50000]	Loss: 1.7575	LR: 0.050000
Training Epoch: 9 [42880/50000]	Loss: 1.4542	LR: 0.050000
Training Epoch: 9 [43008/50000]	Loss: 1.9796	LR: 0.050000
Training Epoch: 9 [43136/50000]	Loss: 1.4286	LR: 0.050000
Training Epoch: 9 [43264/50000]	Loss: 1.6237	LR: 0.050000
Training Epoch: 9 [43392/50000]	Loss: 1.8273	LR: 0.050000
Training Epoch: 9 [43520/50000]	Loss: 1.6458	LR: 0.050000
Training Epoch: 9 [43648/50000]	Loss: 1.9449	LR: 0.050000
Training Epoch: 9 [43776/50000]	Loss: 1.4899	LR: 0.050000
Training Epoch: 9 [43904/50000]	Loss: 1.7320	LR: 0.050000
Training Epoch: 9 [44032/50000]	Loss: 1.4760	LR: 0.050000
Training Epoch: 9 [44160/50000]	Loss: 1.5038	LR: 0.050000
Training Epoch: 9 [44288/50000]	Loss: 1.5494	LR: 0.050000
Training Epoch: 9 [44416/50000]	Loss: 1.7796	LR: 0.050000
Training Epoch: 9 [44544/50000]	Loss: 1.8527	LR: 0.050000
Training Epoch: 9 [44672/50000]	Loss: 1.6865	LR: 0.050000
Training Epoch: 9 [44800/50000]	Loss: 1.7479	LR: 0.050000
Training Epoch: 9 [44928/50000]	Loss: 1.7063	LR: 0.050000
Training Epoch: 9 [45056/50000]	Loss: 1.6273	LR: 0.050000
Training Epoch: 9 [45184/50000]	Loss: 1.5944	LR: 0.050000
Training Epoch: 9 [45312/50000]	Loss: 1.5893	LR: 0.050000
Training Epoch: 9 [45440/50000]	Loss: 1.6152	LR: 0.050000
Training Epoch: 9 [45568/50000]	Loss: 1.6788	LR: 0.050000
Training Epoch: 9 [45696/50000]	Loss: 1.5824	LR: 0.050000
Training Epoch: 9 [45824/50000]	Loss: 1.6769	LR: 0.050000
Training Epoch: 9 [45952/50000]	Loss: 1.6388	LR: 0.050000
Training Epoch: 9 [46080/50000]	Loss: 1.5955	LR: 0.050000
Training Epoch: 9 [46208/50000]	Loss: 1.4642	LR: 0.050000
Training Epoch: 9 [46336/50000]	Loss: 1.4854	LR: 0.050000
Training Epoch: 9 [46464/50000]	Loss: 1.6044	LR: 0.050000
Training Epoch: 9 [46592/50000]	Loss: 1.6945	LR: 0.050000
Training Epoch: 9 [46720/50000]	Loss: 1.5852	LR: 0.050000
Training Epoch: 9 [46848/50000]	Loss: 1.5563	LR: 0.050000
Training Epoch: 9 [46976/50000]	Loss: 1.4492	LR: 0.050000
Training Epoch: 9 [47104/50000]	Loss: 1.6229	LR: 0.050000
Training Epoch: 9 [47232/50000]	Loss: 1.8231	LR: 0.050000
Training Epoch: 9 [47360/50000]	Loss: 1.7085	LR: 0.050000
Training Epoch: 9 [47488/50000]	Loss: 1.9413	LR: 0.050000
Training Epoch: 9 [47616/50000]	Loss: 1.5060	LR: 0.050000
Training Epoch: 9 [47744/50000]	Loss: 1.6247	LR: 0.050000
Training Epoch: 9 [47872/50000]	Loss: 1.8686	LR: 0.050000
Training Epoch: 9 [48000/50000]	Loss: 1.7376	LR: 0.050000
Training Epoch: 9 [48128/50000]	Loss: 1.7497	LR: 0.050000
Training Epoch: 9 [48256/50000]	Loss: 1.8105	LR: 0.050000
Training Epoch: 9 [48384/50000]	Loss: 1.6511	LR: 0.050000
Training Epoch: 9 [48512/50000]	Loss: 2.0074	LR: 0.050000
Training Epoch: 9 [48640/50000]	Loss: 1.8856	LR: 0.050000
Training Epoch: 9 [48768/50000]	Loss: 1.8153	LR: 0.050000
Training Epoch: 9 [48896/50000]	Loss: 1.9442	LR: 0.050000
Training Epoch: 9 [49024/50000]	Loss: 1.7512	LR: 0.050000
Training Epoch: 9 [49152/50000]	Loss: 1.6185	LR: 0.050000
Training Epoch: 9 [49280/50000]	Loss: 1.8706	LR: 0.050000
Training Epoch: 9 [49408/50000]	Loss: 1.7486	LR: 0.050000
Training Epoch: 9 [49536/50000]	Loss: 1.7651	LR: 0.050000
Training Epoch: 9 [49664/50000]	Loss: 1.5132	LR: 0.050000
Training Epoch: 9 [49792/50000]	Loss: 1.7636	LR: 0.050000
Training Epoch: 9 [49920/50000]	Loss: 1.4418	LR: 0.050000
Training Epoch: 9 [50000/50000]	Loss: 1.7036	LR: 0.050000
epoch 9 training time consumed: 53.86s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   33031 GB |   33031 GB |
|       from large pool |  123392 KB |    1034 MB |   32998 GB |   32998 GB |
|       from small pool |   10798 KB |      13 MB |      32 GB |      32 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   33031 GB |   33031 GB |
|       from large pool |  123392 KB |    1034 MB |   32998 GB |   32998 GB |
|       from small pool |   10798 KB |      13 MB |      32 GB |      32 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   14538 GB |   14538 GB |
|       from large pool |  155136 KB |  433088 KB |   14502 GB |   14502 GB |
|       from small pool |    1490 KB |    3494 KB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    1275 K  |    1274 K  |
|       from large pool |      24    |      65    |     665 K  |     665 K  |
|       from small pool |     231    |     274    |     609 K  |     609 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    1275 K  |    1274 K  |
|       from large pool |      24    |      65    |     665 K  |     665 K  |
|       from small pool |     231    |     274    |     609 K  |     609 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |  633479    |  633458    |
|       from large pool |       9    |      14    |  322037    |  322028    |
|       from small pool |      12    |      16    |  311442    |  311430    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 9, Average loss: 0.0144, Accuracy: 0.5048, Time consumed:3.43s

Training Epoch: 10 [128/50000]	Loss: 1.4978	LR: 0.050000
Training Epoch: 10 [256/50000]	Loss: 1.7274	LR: 0.050000
Training Epoch: 10 [384/50000]	Loss: 1.7408	LR: 0.050000
Training Epoch: 10 [512/50000]	Loss: 1.5786	LR: 0.050000
Training Epoch: 10 [640/50000]	Loss: 1.8453	LR: 0.050000
Training Epoch: 10 [768/50000]	Loss: 1.1136	LR: 0.050000
Training Epoch: 10 [896/50000]	Loss: 1.5691	LR: 0.050000
Training Epoch: 10 [1024/50000]	Loss: 1.5514	LR: 0.050000
Training Epoch: 10 [1152/50000]	Loss: 1.7132	LR: 0.050000
Training Epoch: 10 [1280/50000]	Loss: 1.7419	LR: 0.050000
Training Epoch: 10 [1408/50000]	Loss: 1.6552	LR: 0.050000
Training Epoch: 10 [1536/50000]	Loss: 1.5111	LR: 0.050000
Training Epoch: 10 [1664/50000]	Loss: 1.4376	LR: 0.050000
Training Epoch: 10 [1792/50000]	Loss: 1.3978	LR: 0.050000
Training Epoch: 10 [1920/50000]	Loss: 1.5647	LR: 0.050000
Training Epoch: 10 [2048/50000]	Loss: 1.5764	LR: 0.050000
Training Epoch: 10 [2176/50000]	Loss: 1.5397	LR: 0.050000
Training Epoch: 10 [2304/50000]	Loss: 1.7713	LR: 0.050000
Training Epoch: 10 [2432/50000]	Loss: 1.7481	LR: 0.050000
Training Epoch: 10 [2560/50000]	Loss: 1.6624	LR: 0.050000
Training Epoch: 10 [2688/50000]	Loss: 1.5476	LR: 0.050000
Training Epoch: 10 [2816/50000]	Loss: 1.6790	LR: 0.050000
Training Epoch: 10 [2944/50000]	Loss: 1.6117	LR: 0.050000
Training Epoch: 10 [3072/50000]	Loss: 1.6008	LR: 0.050000
Training Epoch: 10 [3200/50000]	Loss: 1.3170	LR: 0.050000
Training Epoch: 10 [3328/50000]	Loss: 1.5712	LR: 0.050000
Training Epoch: 10 [3456/50000]	Loss: 1.7834	LR: 0.050000
Training Epoch: 10 [3584/50000]	Loss: 1.5549	LR: 0.050000
Training Epoch: 10 [3712/50000]	Loss: 1.7737	LR: 0.050000
Training Epoch: 10 [3840/50000]	Loss: 1.6544	LR: 0.050000
Training Epoch: 10 [3968/50000]	Loss: 1.3565	LR: 0.050000
Training Epoch: 10 [4096/50000]	Loss: 1.3415	LR: 0.050000
Training Epoch: 10 [4224/50000]	Loss: 1.6126	LR: 0.050000
Training Epoch: 10 [4352/50000]	Loss: 1.5389	LR: 0.050000
Training Epoch: 10 [4480/50000]	Loss: 1.4513	LR: 0.050000
Training Epoch: 10 [4608/50000]	Loss: 1.6416	LR: 0.050000
Training Epoch: 10 [4736/50000]	Loss: 1.4832	LR: 0.050000
Training Epoch: 10 [4864/50000]	Loss: 1.8085	LR: 0.050000
Training Epoch: 10 [4992/50000]	Loss: 1.5211	LR: 0.050000
Training Epoch: 10 [5120/50000]	Loss: 1.4174	LR: 0.050000
Training Epoch: 10 [5248/50000]	Loss: 1.8043	LR: 0.050000
Training Epoch: 10 [5376/50000]	Loss: 1.6747	LR: 0.050000
Training Epoch: 10 [5504/50000]	Loss: 1.5516	LR: 0.050000
Training Epoch: 10 [5632/50000]	Loss: 1.4891	LR: 0.050000
Training Epoch: 10 [5760/50000]	Loss: 1.5165	LR: 0.050000
Training Epoch: 10 [5888/50000]	Loss: 1.5636	LR: 0.050000
Training Epoch: 10 [6016/50000]	Loss: 1.5481	LR: 0.050000
Training Epoch: 10 [6144/50000]	Loss: 1.5540	LR: 0.050000
Training Epoch: 10 [6272/50000]	Loss: 1.7501	LR: 0.050000
Training Epoch: 10 [6400/50000]	Loss: 1.6176	LR: 0.050000
Training Epoch: 10 [6528/50000]	Loss: 1.8836	LR: 0.050000
Training Epoch: 10 [6656/50000]	Loss: 1.5244	LR: 0.050000
Training Epoch: 10 [6784/50000]	Loss: 1.8971	LR: 0.050000
Training Epoch: 10 [6912/50000]	Loss: 1.6784	LR: 0.050000
Training Epoch: 10 [7040/50000]	Loss: 1.6462	LR: 0.050000
Training Epoch: 10 [7168/50000]	Loss: 1.7070	LR: 0.050000
Training Epoch: 10 [7296/50000]	Loss: 1.6353	LR: 0.050000
Training Epoch: 10 [7424/50000]	Loss: 1.6404	LR: 0.050000
Training Epoch: 10 [7552/50000]	Loss: 1.7856	LR: 0.050000
Training Epoch: 10 [7680/50000]	Loss: 1.5628	LR: 0.050000
Training Epoch: 10 [7808/50000]	Loss: 1.7741	LR: 0.050000
Training Epoch: 10 [7936/50000]	Loss: 1.7853	LR: 0.050000
Training Epoch: 10 [8064/50000]	Loss: 1.5739	LR: 0.050000
Training Epoch: 10 [8192/50000]	Loss: 1.6931	LR: 0.050000
Training Epoch: 10 [8320/50000]	Loss: 1.3044	LR: 0.050000
Training Epoch: 10 [8448/50000]	Loss: 1.2861	LR: 0.050000
Training Epoch: 10 [8576/50000]	Loss: 1.9629	LR: 0.050000
Training Epoch: 10 [8704/50000]	Loss: 1.5100	LR: 0.050000
Training Epoch: 10 [8832/50000]	Loss: 1.6122	LR: 0.050000
Training Epoch: 10 [8960/50000]	Loss: 1.6933	LR: 0.050000
Training Epoch: 10 [9088/50000]	Loss: 1.8069	LR: 0.050000
Training Epoch: 10 [9216/50000]	Loss: 1.6028	LR: 0.050000
Training Epoch: 10 [9344/50000]	Loss: 1.7928	LR: 0.050000
Training Epoch: 10 [9472/50000]	Loss: 1.5337	LR: 0.050000
Training Epoch: 10 [9600/50000]	Loss: 1.7245	LR: 0.050000
Training Epoch: 10 [9728/50000]	Loss: 1.4142	LR: 0.050000
Training Epoch: 10 [9856/50000]	Loss: 1.7809	LR: 0.050000
Training Epoch: 10 [9984/50000]	Loss: 1.7622	LR: 0.050000
Training Epoch: 10 [10112/50000]	Loss: 1.5766	LR: 0.050000
Training Epoch: 10 [10240/50000]	Loss: 1.5259	LR: 0.050000
Training Epoch: 10 [10368/50000]	Loss: 1.7384	LR: 0.050000
Training Epoch: 10 [10496/50000]	Loss: 1.6697	LR: 0.050000
Training Epoch: 10 [10624/50000]	Loss: 1.3974	LR: 0.050000
Training Epoch: 10 [10752/50000]	Loss: 1.4980	LR: 0.050000
Training Epoch: 10 [10880/50000]	Loss: 1.5744	LR: 0.050000
Training Epoch: 10 [11008/50000]	Loss: 1.9301	LR: 0.050000
Training Epoch: 10 [11136/50000]	Loss: 1.9485	LR: 0.050000
Training Epoch: 10 [11264/50000]	Loss: 1.6581	LR: 0.050000
Training Epoch: 10 [11392/50000]	Loss: 1.6984	LR: 0.050000
Training Epoch: 10 [11520/50000]	Loss: 1.7798	LR: 0.050000
Training Epoch: 10 [11648/50000]	Loss: 1.4067	LR: 0.050000
Training Epoch: 10 [11776/50000]	Loss: 1.6355	LR: 0.050000
Training Epoch: 10 [11904/50000]	Loss: 1.7511	LR: 0.050000
Training Epoch: 10 [12032/50000]	Loss: 1.4737	LR: 0.050000
Training Epoch: 10 [12160/50000]	Loss: 1.8068	LR: 0.050000
Training Epoch: 10 [12288/50000]	Loss: 1.5725	LR: 0.050000
Training Epoch: 10 [12416/50000]	Loss: 1.6325	LR: 0.050000
Training Epoch: 10 [12544/50000]	Loss: 1.6234	LR: 0.050000
Training Epoch: 10 [12672/50000]	Loss: 1.4312	LR: 0.050000
Training Epoch: 10 [12800/50000]	Loss: 1.7126	LR: 0.050000
Training Epoch: 10 [12928/50000]	Loss: 1.6688	LR: 0.050000
Training Epoch: 10 [13056/50000]	Loss: 1.6504	LR: 0.050000
Training Epoch: 10 [13184/50000]	Loss: 1.5887	LR: 0.050000
Training Epoch: 10 [13312/50000]	Loss: 1.6802	LR: 0.050000
Training Epoch: 10 [13440/50000]	Loss: 1.6710	LR: 0.050000
Training Epoch: 10 [13568/50000]	Loss: 1.6953	LR: 0.050000
Training Epoch: 10 [13696/50000]	Loss: 1.5505	LR: 0.050000
Training Epoch: 10 [13824/50000]	Loss: 1.3612	LR: 0.050000
Training Epoch: 10 [13952/50000]	Loss: 1.7787	LR: 0.050000
Training Epoch: 10 [14080/50000]	Loss: 1.6769	LR: 0.050000
Training Epoch: 10 [14208/50000]	Loss: 1.3285	LR: 0.050000
Training Epoch: 10 [14336/50000]	Loss: 1.4801	LR: 0.050000
Training Epoch: 10 [14464/50000]	Loss: 1.8003	LR: 0.050000
Training Epoch: 10 [14592/50000]	Loss: 1.6436	LR: 0.050000
Training Epoch: 10 [14720/50000]	Loss: 1.7450	LR: 0.050000
Training Epoch: 10 [14848/50000]	Loss: 1.4586	LR: 0.050000
Training Epoch: 10 [14976/50000]	Loss: 1.4652	LR: 0.050000
Training Epoch: 10 [15104/50000]	Loss: 1.4823	LR: 0.050000
Training Epoch: 10 [15232/50000]	Loss: 1.6483	LR: 0.050000
Training Epoch: 10 [15360/50000]	Loss: 1.6603	LR: 0.050000
Training Epoch: 10 [15488/50000]	Loss: 1.6742	LR: 0.050000
Training Epoch: 10 [15616/50000]	Loss: 1.7699	LR: 0.050000
Training Epoch: 10 [15744/50000]	Loss: 1.7776	LR: 0.050000
Training Epoch: 10 [15872/50000]	Loss: 1.7023	LR: 0.050000
Training Epoch: 10 [16000/50000]	Loss: 1.6735	LR: 0.050000
Training Epoch: 10 [16128/50000]	Loss: 1.7303	LR: 0.050000
Training Epoch: 10 [16256/50000]	Loss: 1.8349	LR: 0.050000
Training Epoch: 10 [16384/50000]	Loss: 1.8239	LR: 0.050000
Training Epoch: 10 [16512/50000]	Loss: 1.9053	LR: 0.050000
Training Epoch: 10 [16640/50000]	Loss: 1.6990	LR: 0.050000
Training Epoch: 10 [16768/50000]	Loss: 1.5630	LR: 0.050000
Training Epoch: 10 [16896/50000]	Loss: 1.6804	LR: 0.050000
Training Epoch: 10 [17024/50000]	Loss: 1.6172	LR: 0.050000
Training Epoch: 10 [17152/50000]	Loss: 1.4515	LR: 0.050000
Training Epoch: 10 [17280/50000]	Loss: 1.4278	LR: 0.050000
Training Epoch: 10 [17408/50000]	Loss: 1.5428	LR: 0.050000
Training Epoch: 10 [17536/50000]	Loss: 1.7294	LR: 0.050000
Training Epoch: 10 [17664/50000]	Loss: 1.4940	LR: 0.050000
Training Epoch: 10 [17792/50000]	Loss: 1.4695	LR: 0.050000
Training Epoch: 10 [17920/50000]	Loss: 1.4814	LR: 0.050000
Training Epoch: 10 [18048/50000]	Loss: 1.7291	LR: 0.050000
Training Epoch: 10 [18176/50000]	Loss: 1.5437	LR: 0.050000
Training Epoch: 10 [18304/50000]	Loss: 1.5102	LR: 0.050000
Training Epoch: 10 [18432/50000]	Loss: 1.6351	LR: 0.050000
Training Epoch: 10 [18560/50000]	Loss: 1.5454	LR: 0.050000
Training Epoch: 10 [18688/50000]	Loss: 1.4764	LR: 0.050000
Training Epoch: 10 [18816/50000]	Loss: 1.4474	LR: 0.050000
Training Epoch: 10 [18944/50000]	Loss: 1.4719	LR: 0.050000
Training Epoch: 10 [19072/50000]	Loss: 1.4218	LR: 0.050000
Training Epoch: 10 [19200/50000]	Loss: 1.7272	LR: 0.050000
Training Epoch: 10 [19328/50000]	Loss: 1.6715	LR: 0.050000
Training Epoch: 10 [19456/50000]	Loss: 1.6453	LR: 0.050000
Training Epoch: 10 [19584/50000]	Loss: 1.7901	LR: 0.050000
Training Epoch: 10 [19712/50000]	Loss: 1.5812	LR: 0.050000
Training Epoch: 10 [19840/50000]	Loss: 1.5749	LR: 0.050000
Training Epoch: 10 [19968/50000]	Loss: 1.4484	LR: 0.050000
Training Epoch: 10 [20096/50000]	Loss: 1.4190	LR: 0.050000
Training Epoch: 10 [20224/50000]	Loss: 1.3594	LR: 0.050000
Training Epoch: 10 [20352/50000]	Loss: 1.8143	LR: 0.050000
Training Epoch: 10 [20480/50000]	Loss: 1.6101	LR: 0.050000
Training Epoch: 10 [20608/50000]	Loss: 1.5978	LR: 0.050000
Training Epoch: 10 [20736/50000]	Loss: 1.5504	LR: 0.050000
Training Epoch: 10 [20864/50000]	Loss: 1.6150	LR: 0.050000
Training Epoch: 10 [20992/50000]	Loss: 1.5705	LR: 0.050000
Training Epoch: 10 [21120/50000]	Loss: 1.6848	LR: 0.050000
Training Epoch: 10 [21248/50000]	Loss: 1.7617	LR: 0.050000
Training Epoch: 10 [21376/50000]	Loss: 1.8080	LR: 0.050000
Training Epoch: 10 [21504/50000]	Loss: 1.6012	LR: 0.050000
Training Epoch: 10 [21632/50000]	Loss: 1.5131	LR: 0.050000
Training Epoch: 10 [21760/50000]	Loss: 1.7922	LR: 0.050000
Training Epoch: 10 [21888/50000]	Loss: 1.8190	LR: 0.050000
Training Epoch: 10 [22016/50000]	Loss: 1.5726	LR: 0.050000
Training Epoch: 10 [22144/50000]	Loss: 1.6307	LR: 0.050000
Training Epoch: 10 [22272/50000]	Loss: 1.8944	LR: 0.050000
Training Epoch: 10 [22400/50000]	Loss: 1.7954	LR: 0.050000
Training Epoch: 10 [22528/50000]	Loss: 1.5808	LR: 0.050000
Training Epoch: 10 [22656/50000]	Loss: 1.6139	LR: 0.050000
Training Epoch: 10 [22784/50000]	Loss: 1.6412	LR: 0.050000
Training Epoch: 10 [22912/50000]	Loss: 1.7829	LR: 0.050000
Training Epoch: 10 [23040/50000]	Loss: 1.7595	LR: 0.050000
Training Epoch: 10 [23168/50000]	Loss: 1.5339	LR: 0.050000
Training Epoch: 10 [23296/50000]	Loss: 1.8809	LR: 0.050000
Training Epoch: 10 [23424/50000]	Loss: 1.5732	LR: 0.050000
Training Epoch: 10 [23552/50000]	Loss: 1.5904	LR: 0.050000
Training Epoch: 10 [23680/50000]	Loss: 1.7475	LR: 0.050000
Training Epoch: 10 [23808/50000]	Loss: 1.5955	LR: 0.050000
Training Epoch: 10 [23936/50000]	Loss: 1.8466	LR: 0.050000
Training Epoch: 10 [24064/50000]	Loss: 1.8442	LR: 0.050000
Training Epoch: 10 [24192/50000]	Loss: 1.8029	LR: 0.050000
Training Epoch: 10 [24320/50000]	Loss: 1.9216	LR: 0.050000
Training Epoch: 10 [24448/50000]	Loss: 1.8366	LR: 0.050000
Training Epoch: 10 [24576/50000]	Loss: 1.4997	LR: 0.050000
Training Epoch: 10 [24704/50000]	Loss: 1.7189	LR: 0.050000
Training Epoch: 10 [24832/50000]	Loss: 1.6742	LR: 0.050000
Training Epoch: 10 [24960/50000]	Loss: 1.6366	LR: 0.050000
Training Epoch: 10 [25088/50000]	Loss: 1.6194	LR: 0.050000
Training Epoch: 10 [25216/50000]	Loss: 1.5933	LR: 0.050000
Training Epoch: 10 [25344/50000]	Loss: 1.7733	LR: 0.050000
Training Epoch: 10 [25472/50000]	Loss: 1.7577	LR: 0.050000
Training Epoch: 10 [25600/50000]	Loss: 1.8101	LR: 0.050000
Training Epoch: 10 [25728/50000]	Loss: 1.5192	LR: 0.050000
Training Epoch: 10 [25856/50000]	Loss: 1.9066	LR: 0.050000
Training Epoch: 10 [25984/50000]	Loss: 1.5079	LR: 0.050000
Training Epoch: 10 [26112/50000]	Loss: 1.5560	LR: 0.050000
Training Epoch: 10 [26240/50000]	Loss: 1.5419	LR: 0.050000
Training Epoch: 10 [26368/50000]	Loss: 1.7723	LR: 0.050000
Training Epoch: 10 [26496/50000]	Loss: 1.6481	LR: 0.050000
Training Epoch: 10 [26624/50000]	Loss: 1.7691	LR: 0.050000
Training Epoch: 10 [26752/50000]	Loss: 1.6309	LR: 0.050000
Training Epoch: 10 [26880/50000]	Loss: 1.5575	LR: 0.050000
Training Epoch: 10 [27008/50000]	Loss: 1.6883	LR: 0.050000
Training Epoch: 10 [27136/50000]	Loss: 1.5299	LR: 0.050000
Training Epoch: 10 [27264/50000]	Loss: 1.8431	LR: 0.050000
Training Epoch: 10 [27392/50000]	Loss: 1.4367	LR: 0.050000
Training Epoch: 10 [27520/50000]	Loss: 1.5209	LR: 0.050000
Training Epoch: 10 [27648/50000]	Loss: 1.6270	LR: 0.050000
Training Epoch: 10 [27776/50000]	Loss: 1.8186	LR: 0.050000
Training Epoch: 10 [27904/50000]	Loss: 1.5549	LR: 0.050000
Training Epoch: 10 [28032/50000]	Loss: 1.8607	LR: 0.050000
Training Epoch: 10 [28160/50000]	Loss: 1.7115	LR: 0.050000
Training Epoch: 10 [28288/50000]	Loss: 1.6947	LR: 0.050000
Training Epoch: 10 [28416/50000]	Loss: 1.6767	LR: 0.050000
Training Epoch: 10 [28544/50000]	Loss: 1.5046	LR: 0.050000
Training Epoch: 10 [28672/50000]	Loss: 1.6116	LR: 0.050000
Training Epoch: 10 [28800/50000]	Loss: 1.7494	LR: 0.050000
Training Epoch: 10 [28928/50000]	Loss: 1.6924	LR: 0.050000
Training Epoch: 10 [29056/50000]	Loss: 1.6786	LR: 0.050000
Training Epoch: 10 [29184/50000]	Loss: 1.6934	LR: 0.050000
Training Epoch: 10 [29312/50000]	Loss: 1.6432	LR: 0.050000
Training Epoch: 10 [29440/50000]	Loss: 1.5677	LR: 0.050000
Training Epoch: 10 [29568/50000]	Loss: 1.5368	LR: 0.050000
Training Epoch: 10 [29696/50000]	Loss: 2.0630	LR: 0.050000
Training Epoch: 10 [29824/50000]	Loss: 1.6288	LR: 0.050000
Training Epoch: 10 [29952/50000]	Loss: 1.6562	LR: 0.050000
Training Epoch: 10 [30080/50000]	Loss: 1.6498	LR: 0.050000
Training Epoch: 10 [30208/50000]	Loss: 1.8070	LR: 0.050000
Training Epoch: 10 [30336/50000]	Loss: 1.4071	LR: 0.050000
Training Epoch: 10 [30464/50000]	Loss: 1.6185	LR: 0.050000
Training Epoch: 10 [30592/50000]	Loss: 1.3377	LR: 0.050000
Training Epoch: 10 [30720/50000]	Loss: 1.6596	LR: 0.050000
Training Epoch: 10 [30848/50000]	Loss: 1.7232	LR: 0.050000
Training Epoch: 10 [30976/50000]	Loss: 1.4338	LR: 0.050000
Training Epoch: 10 [31104/50000]	Loss: 1.3968	LR: 0.050000
Training Epoch: 10 [31232/50000]	Loss: 1.7047	LR: 0.050000
Training Epoch: 10 [31360/50000]	Loss: 1.4848	LR: 0.050000
Training Epoch: 10 [31488/50000]	Loss: 1.5020	LR: 0.050000
Training Epoch: 10 [31616/50000]	Loss: 1.3929	LR: 0.050000
Training Epoch: 10 [31744/50000]	Loss: 1.5899	LR: 0.050000
Training Epoch: 10 [31872/50000]	Loss: 1.6995	LR: 0.050000
Training Epoch: 10 [32000/50000]	Loss: 1.7665	LR: 0.050000
Training Epoch: 10 [32128/50000]	Loss: 1.8514	LR: 0.050000
Training Epoch: 10 [32256/50000]	Loss: 1.5528	LR: 0.050000
Training Epoch: 10 [32384/50000]	Loss: 1.5450	LR: 0.050000
Training Epoch: 10 [32512/50000]	Loss: 1.6826	LR: 0.050000
Training Epoch: 10 [32640/50000]	Loss: 1.5344	LR: 0.050000
Training Epoch: 10 [32768/50000]	Loss: 1.6073	LR: 0.050000
Training Epoch: 10 [32896/50000]	Loss: 1.6848	LR: 0.050000
Training Epoch: 10 [33024/50000]	Loss: 1.6617	LR: 0.050000
Training Epoch: 10 [33152/50000]	Loss: 1.4947	LR: 0.050000
Training Epoch: 10 [33280/50000]	Loss: 1.7373	LR: 0.050000
Training Epoch: 10 [33408/50000]	Loss: 1.6670	LR: 0.050000
Training Epoch: 10 [33536/50000]	Loss: 1.4750	LR: 0.050000
Training Epoch: 10 [33664/50000]	Loss: 1.8325	LR: 0.050000
Training Epoch: 10 [33792/50000]	Loss: 1.8911	LR: 0.050000
Training Epoch: 10 [33920/50000]	Loss: 1.5682	LR: 0.050000
Training Epoch: 10 [34048/50000]	Loss: 1.6426	LR: 0.050000
Training Epoch: 10 [34176/50000]	Loss: 1.6835	LR: 0.050000
Training Epoch: 10 [34304/50000]	Loss: 1.6351	LR: 0.050000
Training Epoch: 10 [34432/50000]	Loss: 1.6246	LR: 0.050000
Training Epoch: 10 [34560/50000]	Loss: 1.5935	LR: 0.050000
Training Epoch: 10 [34688/50000]	Loss: 1.5314	LR: 0.050000
Training Epoch: 10 [34816/50000]	Loss: 1.5465	LR: 0.050000
Training Epoch: 10 [34944/50000]	Loss: 1.5632	LR: 0.050000
Training Epoch: 10 [35072/50000]	Loss: 1.5374	LR: 0.050000
Training Epoch: 10 [35200/50000]	Loss: 1.5299	LR: 0.050000
Training Epoch: 10 [35328/50000]	Loss: 1.5992	LR: 0.050000
Training Epoch: 10 [35456/50000]	Loss: 1.8731	LR: 0.050000
Training Epoch: 10 [35584/50000]	Loss: 1.4395	LR: 0.050000
Training Epoch: 10 [35712/50000]	Loss: 1.2349	LR: 0.050000
Training Epoch: 10 [35840/50000]	Loss: 1.5305	LR: 0.050000
Training Epoch: 10 [35968/50000]	Loss: 1.6129	LR: 0.050000
Training Epoch: 10 [36096/50000]	Loss: 1.6984	LR: 0.050000
Training Epoch: 10 [36224/50000]	Loss: 1.4514	LR: 0.050000
Training Epoch: 10 [36352/50000]	Loss: 1.6604	LR: 0.050000
Training Epoch: 10 [36480/50000]	Loss: 1.6605	LR: 0.050000
Training Epoch: 10 [36608/50000]	Loss: 1.6281	LR: 0.050000
Training Epoch: 10 [36736/50000]	Loss: 1.8800	LR: 0.050000
Training Epoch: 10 [36864/50000]	Loss: 1.5007	LR: 0.050000
Training Epoch: 10 [36992/50000]	Loss: 1.8154	LR: 0.050000
Training Epoch: 10 [37120/50000]	Loss: 1.4504	LR: 0.050000
Training Epoch: 10 [37248/50000]	Loss: 1.8831	LR: 0.050000
Training Epoch: 10 [37376/50000]	Loss: 1.6377	LR: 0.050000
Training Epoch: 10 [37504/50000]	Loss: 1.5810	LR: 0.050000
Training Epoch: 10 [37632/50000]	Loss: 1.6573	LR: 0.050000
Training Epoch: 10 [37760/50000]	Loss: 1.4940	LR: 0.050000
Training Epoch: 10 [37888/50000]	Loss: 1.8133	LR: 0.050000
Training Epoch: 10 [38016/50000]	Loss: 1.6228	LR: 0.050000
Training Epoch: 10 [38144/50000]	Loss: 1.7926	LR: 0.050000
Training Epoch: 10 [38272/50000]	Loss: 1.6753	LR: 0.050000
Training Epoch: 10 [38400/50000]	Loss: 1.5666	LR: 0.050000
Training Epoch: 10 [38528/50000]	Loss: 1.4222	LR: 0.050000
Training Epoch: 10 [38656/50000]	Loss: 1.6055	LR: 0.050000
Training Epoch: 10 [38784/50000]	Loss: 1.7221	LR: 0.050000
Training Epoch: 10 [38912/50000]	Loss: 1.4549	LR: 0.050000
Training Epoch: 10 [39040/50000]	Loss: 1.6441	LR: 0.050000
Training Epoch: 10 [39168/50000]	Loss: 1.7240	LR: 0.050000
Training Epoch: 10 [39296/50000]	Loss: 1.5168	LR: 0.050000
Training Epoch: 10 [39424/50000]	Loss: 1.7783	LR: 0.050000
Training Epoch: 10 [39552/50000]	Loss: 1.7743	LR: 0.050000
Training Epoch: 10 [39680/50000]	Loss: 1.7292	LR: 0.050000
Training Epoch: 10 [39808/50000]	Loss: 1.6855	LR: 0.050000
Training Epoch: 10 [39936/50000]	Loss: 1.6094	LR: 0.050000
Training Epoch: 10 [40064/50000]	Loss: 1.7939	LR: 0.050000
Training Epoch: 10 [40192/50000]	Loss: 1.6516	LR: 0.050000
Training Epoch: 10 [40320/50000]	Loss: 1.7128	LR: 0.050000
Training Epoch: 10 [40448/50000]	Loss: 1.5412	LR: 0.050000
Training Epoch: 10 [40576/50000]	Loss: 1.5627	LR: 0.050000
Training Epoch: 10 [40704/50000]	Loss: 1.6245	LR: 0.050000
Training Epoch: 10 [40832/50000]	Loss: 1.9043	LR: 0.050000
Training Epoch: 10 [40960/50000]	Loss: 1.7786	LR: 0.050000
Training Epoch: 10 [41088/50000]	Loss: 1.5960	LR: 0.050000
Training Epoch: 10 [41216/50000]	Loss: 1.5627	LR: 0.050000
Training Epoch: 10 [41344/50000]	Loss: 1.5592	LR: 0.050000
Training Epoch: 10 [41472/50000]	Loss: 1.7310	LR: 0.050000
Training Epoch: 10 [41600/50000]	Loss: 1.7289	LR: 0.050000
Training Epoch: 10 [41728/50000]	Loss: 1.6751	LR: 0.050000
Training Epoch: 10 [41856/50000]	Loss: 1.5492	LR: 0.050000
Training Epoch: 10 [41984/50000]	Loss: 1.7012	LR: 0.050000
Training Epoch: 10 [42112/50000]	Loss: 1.7570	LR: 0.050000
Training Epoch: 10 [42240/50000]	Loss: 1.4256	LR: 0.050000
Training Epoch: 10 [42368/50000]	Loss: 1.8872	LR: 0.050000
Training Epoch: 10 [42496/50000]	Loss: 1.4103	LR: 0.050000
Training Epoch: 10 [42624/50000]	Loss: 1.5237	LR: 0.050000
Training Epoch: 10 [42752/50000]	Loss: 1.7886	LR: 0.050000
Training Epoch: 10 [42880/50000]	Loss: 1.2998	LR: 0.050000
Training Epoch: 10 [43008/50000]	Loss: 1.6752	LR: 0.050000
Training Epoch: 10 [43136/50000]	Loss: 1.5071	LR: 0.050000
Training Epoch: 10 [43264/50000]	Loss: 1.6967	LR: 0.050000
Training Epoch: 10 [43392/50000]	Loss: 1.5382	LR: 0.050000
Training Epoch: 10 [43520/50000]	Loss: 1.4131	LR: 0.050000
Training Epoch: 10 [43648/50000]	Loss: 1.8620	LR: 0.050000
Training Epoch: 10 [43776/50000]	Loss: 1.4256	LR: 0.050000
Training Epoch: 10 [43904/50000]	Loss: 1.6334	LR: 0.050000
Training Epoch: 10 [44032/50000]	Loss: 1.7158	LR: 0.050000
Training Epoch: 10 [44160/50000]	Loss: 1.6514	LR: 0.050000
Training Epoch: 10 [44288/50000]	Loss: 1.6069	LR: 0.050000
Training Epoch: 10 [44416/50000]	Loss: 1.5035	LR: 0.050000
Training Epoch: 10 [44544/50000]	Loss: 1.3665	LR: 0.050000
Training Epoch: 10 [44672/50000]	Loss: 1.4592	LR: 0.050000
Training Epoch: 10 [44800/50000]	Loss: 1.3559	LR: 0.050000
Training Epoch: 10 [44928/50000]	Loss: 1.3532	LR: 0.050000
Training Epoch: 10 [45056/50000]	Loss: 1.7352	LR: 0.050000
Training Epoch: 10 [45184/50000]	Loss: 1.7455	LR: 0.050000
Training Epoch: 10 [45312/50000]	Loss: 1.5265	LR: 0.050000
Training Epoch: 10 [45440/50000]	Loss: 1.6618	LR: 0.050000
Training Epoch: 10 [45568/50000]	Loss: 1.5382	LR: 0.050000
Training Epoch: 10 [45696/50000]	Loss: 1.5861	LR: 0.050000
Training Epoch: 10 [45824/50000]	Loss: 1.4290	LR: 0.050000
Training Epoch: 10 [45952/50000]	Loss: 1.7142	LR: 0.050000
Training Epoch: 10 [46080/50000]	Loss: 1.5408	LR: 0.050000
Training Epoch: 10 [46208/50000]	Loss: 1.7093	LR: 0.050000
Training Epoch: 10 [46336/50000]	Loss: 1.4636	LR: 0.050000
Training Epoch: 10 [46464/50000]	Loss: 1.5090	LR: 0.050000
Training Epoch: 10 [46592/50000]	Loss: 1.4716	LR: 0.050000
Training Epoch: 10 [46720/50000]	Loss: 1.7326	LR: 0.050000
Training Epoch: 10 [46848/50000]	Loss: 1.7987	LR: 0.050000
Training Epoch: 10 [46976/50000]	Loss: 1.6118	LR: 0.050000
Training Epoch: 10 [47104/50000]	Loss: 1.6512	LR: 0.050000
Training Epoch: 10 [47232/50000]	Loss: 1.6507	LR: 0.050000
Training Epoch: 10 [47360/50000]	Loss: 1.7217	LR: 0.050000
Training Epoch: 10 [47488/50000]	Loss: 1.6272	LR: 0.050000
Training Epoch: 10 [47616/50000]	Loss: 1.7142	LR: 0.050000
Training Epoch: 10 [47744/50000]	Loss: 1.2715	LR: 0.050000
Training Epoch: 10 [47872/50000]	Loss: 1.7206	LR: 0.050000
Training Epoch: 10 [48000/50000]	Loss: 1.6939	LR: 0.050000
Training Epoch: 10 [48128/50000]	Loss: 1.7931	LR: 0.050000
Training Epoch: 10 [48256/50000]	Loss: 1.7563	LR: 0.050000
Training Epoch: 10 [48384/50000]	Loss: 1.8829	LR: 0.050000
Training Epoch: 10 [48512/50000]	Loss: 1.7021	LR: 0.050000
Training Epoch: 10 [48640/50000]	Loss: 1.6445	LR: 0.050000
Training Epoch: 10 [48768/50000]	Loss: 1.5469	LR: 0.050000
Training Epoch: 10 [48896/50000]	Loss: 1.6244	LR: 0.050000
Training Epoch: 10 [49024/50000]	Loss: 1.6789	LR: 0.050000
Training Epoch: 10 [49152/50000]	Loss: 1.5665	LR: 0.050000
Training Epoch: 10 [49280/50000]	Loss: 1.5176	LR: 0.050000
Training Epoch: 10 [49408/50000]	Loss: 1.6541	LR: 0.050000
Training Epoch: 10 [49536/50000]	Loss: 1.4147	LR: 0.050000
Training Epoch: 10 [49664/50000]	Loss: 1.6287	LR: 0.050000
Training Epoch: 10 [49792/50000]	Loss: 1.8710	LR: 0.050000
Training Epoch: 10 [49920/50000]	Loss: 1.4713	LR: 0.050000
Training Epoch: 10 [50000/50000]	Loss: 1.6712	LR: 0.050000
epoch 10 training time consumed: 53.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   36701 GB |   36701 GB |
|       from large pool |  123392 KB |    1034 MB |   36664 GB |   36664 GB |
|       from small pool |   10798 KB |      13 MB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   36701 GB |   36701 GB |
|       from large pool |  123392 KB |    1034 MB |   36664 GB |   36664 GB |
|       from small pool |   10798 KB |      13 MB |      36 GB |      36 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   16153 GB |   16153 GB |
|       from large pool |  155136 KB |  433088 KB |   16113 GB |   16113 GB |
|       from small pool |    1490 KB |    3494 KB |      40 GB |      40 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    1416 K  |    1416 K  |
|       from large pool |      24    |      65    |     739 K  |     739 K  |
|       from small pool |     231    |     274    |     677 K  |     677 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    1416 K  |    1416 K  |
|       from large pool |      24    |      65    |     739 K  |     739 K  |
|       from small pool |     231    |     274    |     677 K  |     677 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |  703345    |  703324    |
|       from large pool |       9    |      14    |  357815    |  357806    |
|       from small pool |      12    |      16    |  345530    |  345518    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 10, Average loss: 0.0134, Accuracy: 0.5346, Time consumed:3.45s

Training Epoch: 11 [128/50000]	Loss: 1.6477	LR: 0.050000
Training Epoch: 11 [256/50000]	Loss: 1.3717	LR: 0.050000
Training Epoch: 11 [384/50000]	Loss: 1.5861	LR: 0.050000
Training Epoch: 11 [512/50000]	Loss: 1.5317	LR: 0.050000
Training Epoch: 11 [640/50000]	Loss: 1.5944	LR: 0.050000
Training Epoch: 11 [768/50000]	Loss: 1.5697	LR: 0.050000
Training Epoch: 11 [896/50000]	Loss: 1.7069	LR: 0.050000
Training Epoch: 11 [1024/50000]	Loss: 1.5452	LR: 0.050000
Training Epoch: 11 [1152/50000]	Loss: 1.4993	LR: 0.050000
Training Epoch: 11 [1280/50000]	Loss: 1.3866	LR: 0.050000
Training Epoch: 11 [1408/50000]	Loss: 1.6036	LR: 0.050000
Training Epoch: 11 [1536/50000]	Loss: 1.6935	LR: 0.050000
Training Epoch: 11 [1664/50000]	Loss: 1.6874	LR: 0.050000
Training Epoch: 11 [1792/50000]	Loss: 1.3732	LR: 0.050000
Training Epoch: 11 [1920/50000]	Loss: 1.5111	LR: 0.050000
Training Epoch: 11 [2048/50000]	Loss: 1.3845	LR: 0.050000
Training Epoch: 11 [2176/50000]	Loss: 1.4894	LR: 0.050000
Training Epoch: 11 [2304/50000]	Loss: 1.6604	LR: 0.050000
Training Epoch: 11 [2432/50000]	Loss: 1.8554	LR: 0.050000
Training Epoch: 11 [2560/50000]	Loss: 1.4659	LR: 0.050000
Training Epoch: 11 [2688/50000]	Loss: 1.4770	LR: 0.050000
Training Epoch: 11 [2816/50000]	Loss: 1.6550	LR: 0.050000
Training Epoch: 11 [2944/50000]	Loss: 1.5450	LR: 0.050000
Training Epoch: 11 [3072/50000]	Loss: 1.5686	LR: 0.050000
Training Epoch: 11 [3200/50000]	Loss: 1.7540	LR: 0.050000
Training Epoch: 11 [3328/50000]	Loss: 1.6096	LR: 0.050000
Training Epoch: 11 [3456/50000]	Loss: 1.6957	LR: 0.050000
Training Epoch: 11 [3584/50000]	Loss: 1.4918	LR: 0.050000
Training Epoch: 11 [3712/50000]	Loss: 1.3846	LR: 0.050000
Training Epoch: 11 [3840/50000]	Loss: 1.7185	LR: 0.050000
Training Epoch: 11 [3968/50000]	Loss: 1.3383	LR: 0.050000
Training Epoch: 11 [4096/50000]	Loss: 1.6656	LR: 0.050000
Training Epoch: 11 [4224/50000]	Loss: 1.3070	LR: 0.050000
Training Epoch: 11 [4352/50000]	Loss: 1.7592	LR: 0.050000
Training Epoch: 11 [4480/50000]	Loss: 1.7407	LR: 0.050000
Training Epoch: 11 [4608/50000]	Loss: 1.3427	LR: 0.050000
Training Epoch: 11 [4736/50000]	Loss: 1.6958	LR: 0.050000
Training Epoch: 11 [4864/50000]	Loss: 1.4704	LR: 0.050000
Training Epoch: 11 [4992/50000]	Loss: 1.4384	LR: 0.050000
Training Epoch: 11 [5120/50000]	Loss: 1.5166	LR: 0.050000
Training Epoch: 11 [5248/50000]	Loss: 1.5545	LR: 0.050000
Training Epoch: 11 [5376/50000]	Loss: 1.6470	LR: 0.050000
Training Epoch: 11 [5504/50000]	Loss: 1.3048	LR: 0.050000
Training Epoch: 11 [5632/50000]	Loss: 1.4987	LR: 0.050000
Training Epoch: 11 [5760/50000]	Loss: 1.3648	LR: 0.050000
Training Epoch: 11 [5888/50000]	Loss: 1.4964	LR: 0.050000
Training Epoch: 11 [6016/50000]	Loss: 1.5786	LR: 0.050000
Training Epoch: 11 [6144/50000]	Loss: 1.4155	LR: 0.050000
Training Epoch: 11 [6272/50000]	Loss: 1.6661	LR: 0.050000
Training Epoch: 11 [6400/50000]	Loss: 1.6369	LR: 0.050000
Training Epoch: 11 [6528/50000]	Loss: 1.4462	LR: 0.050000
Training Epoch: 11 [6656/50000]	Loss: 1.4909	LR: 0.050000
Training Epoch: 11 [6784/50000]	Loss: 1.6120	LR: 0.050000
Training Epoch: 11 [6912/50000]	Loss: 1.6848	LR: 0.050000
Training Epoch: 11 [7040/50000]	Loss: 1.3034	LR: 0.050000
Training Epoch: 11 [7168/50000]	Loss: 1.4067	LR: 0.050000
Training Epoch: 11 [7296/50000]	Loss: 1.4560	LR: 0.050000
Training Epoch: 11 [7424/50000]	Loss: 1.4783	LR: 0.050000
Training Epoch: 11 [7552/50000]	Loss: 1.4844	LR: 0.050000
Training Epoch: 11 [7680/50000]	Loss: 1.5504	LR: 0.050000
Training Epoch: 11 [7808/50000]	Loss: 1.3640	LR: 0.050000
Training Epoch: 11 [7936/50000]	Loss: 1.4819	LR: 0.050000
Training Epoch: 11 [8064/50000]	Loss: 1.6469	LR: 0.050000
Training Epoch: 11 [8192/50000]	Loss: 1.6460	LR: 0.050000
Training Epoch: 11 [8320/50000]	Loss: 1.3922	LR: 0.050000
Training Epoch: 11 [8448/50000]	Loss: 1.3828	LR: 0.050000
Training Epoch: 11 [8576/50000]	Loss: 1.5479	LR: 0.050000
Training Epoch: 11 [8704/50000]	Loss: 1.4458	LR: 0.050000
Training Epoch: 11 [8832/50000]	Loss: 1.6634	LR: 0.050000
Training Epoch: 11 [8960/50000]	Loss: 1.5643	LR: 0.050000
Training Epoch: 11 [9088/50000]	Loss: 1.4651	LR: 0.050000
Training Epoch: 11 [9216/50000]	Loss: 1.4431	LR: 0.050000
Training Epoch: 11 [9344/50000]	Loss: 1.6744	LR: 0.050000
Training Epoch: 11 [9472/50000]	Loss: 1.6340	LR: 0.050000
Training Epoch: 11 [9600/50000]	Loss: 1.5268	LR: 0.050000
Training Epoch: 11 [9728/50000]	Loss: 1.5611	LR: 0.050000
Training Epoch: 11 [9856/50000]	Loss: 1.6513	LR: 0.050000
Training Epoch: 11 [9984/50000]	Loss: 1.2664	LR: 0.050000
Training Epoch: 11 [10112/50000]	Loss: 1.6136	LR: 0.050000
Training Epoch: 11 [10240/50000]	Loss: 1.6663	LR: 0.050000
Training Epoch: 11 [10368/50000]	Loss: 1.3873	LR: 0.050000
Training Epoch: 11 [10496/50000]	Loss: 1.6248	LR: 0.050000
Training Epoch: 11 [10624/50000]	Loss: 1.6154	LR: 0.050000
Training Epoch: 11 [10752/50000]	Loss: 1.5981	LR: 0.050000
Training Epoch: 11 [10880/50000]	Loss: 1.5736	LR: 0.050000
Training Epoch: 11 [11008/50000]	Loss: 1.4271	LR: 0.050000
Training Epoch: 11 [11136/50000]	Loss: 1.6408	LR: 0.050000
Training Epoch: 11 [11264/50000]	Loss: 1.5967	LR: 0.050000
Training Epoch: 11 [11392/50000]	Loss: 1.3746	LR: 0.050000
Training Epoch: 11 [11520/50000]	Loss: 1.2657	LR: 0.050000
Training Epoch: 11 [11648/50000]	Loss: 1.6143	LR: 0.050000
Training Epoch: 11 [11776/50000]	Loss: 1.5643	LR: 0.050000
Training Epoch: 11 [11904/50000]	Loss: 1.6407	LR: 0.050000
Training Epoch: 11 [12032/50000]	Loss: 1.3851	LR: 0.050000
Training Epoch: 11 [12160/50000]	Loss: 1.5377	LR: 0.050000
Training Epoch: 11 [12288/50000]	Loss: 1.6008	LR: 0.050000
Training Epoch: 11 [12416/50000]	Loss: 1.5041	LR: 0.050000
Training Epoch: 11 [12544/50000]	Loss: 1.5376	LR: 0.050000
Training Epoch: 11 [12672/50000]	Loss: 1.5351	LR: 0.050000
Training Epoch: 11 [12800/50000]	Loss: 1.6202	LR: 0.050000
Training Epoch: 11 [12928/50000]	Loss: 1.3542	LR: 0.050000
Training Epoch: 11 [13056/50000]	Loss: 1.6405	LR: 0.050000
Training Epoch: 11 [13184/50000]	Loss: 1.4910	LR: 0.050000
Training Epoch: 11 [13312/50000]	Loss: 1.8976	LR: 0.050000
Training Epoch: 11 [13440/50000]	Loss: 1.8191	LR: 0.050000
Training Epoch: 11 [13568/50000]	Loss: 1.3994	LR: 0.050000
Training Epoch: 11 [13696/50000]	Loss: 1.6832	LR: 0.050000
Training Epoch: 11 [13824/50000]	Loss: 1.3323	LR: 0.050000
Training Epoch: 11 [13952/50000]	Loss: 1.7072	LR: 0.050000
Training Epoch: 11 [14080/50000]	Loss: 1.4174	LR: 0.050000
Training Epoch: 11 [14208/50000]	Loss: 1.5005	LR: 0.050000
Training Epoch: 11 [14336/50000]	Loss: 1.6265	LR: 0.050000
Training Epoch: 11 [14464/50000]	Loss: 1.5271	LR: 0.050000
Training Epoch: 11 [14592/50000]	Loss: 1.5351	LR: 0.050000
Training Epoch: 11 [14720/50000]	Loss: 1.7766	LR: 0.050000
Training Epoch: 11 [14848/50000]	Loss: 1.5640	LR: 0.050000
Training Epoch: 11 [14976/50000]	Loss: 1.5169	LR: 0.050000
Training Epoch: 11 [15104/50000]	Loss: 1.5088	LR: 0.050000
Training Epoch: 11 [15232/50000]	Loss: 1.3824	LR: 0.050000
Training Epoch: 11 [15360/50000]	Loss: 1.6634	LR: 0.050000
Training Epoch: 11 [15488/50000]	Loss: 1.6155	LR: 0.050000
Training Epoch: 11 [15616/50000]	Loss: 1.2574	LR: 0.050000
Training Epoch: 11 [15744/50000]	Loss: 1.3361	LR: 0.050000
Training Epoch: 11 [15872/50000]	Loss: 1.4317	LR: 0.050000
Training Epoch: 11 [16000/50000]	Loss: 1.3141	LR: 0.050000
Training Epoch: 11 [16128/50000]	Loss: 1.6805	LR: 0.050000
Training Epoch: 11 [16256/50000]	Loss: 1.7805	LR: 0.050000
Training Epoch: 11 [16384/50000]	Loss: 1.4771	LR: 0.050000
Training Epoch: 11 [16512/50000]	Loss: 1.3826	LR: 0.050000
Training Epoch: 11 [16640/50000]	Loss: 1.7688	LR: 0.050000
Training Epoch: 11 [16768/50000]	Loss: 1.6992	LR: 0.050000
Training Epoch: 11 [16896/50000]	Loss: 1.4932	LR: 0.050000
Training Epoch: 11 [17024/50000]	Loss: 1.5235	LR: 0.050000
Training Epoch: 11 [17152/50000]	Loss: 1.4983	LR: 0.050000
Training Epoch: 11 [17280/50000]	Loss: 1.6531	LR: 0.050000
Training Epoch: 11 [17408/50000]	Loss: 1.6537	LR: 0.050000
Training Epoch: 11 [17536/50000]	Loss: 1.6161	LR: 0.050000
Training Epoch: 11 [17664/50000]	Loss: 1.4616	LR: 0.050000
Training Epoch: 11 [17792/50000]	Loss: 1.5192	LR: 0.050000
Training Epoch: 11 [17920/50000]	Loss: 1.4060	LR: 0.050000
Training Epoch: 11 [18048/50000]	Loss: 1.3166	LR: 0.050000
Training Epoch: 11 [18176/50000]	Loss: 1.4090	LR: 0.050000
Training Epoch: 11 [18304/50000]	Loss: 1.4783	LR: 0.050000
Training Epoch: 11 [18432/50000]	Loss: 1.7581	LR: 0.050000
Training Epoch: 11 [18560/50000]	Loss: 1.4725	LR: 0.050000
Training Epoch: 11 [18688/50000]	Loss: 1.4676	LR: 0.050000
Training Epoch: 11 [18816/50000]	Loss: 1.6414	LR: 0.050000
Training Epoch: 11 [18944/50000]	Loss: 1.4977	LR: 0.050000
Training Epoch: 11 [19072/50000]	Loss: 1.5946	LR: 0.050000
Training Epoch: 11 [19200/50000]	Loss: 1.6003	LR: 0.050000
Training Epoch: 11 [19328/50000]	Loss: 1.3394	LR: 0.050000
Training Epoch: 11 [19456/50000]	Loss: 1.6624	LR: 0.050000
Training Epoch: 11 [19584/50000]	Loss: 1.4556	LR: 0.050000
Training Epoch: 11 [19712/50000]	Loss: 1.5715	LR: 0.050000
Training Epoch: 11 [19840/50000]	Loss: 1.6188	LR: 0.050000
Training Epoch: 11 [19968/50000]	Loss: 1.8524	LR: 0.050000
Training Epoch: 11 [20096/50000]	Loss: 1.3435	LR: 0.050000
Training Epoch: 11 [20224/50000]	Loss: 1.5531	LR: 0.050000
Training Epoch: 11 [20352/50000]	Loss: 1.5949	LR: 0.050000
Training Epoch: 11 [20480/50000]	Loss: 1.4799	LR: 0.050000
Training Epoch: 11 [20608/50000]	Loss: 1.9832	LR: 0.050000
Training Epoch: 11 [20736/50000]	Loss: 1.6200	LR: 0.050000
Training Epoch: 11 [20864/50000]	Loss: 1.8897	LR: 0.050000
Training Epoch: 11 [20992/50000]	Loss: 1.5232	LR: 0.050000
Training Epoch: 11 [21120/50000]	Loss: 1.5692	LR: 0.050000
Training Epoch: 11 [21248/50000]	Loss: 1.5246	LR: 0.050000
Training Epoch: 11 [21376/50000]	Loss: 1.5759	LR: 0.050000
Training Epoch: 11 [21504/50000]	Loss: 1.7349	LR: 0.050000
Training Epoch: 11 [21632/50000]	Loss: 1.6075	LR: 0.050000
Training Epoch: 11 [21760/50000]	Loss: 1.5923	LR: 0.050000
Training Epoch: 11 [21888/50000]	Loss: 1.5387	LR: 0.050000
Training Epoch: 11 [22016/50000]	Loss: 1.6154	LR: 0.050000
Training Epoch: 11 [22144/50000]	Loss: 1.4891	LR: 0.050000
Training Epoch: 11 [22272/50000]	Loss: 1.7896	LR: 0.050000
Training Epoch: 11 [22400/50000]	Loss: 1.5296	LR: 0.050000
Training Epoch: 11 [22528/50000]	Loss: 1.2558	LR: 0.050000
Training Epoch: 11 [22656/50000]	Loss: 1.4513	LR: 0.050000
Training Epoch: 11 [22784/50000]	Loss: 1.7236	LR: 0.050000
Training Epoch: 11 [22912/50000]	Loss: 1.6134	LR: 0.050000
Training Epoch: 11 [23040/50000]	Loss: 1.6118	LR: 0.050000
Training Epoch: 11 [23168/50000]	Loss: 1.7375	LR: 0.050000
Training Epoch: 11 [23296/50000]	Loss: 1.5663	LR: 0.050000
Training Epoch: 11 [23424/50000]	Loss: 1.6251	LR: 0.050000
Training Epoch: 11 [23552/50000]	Loss: 1.7301	LR: 0.050000
Training Epoch: 11 [23680/50000]	Loss: 1.7063	LR: 0.050000
Training Epoch: 11 [23808/50000]	Loss: 1.5063	LR: 0.050000
Training Epoch: 11 [23936/50000]	Loss: 1.4475	LR: 0.050000
Training Epoch: 11 [24064/50000]	Loss: 1.5646	LR: 0.050000
Training Epoch: 11 [24192/50000]	Loss: 1.5653	LR: 0.050000
Training Epoch: 11 [24320/50000]	Loss: 1.4426	LR: 0.050000
Training Epoch: 11 [24448/50000]	Loss: 1.5618	LR: 0.050000
Training Epoch: 11 [24576/50000]	Loss: 1.5894	LR: 0.050000
Training Epoch: 11 [24704/50000]	Loss: 1.4466	LR: 0.050000
Training Epoch: 11 [24832/50000]	Loss: 1.4798	LR: 0.050000
Training Epoch: 11 [24960/50000]	Loss: 1.6645	LR: 0.050000
Training Epoch: 11 [25088/50000]	Loss: 1.5059	LR: 0.050000
Training Epoch: 11 [25216/50000]	Loss: 1.4742	LR: 0.050000
Training Epoch: 11 [25344/50000]	Loss: 1.5884	LR: 0.050000
Training Epoch: 11 [25472/50000]	Loss: 1.4849	LR: 0.050000
Training Epoch: 11 [25600/50000]	Loss: 1.5080	LR: 0.050000
Training Epoch: 11 [25728/50000]	Loss: 1.3103	LR: 0.050000
Training Epoch: 11 [25856/50000]	Loss: 1.4862	LR: 0.050000
Training Epoch: 11 [25984/50000]	Loss: 1.6927	LR: 0.050000
Training Epoch: 11 [26112/50000]	Loss: 1.3257	LR: 0.050000
Training Epoch: 11 [26240/50000]	Loss: 1.6921	LR: 0.050000
Training Epoch: 11 [26368/50000]	Loss: 1.4234	LR: 0.050000
Training Epoch: 11 [26496/50000]	Loss: 1.4169	LR: 0.050000
Training Epoch: 11 [26624/50000]	Loss: 1.4312	LR: 0.050000
Training Epoch: 11 [26752/50000]	Loss: 1.6324	LR: 0.050000
Training Epoch: 11 [26880/50000]	Loss: 1.4697	LR: 0.050000
Training Epoch: 11 [27008/50000]	Loss: 1.6214	LR: 0.050000
Training Epoch: 11 [27136/50000]	Loss: 1.6256	LR: 0.050000
Training Epoch: 11 [27264/50000]	Loss: 1.9736	LR: 0.050000
Training Epoch: 11 [27392/50000]	Loss: 1.4719	LR: 0.050000
Training Epoch: 11 [27520/50000]	Loss: 1.4876	LR: 0.050000
Training Epoch: 11 [27648/50000]	Loss: 1.5224	LR: 0.050000
Training Epoch: 11 [27776/50000]	Loss: 1.2557	LR: 0.050000
Training Epoch: 11 [27904/50000]	Loss: 1.7621	LR: 0.050000
Training Epoch: 11 [28032/50000]	Loss: 1.7842	LR: 0.050000
Training Epoch: 11 [28160/50000]	Loss: 1.5958	LR: 0.050000
Training Epoch: 11 [28288/50000]	Loss: 1.3681	LR: 0.050000
Training Epoch: 11 [28416/50000]	Loss: 1.4277	LR: 0.050000
Training Epoch: 11 [28544/50000]	Loss: 1.6327	LR: 0.050000
Training Epoch: 11 [28672/50000]	Loss: 1.6897	LR: 0.050000
Training Epoch: 11 [28800/50000]	Loss: 1.5651	LR: 0.050000
Training Epoch: 11 [28928/50000]	Loss: 1.6807	LR: 0.050000
Training Epoch: 11 [29056/50000]	Loss: 1.4531	LR: 0.050000
Training Epoch: 11 [29184/50000]	Loss: 1.8216	LR: 0.050000
Training Epoch: 11 [29312/50000]	Loss: 1.5264	LR: 0.050000
Training Epoch: 11 [29440/50000]	Loss: 1.4542	LR: 0.050000
Training Epoch: 11 [29568/50000]	Loss: 1.7589	LR: 0.050000
Training Epoch: 11 [29696/50000]	Loss: 1.3869	LR: 0.050000
Training Epoch: 11 [29824/50000]	Loss: 1.6363	LR: 0.050000
Training Epoch: 11 [29952/50000]	Loss: 1.6508	LR: 0.050000
Training Epoch: 11 [30080/50000]	Loss: 1.6822	LR: 0.050000
Training Epoch: 11 [30208/50000]	Loss: 1.5598	LR: 0.050000
Training Epoch: 11 [30336/50000]	Loss: 1.6675	LR: 0.050000
Training Epoch: 11 [30464/50000]	Loss: 1.3264	LR: 0.050000
Training Epoch: 11 [30592/50000]	Loss: 1.6775	LR: 0.050000
Training Epoch: 11 [30720/50000]	Loss: 1.4958	LR: 0.050000
Training Epoch: 11 [30848/50000]	Loss: 1.6224	LR: 0.050000
Training Epoch: 11 [30976/50000]	Loss: 1.4184	LR: 0.050000
Training Epoch: 11 [31104/50000]	Loss: 1.5347	LR: 0.050000
Training Epoch: 11 [31232/50000]	Loss: 1.5776	LR: 0.050000
Training Epoch: 11 [31360/50000]	Loss: 1.4365	LR: 0.050000
Training Epoch: 11 [31488/50000]	Loss: 1.6265	LR: 0.050000
Training Epoch: 11 [31616/50000]	Loss: 1.4592	LR: 0.050000
Training Epoch: 11 [31744/50000]	Loss: 1.6575	LR: 0.050000
Training Epoch: 11 [31872/50000]	Loss: 1.2395	LR: 0.050000
Training Epoch: 11 [32000/50000]	Loss: 1.5762	LR: 0.050000
Training Epoch: 11 [32128/50000]	Loss: 1.3843	LR: 0.050000
Training Epoch: 11 [32256/50000]	Loss: 1.6466	LR: 0.050000
Training Epoch: 11 [32384/50000]	Loss: 1.4503	LR: 0.050000
Training Epoch: 11 [32512/50000]	Loss: 1.6046	LR: 0.050000
Training Epoch: 11 [32640/50000]	Loss: 1.4112	LR: 0.050000
Training Epoch: 11 [32768/50000]	Loss: 1.5254	LR: 0.050000
Training Epoch: 11 [32896/50000]	Loss: 1.6538	LR: 0.050000
Training Epoch: 11 [33024/50000]	Loss: 1.5430	LR: 0.050000
Training Epoch: 11 [33152/50000]	Loss: 1.4622	LR: 0.050000
Training Epoch: 11 [33280/50000]	Loss: 1.6165	LR: 0.050000
Training Epoch: 11 [33408/50000]	Loss: 1.6158	LR: 0.050000
Training Epoch: 11 [33536/50000]	Loss: 1.5849	LR: 0.050000
Training Epoch: 11 [33664/50000]	Loss: 1.7470	LR: 0.050000
Training Epoch: 11 [33792/50000]	Loss: 1.2547	LR: 0.050000
Training Epoch: 11 [33920/50000]	Loss: 1.4634	LR: 0.050000
Training Epoch: 11 [34048/50000]	Loss: 1.5685	LR: 0.050000
Training Epoch: 11 [34176/50000]	Loss: 1.4729	LR: 0.050000
Training Epoch: 11 [34304/50000]	Loss: 1.4488	LR: 0.050000
Training Epoch: 11 [34432/50000]	Loss: 1.8019	LR: 0.050000
Training Epoch: 11 [34560/50000]	Loss: 1.4327	LR: 0.050000
Training Epoch: 11 [34688/50000]	Loss: 1.3666	LR: 0.050000
Training Epoch: 11 [34816/50000]	Loss: 1.5166	LR: 0.050000
Training Epoch: 11 [34944/50000]	Loss: 1.4861	LR: 0.050000
Training Epoch: 11 [35072/50000]	Loss: 1.6323	LR: 0.050000
Training Epoch: 11 [35200/50000]	Loss: 1.4245	LR: 0.050000
Training Epoch: 11 [35328/50000]	Loss: 1.4203	LR: 0.050000
Training Epoch: 11 [35456/50000]	Loss: 1.5268	LR: 0.050000
Training Epoch: 11 [35584/50000]	Loss: 1.6780	LR: 0.050000
Training Epoch: 11 [35712/50000]	Loss: 1.4788	LR: 0.050000
Training Epoch: 11 [35840/50000]	Loss: 1.4425	LR: 0.050000
Training Epoch: 11 [35968/50000]	Loss: 1.9613	LR: 0.050000
Training Epoch: 11 [36096/50000]	Loss: 1.6569	LR: 0.050000
Training Epoch: 11 [36224/50000]	Loss: 1.3328	LR: 0.050000
Training Epoch: 11 [36352/50000]	Loss: 1.5770	LR: 0.050000
Training Epoch: 11 [36480/50000]	Loss: 1.3179	LR: 0.050000
Training Epoch: 11 [36608/50000]	Loss: 1.4815	LR: 0.050000
Training Epoch: 11 [36736/50000]	Loss: 1.5014	LR: 0.050000
Training Epoch: 11 [36864/50000]	Loss: 1.4170	LR: 0.050000
Training Epoch: 11 [36992/50000]	Loss: 1.6738	LR: 0.050000
Training Epoch: 11 [37120/50000]	Loss: 1.4863	LR: 0.050000
Training Epoch: 11 [37248/50000]	Loss: 1.5633	LR: 0.050000
Training Epoch: 11 [37376/50000]	Loss: 1.4756	LR: 0.050000
Training Epoch: 11 [37504/50000]	Loss: 1.4731	LR: 0.050000
Training Epoch: 11 [37632/50000]	Loss: 1.5525	LR: 0.050000
Training Epoch: 11 [37760/50000]	Loss: 1.7594	LR: 0.050000
Training Epoch: 11 [37888/50000]	Loss: 1.6701	LR: 0.050000
Training Epoch: 11 [38016/50000]	Loss: 1.6298	LR: 0.050000
Training Epoch: 11 [38144/50000]	Loss: 1.4804	LR: 0.050000
Training Epoch: 11 [38272/50000]	Loss: 1.4692	LR: 0.050000
Training Epoch: 11 [38400/50000]	Loss: 1.7357	LR: 0.050000
Training Epoch: 11 [38528/50000]	Loss: 1.5457	LR: 0.050000
Training Epoch: 11 [38656/50000]	Loss: 1.6022	LR: 0.050000
Training Epoch: 11 [38784/50000]	Loss: 1.4151	LR: 0.050000
Training Epoch: 11 [38912/50000]	Loss: 1.5081	LR: 0.050000
Training Epoch: 11 [39040/50000]	Loss: 1.4156	LR: 0.050000
Training Epoch: 11 [39168/50000]	Loss: 1.7852	LR: 0.050000
Training Epoch: 11 [39296/50000]	Loss: 1.6410	LR: 0.050000
Training Epoch: 11 [39424/50000]	Loss: 1.4149	LR: 0.050000
Training Epoch: 11 [39552/50000]	Loss: 1.3278	LR: 0.050000
Training Epoch: 11 [39680/50000]	Loss: 1.4503	LR: 0.050000
Training Epoch: 11 [39808/50000]	Loss: 1.4583	LR: 0.050000
Training Epoch: 11 [39936/50000]	Loss: 1.4513	LR: 0.050000
Training Epoch: 11 [40064/50000]	Loss: 1.6703	LR: 0.050000
Training Epoch: 11 [40192/50000]	Loss: 1.4405	LR: 0.050000
Training Epoch: 11 [40320/50000]	Loss: 1.6191	LR: 0.050000
Training Epoch: 11 [40448/50000]	Loss: 1.6536	LR: 0.050000
Training Epoch: 11 [40576/50000]	Loss: 1.4246	LR: 0.050000
Training Epoch: 11 [40704/50000]	Loss: 1.4150	LR: 0.050000
Training Epoch: 11 [40832/50000]	Loss: 1.5234	LR: 0.050000
Training Epoch: 11 [40960/50000]	Loss: 1.4443	LR: 0.050000
Training Epoch: 11 [41088/50000]	Loss: 1.6036	LR: 0.050000
Training Epoch: 11 [41216/50000]	Loss: 1.4780	LR: 0.050000
Training Epoch: 11 [41344/50000]	Loss: 1.4333	LR: 0.050000
Training Epoch: 11 [41472/50000]	Loss: 1.6447	LR: 0.050000
Training Epoch: 11 [41600/50000]	Loss: 1.5684	LR: 0.050000
Training Epoch: 11 [41728/50000]	Loss: 1.5639	LR: 0.050000
Training Epoch: 11 [41856/50000]	Loss: 1.3447	LR: 0.050000
Training Epoch: 11 [41984/50000]	Loss: 1.7564	LR: 0.050000
Training Epoch: 11 [42112/50000]	Loss: 1.5772	LR: 0.050000
Training Epoch: 11 [42240/50000]	Loss: 1.4722	LR: 0.050000
Training Epoch: 11 [42368/50000]	Loss: 1.4093	LR: 0.050000
Training Epoch: 11 [42496/50000]	Loss: 1.3518	LR: 0.050000
Training Epoch: 11 [42624/50000]	Loss: 1.4892	LR: 0.050000
Training Epoch: 11 [42752/50000]	Loss: 1.5812	LR: 0.050000
Training Epoch: 11 [42880/50000]	Loss: 1.7244	LR: 0.050000
Training Epoch: 11 [43008/50000]	Loss: 1.3886	LR: 0.050000
Training Epoch: 11 [43136/50000]	Loss: 1.4892	LR: 0.050000
Training Epoch: 11 [43264/50000]	Loss: 1.8541	LR: 0.050000
Training Epoch: 11 [43392/50000]	Loss: 1.6440	LR: 0.050000
Training Epoch: 11 [43520/50000]	Loss: 1.4303	LR: 0.050000
Training Epoch: 11 [43648/50000]	Loss: 1.5779	LR: 0.050000
Training Epoch: 11 [43776/50000]	Loss: 1.5289	LR: 0.050000
Training Epoch: 11 [43904/50000]	Loss: 1.7213	LR: 0.050000
Training Epoch: 11 [44032/50000]	Loss: 1.2763	LR: 0.050000
Training Epoch: 11 [44160/50000]	Loss: 1.6250	LR: 0.050000
Training Epoch: 11 [44288/50000]	Loss: 1.6035	LR: 0.050000
Training Epoch: 11 [44416/50000]	Loss: 1.5230	LR: 0.050000
Training Epoch: 11 [44544/50000]	Loss: 1.4997	LR: 0.050000
Training Epoch: 11 [44672/50000]	Loss: 1.7615	LR: 0.050000
Training Epoch: 11 [44800/50000]	Loss: 1.6116	LR: 0.050000
Training Epoch: 11 [44928/50000]	Loss: 1.3417	LR: 0.050000
Training Epoch: 11 [45056/50000]	Loss: 1.7804	LR: 0.050000
Training Epoch: 11 [45184/50000]	Loss: 1.4670	LR: 0.050000
Training Epoch: 11 [45312/50000]	Loss: 1.5701	LR: 0.050000
Training Epoch: 11 [45440/50000]	Loss: 1.4699	LR: 0.050000
Training Epoch: 11 [45568/50000]	Loss: 1.7410	LR: 0.050000
Training Epoch: 11 [45696/50000]	Loss: 1.5259	LR: 0.050000
Training Epoch: 11 [45824/50000]	Loss: 1.7043	LR: 0.050000
Training Epoch: 11 [45952/50000]	Loss: 1.6701	LR: 0.050000
Training Epoch: 11 [46080/50000]	Loss: 1.3252	LR: 0.050000
Training Epoch: 11 [46208/50000]	Loss: 1.7106	LR: 0.050000
Training Epoch: 11 [46336/50000]	Loss: 1.7135	LR: 0.050000
Training Epoch: 11 [46464/50000]	Loss: 1.5760	LR: 0.050000
Training Epoch: 11 [46592/50000]	Loss: 1.7132	LR: 0.050000
Training Epoch: 11 [46720/50000]	Loss: 1.8810	LR: 0.050000
Training Epoch: 11 [46848/50000]	Loss: 1.4980	LR: 0.050000
Training Epoch: 11 [46976/50000]	Loss: 1.6966	LR: 0.050000
Training Epoch: 11 [47104/50000]	Loss: 1.2666	LR: 0.050000
Training Epoch: 11 [47232/50000]	Loss: 1.7049	LR: 0.050000
Training Epoch: 11 [47360/50000]	Loss: 1.6933	LR: 0.050000
Training Epoch: 11 [47488/50000]	Loss: 1.5119	LR: 0.050000
Training Epoch: 11 [47616/50000]	Loss: 1.5914	LR: 0.050000
Training Epoch: 11 [47744/50000]	Loss: 1.3757	LR: 0.050000
Training Epoch: 11 [47872/50000]	Loss: 1.4439	LR: 0.050000
Training Epoch: 11 [48000/50000]	Loss: 1.5823	LR: 0.050000
Training Epoch: 11 [48128/50000]	Loss: 1.5900	LR: 0.050000
Training Epoch: 11 [48256/50000]	Loss: 1.6935	LR: 0.050000
Training Epoch: 11 [48384/50000]	Loss: 1.4421	LR: 0.050000
Training Epoch: 11 [48512/50000]	Loss: 1.5071	LR: 0.050000
Training Epoch: 11 [48640/50000]	Loss: 1.3378	LR: 0.050000
Training Epoch: 11 [48768/50000]	Loss: 1.6241	LR: 0.050000
Training Epoch: 11 [48896/50000]	Loss: 1.4081	LR: 0.050000
Training Epoch: 11 [49024/50000]	Loss: 1.5126	LR: 0.050000
Training Epoch: 11 [49152/50000]	Loss: 1.2737	LR: 0.050000
Training Epoch: 11 [49280/50000]	Loss: 1.2706	LR: 0.050000
Training Epoch: 11 [49408/50000]	Loss: 1.5621	LR: 0.050000
Training Epoch: 11 [49536/50000]	Loss: 1.5582	LR: 0.050000
Training Epoch: 11 [49664/50000]	Loss: 1.5115	LR: 0.050000
Training Epoch: 11 [49792/50000]	Loss: 1.5551	LR: 0.050000
Training Epoch: 11 [49920/50000]	Loss: 1.7896	LR: 0.050000
Training Epoch: 11 [50000/50000]	Loss: 1.5132	LR: 0.050000
epoch 11 training time consumed: 54.03s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   40371 GB |   40371 GB |
|       from large pool |  123392 KB |    1034 MB |   40331 GB |   40331 GB |
|       from small pool |   10798 KB |      13 MB |      39 GB |      39 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   40371 GB |   40371 GB |
|       from large pool |  123392 KB |    1034 MB |   40331 GB |   40331 GB |
|       from small pool |   10798 KB |      13 MB |      39 GB |      39 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   17768 GB |   17768 GB |
|       from large pool |  155136 KB |  433088 KB |   17724 GB |   17724 GB |
|       from small pool |    1490 KB |    3494 KB |      44 GB |      44 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    1558 K  |    1558 K  |
|       from large pool |      24    |      65    |     813 K  |     813 K  |
|       from small pool |     231    |     274    |     745 K  |     744 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    1558 K  |    1558 K  |
|       from large pool |      24    |      65    |     813 K  |     813 K  |
|       from small pool |     231    |     274    |     745 K  |     744 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |     773 K  |     773 K  |
|       from large pool |       9    |      14    |     393 K  |     393 K  |
|       from small pool |      12    |      16    |     379 K  |     379 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 11, Average loss: 0.0126, Accuracy: 0.5557, Time consumed:3.48s

Training Epoch: 12 [128/50000]	Loss: 1.1673	LR: 0.050000
Training Epoch: 12 [256/50000]	Loss: 1.3760	LR: 0.050000
Training Epoch: 12 [384/50000]	Loss: 1.5964	LR: 0.050000
Training Epoch: 12 [512/50000]	Loss: 1.4351	LR: 0.050000
Training Epoch: 12 [640/50000]	Loss: 1.5004	LR: 0.050000
Training Epoch: 12 [768/50000]	Loss: 1.4803	LR: 0.050000
Training Epoch: 12 [896/50000]	Loss: 1.5544	LR: 0.050000
Training Epoch: 12 [1024/50000]	Loss: 1.4281	LR: 0.050000
Training Epoch: 12 [1152/50000]	Loss: 1.4956	LR: 0.050000
Training Epoch: 12 [1280/50000]	Loss: 1.4058	LR: 0.050000
Training Epoch: 12 [1408/50000]	Loss: 1.4647	LR: 0.050000
Training Epoch: 12 [1536/50000]	Loss: 1.4598	LR: 0.050000
Training Epoch: 12 [1664/50000]	Loss: 1.6886	LR: 0.050000
Training Epoch: 12 [1792/50000]	Loss: 1.3639	LR: 0.050000
Training Epoch: 12 [1920/50000]	Loss: 1.1601	LR: 0.050000
Training Epoch: 12 [2048/50000]	Loss: 1.3772	LR: 0.050000
Training Epoch: 12 [2176/50000]	Loss: 1.4461	LR: 0.050000
Training Epoch: 12 [2304/50000]	Loss: 1.5097	LR: 0.050000
Training Epoch: 12 [2432/50000]	Loss: 1.3098	LR: 0.050000
Training Epoch: 12 [2560/50000]	Loss: 1.3492	LR: 0.050000
Training Epoch: 12 [2688/50000]	Loss: 1.4962	LR: 0.050000
Training Epoch: 12 [2816/50000]	Loss: 1.2912	LR: 0.050000
Training Epoch: 12 [2944/50000]	Loss: 1.6130	LR: 0.050000
Training Epoch: 12 [3072/50000]	Loss: 1.6314	LR: 0.050000
Training Epoch: 12 [3200/50000]	Loss: 1.3674	LR: 0.050000
Training Epoch: 12 [3328/50000]	Loss: 1.5834	LR: 0.050000
Training Epoch: 12 [3456/50000]	Loss: 1.2897	LR: 0.050000
Training Epoch: 12 [3584/50000]	Loss: 1.4063	LR: 0.050000
Training Epoch: 12 [3712/50000]	Loss: 1.3500	LR: 0.050000
Training Epoch: 12 [3840/50000]	Loss: 1.2793	LR: 0.050000
Training Epoch: 12 [3968/50000]	Loss: 1.6010	LR: 0.050000
Training Epoch: 12 [4096/50000]	Loss: 1.4293	LR: 0.050000
Training Epoch: 12 [4224/50000]	Loss: 1.7956	LR: 0.050000
Training Epoch: 12 [4352/50000]	Loss: 1.3818	LR: 0.050000
Training Epoch: 12 [4480/50000]	Loss: 1.5721	LR: 0.050000
Training Epoch: 12 [4608/50000]	Loss: 1.3705	LR: 0.050000
Training Epoch: 12 [4736/50000]	Loss: 1.4503	LR: 0.050000
Training Epoch: 12 [4864/50000]	Loss: 1.7049	LR: 0.050000
Training Epoch: 12 [4992/50000]	Loss: 1.2935	LR: 0.050000
Training Epoch: 12 [5120/50000]	Loss: 1.3234	LR: 0.050000
Training Epoch: 12 [5248/50000]	Loss: 1.2604	LR: 0.050000
Training Epoch: 12 [5376/50000]	Loss: 1.1352	LR: 0.050000
Training Epoch: 12 [5504/50000]	Loss: 1.6736	LR: 0.050000
Training Epoch: 12 [5632/50000]	Loss: 1.2809	LR: 0.050000
Training Epoch: 12 [5760/50000]	Loss: 1.8312	LR: 0.050000
Training Epoch: 12 [5888/50000]	Loss: 1.3618	LR: 0.050000
Training Epoch: 12 [6016/50000]	Loss: 1.5147	LR: 0.050000
Training Epoch: 12 [6144/50000]	Loss: 1.2246	LR: 0.050000
Training Epoch: 12 [6272/50000]	Loss: 1.4904	LR: 0.050000
Training Epoch: 12 [6400/50000]	Loss: 1.4485	LR: 0.050000
Training Epoch: 12 [6528/50000]	Loss: 1.5065	LR: 0.050000
Training Epoch: 12 [6656/50000]	Loss: 1.5664	LR: 0.050000
Training Epoch: 12 [6784/50000]	Loss: 1.3363	LR: 0.050000
Training Epoch: 12 [6912/50000]	Loss: 1.4921	LR: 0.050000
Training Epoch: 12 [7040/50000]	Loss: 1.4301	LR: 0.050000
Training Epoch: 12 [7168/50000]	Loss: 1.3558	LR: 0.050000
Training Epoch: 12 [7296/50000]	Loss: 1.5253	LR: 0.050000
Training Epoch: 12 [7424/50000]	Loss: 1.6821	LR: 0.050000
Training Epoch: 12 [7552/50000]	Loss: 1.6820	LR: 0.050000
Training Epoch: 12 [7680/50000]	Loss: 1.3405	LR: 0.050000
Training Epoch: 12 [7808/50000]	Loss: 1.6256	LR: 0.050000
Training Epoch: 12 [7936/50000]	Loss: 1.5117	LR: 0.050000
Training Epoch: 12 [8064/50000]	Loss: 1.4604	LR: 0.050000
Training Epoch: 12 [8192/50000]	Loss: 1.5937	LR: 0.050000
Training Epoch: 12 [8320/50000]	Loss: 1.2632	LR: 0.050000
Training Epoch: 12 [8448/50000]	Loss: 1.3440	LR: 0.050000
Training Epoch: 12 [8576/50000]	Loss: 1.5687	LR: 0.050000
Training Epoch: 12 [8704/50000]	Loss: 1.5393	LR: 0.050000
Training Epoch: 12 [8832/50000]	Loss: 1.5396	LR: 0.050000
Training Epoch: 12 [8960/50000]	Loss: 1.4448	LR: 0.050000
Training Epoch: 12 [9088/50000]	Loss: 1.5164	LR: 0.050000
Training Epoch: 12 [9216/50000]	Loss: 1.3470	LR: 0.050000
Training Epoch: 12 [9344/50000]	Loss: 1.4692	LR: 0.050000
Training Epoch: 12 [9472/50000]	Loss: 1.6107	LR: 0.050000
Training Epoch: 12 [9600/50000]	Loss: 1.3919	LR: 0.050000
Training Epoch: 12 [9728/50000]	Loss: 1.8012	LR: 0.050000
Training Epoch: 12 [9856/50000]	Loss: 1.4589	LR: 0.050000
Training Epoch: 12 [9984/50000]	Loss: 1.3043	LR: 0.050000
Training Epoch: 12 [10112/50000]	Loss: 1.1580	LR: 0.050000
Training Epoch: 12 [10240/50000]	Loss: 1.4959	LR: 0.050000
Training Epoch: 12 [10368/50000]	Loss: 1.5483	LR: 0.050000
Training Epoch: 12 [10496/50000]	Loss: 1.4007	LR: 0.050000
Training Epoch: 12 [10624/50000]	Loss: 1.3211	LR: 0.050000
Training Epoch: 12 [10752/50000]	Loss: 1.4590	LR: 0.050000
Training Epoch: 12 [10880/50000]	Loss: 1.4255	LR: 0.050000
Training Epoch: 12 [11008/50000]	Loss: 1.2541	LR: 0.050000
Training Epoch: 12 [11136/50000]	Loss: 1.7642	LR: 0.050000
Training Epoch: 12 [11264/50000]	Loss: 1.5042	LR: 0.050000
Training Epoch: 12 [11392/50000]	Loss: 1.4722	LR: 0.050000
Training Epoch: 12 [11520/50000]	Loss: 1.5762	LR: 0.050000
Training Epoch: 12 [11648/50000]	Loss: 1.2286	LR: 0.050000
Training Epoch: 12 [11776/50000]	Loss: 1.5623	LR: 0.050000
Training Epoch: 12 [11904/50000]	Loss: 1.5188	LR: 0.050000
Training Epoch: 12 [12032/50000]	Loss: 1.3500	LR: 0.050000
Training Epoch: 12 [12160/50000]	Loss: 1.6049	LR: 0.050000
Training Epoch: 12 [12288/50000]	Loss: 1.3776	LR: 0.050000
Training Epoch: 12 [12416/50000]	Loss: 1.3485	LR: 0.050000
Training Epoch: 12 [12544/50000]	Loss: 1.3525	LR: 0.050000
Training Epoch: 12 [12672/50000]	Loss: 1.6534	LR: 0.050000
Training Epoch: 12 [12800/50000]	Loss: 1.3405	LR: 0.050000
Training Epoch: 12 [12928/50000]	Loss: 1.5420	LR: 0.050000
Training Epoch: 12 [13056/50000]	Loss: 1.3004	LR: 0.050000
Training Epoch: 12 [13184/50000]	Loss: 1.8601	LR: 0.050000
Training Epoch: 12 [13312/50000]	Loss: 1.2605	LR: 0.050000
Training Epoch: 12 [13440/50000]	Loss: 1.2853	LR: 0.050000
Training Epoch: 12 [13568/50000]	Loss: 1.2797	LR: 0.050000
Training Epoch: 12 [13696/50000]	Loss: 1.2421	LR: 0.050000
Training Epoch: 12 [13824/50000]	Loss: 1.5081	LR: 0.050000
Training Epoch: 12 [13952/50000]	Loss: 1.6043	LR: 0.050000
Training Epoch: 12 [14080/50000]	Loss: 1.5706	LR: 0.050000
Training Epoch: 12 [14208/50000]	Loss: 1.2798	LR: 0.050000
Training Epoch: 12 [14336/50000]	Loss: 1.6450	LR: 0.050000
Training Epoch: 12 [14464/50000]	Loss: 1.3876	LR: 0.050000
Training Epoch: 12 [14592/50000]	Loss: 1.5250	LR: 0.050000
Training Epoch: 12 [14720/50000]	Loss: 1.3008	LR: 0.050000
Training Epoch: 12 [14848/50000]	Loss: 1.2929	LR: 0.050000
Training Epoch: 12 [14976/50000]	Loss: 1.7347	LR: 0.050000
Training Epoch: 12 [15104/50000]	Loss: 1.3886	LR: 0.050000
Training Epoch: 12 [15232/50000]	Loss: 1.3636	LR: 0.050000
Training Epoch: 12 [15360/50000]	Loss: 1.5379	LR: 0.050000
Training Epoch: 12 [15488/50000]	Loss: 1.5854	LR: 0.050000
Training Epoch: 12 [15616/50000]	Loss: 1.6759	LR: 0.050000
Training Epoch: 12 [15744/50000]	Loss: 1.3824	LR: 0.050000
Training Epoch: 12 [15872/50000]	Loss: 1.3450	LR: 0.050000
Training Epoch: 12 [16000/50000]	Loss: 1.4347	LR: 0.050000
Training Epoch: 12 [16128/50000]	Loss: 1.5025	LR: 0.050000
Training Epoch: 12 [16256/50000]	Loss: 1.6607	LR: 0.050000
Training Epoch: 12 [16384/50000]	Loss: 1.3243	LR: 0.050000
Training Epoch: 12 [16512/50000]	Loss: 1.3321	LR: 0.050000
Training Epoch: 12 [16640/50000]	Loss: 1.4176	LR: 0.050000
Training Epoch: 12 [16768/50000]	Loss: 1.2848	LR: 0.050000
Training Epoch: 12 [16896/50000]	Loss: 1.3118	LR: 0.050000
Training Epoch: 12 [17024/50000]	Loss: 1.4347	LR: 0.050000
Training Epoch: 12 [17152/50000]	Loss: 1.4657	LR: 0.050000
Training Epoch: 12 [17280/50000]	Loss: 1.5592	LR: 0.050000
Training Epoch: 12 [17408/50000]	Loss: 1.4145	LR: 0.050000
Training Epoch: 12 [17536/50000]	Loss: 1.4143	LR: 0.050000
Training Epoch: 12 [17664/50000]	Loss: 1.3820	LR: 0.050000
Training Epoch: 12 [17792/50000]	Loss: 1.2759	LR: 0.050000
Training Epoch: 12 [17920/50000]	Loss: 1.4732	LR: 0.050000
Training Epoch: 12 [18048/50000]	Loss: 1.5233	LR: 0.050000
Training Epoch: 12 [18176/50000]	Loss: 1.2638	LR: 0.050000
Training Epoch: 12 [18304/50000]	Loss: 1.6014	LR: 0.050000
Training Epoch: 12 [18432/50000]	Loss: 1.5402	LR: 0.050000
Training Epoch: 12 [18560/50000]	Loss: 1.4872	LR: 0.050000
Training Epoch: 12 [18688/50000]	Loss: 1.7135	LR: 0.050000
Training Epoch: 12 [18816/50000]	Loss: 1.4177	LR: 0.050000
Training Epoch: 12 [18944/50000]	Loss: 1.1744	LR: 0.050000
Training Epoch: 12 [19072/50000]	Loss: 1.4189	LR: 0.050000
Training Epoch: 12 [19200/50000]	Loss: 1.3462	LR: 0.050000
Training Epoch: 12 [19328/50000]	Loss: 1.5229	LR: 0.050000
Training Epoch: 12 [19456/50000]	Loss: 1.2861	LR: 0.050000
Training Epoch: 12 [19584/50000]	Loss: 1.4640	LR: 0.050000
Training Epoch: 12 [19712/50000]	Loss: 1.4921	LR: 0.050000
Training Epoch: 12 [19840/50000]	Loss: 1.6220	LR: 0.050000
Training Epoch: 12 [19968/50000]	Loss: 1.3525	LR: 0.050000
Training Epoch: 12 [20096/50000]	Loss: 1.6311	LR: 0.050000
Training Epoch: 12 [20224/50000]	Loss: 1.3459	LR: 0.050000
Training Epoch: 12 [20352/50000]	Loss: 1.4977	LR: 0.050000
Training Epoch: 12 [20480/50000]	Loss: 1.6373	LR: 0.050000
Training Epoch: 12 [20608/50000]	Loss: 1.2171	LR: 0.050000
Training Epoch: 12 [20736/50000]	Loss: 1.9242	LR: 0.050000
Training Epoch: 12 [20864/50000]	Loss: 1.4083	LR: 0.050000
Training Epoch: 12 [20992/50000]	Loss: 1.7042	LR: 0.050000
Training Epoch: 12 [21120/50000]	Loss: 1.3962	LR: 0.050000
Training Epoch: 12 [21248/50000]	Loss: 1.3740	LR: 0.050000
Training Epoch: 12 [21376/50000]	Loss: 1.7588	LR: 0.050000
Training Epoch: 12 [21504/50000]	Loss: 1.5562	LR: 0.050000
Training Epoch: 12 [21632/50000]	Loss: 1.6028	LR: 0.050000
Training Epoch: 12 [21760/50000]	Loss: 1.3058	LR: 0.050000
Training Epoch: 12 [21888/50000]	Loss: 1.2781	LR: 0.050000
Training Epoch: 12 [22016/50000]	Loss: 1.5534	LR: 0.050000
Training Epoch: 12 [22144/50000]	Loss: 1.3785	LR: 0.050000
Training Epoch: 12 [22272/50000]	Loss: 1.4822	LR: 0.050000
Training Epoch: 12 [22400/50000]	Loss: 1.7801	LR: 0.050000
Training Epoch: 12 [22528/50000]	Loss: 1.3409	LR: 0.050000
Training Epoch: 12 [22656/50000]	Loss: 1.4906	LR: 0.050000
Training Epoch: 12 [22784/50000]	Loss: 1.5454	LR: 0.050000
Training Epoch: 12 [22912/50000]	Loss: 1.6654	LR: 0.050000
Training Epoch: 12 [23040/50000]	Loss: 1.6180	LR: 0.050000
Training Epoch: 12 [23168/50000]	Loss: 1.7274	LR: 0.050000
Training Epoch: 12 [23296/50000]	Loss: 1.6263	LR: 0.050000
Training Epoch: 12 [23424/50000]	Loss: 1.3702	LR: 0.050000
Training Epoch: 12 [23552/50000]	Loss: 1.2492	LR: 0.050000
Training Epoch: 12 [23680/50000]	Loss: 1.3903	LR: 0.050000
Training Epoch: 12 [23808/50000]	Loss: 1.3375	LR: 0.050000
Training Epoch: 12 [23936/50000]	Loss: 1.4724	LR: 0.050000
Training Epoch: 12 [24064/50000]	Loss: 1.4741	LR: 0.050000
Training Epoch: 12 [24192/50000]	Loss: 1.4829	LR: 0.050000
Training Epoch: 12 [24320/50000]	Loss: 1.4812	LR: 0.050000
Training Epoch: 12 [24448/50000]	Loss: 1.5559	LR: 0.050000
Training Epoch: 12 [24576/50000]	Loss: 1.5711	LR: 0.050000
Training Epoch: 12 [24704/50000]	Loss: 1.3553	LR: 0.050000
Training Epoch: 12 [24832/50000]	Loss: 1.3011	LR: 0.050000
Training Epoch: 12 [24960/50000]	Loss: 1.4794	LR: 0.050000
Training Epoch: 12 [25088/50000]	Loss: 1.6695	LR: 0.050000
Training Epoch: 12 [25216/50000]	Loss: 1.3808	LR: 0.050000
Training Epoch: 12 [25344/50000]	Loss: 1.7054	LR: 0.050000
Training Epoch: 12 [25472/50000]	Loss: 1.6330	LR: 0.050000
Training Epoch: 12 [25600/50000]	Loss: 1.4885	LR: 0.050000
Training Epoch: 12 [25728/50000]	Loss: 1.4857	LR: 0.050000
Training Epoch: 12 [25856/50000]	Loss: 1.5046	LR: 0.050000
Training Epoch: 12 [25984/50000]	Loss: 1.5117	LR: 0.050000
Training Epoch: 12 [26112/50000]	Loss: 1.4636	LR: 0.050000
Training Epoch: 12 [26240/50000]	Loss: 1.4103	LR: 0.050000
Training Epoch: 12 [26368/50000]	Loss: 1.8219	LR: 0.050000
Training Epoch: 12 [26496/50000]	Loss: 1.3766	LR: 0.050000
Training Epoch: 12 [26624/50000]	Loss: 1.5672	LR: 0.050000
Training Epoch: 12 [26752/50000]	Loss: 1.7268	LR: 0.050000
Training Epoch: 12 [26880/50000]	Loss: 1.4133	LR: 0.050000
Training Epoch: 12 [27008/50000]	Loss: 1.4874	LR: 0.050000
Training Epoch: 12 [27136/50000]	Loss: 1.7294	LR: 0.050000
Training Epoch: 12 [27264/50000]	Loss: 1.6368	LR: 0.050000
Training Epoch: 12 [27392/50000]	Loss: 1.6615	LR: 0.050000
Training Epoch: 12 [27520/50000]	Loss: 1.3906	LR: 0.050000
Training Epoch: 12 [27648/50000]	Loss: 1.6767	LR: 0.050000
Training Epoch: 12 [27776/50000]	Loss: 1.7024	LR: 0.050000
Training Epoch: 12 [27904/50000]	Loss: 1.4913	LR: 0.050000
Training Epoch: 12 [28032/50000]	Loss: 1.7140	LR: 0.050000
Training Epoch: 12 [28160/50000]	Loss: 1.5511	LR: 0.050000
Training Epoch: 12 [28288/50000]	Loss: 1.6394	LR: 0.050000
Training Epoch: 12 [28416/50000]	Loss: 1.4329	LR: 0.050000
Training Epoch: 12 [28544/50000]	Loss: 1.5354	LR: 0.050000
Training Epoch: 12 [28672/50000]	Loss: 1.5653	LR: 0.050000
Training Epoch: 12 [28800/50000]	Loss: 1.7526	LR: 0.050000
Training Epoch: 12 [28928/50000]	Loss: 1.5207	LR: 0.050000
Training Epoch: 12 [29056/50000]	Loss: 1.5159	LR: 0.050000
Training Epoch: 12 [29184/50000]	Loss: 1.7922	LR: 0.050000
Training Epoch: 12 [29312/50000]	Loss: 1.3697	LR: 0.050000
Training Epoch: 12 [29440/50000]	Loss: 1.4055	LR: 0.050000
Training Epoch: 12 [29568/50000]	Loss: 1.4162	LR: 0.050000
Training Epoch: 12 [29696/50000]	Loss: 1.6785	LR: 0.050000
Training Epoch: 12 [29824/50000]	Loss: 1.5033	LR: 0.050000
Training Epoch: 12 [29952/50000]	Loss: 1.6041	LR: 0.050000
Training Epoch: 12 [30080/50000]	Loss: 1.6127	LR: 0.050000
Training Epoch: 12 [30208/50000]	Loss: 1.0825	LR: 0.050000
Training Epoch: 12 [30336/50000]	Loss: 1.5786	LR: 0.050000
Training Epoch: 12 [30464/50000]	Loss: 1.3007	LR: 0.050000
Training Epoch: 12 [30592/50000]	Loss: 1.4619	LR: 0.050000
Training Epoch: 12 [30720/50000]	Loss: 1.5390	LR: 0.050000
Training Epoch: 12 [30848/50000]	Loss: 1.5482	LR: 0.050000
Training Epoch: 12 [30976/50000]	Loss: 1.3390	LR: 0.050000
Training Epoch: 12 [31104/50000]	Loss: 1.5257	LR: 0.050000
Training Epoch: 12 [31232/50000]	Loss: 1.7171	LR: 0.050000
Training Epoch: 12 [31360/50000]	Loss: 1.6527	LR: 0.050000
Training Epoch: 12 [31488/50000]	Loss: 1.3656	LR: 0.050000
Training Epoch: 12 [31616/50000]	Loss: 1.5188	LR: 0.050000
Training Epoch: 12 [31744/50000]	Loss: 1.6140	LR: 0.050000
Training Epoch: 12 [31872/50000]	Loss: 1.9124	LR: 0.050000
Training Epoch: 12 [32000/50000]	Loss: 1.5099	LR: 0.050000
Training Epoch: 12 [32128/50000]	Loss: 1.4803	LR: 0.050000
Training Epoch: 12 [32256/50000]	Loss: 1.5965	LR: 0.050000
Training Epoch: 12 [32384/50000]	Loss: 1.7369	LR: 0.050000
Training Epoch: 12 [32512/50000]	Loss: 1.4148	LR: 0.050000
Training Epoch: 12 [32640/50000]	Loss: 1.4291	LR: 0.050000
Training Epoch: 12 [32768/50000]	Loss: 1.6303	LR: 0.050000
Training Epoch: 12 [32896/50000]	Loss: 1.3882	LR: 0.050000
Training Epoch: 12 [33024/50000]	Loss: 1.6827	LR: 0.050000
Training Epoch: 12 [33152/50000]	Loss: 1.3750	LR: 0.050000
Training Epoch: 12 [33280/50000]	Loss: 1.5180	LR: 0.050000
Training Epoch: 12 [33408/50000]	Loss: 1.4135	LR: 0.050000
Training Epoch: 12 [33536/50000]	Loss: 1.4407	LR: 0.050000
Training Epoch: 12 [33664/50000]	Loss: 1.7240	LR: 0.050000
Training Epoch: 12 [33792/50000]	Loss: 1.6031	LR: 0.050000
Training Epoch: 12 [33920/50000]	Loss: 1.6187	LR: 0.050000
Training Epoch: 12 [34048/50000]	Loss: 1.5678	LR: 0.050000
Training Epoch: 12 [34176/50000]	Loss: 1.5696	LR: 0.050000
Training Epoch: 12 [34304/50000]	Loss: 1.3996	LR: 0.050000
Training Epoch: 12 [34432/50000]	Loss: 1.4365	LR: 0.050000
Training Epoch: 12 [34560/50000]	Loss: 1.4979	LR: 0.050000
Training Epoch: 12 [34688/50000]	Loss: 1.5813	LR: 0.050000
Training Epoch: 12 [34816/50000]	Loss: 1.4029	LR: 0.050000
Training Epoch: 12 [34944/50000]	Loss: 1.5930	LR: 0.050000
Training Epoch: 12 [35072/50000]	Loss: 1.1514	LR: 0.050000
Training Epoch: 12 [35200/50000]	Loss: 1.4248	LR: 0.050000
Training Epoch: 12 [35328/50000]	Loss: 1.3685	LR: 0.050000
Training Epoch: 12 [35456/50000]	Loss: 1.4696	LR: 0.050000
Training Epoch: 12 [35584/50000]	Loss: 1.6493	LR: 0.050000
Training Epoch: 12 [35712/50000]	Loss: 1.3972	LR: 0.050000
Training Epoch: 12 [35840/50000]	Loss: 1.6452	LR: 0.050000
Training Epoch: 12 [35968/50000]	Loss: 1.4722	LR: 0.050000
Training Epoch: 12 [36096/50000]	Loss: 1.4973	LR: 0.050000
Training Epoch: 12 [36224/50000]	Loss: 1.6059	LR: 0.050000
Training Epoch: 12 [36352/50000]	Loss: 1.5163	LR: 0.050000
Training Epoch: 12 [36480/50000]	Loss: 1.4931	LR: 0.050000
Training Epoch: 12 [36608/50000]	Loss: 1.3694	LR: 0.050000
Training Epoch: 12 [36736/50000]	Loss: 1.5488	LR: 0.050000
Training Epoch: 12 [36864/50000]	Loss: 1.3889	LR: 0.050000
Training Epoch: 12 [36992/50000]	Loss: 1.5248	LR: 0.050000
Training Epoch: 12 [37120/50000]	Loss: 1.5540	LR: 0.050000
Training Epoch: 12 [37248/50000]	Loss: 1.3302	LR: 0.050000
Training Epoch: 12 [37376/50000]	Loss: 1.4625	LR: 0.050000
Training Epoch: 12 [37504/50000]	Loss: 1.5283	LR: 0.050000
Training Epoch: 12 [37632/50000]	Loss: 1.7278	LR: 0.050000
Training Epoch: 12 [37760/50000]	Loss: 1.4636	LR: 0.050000
Training Epoch: 12 [37888/50000]	Loss: 1.4385	LR: 0.050000
Training Epoch: 12 [38016/50000]	Loss: 1.4123	LR: 0.050000
Training Epoch: 12 [38144/50000]	Loss: 1.7141	LR: 0.050000
Training Epoch: 12 [38272/50000]	Loss: 1.6080	LR: 0.050000
Training Epoch: 12 [38400/50000]	Loss: 1.4486	LR: 0.050000
Training Epoch: 12 [38528/50000]	Loss: 1.2963	LR: 0.050000
Training Epoch: 12 [38656/50000]	Loss: 1.6722	LR: 0.050000
Training Epoch: 12 [38784/50000]	Loss: 1.4981	LR: 0.050000
Training Epoch: 12 [38912/50000]	Loss: 1.4339	LR: 0.050000
Training Epoch: 12 [39040/50000]	Loss: 1.4244	LR: 0.050000
Training Epoch: 12 [39168/50000]	Loss: 1.4329	LR: 0.050000
Training Epoch: 12 [39296/50000]	Loss: 1.6317	LR: 0.050000
Training Epoch: 12 [39424/50000]	Loss: 1.5239	LR: 0.050000
Training Epoch: 12 [39552/50000]	Loss: 1.2507	LR: 0.050000
Training Epoch: 12 [39680/50000]	Loss: 1.4386	LR: 0.050000
Training Epoch: 12 [39808/50000]	Loss: 1.3123	LR: 0.050000
Training Epoch: 12 [39936/50000]	Loss: 1.6930	LR: 0.050000
Training Epoch: 12 [40064/50000]	Loss: 1.6750	LR: 0.050000
Training Epoch: 12 [40192/50000]	Loss: 1.6059	LR: 0.050000
Training Epoch: 12 [40320/50000]	Loss: 1.4036	LR: 0.050000
Training Epoch: 12 [40448/50000]	Loss: 1.5047	LR: 0.050000
Training Epoch: 12 [40576/50000]	Loss: 1.5741	LR: 0.050000
Training Epoch: 12 [40704/50000]	Loss: 1.1111	LR: 0.050000
Training Epoch: 12 [40832/50000]	Loss: 1.4031	LR: 0.050000
Training Epoch: 12 [40960/50000]	Loss: 1.7147	LR: 0.050000
Training Epoch: 12 [41088/50000]	Loss: 1.4678	LR: 0.050000
Training Epoch: 12 [41216/50000]	Loss: 1.8270	LR: 0.050000
Training Epoch: 12 [41344/50000]	Loss: 1.5409	LR: 0.050000
Training Epoch: 12 [41472/50000]	Loss: 1.2419	LR: 0.050000
Training Epoch: 12 [41600/50000]	Loss: 1.3575	LR: 0.050000
Training Epoch: 12 [41728/50000]	Loss: 1.3781	LR: 0.050000
Training Epoch: 12 [41856/50000]	Loss: 1.4587	LR: 0.050000
Training Epoch: 12 [41984/50000]	Loss: 1.4580	LR: 0.050000
Training Epoch: 12 [42112/50000]	Loss: 1.6489	LR: 0.050000
Training Epoch: 12 [42240/50000]	Loss: 1.4195	LR: 0.050000
Training Epoch: 12 [42368/50000]	Loss: 1.6536	LR: 0.050000
Training Epoch: 12 [42496/50000]	Loss: 1.5620	LR: 0.050000
Training Epoch: 12 [42624/50000]	Loss: 1.6802	LR: 0.050000
Training Epoch: 12 [42752/50000]	Loss: 1.4545	LR: 0.050000
Training Epoch: 12 [42880/50000]	Loss: 1.7027	LR: 0.050000
Training Epoch: 12 [43008/50000]	Loss: 1.3765	LR: 0.050000
Training Epoch: 12 [43136/50000]	Loss: 1.4917	LR: 0.050000
Training Epoch: 12 [43264/50000]	Loss: 1.6483	LR: 0.050000
Training Epoch: 12 [43392/50000]	Loss: 1.5892	LR: 0.050000
Training Epoch: 12 [43520/50000]	Loss: 1.6174	LR: 0.050000
Training Epoch: 12 [43648/50000]	Loss: 1.5092	LR: 0.050000
Training Epoch: 12 [43776/50000]	Loss: 1.4958	LR: 0.050000
Training Epoch: 12 [43904/50000]	Loss: 1.1986	LR: 0.050000
Training Epoch: 12 [44032/50000]	Loss: 1.1671	LR: 0.050000
Training Epoch: 12 [44160/50000]	Loss: 1.6081	LR: 0.050000
Training Epoch: 12 [44288/50000]	Loss: 1.6300	LR: 0.050000
Training Epoch: 12 [44416/50000]	Loss: 1.6453	LR: 0.050000
Training Epoch: 12 [44544/50000]	Loss: 1.4031	LR: 0.050000
Training Epoch: 12 [44672/50000]	Loss: 1.7434	LR: 0.050000
Training Epoch: 12 [44800/50000]	Loss: 1.5454	LR: 0.050000
Training Epoch: 12 [44928/50000]	Loss: 1.5329	LR: 0.050000
Training Epoch: 12 [45056/50000]	Loss: 1.4416	LR: 0.050000
Training Epoch: 12 [45184/50000]	Loss: 1.6558	LR: 0.050000
Training Epoch: 12 [45312/50000]	Loss: 1.3965	LR: 0.050000
Training Epoch: 12 [45440/50000]	Loss: 1.5561	LR: 0.050000
Training Epoch: 12 [45568/50000]	Loss: 1.7751	LR: 0.050000
Training Epoch: 12 [45696/50000]	Loss: 1.4727	LR: 0.050000
Training Epoch: 12 [45824/50000]	Loss: 1.3580	LR: 0.050000
Training Epoch: 12 [45952/50000]	Loss: 1.4685	LR: 0.050000
Training Epoch: 12 [46080/50000]	Loss: 1.5733	LR: 0.050000
Training Epoch: 12 [46208/50000]	Loss: 1.7063	LR: 0.050000
Training Epoch: 12 [46336/50000]	Loss: 1.5641	LR: 0.050000
Training Epoch: 12 [46464/50000]	Loss: 1.8209	LR: 0.050000
Training Epoch: 12 [46592/50000]	Loss: 1.4629	LR: 0.050000
Training Epoch: 12 [46720/50000]	Loss: 1.4473	LR: 0.050000
Training Epoch: 12 [46848/50000]	Loss: 1.4388	LR: 0.050000
Training Epoch: 12 [46976/50000]	Loss: 1.3892	LR: 0.050000
Training Epoch: 12 [47104/50000]	Loss: 1.3605	LR: 0.050000
Training Epoch: 12 [47232/50000]	Loss: 1.4586	LR: 0.050000
Training Epoch: 12 [47360/50000]	Loss: 1.3596	LR: 0.050000
Training Epoch: 12 [47488/50000]	Loss: 1.2491	LR: 0.050000
Training Epoch: 12 [47616/50000]	Loss: 1.5504	LR: 0.050000
Training Epoch: 12 [47744/50000]	Loss: 1.4671	LR: 0.050000
Training Epoch: 12 [47872/50000]	Loss: 1.4224	LR: 0.050000
Training Epoch: 12 [48000/50000]	Loss: 1.7033	LR: 0.050000
Training Epoch: 12 [48128/50000]	Loss: 1.5202	LR: 0.050000
Training Epoch: 12 [48256/50000]	Loss: 1.4545	LR: 0.050000
Training Epoch: 12 [48384/50000]	Loss: 1.6266	LR: 0.050000
Training Epoch: 12 [48512/50000]	Loss: 1.4565	LR: 0.050000
Training Epoch: 12 [48640/50000]	Loss: 1.3084	LR: 0.050000
Training Epoch: 12 [48768/50000]	Loss: 1.8219	LR: 0.050000
Training Epoch: 12 [48896/50000]	Loss: 1.5650	LR: 0.050000
Training Epoch: 12 [49024/50000]	Loss: 1.5311	LR: 0.050000
Training Epoch: 12 [49152/50000]	Loss: 1.5769	LR: 0.050000
Training Epoch: 12 [49280/50000]	Loss: 1.2842	LR: 0.050000
Training Epoch: 12 [49408/50000]	Loss: 1.7308	LR: 0.050000
Training Epoch: 12 [49536/50000]	Loss: 1.5783	LR: 0.050000
Training Epoch: 12 [49664/50000]	Loss: 1.4914	LR: 0.050000
Training Epoch: 12 [49792/50000]	Loss: 1.6300	LR: 0.050000
Training Epoch: 12 [49920/50000]	Loss: 1.2943	LR: 0.050000
Training Epoch: 12 [50000/50000]	Loss: 1.5992	LR: 0.050000
epoch 12 training time consumed: 53.99s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   44041 GB |   44041 GB |
|       from large pool |  123392 KB |    1034 MB |   43997 GB |   43997 GB |
|       from small pool |   10798 KB |      13 MB |      43 GB |      43 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   44041 GB |   44041 GB |
|       from large pool |  123392 KB |    1034 MB |   43997 GB |   43997 GB |
|       from small pool |   10798 KB |      13 MB |      43 GB |      43 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   19383 GB |   19383 GB |
|       from large pool |  155136 KB |  433088 KB |   19335 GB |   19335 GB |
|       from small pool |    1490 KB |    3494 KB |      48 GB |      48 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    1699 K  |    1699 K  |
|       from large pool |      24    |      65    |     887 K  |     887 K  |
|       from small pool |     231    |     274    |     812 K  |     812 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    1699 K  |    1699 K  |
|       from large pool |      24    |      65    |     887 K  |     887 K  |
|       from small pool |     231    |     274    |     812 K  |     812 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |     843 K  |     843 K  |
|       from large pool |       9    |      14    |     429 K  |     429 K  |
|       from small pool |      12    |      16    |     413 K  |     413 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 12, Average loss: 0.0133, Accuracy: 0.5331, Time consumed:3.48s

Training Epoch: 13 [128/50000]	Loss: 1.3392	LR: 0.050000
Training Epoch: 13 [256/50000]	Loss: 1.3209	LR: 0.050000
Training Epoch: 13 [384/50000]	Loss: 1.3610	LR: 0.050000
Training Epoch: 13 [512/50000]	Loss: 1.3727	LR: 0.050000
Training Epoch: 13 [640/50000]	Loss: 1.3175	LR: 0.050000
Training Epoch: 13 [768/50000]	Loss: 1.4171	LR: 0.050000
Training Epoch: 13 [896/50000]	Loss: 1.4370	LR: 0.050000
Training Epoch: 13 [1024/50000]	Loss: 1.3633	LR: 0.050000
Training Epoch: 13 [1152/50000]	Loss: 1.4901	LR: 0.050000
Training Epoch: 13 [1280/50000]	Loss: 1.3416	LR: 0.050000
Training Epoch: 13 [1408/50000]	Loss: 1.4729	LR: 0.050000
Training Epoch: 13 [1536/50000]	Loss: 1.3855	LR: 0.050000
Training Epoch: 13 [1664/50000]	Loss: 1.2687	LR: 0.050000
Training Epoch: 13 [1792/50000]	Loss: 1.3605	LR: 0.050000
Training Epoch: 13 [1920/50000]	Loss: 1.5874	LR: 0.050000
Training Epoch: 13 [2048/50000]	Loss: 1.3521	LR: 0.050000
Training Epoch: 13 [2176/50000]	Loss: 1.5695	LR: 0.050000
Training Epoch: 13 [2304/50000]	Loss: 1.3852	LR: 0.050000
Training Epoch: 13 [2432/50000]	Loss: 1.1731	LR: 0.050000
Training Epoch: 13 [2560/50000]	Loss: 1.1878	LR: 0.050000
Training Epoch: 13 [2688/50000]	Loss: 1.4704	LR: 0.050000
Training Epoch: 13 [2816/50000]	Loss: 1.5521	LR: 0.050000
Training Epoch: 13 [2944/50000]	Loss: 1.2253	LR: 0.050000
Training Epoch: 13 [3072/50000]	Loss: 1.4421	LR: 0.050000
Training Epoch: 13 [3200/50000]	Loss: 1.4523	LR: 0.050000
Training Epoch: 13 [3328/50000]	Loss: 1.4703	LR: 0.050000
Training Epoch: 13 [3456/50000]	Loss: 1.6942	LR: 0.050000
Training Epoch: 13 [3584/50000]	Loss: 1.4164	LR: 0.050000
Training Epoch: 13 [3712/50000]	Loss: 1.3791	LR: 0.050000
Training Epoch: 13 [3840/50000]	Loss: 1.4004	LR: 0.050000
Training Epoch: 13 [3968/50000]	Loss: 1.1408	LR: 0.050000
Training Epoch: 13 [4096/50000]	Loss: 1.3502	LR: 0.050000
Training Epoch: 13 [4224/50000]	Loss: 1.3569	LR: 0.050000
Training Epoch: 13 [4352/50000]	Loss: 1.3236	LR: 0.050000
Training Epoch: 13 [4480/50000]	Loss: 1.6077	LR: 0.050000
Training Epoch: 13 [4608/50000]	Loss: 1.3377	LR: 0.050000
Training Epoch: 13 [4736/50000]	Loss: 1.2257	LR: 0.050000
Training Epoch: 13 [4864/50000]	Loss: 1.3917	LR: 0.050000
Training Epoch: 13 [4992/50000]	Loss: 1.1809	LR: 0.050000
Training Epoch: 13 [5120/50000]	Loss: 1.4185	LR: 0.050000
Training Epoch: 13 [5248/50000]	Loss: 1.5668	LR: 0.050000
Training Epoch: 13 [5376/50000]	Loss: 1.6421	LR: 0.050000
Training Epoch: 13 [5504/50000]	Loss: 1.2572	LR: 0.050000
Training Epoch: 13 [5632/50000]	Loss: 1.2496	LR: 0.050000
Training Epoch: 13 [5760/50000]	Loss: 1.4868	LR: 0.050000
Training Epoch: 13 [5888/50000]	Loss: 1.4840	LR: 0.050000
Training Epoch: 13 [6016/50000]	Loss: 1.5556	LR: 0.050000
Training Epoch: 13 [6144/50000]	Loss: 1.4816	LR: 0.050000
Training Epoch: 13 [6272/50000]	Loss: 1.4719	LR: 0.050000
Training Epoch: 13 [6400/50000]	Loss: 1.1772	LR: 0.050000
Training Epoch: 13 [6528/50000]	Loss: 1.4403	LR: 0.050000
Training Epoch: 13 [6656/50000]	Loss: 1.3477	LR: 0.050000
Training Epoch: 13 [6784/50000]	Loss: 1.7204	LR: 0.050000
Training Epoch: 13 [6912/50000]	Loss: 1.1886	LR: 0.050000
Training Epoch: 13 [7040/50000]	Loss: 1.3372	LR: 0.050000
Training Epoch: 13 [7168/50000]	Loss: 1.4030	LR: 0.050000
Training Epoch: 13 [7296/50000]	Loss: 1.5121	LR: 0.050000
Training Epoch: 13 [7424/50000]	Loss: 1.5170	LR: 0.050000
Training Epoch: 13 [7552/50000]	Loss: 1.5376	LR: 0.050000
Training Epoch: 13 [7680/50000]	Loss: 1.4362	LR: 0.050000
Training Epoch: 13 [7808/50000]	Loss: 1.4933	LR: 0.050000
Training Epoch: 13 [7936/50000]	Loss: 1.2610	LR: 0.050000
Training Epoch: 13 [8064/50000]	Loss: 1.6907	LR: 0.050000
Training Epoch: 13 [8192/50000]	Loss: 1.4369	LR: 0.050000
Training Epoch: 13 [8320/50000]	Loss: 1.3156	LR: 0.050000
Training Epoch: 13 [8448/50000]	Loss: 1.3297	LR: 0.050000
Training Epoch: 13 [8576/50000]	Loss: 1.3719	LR: 0.050000
Training Epoch: 13 [8704/50000]	Loss: 1.3926	LR: 0.050000
Training Epoch: 13 [8832/50000]	Loss: 1.3537	LR: 0.050000
Training Epoch: 13 [8960/50000]	Loss: 1.3485	LR: 0.050000
Training Epoch: 13 [9088/50000]	Loss: 1.5239	LR: 0.050000
Training Epoch: 13 [9216/50000]	Loss: 1.4616	LR: 0.050000
Training Epoch: 13 [9344/50000]	Loss: 1.3316	LR: 0.050000
Training Epoch: 13 [9472/50000]	Loss: 1.4574	LR: 0.050000
Training Epoch: 13 [9600/50000]	Loss: 1.5073	LR: 0.050000
Training Epoch: 13 [9728/50000]	Loss: 1.6615	LR: 0.050000
Training Epoch: 13 [9856/50000]	Loss: 1.3572	LR: 0.050000
Training Epoch: 13 [9984/50000]	Loss: 1.3741	LR: 0.050000
Training Epoch: 13 [10112/50000]	Loss: 1.6070	LR: 0.050000
Training Epoch: 13 [10240/50000]	Loss: 1.3742	LR: 0.050000
Training Epoch: 13 [10368/50000]	Loss: 1.3424	LR: 0.050000
Training Epoch: 13 [10496/50000]	Loss: 1.4426	LR: 0.050000
Training Epoch: 13 [10624/50000]	Loss: 1.6475	LR: 0.050000
Training Epoch: 13 [10752/50000]	Loss: 1.3991	LR: 0.050000
Training Epoch: 13 [10880/50000]	Loss: 1.6456	LR: 0.050000
Training Epoch: 13 [11008/50000]	Loss: 1.5644	LR: 0.050000
Training Epoch: 13 [11136/50000]	Loss: 1.4582	LR: 0.050000
Training Epoch: 13 [11264/50000]	Loss: 1.1630	LR: 0.050000
Training Epoch: 13 [11392/50000]	Loss: 1.3805	LR: 0.050000
Training Epoch: 13 [11520/50000]	Loss: 1.2225	LR: 0.050000
Training Epoch: 13 [11648/50000]	Loss: 1.3205	LR: 0.050000
Training Epoch: 13 [11776/50000]	Loss: 1.5643	LR: 0.050000
Training Epoch: 13 [11904/50000]	Loss: 1.3760	LR: 0.050000
Training Epoch: 13 [12032/50000]	Loss: 1.3319	LR: 0.050000
Training Epoch: 13 [12160/50000]	Loss: 1.2311	LR: 0.050000
Training Epoch: 13 [12288/50000]	Loss: 1.5191	LR: 0.050000
Training Epoch: 13 [12416/50000]	Loss: 1.3863	LR: 0.050000
Training Epoch: 13 [12544/50000]	Loss: 1.3220	LR: 0.050000
Training Epoch: 13 [12672/50000]	Loss: 1.5035	LR: 0.050000
Training Epoch: 13 [12800/50000]	Loss: 1.4093	LR: 0.050000
Training Epoch: 13 [12928/50000]	Loss: 1.5441	LR: 0.050000
Training Epoch: 13 [13056/50000]	Loss: 1.6910	LR: 0.050000
Training Epoch: 13 [13184/50000]	Loss: 1.3891	LR: 0.050000
Training Epoch: 13 [13312/50000]	Loss: 1.4484	LR: 0.050000
Training Epoch: 13 [13440/50000]	Loss: 1.4505	LR: 0.050000
Training Epoch: 13 [13568/50000]	Loss: 1.3458	LR: 0.050000
Training Epoch: 13 [13696/50000]	Loss: 1.3663	LR: 0.050000
Training Epoch: 13 [13824/50000]	Loss: 1.3405	LR: 0.050000
Training Epoch: 13 [13952/50000]	Loss: 1.4013	LR: 0.050000
Training Epoch: 13 [14080/50000]	Loss: 1.2699	LR: 0.050000
Training Epoch: 13 [14208/50000]	Loss: 1.2933	LR: 0.050000
Training Epoch: 13 [14336/50000]	Loss: 1.4271	LR: 0.050000
Training Epoch: 13 [14464/50000]	Loss: 1.6242	LR: 0.050000
Training Epoch: 13 [14592/50000]	Loss: 1.5535	LR: 0.050000
Training Epoch: 13 [14720/50000]	Loss: 1.4959	LR: 0.050000
Training Epoch: 13 [14848/50000]	Loss: 1.3101	LR: 0.050000
Training Epoch: 13 [14976/50000]	Loss: 1.5057	LR: 0.050000
Training Epoch: 13 [15104/50000]	Loss: 1.5883	LR: 0.050000
Training Epoch: 13 [15232/50000]	Loss: 1.3504	LR: 0.050000
Training Epoch: 13 [15360/50000]	Loss: 1.4684	LR: 0.050000
Training Epoch: 13 [15488/50000]	Loss: 1.3649	LR: 0.050000
Training Epoch: 13 [15616/50000]	Loss: 1.2452	LR: 0.050000
Training Epoch: 13 [15744/50000]	Loss: 1.6054	LR: 0.050000
Training Epoch: 13 [15872/50000]	Loss: 1.2309	LR: 0.050000
Training Epoch: 13 [16000/50000]	Loss: 1.6697	LR: 0.050000
Training Epoch: 13 [16128/50000]	Loss: 1.4019	LR: 0.050000
Training Epoch: 13 [16256/50000]	Loss: 1.3366	LR: 0.050000
Training Epoch: 13 [16384/50000]	Loss: 1.5261	LR: 0.050000
Training Epoch: 13 [16512/50000]	Loss: 1.4387	LR: 0.050000
Training Epoch: 13 [16640/50000]	Loss: 1.5306	LR: 0.050000
Training Epoch: 13 [16768/50000]	Loss: 1.4349	LR: 0.050000
Training Epoch: 13 [16896/50000]	Loss: 1.2502	LR: 0.050000
Training Epoch: 13 [17024/50000]	Loss: 1.4341	LR: 0.050000
Training Epoch: 13 [17152/50000]	Loss: 1.4541	LR: 0.050000
Training Epoch: 13 [17280/50000]	Loss: 1.4331	LR: 0.050000
Training Epoch: 13 [17408/50000]	Loss: 1.4731	LR: 0.050000
Training Epoch: 13 [17536/50000]	Loss: 1.4588	LR: 0.050000
Training Epoch: 13 [17664/50000]	Loss: 1.1455	LR: 0.050000
Training Epoch: 13 [17792/50000]	Loss: 1.3267	LR: 0.050000
Training Epoch: 13 [17920/50000]	Loss: 1.5391	LR: 0.050000
Training Epoch: 13 [18048/50000]	Loss: 1.2032	LR: 0.050000
Training Epoch: 13 [18176/50000]	Loss: 1.1866	LR: 0.050000
Training Epoch: 13 [18304/50000]	Loss: 1.6274	LR: 0.050000
Training Epoch: 13 [18432/50000]	Loss: 1.4293	LR: 0.050000
Training Epoch: 13 [18560/50000]	Loss: 1.3800	LR: 0.050000
Training Epoch: 13 [18688/50000]	Loss: 1.5655	LR: 0.050000
Training Epoch: 13 [18816/50000]	Loss: 1.5531	LR: 0.050000
Training Epoch: 13 [18944/50000]	Loss: 1.1040	LR: 0.050000
Training Epoch: 13 [19072/50000]	Loss: 1.4695	LR: 0.050000
Training Epoch: 13 [19200/50000]	Loss: 1.3414	LR: 0.050000
Training Epoch: 13 [19328/50000]	Loss: 1.4899	LR: 0.050000
Training Epoch: 13 [19456/50000]	Loss: 1.4486	LR: 0.050000
Training Epoch: 13 [19584/50000]	Loss: 1.4557	LR: 0.050000
Training Epoch: 13 [19712/50000]	Loss: 1.2900	LR: 0.050000
Training Epoch: 13 [19840/50000]	Loss: 1.2344	LR: 0.050000
Training Epoch: 13 [19968/50000]	Loss: 1.5686	LR: 0.050000
Training Epoch: 13 [20096/50000]	Loss: 1.4607	LR: 0.050000
Training Epoch: 13 [20224/50000]	Loss: 1.5374	LR: 0.050000
Training Epoch: 13 [20352/50000]	Loss: 1.2795	LR: 0.050000
Training Epoch: 13 [20480/50000]	Loss: 1.5184	LR: 0.050000
Training Epoch: 13 [20608/50000]	Loss: 1.3293	LR: 0.050000
Training Epoch: 13 [20736/50000]	Loss: 1.4810	LR: 0.050000
Training Epoch: 13 [20864/50000]	Loss: 1.3414	LR: 0.050000
Training Epoch: 13 [20992/50000]	Loss: 1.2172	LR: 0.050000
Training Epoch: 13 [21120/50000]	Loss: 1.3897	LR: 0.050000
Training Epoch: 13 [21248/50000]	Loss: 1.2706	LR: 0.050000
Training Epoch: 13 [21376/50000]	Loss: 1.6366	LR: 0.050000
Training Epoch: 13 [21504/50000]	Loss: 1.3294	LR: 0.050000
Training Epoch: 13 [21632/50000]	Loss: 1.4585	LR: 0.050000
Training Epoch: 13 [21760/50000]	Loss: 1.2756	LR: 0.050000
Training Epoch: 13 [21888/50000]	Loss: 1.4864	LR: 0.050000
Training Epoch: 13 [22016/50000]	Loss: 1.5158	LR: 0.050000
Training Epoch: 13 [22144/50000]	Loss: 1.6876	LR: 0.050000
Training Epoch: 13 [22272/50000]	Loss: 1.2301	LR: 0.050000
Training Epoch: 13 [22400/50000]	Loss: 1.4292	LR: 0.050000
Training Epoch: 13 [22528/50000]	Loss: 1.4048	LR: 0.050000
Training Epoch: 13 [22656/50000]	Loss: 1.4238	LR: 0.050000
Training Epoch: 13 [22784/50000]	Loss: 1.4032	LR: 0.050000
Training Epoch: 13 [22912/50000]	Loss: 1.4376	LR: 0.050000
Training Epoch: 13 [23040/50000]	Loss: 1.4184	LR: 0.050000
Training Epoch: 13 [23168/50000]	Loss: 1.5885	LR: 0.050000
Training Epoch: 13 [23296/50000]	Loss: 1.3473	LR: 0.050000
Training Epoch: 13 [23424/50000]	Loss: 1.6131	LR: 0.050000
Training Epoch: 13 [23552/50000]	Loss: 1.4345	LR: 0.050000
Training Epoch: 13 [23680/50000]	Loss: 1.4535	LR: 0.050000
Training Epoch: 13 [23808/50000]	Loss: 1.5453	LR: 0.050000
Training Epoch: 13 [23936/50000]	Loss: 1.5301	LR: 0.050000
Training Epoch: 13 [24064/50000]	Loss: 1.5459	LR: 0.050000
Training Epoch: 13 [24192/50000]	Loss: 1.4694	LR: 0.050000
Training Epoch: 13 [24320/50000]	Loss: 1.4947	LR: 0.050000
Training Epoch: 13 [24448/50000]	Loss: 1.5029	LR: 0.050000
Training Epoch: 13 [24576/50000]	Loss: 1.5237	LR: 0.050000
Training Epoch: 13 [24704/50000]	Loss: 1.5672	LR: 0.050000
Training Epoch: 13 [24832/50000]	Loss: 1.6021	LR: 0.050000
Training Epoch: 13 [24960/50000]	Loss: 1.3979	LR: 0.050000
Training Epoch: 13 [25088/50000]	Loss: 1.5895	LR: 0.050000
Training Epoch: 13 [25216/50000]	Loss: 1.5563	LR: 0.050000
Training Epoch: 13 [25344/50000]	Loss: 1.4363	LR: 0.050000
Training Epoch: 13 [25472/50000]	Loss: 1.1552	LR: 0.050000
Training Epoch: 13 [25600/50000]	Loss: 1.4019	LR: 0.050000
Training Epoch: 13 [25728/50000]	Loss: 1.3775	LR: 0.050000
Training Epoch: 13 [25856/50000]	Loss: 1.6576	LR: 0.050000
Training Epoch: 13 [25984/50000]	Loss: 1.7198	LR: 0.050000
Training Epoch: 13 [26112/50000]	Loss: 1.6329	LR: 0.050000
Training Epoch: 13 [26240/50000]	Loss: 1.4676	LR: 0.050000
Training Epoch: 13 [26368/50000]	Loss: 1.3736	LR: 0.050000
Training Epoch: 13 [26496/50000]	Loss: 1.3688	LR: 0.050000
Training Epoch: 13 [26624/50000]	Loss: 1.4251	LR: 0.050000
Training Epoch: 13 [26752/50000]	Loss: 1.2865	LR: 0.050000
Training Epoch: 13 [26880/50000]	Loss: 1.2637	LR: 0.050000
Training Epoch: 13 [27008/50000]	Loss: 1.5468	LR: 0.050000
Training Epoch: 13 [27136/50000]	Loss: 1.3732	LR: 0.050000
Training Epoch: 13 [27264/50000]	Loss: 1.5088	LR: 0.050000
Training Epoch: 13 [27392/50000]	Loss: 1.5927	LR: 0.050000
Training Epoch: 13 [27520/50000]	Loss: 1.4718	LR: 0.050000
Training Epoch: 13 [27648/50000]	Loss: 1.3931	LR: 0.050000
Training Epoch: 13 [27776/50000]	Loss: 1.4372	LR: 0.050000
Training Epoch: 13 [27904/50000]	Loss: 1.2927	LR: 0.050000
Training Epoch: 13 [28032/50000]	Loss: 1.4417	LR: 0.050000
Training Epoch: 13 [28160/50000]	Loss: 1.3635	LR: 0.050000
Training Epoch: 13 [28288/50000]	Loss: 1.5746	LR: 0.050000
Training Epoch: 13 [28416/50000]	Loss: 1.2113	LR: 0.050000
Training Epoch: 13 [28544/50000]	Loss: 1.3267	LR: 0.050000
Training Epoch: 13 [28672/50000]	Loss: 1.2751	LR: 0.050000
Training Epoch: 13 [28800/50000]	Loss: 1.4419	LR: 0.050000
Training Epoch: 13 [28928/50000]	Loss: 1.3836	LR: 0.050000
Training Epoch: 13 [29056/50000]	Loss: 1.3187	LR: 0.050000
Training Epoch: 13 [29184/50000]	Loss: 1.6520	LR: 0.050000
Training Epoch: 13 [29312/50000]	Loss: 1.3988	LR: 0.050000
Training Epoch: 13 [29440/50000]	Loss: 1.4693	LR: 0.050000
Training Epoch: 13 [29568/50000]	Loss: 1.4459	LR: 0.050000
Training Epoch: 13 [29696/50000]	Loss: 1.6105	LR: 0.050000
Training Epoch: 13 [29824/50000]	Loss: 1.5535	LR: 0.050000
Training Epoch: 13 [29952/50000]	Loss: 1.5219	LR: 0.050000
Training Epoch: 13 [30080/50000]	Loss: 1.4610	LR: 0.050000
Training Epoch: 13 [30208/50000]	Loss: 1.2795	LR: 0.050000
Training Epoch: 13 [30336/50000]	Loss: 1.3483	LR: 0.050000
Training Epoch: 13 [30464/50000]	Loss: 1.4458	LR: 0.050000
Training Epoch: 13 [30592/50000]	Loss: 1.2748	LR: 0.050000
Training Epoch: 13 [30720/50000]	Loss: 1.3464	LR: 0.050000
Training Epoch: 13 [30848/50000]	Loss: 1.2870	LR: 0.050000
Training Epoch: 13 [30976/50000]	Loss: 1.4366	LR: 0.050000
Training Epoch: 13 [31104/50000]	Loss: 1.3184	LR: 0.050000
Training Epoch: 13 [31232/50000]	Loss: 1.4650	LR: 0.050000
Training Epoch: 13 [31360/50000]	Loss: 1.2652	LR: 0.050000
Training Epoch: 13 [31488/50000]	Loss: 1.5447	LR: 0.050000
Training Epoch: 13 [31616/50000]	Loss: 1.3628	LR: 0.050000
Training Epoch: 13 [31744/50000]	Loss: 1.3961	LR: 0.050000
Training Epoch: 13 [31872/50000]	Loss: 1.5061	LR: 0.050000
Training Epoch: 13 [32000/50000]	Loss: 1.2394	LR: 0.050000
Training Epoch: 13 [32128/50000]	Loss: 1.5313	LR: 0.050000
Training Epoch: 13 [32256/50000]	Loss: 1.2803	LR: 0.050000
Training Epoch: 13 [32384/50000]	Loss: 1.4946	LR: 0.050000
Training Epoch: 13 [32512/50000]	Loss: 1.5197	LR: 0.050000
Training Epoch: 13 [32640/50000]	Loss: 1.3639	LR: 0.050000
Training Epoch: 13 [32768/50000]	Loss: 1.5332	LR: 0.050000
Training Epoch: 13 [32896/50000]	Loss: 1.5495	LR: 0.050000
Training Epoch: 13 [33024/50000]	Loss: 1.4104	LR: 0.050000
Training Epoch: 13 [33152/50000]	Loss: 1.5080	LR: 0.050000
Training Epoch: 13 [33280/50000]	Loss: 1.5959	LR: 0.050000
Training Epoch: 13 [33408/50000]	Loss: 1.4781	LR: 0.050000
Training Epoch: 13 [33536/50000]	Loss: 1.6407	LR: 0.050000
Training Epoch: 13 [33664/50000]	Loss: 1.3150	LR: 0.050000
Training Epoch: 13 [33792/50000]	Loss: 1.2999	LR: 0.050000
Training Epoch: 13 [33920/50000]	Loss: 1.4117	LR: 0.050000
Training Epoch: 13 [34048/50000]	Loss: 1.5805	LR: 0.050000
Training Epoch: 13 [34176/50000]	Loss: 1.4765	LR: 0.050000
Training Epoch: 13 [34304/50000]	Loss: 1.5219	LR: 0.050000
Training Epoch: 13 [34432/50000]	Loss: 1.4641	LR: 0.050000
Training Epoch: 13 [34560/50000]	Loss: 1.2903	LR: 0.050000
Training Epoch: 13 [34688/50000]	Loss: 1.3787	LR: 0.050000
Training Epoch: 13 [34816/50000]	Loss: 1.5723	LR: 0.050000
Training Epoch: 13 [34944/50000]	Loss: 1.4595	LR: 0.050000
Training Epoch: 13 [35072/50000]	Loss: 1.4801	LR: 0.050000
Training Epoch: 13 [35200/50000]	Loss: 1.4043	LR: 0.050000
Training Epoch: 13 [35328/50000]	Loss: 1.6405	LR: 0.050000
Training Epoch: 13 [35456/50000]	Loss: 1.2692	LR: 0.050000
Training Epoch: 13 [35584/50000]	Loss: 1.1850	LR: 0.050000
Training Epoch: 13 [35712/50000]	Loss: 1.3841	LR: 0.050000
Training Epoch: 13 [35840/50000]	Loss: 1.6238	LR: 0.050000
Training Epoch: 13 [35968/50000]	Loss: 1.3237	LR: 0.050000
Training Epoch: 13 [36096/50000]	Loss: 1.4756	LR: 0.050000
Training Epoch: 13 [36224/50000]	Loss: 1.6197	LR: 0.050000
Training Epoch: 13 [36352/50000]	Loss: 1.4570	LR: 0.050000
Training Epoch: 13 [36480/50000]	Loss: 1.3269	LR: 0.050000
Training Epoch: 13 [36608/50000]	Loss: 1.5439	LR: 0.050000
Training Epoch: 13 [36736/50000]	Loss: 1.4063	LR: 0.050000
Training Epoch: 13 [36864/50000]	Loss: 1.4941	LR: 0.050000
Training Epoch: 13 [36992/50000]	Loss: 1.2986	LR: 0.050000
Training Epoch: 13 [37120/50000]	Loss: 1.6422	LR: 0.050000
Training Epoch: 13 [37248/50000]	Loss: 1.6086	LR: 0.050000
Training Epoch: 13 [37376/50000]	Loss: 1.5953	LR: 0.050000
Training Epoch: 13 [37504/50000]	Loss: 1.1474	LR: 0.050000
Training Epoch: 13 [37632/50000]	Loss: 1.6684	LR: 0.050000
Training Epoch: 13 [37760/50000]	Loss: 1.4361	LR: 0.050000
Training Epoch: 13 [37888/50000]	Loss: 1.5177	LR: 0.050000
Training Epoch: 13 [38016/50000]	Loss: 1.6055	LR: 0.050000
Training Epoch: 13 [38144/50000]	Loss: 1.2883	LR: 0.050000
Training Epoch: 13 [38272/50000]	Loss: 1.5999	LR: 0.050000
Training Epoch: 13 [38400/50000]	Loss: 1.6531	LR: 0.050000
Training Epoch: 13 [38528/50000]	Loss: 1.4029	LR: 0.050000
Training Epoch: 13 [38656/50000]	Loss: 1.4891	LR: 0.050000
Training Epoch: 13 [38784/50000]	Loss: 1.5029	LR: 0.050000
Training Epoch: 13 [38912/50000]	Loss: 1.3441	LR: 0.050000
Training Epoch: 13 [39040/50000]	Loss: 1.5685	LR: 0.050000
Training Epoch: 13 [39168/50000]	Loss: 1.6628	LR: 0.050000
Training Epoch: 13 [39296/50000]	Loss: 1.4907	LR: 0.050000
Training Epoch: 13 [39424/50000]	Loss: 1.6087	LR: 0.050000
Training Epoch: 13 [39552/50000]	Loss: 1.2540	LR: 0.050000
Training Epoch: 13 [39680/50000]	Loss: 1.4583	LR: 0.050000
Training Epoch: 13 [39808/50000]	Loss: 1.6542	LR: 0.050000
Training Epoch: 13 [39936/50000]	Loss: 1.4885	LR: 0.050000
Training Epoch: 13 [40064/50000]	Loss: 1.7766	LR: 0.050000
Training Epoch: 13 [40192/50000]	Loss: 1.3363	LR: 0.050000
Training Epoch: 13 [40320/50000]	Loss: 1.1866	LR: 0.050000
Training Epoch: 13 [40448/50000]	Loss: 1.5799	LR: 0.050000
Training Epoch: 13 [40576/50000]	Loss: 1.3669	LR: 0.050000
Training Epoch: 13 [40704/50000]	Loss: 1.6647	LR: 0.050000
Training Epoch: 13 [40832/50000]	Loss: 1.1449	LR: 0.050000
Training Epoch: 13 [40960/50000]	Loss: 1.7191	LR: 0.050000
Training Epoch: 13 [41088/50000]	Loss: 1.3062	LR: 0.050000
Training Epoch: 13 [41216/50000]	Loss: 1.4602	LR: 0.050000
Training Epoch: 13 [41344/50000]	Loss: 1.3953	LR: 0.050000
Training Epoch: 13 [41472/50000]	Loss: 1.3585	LR: 0.050000
Training Epoch: 13 [41600/50000]	Loss: 1.3052	LR: 0.050000
Training Epoch: 13 [41728/50000]	Loss: 1.4790	LR: 0.050000
Training Epoch: 13 [41856/50000]	Loss: 1.5927	LR: 0.050000
Training Epoch: 13 [41984/50000]	Loss: 1.4191	LR: 0.050000
Training Epoch: 13 [42112/50000]	Loss: 1.5694	LR: 0.050000
Training Epoch: 13 [42240/50000]	Loss: 1.6462	LR: 0.050000
Training Epoch: 13 [42368/50000]	Loss: 1.6645	LR: 0.050000
Training Epoch: 13 [42496/50000]	Loss: 1.4251	LR: 0.050000
Training Epoch: 13 [42624/50000]	Loss: 1.8254	LR: 0.050000
Training Epoch: 13 [42752/50000]	Loss: 1.3822	LR: 0.050000
Training Epoch: 13 [42880/50000]	Loss: 1.5088	LR: 0.050000
Training Epoch: 13 [43008/50000]	Loss: 1.5925	LR: 0.050000
Training Epoch: 13 [43136/50000]	Loss: 1.4281	LR: 0.050000
Training Epoch: 13 [43264/50000]	Loss: 1.3957	LR: 0.050000
Training Epoch: 13 [43392/50000]	Loss: 1.3633	LR: 0.050000
Training Epoch: 13 [43520/50000]	Loss: 1.7168	LR: 0.050000
Training Epoch: 13 [43648/50000]	Loss: 1.6565	LR: 0.050000
Training Epoch: 13 [43776/50000]	Loss: 1.6921	LR: 0.050000
Training Epoch: 13 [43904/50000]	Loss: 1.1864	LR: 0.050000
Training Epoch: 13 [44032/50000]	Loss: 1.4520	LR: 0.050000
Training Epoch: 13 [44160/50000]	Loss: 1.4685	LR: 0.050000
Training Epoch: 13 [44288/50000]	Loss: 1.3728	LR: 0.050000
Training Epoch: 13 [44416/50000]	Loss: 1.4253	LR: 0.050000
Training Epoch: 13 [44544/50000]	Loss: 1.4195	LR: 0.050000
Training Epoch: 13 [44672/50000]	Loss: 1.5480	LR: 0.050000
Training Epoch: 13 [44800/50000]	Loss: 1.6837	LR: 0.050000
Training Epoch: 13 [44928/50000]	Loss: 1.4684	LR: 0.050000
Training Epoch: 13 [45056/50000]	Loss: 1.4770	LR: 0.050000
Training Epoch: 13 [45184/50000]	Loss: 1.7113	LR: 0.050000
Training Epoch: 13 [45312/50000]	Loss: 1.4207	LR: 0.050000
Training Epoch: 13 [45440/50000]	Loss: 1.5618	LR: 0.050000
Training Epoch: 13 [45568/50000]	Loss: 1.8406	LR: 0.050000
Training Epoch: 13 [45696/50000]	Loss: 1.6894	LR: 0.050000
Training Epoch: 13 [45824/50000]	Loss: 1.4784	LR: 0.050000
Training Epoch: 13 [45952/50000]	Loss: 1.4389	LR: 0.050000
Training Epoch: 13 [46080/50000]	Loss: 1.5209	LR: 0.050000
Training Epoch: 13 [46208/50000]	Loss: 1.4873	LR: 0.050000
Training Epoch: 13 [46336/50000]	Loss: 1.4093	LR: 0.050000
Training Epoch: 13 [46464/50000]	Loss: 1.4744	LR: 0.050000
Training Epoch: 13 [46592/50000]	Loss: 1.6552	LR: 0.050000
Training Epoch: 13 [46720/50000]	Loss: 1.3862	LR: 0.050000
Training Epoch: 13 [46848/50000]	Loss: 1.5540	LR: 0.050000
Training Epoch: 13 [46976/50000]	Loss: 1.3920	LR: 0.050000
Training Epoch: 13 [47104/50000]	Loss: 1.2631	LR: 0.050000
Training Epoch: 13 [47232/50000]	Loss: 1.5208	LR: 0.050000
Training Epoch: 13 [47360/50000]	Loss: 1.1922	LR: 0.050000
Training Epoch: 13 [47488/50000]	Loss: 1.7274	LR: 0.050000
Training Epoch: 13 [47616/50000]	Loss: 1.4166	LR: 0.050000
Training Epoch: 13 [47744/50000]	Loss: 1.3367	LR: 0.050000
Training Epoch: 13 [47872/50000]	Loss: 1.3532	LR: 0.050000
Training Epoch: 13 [48000/50000]	Loss: 1.9028	LR: 0.050000
Training Epoch: 13 [48128/50000]	Loss: 1.3880	LR: 0.050000
Training Epoch: 13 [48256/50000]	Loss: 1.5446	LR: 0.050000
Training Epoch: 13 [48384/50000]	Loss: 1.5872	LR: 0.050000
Training Epoch: 13 [48512/50000]	Loss: 1.3780	LR: 0.050000
Training Epoch: 13 [48640/50000]	Loss: 1.6539	LR: 0.050000
Training Epoch: 13 [48768/50000]	Loss: 1.6440	LR: 0.050000
Training Epoch: 13 [48896/50000]	Loss: 1.5635	LR: 0.050000
Training Epoch: 13 [49024/50000]	Loss: 1.2230	LR: 0.050000
Training Epoch: 13 [49152/50000]	Loss: 1.2490	LR: 0.050000
Training Epoch: 13 [49280/50000]	Loss: 1.4579	LR: 0.050000
Training Epoch: 13 [49408/50000]	Loss: 1.4180	LR: 0.050000
Training Epoch: 13 [49536/50000]	Loss: 1.5501	LR: 0.050000
Training Epoch: 13 [49664/50000]	Loss: 1.3605	LR: 0.050000
Training Epoch: 13 [49792/50000]	Loss: 1.8362	LR: 0.050000
Training Epoch: 13 [49920/50000]	Loss: 1.3180	LR: 0.050000
Training Epoch: 13 [50000/50000]	Loss: 1.4198	LR: 0.050000
epoch 13 training time consumed: 53.97s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   47711 GB |   47711 GB |
|       from large pool |  123392 KB |    1034 MB |   47664 GB |   47664 GB |
|       from small pool |   10798 KB |      13 MB |      47 GB |      47 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   47711 GB |   47711 GB |
|       from large pool |  123392 KB |    1034 MB |   47664 GB |   47664 GB |
|       from small pool |   10798 KB |      13 MB |      47 GB |      47 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   20998 GB |   20998 GB |
|       from large pool |  155136 KB |  433088 KB |   20946 GB |   20946 GB |
|       from small pool |    1490 KB |    3494 KB |      51 GB |      51 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    1841 K  |    1841 K  |
|       from large pool |      24    |      65    |     961 K  |     961 K  |
|       from small pool |     231    |     274    |     880 K  |     880 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    1841 K  |    1841 K  |
|       from large pool |      24    |      65    |     961 K  |     961 K  |
|       from small pool |     231    |     274    |     880 K  |     880 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |     913 K  |     913 K  |
|       from large pool |       9    |      14    |     465 K  |     465 K  |
|       from small pool |      12    |      16    |     447 K  |     447 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 13, Average loss: 0.0127, Accuracy: 0.5580, Time consumed:3.45s

Training Epoch: 14 [128/50000]	Loss: 1.2630	LR: 0.050000
Training Epoch: 14 [256/50000]	Loss: 1.1577	LR: 0.050000
Training Epoch: 14 [384/50000]	Loss: 1.5365	LR: 0.050000
Training Epoch: 14 [512/50000]	Loss: 1.2780	LR: 0.050000
Training Epoch: 14 [640/50000]	Loss: 1.4079	LR: 0.050000
Training Epoch: 14 [768/50000]	Loss: 1.1997	LR: 0.050000
Training Epoch: 14 [896/50000]	Loss: 1.5450	LR: 0.050000
Training Epoch: 14 [1024/50000]	Loss: 1.2598	LR: 0.050000
Training Epoch: 14 [1152/50000]	Loss: 1.2779	LR: 0.050000
Training Epoch: 14 [1280/50000]	Loss: 1.2434	LR: 0.050000
Training Epoch: 14 [1408/50000]	Loss: 1.2408	LR: 0.050000
Training Epoch: 14 [1536/50000]	Loss: 1.1199	LR: 0.050000
Training Epoch: 14 [1664/50000]	Loss: 1.2588	LR: 0.050000
Training Epoch: 14 [1792/50000]	Loss: 1.3608	LR: 0.050000
Training Epoch: 14 [1920/50000]	Loss: 1.3232	LR: 0.050000
Training Epoch: 14 [2048/50000]	Loss: 1.3680	LR: 0.050000
Training Epoch: 14 [2176/50000]	Loss: 1.1881	LR: 0.050000
Training Epoch: 14 [2304/50000]	Loss: 1.5035	LR: 0.050000
Training Epoch: 14 [2432/50000]	Loss: 1.5048	LR: 0.050000
Training Epoch: 14 [2560/50000]	Loss: 1.3079	LR: 0.050000
Training Epoch: 14 [2688/50000]	Loss: 1.4920	LR: 0.050000
Training Epoch: 14 [2816/50000]	Loss: 1.4058	LR: 0.050000
Training Epoch: 14 [2944/50000]	Loss: 1.4391	LR: 0.050000
Training Epoch: 14 [3072/50000]	Loss: 1.1714	LR: 0.050000
Training Epoch: 14 [3200/50000]	Loss: 1.5258	LR: 0.050000
Training Epoch: 14 [3328/50000]	Loss: 1.5722	LR: 0.050000
Training Epoch: 14 [3456/50000]	Loss: 1.2869	LR: 0.050000
Training Epoch: 14 [3584/50000]	Loss: 1.1541	LR: 0.050000
Training Epoch: 14 [3712/50000]	Loss: 1.2053	LR: 0.050000
Training Epoch: 14 [3840/50000]	Loss: 1.4506	LR: 0.050000
Training Epoch: 14 [3968/50000]	Loss: 1.3226	LR: 0.050000
Training Epoch: 14 [4096/50000]	Loss: 1.4273	LR: 0.050000
Training Epoch: 14 [4224/50000]	Loss: 1.2773	LR: 0.050000
Training Epoch: 14 [4352/50000]	Loss: 1.3148	LR: 0.050000
Training Epoch: 14 [4480/50000]	Loss: 1.2994	LR: 0.050000
Training Epoch: 14 [4608/50000]	Loss: 1.3038	LR: 0.050000
Training Epoch: 14 [4736/50000]	Loss: 1.1206	LR: 0.050000
Training Epoch: 14 [4864/50000]	Loss: 1.2586	LR: 0.050000
Training Epoch: 14 [4992/50000]	Loss: 1.5732	LR: 0.050000
Training Epoch: 14 [5120/50000]	Loss: 1.1600	LR: 0.050000
Training Epoch: 14 [5248/50000]	Loss: 1.2698	LR: 0.050000
Training Epoch: 14 [5376/50000]	Loss: 1.2050	LR: 0.050000
Training Epoch: 14 [5504/50000]	Loss: 1.6227	LR: 0.050000
Training Epoch: 14 [5632/50000]	Loss: 1.3507	LR: 0.050000
Training Epoch: 14 [5760/50000]	Loss: 1.3782	LR: 0.050000
Training Epoch: 14 [5888/50000]	Loss: 1.2104	LR: 0.050000
Training Epoch: 14 [6016/50000]	Loss: 1.3826	LR: 0.050000
Training Epoch: 14 [6144/50000]	Loss: 1.1925	LR: 0.050000
Training Epoch: 14 [6272/50000]	Loss: 1.3534	LR: 0.050000
Training Epoch: 14 [6400/50000]	Loss: 1.6370	LR: 0.050000
Training Epoch: 14 [6528/50000]	Loss: 1.3270	LR: 0.050000
Training Epoch: 14 [6656/50000]	Loss: 1.5886	LR: 0.050000
Training Epoch: 14 [6784/50000]	Loss: 1.4765	LR: 0.050000
Training Epoch: 14 [6912/50000]	Loss: 1.4037	LR: 0.050000
Training Epoch: 14 [7040/50000]	Loss: 1.2015	LR: 0.050000
Training Epoch: 14 [7168/50000]	Loss: 1.4700	LR: 0.050000
Training Epoch: 14 [7296/50000]	Loss: 1.4974	LR: 0.050000
Training Epoch: 14 [7424/50000]	Loss: 1.7256	LR: 0.050000
Training Epoch: 14 [7552/50000]	Loss: 1.3527	LR: 0.050000
Training Epoch: 14 [7680/50000]	Loss: 1.2665	LR: 0.050000
Training Epoch: 14 [7808/50000]	Loss: 1.2613	LR: 0.050000
Training Epoch: 14 [7936/50000]	Loss: 1.4770	LR: 0.050000
Training Epoch: 14 [8064/50000]	Loss: 1.4088	LR: 0.050000
Training Epoch: 14 [8192/50000]	Loss: 1.4278	LR: 0.050000
Training Epoch: 14 [8320/50000]	Loss: 1.5260	LR: 0.050000
Training Epoch: 14 [8448/50000]	Loss: 1.4216	LR: 0.050000
Training Epoch: 14 [8576/50000]	Loss: 1.3695	LR: 0.050000
Training Epoch: 14 [8704/50000]	Loss: 1.3818	LR: 0.050000
Training Epoch: 14 [8832/50000]	Loss: 1.4583	LR: 0.050000
Training Epoch: 14 [8960/50000]	Loss: 1.4513	LR: 0.050000
Training Epoch: 14 [9088/50000]	Loss: 1.5734	LR: 0.050000
Training Epoch: 14 [9216/50000]	Loss: 1.3045	LR: 0.050000
Training Epoch: 14 [9344/50000]	Loss: 1.2552	LR: 0.050000
Training Epoch: 14 [9472/50000]	Loss: 1.4017	LR: 0.050000
Training Epoch: 14 [9600/50000]	Loss: 1.4238	LR: 0.050000
Training Epoch: 14 [9728/50000]	Loss: 1.5848	LR: 0.050000
Training Epoch: 14 [9856/50000]	Loss: 1.2074	LR: 0.050000
Training Epoch: 14 [9984/50000]	Loss: 1.2564	LR: 0.050000
Training Epoch: 14 [10112/50000]	Loss: 1.5081	LR: 0.050000
Training Epoch: 14 [10240/50000]	Loss: 1.1801	LR: 0.050000
Training Epoch: 14 [10368/50000]	Loss: 1.2859	LR: 0.050000
Training Epoch: 14 [10496/50000]	Loss: 1.5148	LR: 0.050000
Training Epoch: 14 [10624/50000]	Loss: 1.5255	LR: 0.050000
Training Epoch: 14 [10752/50000]	Loss: 1.5190	LR: 0.050000
Training Epoch: 14 [10880/50000]	Loss: 1.4319	LR: 0.050000
Training Epoch: 14 [11008/50000]	Loss: 1.2986	LR: 0.050000
Training Epoch: 14 [11136/50000]	Loss: 1.3886	LR: 0.050000
Training Epoch: 14 [11264/50000]	Loss: 1.4498	LR: 0.050000
Training Epoch: 14 [11392/50000]	Loss: 1.5014	LR: 0.050000
Training Epoch: 14 [11520/50000]	Loss: 1.1712	LR: 0.050000
Training Epoch: 14 [11648/50000]	Loss: 1.4033	LR: 0.050000
Training Epoch: 14 [11776/50000]	Loss: 1.4027	LR: 0.050000
Training Epoch: 14 [11904/50000]	Loss: 1.3378	LR: 0.050000
Training Epoch: 14 [12032/50000]	Loss: 1.2512	LR: 0.050000
Training Epoch: 14 [12160/50000]	Loss: 1.4362	LR: 0.050000
Training Epoch: 14 [12288/50000]	Loss: 1.3835	LR: 0.050000
Training Epoch: 14 [12416/50000]	Loss: 1.4229	LR: 0.050000
Training Epoch: 14 [12544/50000]	Loss: 1.3369	LR: 0.050000
Training Epoch: 14 [12672/50000]	Loss: 1.3026	LR: 0.050000
Training Epoch: 14 [12800/50000]	Loss: 1.5811	LR: 0.050000
Training Epoch: 14 [12928/50000]	Loss: 1.4238	LR: 0.050000
Training Epoch: 14 [13056/50000]	Loss: 1.5418	LR: 0.050000
Training Epoch: 14 [13184/50000]	Loss: 1.2898	LR: 0.050000
Training Epoch: 14 [13312/50000]	Loss: 1.2904	LR: 0.050000
Training Epoch: 14 [13440/50000]	Loss: 1.4587	LR: 0.050000
Training Epoch: 14 [13568/50000]	Loss: 1.2940	LR: 0.050000
Training Epoch: 14 [13696/50000]	Loss: 1.0496	LR: 0.050000
Training Epoch: 14 [13824/50000]	Loss: 1.3801	LR: 0.050000
Training Epoch: 14 [13952/50000]	Loss: 1.3896	LR: 0.050000
Training Epoch: 14 [14080/50000]	Loss: 1.3784	LR: 0.050000
Training Epoch: 14 [14208/50000]	Loss: 1.5250	LR: 0.050000
Training Epoch: 14 [14336/50000]	Loss: 1.3011	LR: 0.050000
Training Epoch: 14 [14464/50000]	Loss: 1.3737	LR: 0.050000
Training Epoch: 14 [14592/50000]	Loss: 1.2227	LR: 0.050000
Training Epoch: 14 [14720/50000]	Loss: 1.3219	LR: 0.050000
Training Epoch: 14 [14848/50000]	Loss: 1.6030	LR: 0.050000
Training Epoch: 14 [14976/50000]	Loss: 1.4409	LR: 0.050000
Training Epoch: 14 [15104/50000]	Loss: 1.3918	LR: 0.050000
Training Epoch: 14 [15232/50000]	Loss: 1.6036	LR: 0.050000
Training Epoch: 14 [15360/50000]	Loss: 1.3455	LR: 0.050000
Training Epoch: 14 [15488/50000]	Loss: 1.3126	LR: 0.050000
Training Epoch: 14 [15616/50000]	Loss: 1.3909	LR: 0.050000
Training Epoch: 14 [15744/50000]	Loss: 1.3877	LR: 0.050000
Training Epoch: 14 [15872/50000]	Loss: 1.4137	LR: 0.050000
Training Epoch: 14 [16000/50000]	Loss: 1.4203	LR: 0.050000
Training Epoch: 14 [16128/50000]	Loss: 1.2993	LR: 0.050000
Training Epoch: 14 [16256/50000]	Loss: 1.5024	LR: 0.050000
Training Epoch: 14 [16384/50000]	Loss: 1.4432	LR: 0.050000
Training Epoch: 14 [16512/50000]	Loss: 1.2035	LR: 0.050000
Training Epoch: 14 [16640/50000]	Loss: 1.3262	LR: 0.050000
Training Epoch: 14 [16768/50000]	Loss: 1.2270	LR: 0.050000
Training Epoch: 14 [16896/50000]	Loss: 1.2340	LR: 0.050000
Training Epoch: 14 [17024/50000]	Loss: 1.2036	LR: 0.050000
Training Epoch: 14 [17152/50000]	Loss: 1.3434	LR: 0.050000
Training Epoch: 14 [17280/50000]	Loss: 1.5160	LR: 0.050000
Training Epoch: 14 [17408/50000]	Loss: 1.3799	LR: 0.050000
Training Epoch: 14 [17536/50000]	Loss: 1.2461	LR: 0.050000
Training Epoch: 14 [17664/50000]	Loss: 1.2254	LR: 0.050000
Training Epoch: 14 [17792/50000]	Loss: 1.1534	LR: 0.050000
Training Epoch: 14 [17920/50000]	Loss: 1.3439	LR: 0.050000
Training Epoch: 14 [18048/50000]	Loss: 1.4626	LR: 0.050000
Training Epoch: 14 [18176/50000]	Loss: 1.3110	LR: 0.050000
Training Epoch: 14 [18304/50000]	Loss: 1.4445	LR: 0.050000
Training Epoch: 14 [18432/50000]	Loss: 1.6335	LR: 0.050000
Training Epoch: 14 [18560/50000]	Loss: 1.5024	LR: 0.050000
Training Epoch: 14 [18688/50000]	Loss: 1.2718	LR: 0.050000
Training Epoch: 14 [18816/50000]	Loss: 1.3676	LR: 0.050000
Training Epoch: 14 [18944/50000]	Loss: 1.4523	LR: 0.050000
Training Epoch: 14 [19072/50000]	Loss: 1.3878	LR: 0.050000
Training Epoch: 14 [19200/50000]	Loss: 1.6921	LR: 0.050000
Training Epoch: 14 [19328/50000]	Loss: 1.4342	LR: 0.050000
Training Epoch: 14 [19456/50000]	Loss: 1.1585	LR: 0.050000
Training Epoch: 14 [19584/50000]	Loss: 1.4732	LR: 0.050000
Training Epoch: 14 [19712/50000]	Loss: 1.4670	LR: 0.050000
Training Epoch: 14 [19840/50000]	Loss: 1.2654	LR: 0.050000
Training Epoch: 14 [19968/50000]	Loss: 1.5381	LR: 0.050000
Training Epoch: 14 [20096/50000]	Loss: 1.4084	LR: 0.050000
Training Epoch: 14 [20224/50000]	Loss: 1.4982	LR: 0.050000
Training Epoch: 14 [20352/50000]	Loss: 1.5428	LR: 0.050000
Training Epoch: 14 [20480/50000]	Loss: 1.3706	LR: 0.050000
Training Epoch: 14 [20608/50000]	Loss: 1.5162	LR: 0.050000
Training Epoch: 14 [20736/50000]	Loss: 1.3534	LR: 0.050000
Training Epoch: 14 [20864/50000]	Loss: 1.5973	LR: 0.050000
Training Epoch: 14 [20992/50000]	Loss: 1.3916	LR: 0.050000
Training Epoch: 14 [21120/50000]	Loss: 1.2710	LR: 0.050000
Training Epoch: 14 [21248/50000]	Loss: 1.4676	LR: 0.050000
Training Epoch: 14 [21376/50000]	Loss: 1.4167	LR: 0.050000
Training Epoch: 14 [21504/50000]	Loss: 1.2477	LR: 0.050000
Training Epoch: 14 [21632/50000]	Loss: 1.0923	LR: 0.050000
Training Epoch: 14 [21760/50000]	Loss: 1.4533	LR: 0.050000
Training Epoch: 14 [21888/50000]	Loss: 1.3821	LR: 0.050000
Training Epoch: 14 [22016/50000]	Loss: 1.2173	LR: 0.050000
Training Epoch: 14 [22144/50000]	Loss: 1.5222	LR: 0.050000
Training Epoch: 14 [22272/50000]	Loss: 1.2388	LR: 0.050000
Training Epoch: 14 [22400/50000]	Loss: 1.1215	LR: 0.050000
Training Epoch: 14 [22528/50000]	Loss: 1.5840	LR: 0.050000
Training Epoch: 14 [22656/50000]	Loss: 1.3686	LR: 0.050000
Training Epoch: 14 [22784/50000]	Loss: 1.5431	LR: 0.050000
Training Epoch: 14 [22912/50000]	Loss: 1.5351	LR: 0.050000
Training Epoch: 14 [23040/50000]	Loss: 1.3824	LR: 0.050000
Training Epoch: 14 [23168/50000]	Loss: 1.3413	LR: 0.050000
Training Epoch: 14 [23296/50000]	Loss: 1.3268	LR: 0.050000
Training Epoch: 14 [23424/50000]	Loss: 1.4499	LR: 0.050000
Training Epoch: 14 [23552/50000]	Loss: 1.1556	LR: 0.050000
Training Epoch: 14 [23680/50000]	Loss: 1.4931	LR: 0.050000
Training Epoch: 14 [23808/50000]	Loss: 1.2258	LR: 0.050000
Training Epoch: 14 [23936/50000]	Loss: 1.2915	LR: 0.050000
Training Epoch: 14 [24064/50000]	Loss: 1.1926	LR: 0.050000
Training Epoch: 14 [24192/50000]	Loss: 1.2840	LR: 0.050000
Training Epoch: 14 [24320/50000]	Loss: 1.2913	LR: 0.050000
Training Epoch: 14 [24448/50000]	Loss: 1.2735	LR: 0.050000
Training Epoch: 14 [24576/50000]	Loss: 1.5177	LR: 0.050000
Training Epoch: 14 [24704/50000]	Loss: 1.3467	LR: 0.050000
Training Epoch: 14 [24832/50000]	Loss: 1.4499	LR: 0.050000
Training Epoch: 14 [24960/50000]	Loss: 1.5202	LR: 0.050000
Training Epoch: 14 [25088/50000]	Loss: 1.3191	LR: 0.050000
Training Epoch: 14 [25216/50000]	Loss: 1.3243	LR: 0.050000
Training Epoch: 14 [25344/50000]	Loss: 1.3394	LR: 0.050000
Training Epoch: 14 [25472/50000]	Loss: 1.4398	LR: 0.050000
Training Epoch: 14 [25600/50000]	Loss: 1.2637	LR: 0.050000
Training Epoch: 14 [25728/50000]	Loss: 1.4952	LR: 0.050000
Training Epoch: 14 [25856/50000]	Loss: 1.4107	LR: 0.050000
Training Epoch: 14 [25984/50000]	Loss: 1.4797	LR: 0.050000
Training Epoch: 14 [26112/50000]	Loss: 1.4627	LR: 0.050000
Training Epoch: 14 [26240/50000]	Loss: 1.3539	LR: 0.050000
Training Epoch: 14 [26368/50000]	Loss: 1.1427	LR: 0.050000
Training Epoch: 14 [26496/50000]	Loss: 1.1658	LR: 0.050000
Training Epoch: 14 [26624/50000]	Loss: 1.3361	LR: 0.050000
Training Epoch: 14 [26752/50000]	Loss: 1.3518	LR: 0.050000
Training Epoch: 14 [26880/50000]	Loss: 1.3425	LR: 0.050000
Training Epoch: 14 [27008/50000]	Loss: 1.0841	LR: 0.050000
Training Epoch: 14 [27136/50000]	Loss: 1.5104	LR: 0.050000
Training Epoch: 14 [27264/50000]	Loss: 1.3592	LR: 0.050000
Training Epoch: 14 [27392/50000]	Loss: 1.2917	LR: 0.050000
Training Epoch: 14 [27520/50000]	Loss: 1.4237	LR: 0.050000
Training Epoch: 14 [27648/50000]	Loss: 1.3293	LR: 0.050000
Training Epoch: 14 [27776/50000]	Loss: 1.2987	LR: 0.050000
Training Epoch: 14 [27904/50000]	Loss: 1.2685	LR: 0.050000
Training Epoch: 14 [28032/50000]	Loss: 1.5799	LR: 0.050000
Training Epoch: 14 [28160/50000]	Loss: 1.3722	LR: 0.050000
Training Epoch: 14 [28288/50000]	Loss: 1.3123	LR: 0.050000
Training Epoch: 14 [28416/50000]	Loss: 1.3775	LR: 0.050000
Training Epoch: 14 [28544/50000]	Loss: 1.4458	LR: 0.050000
Training Epoch: 14 [28672/50000]	Loss: 1.3881	LR: 0.050000
Training Epoch: 14 [28800/50000]	Loss: 1.2738	LR: 0.050000
Training Epoch: 14 [28928/50000]	Loss: 1.7033	LR: 0.050000
Training Epoch: 14 [29056/50000]	Loss: 1.3153	LR: 0.050000
Training Epoch: 14 [29184/50000]	Loss: 1.4663	LR: 0.050000
Training Epoch: 14 [29312/50000]	Loss: 1.5936	LR: 0.050000
Training Epoch: 14 [29440/50000]	Loss: 1.3461	LR: 0.050000
Training Epoch: 14 [29568/50000]	Loss: 1.1508	LR: 0.050000
Training Epoch: 14 [29696/50000]	Loss: 1.3356	LR: 0.050000
Training Epoch: 14 [29824/50000]	Loss: 1.2672	LR: 0.050000
Training Epoch: 14 [29952/50000]	Loss: 1.2379	LR: 0.050000
Training Epoch: 14 [30080/50000]	Loss: 1.3840	LR: 0.050000
Training Epoch: 14 [30208/50000]	Loss: 1.2827	LR: 0.050000
Training Epoch: 14 [30336/50000]	Loss: 1.3777	LR: 0.050000
Training Epoch: 14 [30464/50000]	Loss: 1.2636	LR: 0.050000
Training Epoch: 14 [30592/50000]	Loss: 1.3697	LR: 0.050000
Training Epoch: 14 [30720/50000]	Loss: 1.5903	LR: 0.050000
Training Epoch: 14 [30848/50000]	Loss: 1.5433	LR: 0.050000
Training Epoch: 14 [30976/50000]	Loss: 1.1635	LR: 0.050000
Training Epoch: 14 [31104/50000]	Loss: 1.5406	LR: 0.050000
Training Epoch: 14 [31232/50000]	Loss: 1.3653	LR: 0.050000
Training Epoch: 14 [31360/50000]	Loss: 1.5095	LR: 0.050000
Training Epoch: 14 [31488/50000]	Loss: 1.4169	LR: 0.050000
Training Epoch: 14 [31616/50000]	Loss: 1.3550	LR: 0.050000
Training Epoch: 14 [31744/50000]	Loss: 1.5448	LR: 0.050000
Training Epoch: 14 [31872/50000]	Loss: 1.3071	LR: 0.050000
Training Epoch: 14 [32000/50000]	Loss: 1.2605	LR: 0.050000
Training Epoch: 14 [32128/50000]	Loss: 1.2588	LR: 0.050000
Training Epoch: 14 [32256/50000]	Loss: 1.3082	LR: 0.050000
Training Epoch: 14 [32384/50000]	Loss: 1.2661	LR: 0.050000
Training Epoch: 14 [32512/50000]	Loss: 1.4800	LR: 0.050000
Training Epoch: 14 [32640/50000]	Loss: 1.5243	LR: 0.050000
Training Epoch: 14 [32768/50000]	Loss: 1.3423	LR: 0.050000
Training Epoch: 14 [32896/50000]	Loss: 1.3319	LR: 0.050000
Training Epoch: 14 [33024/50000]	Loss: 1.5503	LR: 0.050000
Training Epoch: 14 [33152/50000]	Loss: 1.6695	LR: 0.050000
Training Epoch: 14 [33280/50000]	Loss: 1.3966	LR: 0.050000
Training Epoch: 14 [33408/50000]	Loss: 1.3269	LR: 0.050000
Training Epoch: 14 [33536/50000]	Loss: 1.3549	LR: 0.050000
Training Epoch: 14 [33664/50000]	Loss: 1.4580	LR: 0.050000
Training Epoch: 14 [33792/50000]	Loss: 1.3707	LR: 0.050000
Training Epoch: 14 [33920/50000]	Loss: 1.3684	LR: 0.050000
Training Epoch: 14 [34048/50000]	Loss: 1.4923	LR: 0.050000
Training Epoch: 14 [34176/50000]	Loss: 1.2268	LR: 0.050000
Training Epoch: 14 [34304/50000]	Loss: 1.4788	LR: 0.050000
Training Epoch: 14 [34432/50000]	Loss: 1.3338	LR: 0.050000
Training Epoch: 14 [34560/50000]	Loss: 1.2949	LR: 0.050000
Training Epoch: 14 [34688/50000]	Loss: 1.6150	LR: 0.050000
Training Epoch: 14 [34816/50000]	Loss: 1.3370	LR: 0.050000
Training Epoch: 14 [34944/50000]	Loss: 1.6939	LR: 0.050000
Training Epoch: 14 [35072/50000]	Loss: 1.4372	LR: 0.050000
Training Epoch: 14 [35200/50000]	Loss: 1.4510	LR: 0.050000
Training Epoch: 14 [35328/50000]	Loss: 1.3629	LR: 0.050000
Training Epoch: 14 [35456/50000]	Loss: 1.4835	LR: 0.050000
Training Epoch: 14 [35584/50000]	Loss: 1.4914	LR: 0.050000
Training Epoch: 14 [35712/50000]	Loss: 1.6109	LR: 0.050000
Training Epoch: 14 [35840/50000]	Loss: 1.3766	LR: 0.050000
Training Epoch: 14 [35968/50000]	Loss: 1.5726	LR: 0.050000
Training Epoch: 14 [36096/50000]	Loss: 1.4659	LR: 0.050000
Training Epoch: 14 [36224/50000]	Loss: 1.4967	LR: 0.050000
Training Epoch: 14 [36352/50000]	Loss: 1.4797	LR: 0.050000
Training Epoch: 14 [36480/50000]	Loss: 1.4275	LR: 0.050000
Training Epoch: 14 [36608/50000]	Loss: 1.2799	LR: 0.050000
Training Epoch: 14 [36736/50000]	Loss: 1.4167	LR: 0.050000
Training Epoch: 14 [36864/50000]	Loss: 1.5813	LR: 0.050000
Training Epoch: 14 [36992/50000]	Loss: 1.4674	LR: 0.050000
Training Epoch: 14 [37120/50000]	Loss: 1.4858	LR: 0.050000
Training Epoch: 14 [37248/50000]	Loss: 1.3571	LR: 0.050000
Training Epoch: 14 [37376/50000]	Loss: 1.3697	LR: 0.050000
Training Epoch: 14 [37504/50000]	Loss: 1.3724	LR: 0.050000
Training Epoch: 14 [37632/50000]	Loss: 1.1686	LR: 0.050000
Training Epoch: 14 [37760/50000]	Loss: 1.1782	LR: 0.050000
Training Epoch: 14 [37888/50000]	Loss: 1.2737	LR: 0.050000
Training Epoch: 14 [38016/50000]	Loss: 1.4861	LR: 0.050000
Training Epoch: 14 [38144/50000]	Loss: 1.3432	LR: 0.050000
Training Epoch: 14 [38272/50000]	Loss: 1.5219	LR: 0.050000
Training Epoch: 14 [38400/50000]	Loss: 1.2304	LR: 0.050000
Training Epoch: 14 [38528/50000]	Loss: 1.6294	LR: 0.050000
Training Epoch: 14 [38656/50000]	Loss: 1.3124	LR: 0.050000
Training Epoch: 14 [38784/50000]	Loss: 1.4552	LR: 0.050000
Training Epoch: 14 [38912/50000]	Loss: 1.5582	LR: 0.050000
Training Epoch: 14 [39040/50000]	Loss: 1.4295	LR: 0.050000
Training Epoch: 14 [39168/50000]	Loss: 1.5793	LR: 0.050000
Training Epoch: 14 [39296/50000]	Loss: 1.4650	LR: 0.050000
Training Epoch: 14 [39424/50000]	Loss: 1.4503	LR: 0.050000
Training Epoch: 14 [39552/50000]	Loss: 1.4454	LR: 0.050000
Training Epoch: 14 [39680/50000]	Loss: 1.5583	LR: 0.050000
Training Epoch: 14 [39808/50000]	Loss: 1.2682	LR: 0.050000
Training Epoch: 14 [39936/50000]	Loss: 1.5941	LR: 0.050000
Training Epoch: 14 [40064/50000]	Loss: 1.4451	LR: 0.050000
Training Epoch: 14 [40192/50000]	Loss: 1.6886	LR: 0.050000
Training Epoch: 14 [40320/50000]	Loss: 1.3648	LR: 0.050000
Training Epoch: 14 [40448/50000]	Loss: 1.5196	LR: 0.050000
Training Epoch: 14 [40576/50000]	Loss: 1.6343	LR: 0.050000
Training Epoch: 14 [40704/50000]	Loss: 1.4258	LR: 0.050000
Training Epoch: 14 [40832/50000]	Loss: 1.6294	LR: 0.050000
Training Epoch: 14 [40960/50000]	Loss: 1.5566	LR: 0.050000
Training Epoch: 14 [41088/50000]	Loss: 1.5914	LR: 0.050000
Training Epoch: 14 [41216/50000]	Loss: 1.4557	LR: 0.050000
Training Epoch: 14 [41344/50000]	Loss: 1.4791	LR: 0.050000
Training Epoch: 14 [41472/50000]	Loss: 1.2155	LR: 0.050000
Training Epoch: 14 [41600/50000]	Loss: 1.4867	LR: 0.050000
Training Epoch: 14 [41728/50000]	Loss: 1.4530	LR: 0.050000
Training Epoch: 14 [41856/50000]	Loss: 1.1321	LR: 0.050000
Training Epoch: 14 [41984/50000]	Loss: 1.4217	LR: 0.050000
Training Epoch: 14 [42112/50000]	Loss: 1.5464	LR: 0.050000
Training Epoch: 14 [42240/50000]	Loss: 1.4831	LR: 0.050000
Training Epoch: 14 [42368/50000]	Loss: 1.4627	LR: 0.050000
Training Epoch: 14 [42496/50000]	Loss: 1.2558	LR: 0.050000
Training Epoch: 14 [42624/50000]	Loss: 1.3838	LR: 0.050000
Training Epoch: 14 [42752/50000]	Loss: 1.6386	LR: 0.050000
Training Epoch: 14 [42880/50000]	Loss: 1.4934	LR: 0.050000
Training Epoch: 14 [43008/50000]	Loss: 1.6554	LR: 0.050000
Training Epoch: 14 [43136/50000]	Loss: 1.5733	LR: 0.050000
Training Epoch: 14 [43264/50000]	Loss: 1.5573	LR: 0.050000
Training Epoch: 14 [43392/50000]	Loss: 1.4615	LR: 0.050000
Training Epoch: 14 [43520/50000]	Loss: 1.4604	LR: 0.050000
Training Epoch: 14 [43648/50000]	Loss: 1.2497	LR: 0.050000
Training Epoch: 14 [43776/50000]	Loss: 1.2423	LR: 0.050000
Training Epoch: 14 [43904/50000]	Loss: 1.5436	LR: 0.050000
Training Epoch: 14 [44032/50000]	Loss: 1.2203	LR: 0.050000
Training Epoch: 14 [44160/50000]	Loss: 1.2860	LR: 0.050000
Training Epoch: 14 [44288/50000]	Loss: 1.5515	LR: 0.050000
Training Epoch: 14 [44416/50000]	Loss: 1.5439	LR: 0.050000
Training Epoch: 14 [44544/50000]	Loss: 1.6275	LR: 0.050000
Training Epoch: 14 [44672/50000]	Loss: 1.6214	LR: 0.050000
Training Epoch: 14 [44800/50000]	Loss: 1.4489	LR: 0.050000
Training Epoch: 14 [44928/50000]	Loss: 1.2811	LR: 0.050000
Training Epoch: 14 [45056/50000]	Loss: 1.3536	LR: 0.050000
Training Epoch: 14 [45184/50000]	Loss: 1.5579	LR: 0.050000
Training Epoch: 14 [45312/50000]	Loss: 1.1617	LR: 0.050000
Training Epoch: 14 [45440/50000]	Loss: 1.3709	LR: 0.050000
Training Epoch: 14 [45568/50000]	Loss: 1.4367	LR: 0.050000
Training Epoch: 14 [45696/50000]	Loss: 1.1789	LR: 0.050000
Training Epoch: 14 [45824/50000]	Loss: 1.5971	LR: 0.050000
Training Epoch: 14 [45952/50000]	Loss: 1.2785	LR: 0.050000
Training Epoch: 14 [46080/50000]	Loss: 1.3569	LR: 0.050000
Training Epoch: 14 [46208/50000]	Loss: 1.4446	LR: 0.050000
Training Epoch: 14 [46336/50000]	Loss: 1.1748	LR: 0.050000
Training Epoch: 14 [46464/50000]	Loss: 1.3950	LR: 0.050000
Training Epoch: 14 [46592/50000]	Loss: 1.7999	LR: 0.050000
Training Epoch: 14 [46720/50000]	Loss: 1.1821	LR: 0.050000
Training Epoch: 14 [46848/50000]	Loss: 1.4566	LR: 0.050000
Training Epoch: 14 [46976/50000]	Loss: 1.3625	LR: 0.050000
Training Epoch: 14 [47104/50000]	Loss: 1.3821	LR: 0.050000
Training Epoch: 14 [47232/50000]	Loss: 1.5033	LR: 0.050000
Training Epoch: 14 [47360/50000]	Loss: 1.2767	LR: 0.050000
Training Epoch: 14 [47488/50000]	Loss: 1.4324	LR: 0.050000
Training Epoch: 14 [47616/50000]	Loss: 1.4489	LR: 0.050000
Training Epoch: 14 [47744/50000]	Loss: 1.3465	LR: 0.050000
Training Epoch: 14 [47872/50000]	Loss: 1.2754	LR: 0.050000
Training Epoch: 14 [48000/50000]	Loss: 1.3145	LR: 0.050000
Training Epoch: 14 [48128/50000]	Loss: 1.5517	LR: 0.050000
Training Epoch: 14 [48256/50000]	Loss: 1.5906	LR: 0.050000
Training Epoch: 14 [48384/50000]	Loss: 1.2086	LR: 0.050000
Training Epoch: 14 [48512/50000]	Loss: 1.3619	LR: 0.050000
Training Epoch: 14 [48640/50000]	Loss: 1.3075	LR: 0.050000
Training Epoch: 14 [48768/50000]	Loss: 1.4859	LR: 0.050000
Training Epoch: 14 [48896/50000]	Loss: 1.4274	LR: 0.050000
Training Epoch: 14 [49024/50000]	Loss: 1.3836	LR: 0.050000
Training Epoch: 14 [49152/50000]	Loss: 1.4514	LR: 0.050000
Training Epoch: 14 [49280/50000]	Loss: 1.6217	LR: 0.050000
Training Epoch: 14 [49408/50000]	Loss: 1.0536	LR: 0.050000
Training Epoch: 14 [49536/50000]	Loss: 1.4599	LR: 0.050000
Training Epoch: 14 [49664/50000]	Loss: 1.2319	LR: 0.050000
Training Epoch: 14 [49792/50000]	Loss: 1.6817	LR: 0.050000
Training Epoch: 14 [49920/50000]	Loss: 1.5581	LR: 0.050000
Training Epoch: 14 [50000/50000]	Loss: 1.4039	LR: 0.050000
epoch 14 training time consumed: 53.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   51381 GB |   51381 GB |
|       from large pool |  123392 KB |    1034 MB |   51330 GB |   51330 GB |
|       from small pool |   10798 KB |      13 MB |      50 GB |      50 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   51381 GB |   51381 GB |
|       from large pool |  123392 KB |    1034 MB |   51330 GB |   51330 GB |
|       from small pool |   10798 KB |      13 MB |      50 GB |      50 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   22613 GB |   22613 GB |
|       from large pool |  155136 KB |  433088 KB |   22557 GB |   22557 GB |
|       from small pool |    1490 KB |    3494 KB |      55 GB |      55 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    1983 K  |    1982 K  |
|       from large pool |      24    |      65    |    1034 K  |    1034 K  |
|       from small pool |     231    |     274    |     948 K  |     947 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    1983 K  |    1982 K  |
|       from large pool |      24    |      65    |    1034 K  |    1034 K  |
|       from small pool |     231    |     274    |     948 K  |     947 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |     983 K  |     982 K  |
|       from large pool |       9    |      14    |     500 K  |     500 K  |
|       from small pool |      12    |      16    |     482 K  |     482 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 14, Average loss: 0.0134, Accuracy: 0.5462, Time consumed:3.47s

Training Epoch: 15 [128/50000]	Loss: 1.2651	LR: 0.050000
Training Epoch: 15 [256/50000]	Loss: 1.3449	LR: 0.050000
Training Epoch: 15 [384/50000]	Loss: 1.3907	LR: 0.050000
Training Epoch: 15 [512/50000]	Loss: 1.1929	LR: 0.050000
Training Epoch: 15 [640/50000]	Loss: 1.3167	LR: 0.050000
Training Epoch: 15 [768/50000]	Loss: 1.3137	LR: 0.050000
Training Epoch: 15 [896/50000]	Loss: 1.2758	LR: 0.050000
Training Epoch: 15 [1024/50000]	Loss: 1.5378	LR: 0.050000
Training Epoch: 15 [1152/50000]	Loss: 1.3885	LR: 0.050000
Training Epoch: 15 [1280/50000]	Loss: 1.2652	LR: 0.050000
Training Epoch: 15 [1408/50000]	Loss: 1.1178	LR: 0.050000
Training Epoch: 15 [1536/50000]	Loss: 1.2183	LR: 0.050000
Training Epoch: 15 [1664/50000]	Loss: 1.3841	LR: 0.050000
Training Epoch: 15 [1792/50000]	Loss: 1.4242	LR: 0.050000
Training Epoch: 15 [1920/50000]	Loss: 1.3905	LR: 0.050000
Training Epoch: 15 [2048/50000]	Loss: 1.4104	LR: 0.050000
Training Epoch: 15 [2176/50000]	Loss: 1.3286	LR: 0.050000
Training Epoch: 15 [2304/50000]	Loss: 1.0223	LR: 0.050000
Training Epoch: 15 [2432/50000]	Loss: 1.4237	LR: 0.050000
Training Epoch: 15 [2560/50000]	Loss: 1.2427	LR: 0.050000
Training Epoch: 15 [2688/50000]	Loss: 1.3639	LR: 0.050000
Training Epoch: 15 [2816/50000]	Loss: 1.3626	LR: 0.050000
Training Epoch: 15 [2944/50000]	Loss: 1.4969	LR: 0.050000
Training Epoch: 15 [3072/50000]	Loss: 1.2174	LR: 0.050000
Training Epoch: 15 [3200/50000]	Loss: 1.1764	LR: 0.050000
Training Epoch: 15 [3328/50000]	Loss: 1.1176	LR: 0.050000
Training Epoch: 15 [3456/50000]	Loss: 1.3392	LR: 0.050000
Training Epoch: 15 [3584/50000]	Loss: 1.4201	LR: 0.050000
Training Epoch: 15 [3712/50000]	Loss: 1.3085	LR: 0.050000
Training Epoch: 15 [3840/50000]	Loss: 1.1412	LR: 0.050000
Training Epoch: 15 [3968/50000]	Loss: 1.3126	LR: 0.050000
Training Epoch: 15 [4096/50000]	Loss: 1.2937	LR: 0.050000
Training Epoch: 15 [4224/50000]	Loss: 1.3127	LR: 0.050000
Training Epoch: 15 [4352/50000]	Loss: 1.2971	LR: 0.050000
Training Epoch: 15 [4480/50000]	Loss: 1.3090	LR: 0.050000
Training Epoch: 15 [4608/50000]	Loss: 1.4523	LR: 0.050000
Training Epoch: 15 [4736/50000]	Loss: 1.2522	LR: 0.050000
Training Epoch: 15 [4864/50000]	Loss: 1.1215	LR: 0.050000
Training Epoch: 15 [4992/50000]	Loss: 1.1880	LR: 0.050000
Training Epoch: 15 [5120/50000]	Loss: 1.3847	LR: 0.050000
Training Epoch: 15 [5248/50000]	Loss: 1.2720	LR: 0.050000
Training Epoch: 15 [5376/50000]	Loss: 1.3650	LR: 0.050000
Training Epoch: 15 [5504/50000]	Loss: 1.1468	LR: 0.050000
Training Epoch: 15 [5632/50000]	Loss: 1.3025	LR: 0.050000
Training Epoch: 15 [5760/50000]	Loss: 1.4371	LR: 0.050000
Training Epoch: 15 [5888/50000]	Loss: 1.2049	LR: 0.050000
Training Epoch: 15 [6016/50000]	Loss: 1.1870	LR: 0.050000
Training Epoch: 15 [6144/50000]	Loss: 1.3743	LR: 0.050000
Training Epoch: 15 [6272/50000]	Loss: 1.1673	LR: 0.050000
Training Epoch: 15 [6400/50000]	Loss: 1.6456	LR: 0.050000
Training Epoch: 15 [6528/50000]	Loss: 1.2868	LR: 0.050000
Training Epoch: 15 [6656/50000]	Loss: 1.5267	LR: 0.050000
Training Epoch: 15 [6784/50000]	Loss: 1.6008	LR: 0.050000
Training Epoch: 15 [6912/50000]	Loss: 1.1466	LR: 0.050000
Training Epoch: 15 [7040/50000]	Loss: 1.4642	LR: 0.050000
Training Epoch: 15 [7168/50000]	Loss: 1.1993	LR: 0.050000
Training Epoch: 15 [7296/50000]	Loss: 1.1964	LR: 0.050000
Training Epoch: 15 [7424/50000]	Loss: 1.2908	LR: 0.050000
Training Epoch: 15 [7552/50000]	Loss: 1.2454	LR: 0.050000
Training Epoch: 15 [7680/50000]	Loss: 1.2986	LR: 0.050000
Training Epoch: 15 [7808/50000]	Loss: 1.1518	LR: 0.050000
Training Epoch: 15 [7936/50000]	Loss: 1.3538	LR: 0.050000
Training Epoch: 15 [8064/50000]	Loss: 1.1575	LR: 0.050000
Training Epoch: 15 [8192/50000]	Loss: 1.2847	LR: 0.050000
Training Epoch: 15 [8320/50000]	Loss: 1.5676	LR: 0.050000
Training Epoch: 15 [8448/50000]	Loss: 1.2564	LR: 0.050000
Training Epoch: 15 [8576/50000]	Loss: 1.2267	LR: 0.050000
Training Epoch: 15 [8704/50000]	Loss: 1.2492	LR: 0.050000
Training Epoch: 15 [8832/50000]	Loss: 1.2968	LR: 0.050000
Training Epoch: 15 [8960/50000]	Loss: 1.4285	LR: 0.050000
Training Epoch: 15 [9088/50000]	Loss: 1.4460	LR: 0.050000
Training Epoch: 15 [9216/50000]	Loss: 1.4903	LR: 0.050000
Training Epoch: 15 [9344/50000]	Loss: 1.4166	LR: 0.050000
Training Epoch: 15 [9472/50000]	Loss: 1.3410	LR: 0.050000
Training Epoch: 15 [9600/50000]	Loss: 1.2509	LR: 0.050000
Training Epoch: 15 [9728/50000]	Loss: 1.2366	LR: 0.050000
Training Epoch: 15 [9856/50000]	Loss: 1.3273	LR: 0.050000
Training Epoch: 15 [9984/50000]	Loss: 1.1736	LR: 0.050000
Training Epoch: 15 [10112/50000]	Loss: 1.3475	LR: 0.050000
Training Epoch: 15 [10240/50000]	Loss: 1.4892	LR: 0.050000
Training Epoch: 15 [10368/50000]	Loss: 1.2237	LR: 0.050000
Training Epoch: 15 [10496/50000]	Loss: 1.1833	LR: 0.050000
Training Epoch: 15 [10624/50000]	Loss: 1.0313	LR: 0.050000
Training Epoch: 15 [10752/50000]	Loss: 1.0282	LR: 0.050000
Training Epoch: 15 [10880/50000]	Loss: 1.2340	LR: 0.050000
Training Epoch: 15 [11008/50000]	Loss: 1.3734	LR: 0.050000
Training Epoch: 15 [11136/50000]	Loss: 1.3035	LR: 0.050000
Training Epoch: 15 [11264/50000]	Loss: 1.5505	LR: 0.050000
Training Epoch: 15 [11392/50000]	Loss: 1.6421	LR: 0.050000
Training Epoch: 15 [11520/50000]	Loss: 1.3098	LR: 0.050000
Training Epoch: 15 [11648/50000]	Loss: 1.2788	LR: 0.050000
Training Epoch: 15 [11776/50000]	Loss: 1.1679	LR: 0.050000
Training Epoch: 15 [11904/50000]	Loss: 1.3796	LR: 0.050000
Training Epoch: 15 [12032/50000]	Loss: 1.3928	LR: 0.050000
Training Epoch: 15 [12160/50000]	Loss: 1.3117	LR: 0.050000
Training Epoch: 15 [12288/50000]	Loss: 1.1413	LR: 0.050000
Training Epoch: 15 [12416/50000]	Loss: 1.4275	LR: 0.050000
Training Epoch: 15 [12544/50000]	Loss: 1.1803	LR: 0.050000
Training Epoch: 15 [12672/50000]	Loss: 1.4124	LR: 0.050000
Training Epoch: 15 [12800/50000]	Loss: 1.2537	LR: 0.050000
Training Epoch: 15 [12928/50000]	Loss: 1.4040	LR: 0.050000
Training Epoch: 15 [13056/50000]	Loss: 1.1451	LR: 0.050000
Training Epoch: 15 [13184/50000]	Loss: 1.3991	LR: 0.050000
Training Epoch: 15 [13312/50000]	Loss: 1.1916	LR: 0.050000
Training Epoch: 15 [13440/50000]	Loss: 1.4330	LR: 0.050000
Training Epoch: 15 [13568/50000]	Loss: 1.1988	LR: 0.050000
Training Epoch: 15 [13696/50000]	Loss: 0.9898	LR: 0.050000
Training Epoch: 15 [13824/50000]	Loss: 1.2598	LR: 0.050000
Training Epoch: 15 [13952/50000]	Loss: 1.4345	LR: 0.050000
Training Epoch: 15 [14080/50000]	Loss: 1.5168	LR: 0.050000
Training Epoch: 15 [14208/50000]	Loss: 1.4696	LR: 0.050000
Training Epoch: 15 [14336/50000]	Loss: 1.3666	LR: 0.050000
Training Epoch: 15 [14464/50000]	Loss: 1.2478	LR: 0.050000
Training Epoch: 15 [14592/50000]	Loss: 1.2521	LR: 0.050000
Training Epoch: 15 [14720/50000]	Loss: 1.1171	LR: 0.050000
Training Epoch: 15 [14848/50000]	Loss: 1.3835	LR: 0.050000
Training Epoch: 15 [14976/50000]	Loss: 1.2060	LR: 0.050000
Training Epoch: 15 [15104/50000]	Loss: 1.3150	LR: 0.050000
Training Epoch: 15 [15232/50000]	Loss: 1.3159	LR: 0.050000
Training Epoch: 15 [15360/50000]	Loss: 1.7819	LR: 0.050000
Training Epoch: 15 [15488/50000]	Loss: 1.2158	LR: 0.050000
Training Epoch: 15 [15616/50000]	Loss: 1.2942	LR: 0.050000
Training Epoch: 15 [15744/50000]	Loss: 1.5970	LR: 0.050000
Training Epoch: 15 [15872/50000]	Loss: 1.5453	LR: 0.050000
Training Epoch: 15 [16000/50000]	Loss: 1.2018	LR: 0.050000
Training Epoch: 15 [16128/50000]	Loss: 1.3432	LR: 0.050000
Training Epoch: 15 [16256/50000]	Loss: 1.1887	LR: 0.050000
Training Epoch: 15 [16384/50000]	Loss: 1.3882	LR: 0.050000
Training Epoch: 15 [16512/50000]	Loss: 1.3069	LR: 0.050000
Training Epoch: 15 [16640/50000]	Loss: 1.2977	LR: 0.050000
Training Epoch: 15 [16768/50000]	Loss: 1.1638	LR: 0.050000
Training Epoch: 15 [16896/50000]	Loss: 1.2275	LR: 0.050000
Training Epoch: 15 [17024/50000]	Loss: 1.2837	LR: 0.050000
Training Epoch: 15 [17152/50000]	Loss: 1.2646	LR: 0.050000
Training Epoch: 15 [17280/50000]	Loss: 1.3899	LR: 0.050000
Training Epoch: 15 [17408/50000]	Loss: 1.1836	LR: 0.050000
Training Epoch: 15 [17536/50000]	Loss: 1.3221	LR: 0.050000
Training Epoch: 15 [17664/50000]	Loss: 1.2457	LR: 0.050000
Training Epoch: 15 [17792/50000]	Loss: 1.2090	LR: 0.050000
Training Epoch: 15 [17920/50000]	Loss: 1.1330	LR: 0.050000
Training Epoch: 15 [18048/50000]	Loss: 1.5996	LR: 0.050000
Training Epoch: 15 [18176/50000]	Loss: 1.0688	LR: 0.050000
Training Epoch: 15 [18304/50000]	Loss: 1.2522	LR: 0.050000
Training Epoch: 15 [18432/50000]	Loss: 1.3715	LR: 0.050000
Training Epoch: 15 [18560/50000]	Loss: 1.2890	LR: 0.050000
Training Epoch: 15 [18688/50000]	Loss: 1.3496	LR: 0.050000
Training Epoch: 15 [18816/50000]	Loss: 1.3234	LR: 0.050000
Training Epoch: 15 [18944/50000]	Loss: 1.3884	LR: 0.050000
Training Epoch: 15 [19072/50000]	Loss: 1.1655	LR: 0.050000
Training Epoch: 15 [19200/50000]	Loss: 1.1162	LR: 0.050000
Training Epoch: 15 [19328/50000]	Loss: 1.0140	LR: 0.050000
Training Epoch: 15 [19456/50000]	Loss: 1.4350	LR: 0.050000
Training Epoch: 15 [19584/50000]	Loss: 1.4398	LR: 0.050000
Training Epoch: 15 [19712/50000]	Loss: 1.3311	LR: 0.050000
Training Epoch: 15 [19840/50000]	Loss: 1.3126	LR: 0.050000
Training Epoch: 15 [19968/50000]	Loss: 1.3002	LR: 0.050000
Training Epoch: 15 [20096/50000]	Loss: 1.3765	LR: 0.050000
Training Epoch: 15 [20224/50000]	Loss: 1.4356	LR: 0.050000
Training Epoch: 15 [20352/50000]	Loss: 1.1906	LR: 0.050000
Training Epoch: 15 [20480/50000]	Loss: 1.3713	LR: 0.050000
Training Epoch: 15 [20608/50000]	Loss: 1.3555	LR: 0.050000
Training Epoch: 15 [20736/50000]	Loss: 1.5103	LR: 0.050000
Training Epoch: 15 [20864/50000]	Loss: 1.4868	LR: 0.050000
Training Epoch: 15 [20992/50000]	Loss: 1.4353	LR: 0.050000
Training Epoch: 15 [21120/50000]	Loss: 1.5666	LR: 0.050000
Training Epoch: 15 [21248/50000]	Loss: 1.3443	LR: 0.050000
Training Epoch: 15 [21376/50000]	Loss: 1.5361	LR: 0.050000
Training Epoch: 15 [21504/50000]	Loss: 1.4585	LR: 0.050000
Training Epoch: 15 [21632/50000]	Loss: 1.4770	LR: 0.050000
Training Epoch: 15 [21760/50000]	Loss: 1.2613	LR: 0.050000
Training Epoch: 15 [21888/50000]	Loss: 1.3737	LR: 0.050000
Training Epoch: 15 [22016/50000]	Loss: 1.2865	LR: 0.050000
Training Epoch: 15 [22144/50000]	Loss: 1.1981	LR: 0.050000
Training Epoch: 15 [22272/50000]	Loss: 1.2847	LR: 0.050000
Training Epoch: 15 [22400/50000]	Loss: 1.3088	LR: 0.050000
Training Epoch: 15 [22528/50000]	Loss: 1.0630	LR: 0.050000
Training Epoch: 15 [22656/50000]	Loss: 1.2191	LR: 0.050000
Training Epoch: 15 [22784/50000]	Loss: 1.2717	LR: 0.050000
Training Epoch: 15 [22912/50000]	Loss: 1.3825	LR: 0.050000
Training Epoch: 15 [23040/50000]	Loss: 1.3642	LR: 0.050000
Training Epoch: 15 [23168/50000]	Loss: 1.6168	LR: 0.050000
Training Epoch: 15 [23296/50000]	Loss: 1.1974	LR: 0.050000
Training Epoch: 15 [23424/50000]	Loss: 1.3088	LR: 0.050000
Training Epoch: 15 [23552/50000]	Loss: 1.4565	LR: 0.050000
Training Epoch: 15 [23680/50000]	Loss: 1.2473	LR: 0.050000
Training Epoch: 15 [23808/50000]	Loss: 1.3641	LR: 0.050000
Training Epoch: 15 [23936/50000]	Loss: 1.6762	LR: 0.050000
Training Epoch: 15 [24064/50000]	Loss: 1.1957	LR: 0.050000
Training Epoch: 15 [24192/50000]	Loss: 1.6170	LR: 0.050000
Training Epoch: 15 [24320/50000]	Loss: 1.4530	LR: 0.050000
Training Epoch: 15 [24448/50000]	Loss: 1.5631	LR: 0.050000
Training Epoch: 15 [24576/50000]	Loss: 1.1474	LR: 0.050000
Training Epoch: 15 [24704/50000]	Loss: 1.1777	LR: 0.050000
Training Epoch: 15 [24832/50000]	Loss: 1.1632	LR: 0.050000
Training Epoch: 15 [24960/50000]	Loss: 1.4812	LR: 0.050000
Training Epoch: 15 [25088/50000]	Loss: 1.5421	LR: 0.050000
Training Epoch: 15 [25216/50000]	Loss: 1.0575	LR: 0.050000
Training Epoch: 15 [25344/50000]	Loss: 1.4513	LR: 0.050000
Training Epoch: 15 [25472/50000]	Loss: 1.4734	LR: 0.050000
Training Epoch: 15 [25600/50000]	Loss: 1.3675	LR: 0.050000
Training Epoch: 15 [25728/50000]	Loss: 1.2189	LR: 0.050000
Training Epoch: 15 [25856/50000]	Loss: 1.5444	LR: 0.050000
Training Epoch: 15 [25984/50000]	Loss: 1.6716	LR: 0.050000
Training Epoch: 15 [26112/50000]	Loss: 1.3460	LR: 0.050000
Training Epoch: 15 [26240/50000]	Loss: 1.2741	LR: 0.050000
Training Epoch: 15 [26368/50000]	Loss: 1.2986	LR: 0.050000
Training Epoch: 15 [26496/50000]	Loss: 1.4481	LR: 0.050000
Training Epoch: 15 [26624/50000]	Loss: 1.2307	LR: 0.050000
Training Epoch: 15 [26752/50000]	Loss: 1.3288	LR: 0.050000
Training Epoch: 15 [26880/50000]	Loss: 1.3925	LR: 0.050000
Training Epoch: 15 [27008/50000]	Loss: 1.4382	LR: 0.050000
Training Epoch: 15 [27136/50000]	Loss: 1.3609	LR: 0.050000
Training Epoch: 15 [27264/50000]	Loss: 1.4440	LR: 0.050000
Training Epoch: 15 [27392/50000]	Loss: 1.2484	LR: 0.050000
Training Epoch: 15 [27520/50000]	Loss: 1.2791	LR: 0.050000
Training Epoch: 15 [27648/50000]	Loss: 1.2894	LR: 0.050000
Training Epoch: 15 [27776/50000]	Loss: 1.3582	LR: 0.050000
Training Epoch: 15 [27904/50000]	Loss: 1.4559	LR: 0.050000
Training Epoch: 15 [28032/50000]	Loss: 1.5516	LR: 0.050000
Training Epoch: 15 [28160/50000]	Loss: 1.3853	LR: 0.050000
Training Epoch: 15 [28288/50000]	Loss: 1.2491	LR: 0.050000
Training Epoch: 15 [28416/50000]	Loss: 1.2595	LR: 0.050000
Training Epoch: 15 [28544/50000]	Loss: 1.4111	LR: 0.050000
Training Epoch: 15 [28672/50000]	Loss: 1.3000	LR: 0.050000
Training Epoch: 15 [28800/50000]	Loss: 1.5025	LR: 0.050000
Training Epoch: 15 [28928/50000]	Loss: 1.3401	LR: 0.050000
Training Epoch: 15 [29056/50000]	Loss: 1.2793	LR: 0.050000
Training Epoch: 15 [29184/50000]	Loss: 1.3756	LR: 0.050000
Training Epoch: 15 [29312/50000]	Loss: 1.3806	LR: 0.050000
Training Epoch: 15 [29440/50000]	Loss: 1.2537	LR: 0.050000
Training Epoch: 15 [29568/50000]	Loss: 1.4709	LR: 0.050000
Training Epoch: 15 [29696/50000]	Loss: 1.2029	LR: 0.050000
Training Epoch: 15 [29824/50000]	Loss: 1.4227	LR: 0.050000
Training Epoch: 15 [29952/50000]	Loss: 1.2289	LR: 0.050000
Training Epoch: 15 [30080/50000]	Loss: 1.0996	LR: 0.050000
Training Epoch: 15 [30208/50000]	Loss: 1.2550	LR: 0.050000
Training Epoch: 15 [30336/50000]	Loss: 1.3476	LR: 0.050000
Training Epoch: 15 [30464/50000]	Loss: 1.3524	LR: 0.050000
Training Epoch: 15 [30592/50000]	Loss: 1.1469	LR: 0.050000
Training Epoch: 15 [30720/50000]	Loss: 1.1405	LR: 0.050000
Training Epoch: 15 [30848/50000]	Loss: 1.4605	LR: 0.050000
Training Epoch: 15 [30976/50000]	Loss: 1.3837	LR: 0.050000
Training Epoch: 15 [31104/50000]	Loss: 1.2780	LR: 0.050000
Training Epoch: 15 [31232/50000]	Loss: 1.2201	LR: 0.050000
Training Epoch: 15 [31360/50000]	Loss: 1.2367	LR: 0.050000
Training Epoch: 15 [31488/50000]	Loss: 1.5450	LR: 0.050000
Training Epoch: 15 [31616/50000]	Loss: 1.3549	LR: 0.050000
Training Epoch: 15 [31744/50000]	Loss: 1.5535	LR: 0.050000
Training Epoch: 15 [31872/50000]	Loss: 1.4039	LR: 0.050000
Training Epoch: 15 [32000/50000]	Loss: 1.0796	LR: 0.050000
Training Epoch: 15 [32128/50000]	Loss: 1.3770	LR: 0.050000
Training Epoch: 15 [32256/50000]	Loss: 1.2629	LR: 0.050000
Training Epoch: 15 [32384/50000]	Loss: 1.3830	LR: 0.050000
Training Epoch: 15 [32512/50000]	Loss: 1.4102	LR: 0.050000
Training Epoch: 15 [32640/50000]	Loss: 1.3124	LR: 0.050000
Training Epoch: 15 [32768/50000]	Loss: 1.5273	LR: 0.050000
Training Epoch: 15 [32896/50000]	Loss: 1.2182	LR: 0.050000
Training Epoch: 15 [33024/50000]	Loss: 1.3923	LR: 0.050000
Training Epoch: 15 [33152/50000]	Loss: 1.5170	LR: 0.050000
Training Epoch: 15 [33280/50000]	Loss: 1.3435	LR: 0.050000
Training Epoch: 15 [33408/50000]	Loss: 1.5820	LR: 0.050000
Training Epoch: 15 [33536/50000]	Loss: 1.1302	LR: 0.050000
Training Epoch: 15 [33664/50000]	Loss: 1.3690	LR: 0.050000
Training Epoch: 15 [33792/50000]	Loss: 1.5133	LR: 0.050000
Training Epoch: 15 [33920/50000]	Loss: 1.3674	LR: 0.050000
Training Epoch: 15 [34048/50000]	Loss: 1.7159	LR: 0.050000
Training Epoch: 15 [34176/50000]	Loss: 1.3529	LR: 0.050000
Training Epoch: 15 [34304/50000]	Loss: 1.3524	LR: 0.050000
Training Epoch: 15 [34432/50000]	Loss: 1.3464	LR: 0.050000
Training Epoch: 15 [34560/50000]	Loss: 1.2647	LR: 0.050000
Training Epoch: 15 [34688/50000]	Loss: 1.2927	LR: 0.050000
Training Epoch: 15 [34816/50000]	Loss: 1.1744	LR: 0.050000
Training Epoch: 15 [34944/50000]	Loss: 1.4125	LR: 0.050000
Training Epoch: 15 [35072/50000]	Loss: 1.3907	LR: 0.050000
Training Epoch: 15 [35200/50000]	Loss: 1.2229	LR: 0.050000
Training Epoch: 15 [35328/50000]	Loss: 1.2739	LR: 0.050000
Training Epoch: 15 [35456/50000]	Loss: 1.2359	LR: 0.050000
Training Epoch: 15 [35584/50000]	Loss: 1.2706	LR: 0.050000
Training Epoch: 15 [35712/50000]	Loss: 1.4590	LR: 0.050000
Training Epoch: 15 [35840/50000]	Loss: 1.3997	LR: 0.050000
Training Epoch: 15 [35968/50000]	Loss: 1.4751	LR: 0.050000
Training Epoch: 15 [36096/50000]	Loss: 1.3680	LR: 0.050000
Training Epoch: 15 [36224/50000]	Loss: 1.4691	LR: 0.050000
Training Epoch: 15 [36352/50000]	Loss: 1.1573	LR: 0.050000
Training Epoch: 15 [36480/50000]	Loss: 1.3487	LR: 0.050000
Training Epoch: 15 [36608/50000]	Loss: 1.3142	LR: 0.050000
Training Epoch: 15 [36736/50000]	Loss: 1.3718	LR: 0.050000
Training Epoch: 15 [36864/50000]	Loss: 1.3286	LR: 0.050000
Training Epoch: 15 [36992/50000]	Loss: 1.5765	LR: 0.050000
Training Epoch: 15 [37120/50000]	Loss: 1.3682	LR: 0.050000
Training Epoch: 15 [37248/50000]	Loss: 1.5674	LR: 0.050000
Training Epoch: 15 [37376/50000]	Loss: 1.4521	LR: 0.050000
Training Epoch: 15 [37504/50000]	Loss: 1.5613	LR: 0.050000
Training Epoch: 15 [37632/50000]	Loss: 1.4630	LR: 0.050000
Training Epoch: 15 [37760/50000]	Loss: 1.4568	LR: 0.050000
Training Epoch: 15 [37888/50000]	Loss: 1.2833	LR: 0.050000
Training Epoch: 15 [38016/50000]	Loss: 1.1623	LR: 0.050000
Training Epoch: 15 [38144/50000]	Loss: 1.4927	LR: 0.050000
Training Epoch: 15 [38272/50000]	Loss: 1.6026	LR: 0.050000
Training Epoch: 15 [38400/50000]	Loss: 1.3017	LR: 0.050000
Training Epoch: 15 [38528/50000]	Loss: 1.4444	LR: 0.050000
Training Epoch: 15 [38656/50000]	Loss: 1.3511	LR: 0.050000
Training Epoch: 15 [38784/50000]	Loss: 1.6057	LR: 0.050000
Training Epoch: 15 [38912/50000]	Loss: 1.4883	LR: 0.050000
Training Epoch: 15 [39040/50000]	Loss: 1.4787	LR: 0.050000
Training Epoch: 15 [39168/50000]	Loss: 1.2467	LR: 0.050000
Training Epoch: 15 [39296/50000]	Loss: 1.2330	LR: 0.050000
Training Epoch: 15 [39424/50000]	Loss: 1.1715	LR: 0.050000
Training Epoch: 15 [39552/50000]	Loss: 1.2403	LR: 0.050000
Training Epoch: 15 [39680/50000]	Loss: 1.2352	LR: 0.050000
Training Epoch: 15 [39808/50000]	Loss: 1.1188	LR: 0.050000
Training Epoch: 15 [39936/50000]	Loss: 1.2458	LR: 0.050000
Training Epoch: 15 [40064/50000]	Loss: 1.2684	LR: 0.050000
Training Epoch: 15 [40192/50000]	Loss: 1.3576	LR: 0.050000
Training Epoch: 15 [40320/50000]	Loss: 1.3438	LR: 0.050000
Training Epoch: 15 [40448/50000]	Loss: 1.3510	LR: 0.050000
Training Epoch: 15 [40576/50000]	Loss: 1.4872	LR: 0.050000
Training Epoch: 15 [40704/50000]	Loss: 1.2736	LR: 0.050000
Training Epoch: 15 [40832/50000]	Loss: 1.6878	LR: 0.050000
Training Epoch: 15 [40960/50000]	Loss: 1.1914	LR: 0.050000
Training Epoch: 15 [41088/50000]	Loss: 1.4010	LR: 0.050000
Training Epoch: 15 [41216/50000]	Loss: 1.2655	LR: 0.050000
Training Epoch: 15 [41344/50000]	Loss: 1.5903	LR: 0.050000
Training Epoch: 15 [41472/50000]	Loss: 1.5173	LR: 0.050000
Training Epoch: 15 [41600/50000]	Loss: 1.4550	LR: 0.050000
Training Epoch: 15 [41728/50000]	Loss: 1.3833	LR: 0.050000
Training Epoch: 15 [41856/50000]	Loss: 1.4603	LR: 0.050000
Training Epoch: 15 [41984/50000]	Loss: 1.1852	LR: 0.050000
Training Epoch: 15 [42112/50000]	Loss: 1.5120	LR: 0.050000
Training Epoch: 15 [42240/50000]	Loss: 1.2280	LR: 0.050000
Training Epoch: 15 [42368/50000]	Loss: 1.2538	LR: 0.050000
Training Epoch: 15 [42496/50000]	Loss: 1.3194	LR: 0.050000
Training Epoch: 15 [42624/50000]	Loss: 1.1546	LR: 0.050000
Training Epoch: 15 [42752/50000]	Loss: 1.2735	LR: 0.050000
Training Epoch: 15 [42880/50000]	Loss: 1.3696	LR: 0.050000
Training Epoch: 15 [43008/50000]	Loss: 1.3858	LR: 0.050000
Training Epoch: 15 [43136/50000]	Loss: 1.2638	LR: 0.050000
Training Epoch: 15 [43264/50000]	Loss: 1.3926	LR: 0.050000
Training Epoch: 15 [43392/50000]	Loss: 1.4130	LR: 0.050000
Training Epoch: 15 [43520/50000]	Loss: 1.2308	LR: 0.050000
Training Epoch: 15 [43648/50000]	Loss: 1.3586	LR: 0.050000
Training Epoch: 15 [43776/50000]	Loss: 1.2596	LR: 0.050000
Training Epoch: 15 [43904/50000]	Loss: 1.3712	LR: 0.050000
Training Epoch: 15 [44032/50000]	Loss: 1.3223	LR: 0.050000
Training Epoch: 15 [44160/50000]	Loss: 1.2759	LR: 0.050000
Training Epoch: 15 [44288/50000]	Loss: 1.5605	LR: 0.050000
Training Epoch: 15 [44416/50000]	Loss: 1.3460	LR: 0.050000
Training Epoch: 15 [44544/50000]	Loss: 1.5268	LR: 0.050000
Training Epoch: 15 [44672/50000]	Loss: 1.2829	LR: 0.050000
Training Epoch: 15 [44800/50000]	Loss: 1.5319	LR: 0.050000
Training Epoch: 15 [44928/50000]	Loss: 1.3471	LR: 0.050000
Training Epoch: 15 [45056/50000]	Loss: 1.3058	LR: 0.050000
Training Epoch: 15 [45184/50000]	Loss: 1.4462	LR: 0.050000
Training Epoch: 15 [45312/50000]	Loss: 1.4793	LR: 0.050000
Training Epoch: 15 [45440/50000]	Loss: 1.4991	LR: 0.050000
Training Epoch: 15 [45568/50000]	Loss: 1.3731	LR: 0.050000
Training Epoch: 15 [45696/50000]	Loss: 1.6555	LR: 0.050000
Training Epoch: 15 [45824/50000]	Loss: 1.4644	LR: 0.050000
Training Epoch: 15 [45952/50000]	Loss: 1.3460	LR: 0.050000
Training Epoch: 15 [46080/50000]	Loss: 1.0831	LR: 0.050000
Training Epoch: 15 [46208/50000]	Loss: 1.6452	LR: 0.050000
Training Epoch: 15 [46336/50000]	Loss: 1.2739	LR: 0.050000
Training Epoch: 15 [46464/50000]	Loss: 1.3318	LR: 0.050000
Training Epoch: 15 [46592/50000]	Loss: 1.6608	LR: 0.050000
Training Epoch: 15 [46720/50000]	Loss: 1.4647	LR: 0.050000
Training Epoch: 15 [46848/50000]	Loss: 1.1782	LR: 0.050000
Training Epoch: 15 [46976/50000]	Loss: 1.5113	LR: 0.050000
Training Epoch: 15 [47104/50000]	Loss: 1.4117	LR: 0.050000
Training Epoch: 15 [47232/50000]	Loss: 1.2105	LR: 0.050000
Training Epoch: 15 [47360/50000]	Loss: 1.3023	LR: 0.050000
Training Epoch: 15 [47488/50000]	Loss: 1.5450	LR: 0.050000
Training Epoch: 15 [47616/50000]	Loss: 1.4620	LR: 0.050000
Training Epoch: 15 [47744/50000]	Loss: 1.6439	LR: 0.050000
Training Epoch: 15 [47872/50000]	Loss: 1.3508	LR: 0.050000
Training Epoch: 15 [48000/50000]	Loss: 1.2549	LR: 0.050000
Training Epoch: 15 [48128/50000]	Loss: 1.3114	LR: 0.050000
Training Epoch: 15 [48256/50000]	Loss: 1.2862	LR: 0.050000
Training Epoch: 15 [48384/50000]	Loss: 1.1974	LR: 0.050000
Training Epoch: 15 [48512/50000]	Loss: 1.5573	LR: 0.050000
Training Epoch: 15 [48640/50000]	Loss: 1.0659	LR: 0.050000
Training Epoch: 15 [48768/50000]	Loss: 1.6331	LR: 0.050000
Training Epoch: 15 [48896/50000]	Loss: 1.1612	LR: 0.050000
Training Epoch: 15 [49024/50000]	Loss: 1.3308	LR: 0.050000
Training Epoch: 15 [49152/50000]	Loss: 1.3527	LR: 0.050000
Training Epoch: 15 [49280/50000]	Loss: 1.3476	LR: 0.050000
Training Epoch: 15 [49408/50000]	Loss: 1.5283	LR: 0.050000
Training Epoch: 15 [49536/50000]	Loss: 1.7360	LR: 0.050000
Training Epoch: 15 [49664/50000]	Loss: 1.4998	LR: 0.050000
Training Epoch: 15 [49792/50000]	Loss: 1.1970	LR: 0.050000
Training Epoch: 15 [49920/50000]	Loss: 1.6718	LR: 0.050000
Training Epoch: 15 [50000/50000]	Loss: 1.5329	LR: 0.050000
epoch 15 training time consumed: 53.87s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   55051 GB |   55051 GB |
|       from large pool |  123392 KB |    1034 MB |   54997 GB |   54997 GB |
|       from small pool |   10798 KB |      13 MB |      54 GB |      54 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   55051 GB |   55051 GB |
|       from large pool |  123392 KB |    1034 MB |   54997 GB |   54997 GB |
|       from small pool |   10798 KB |      13 MB |      54 GB |      54 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   24228 GB |   24228 GB |
|       from large pool |  155136 KB |  433088 KB |   24168 GB |   24168 GB |
|       from small pool |    1490 KB |    3494 KB |      59 GB |      59 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    2124 K  |    2124 K  |
|       from large pool |      24    |      65    |    1108 K  |    1108 K  |
|       from small pool |     231    |     274    |    1015 K  |    1015 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    2124 K  |    2124 K  |
|       from large pool |      24    |      65    |    1108 K  |    1108 K  |
|       from small pool |     231    |     274    |    1015 K  |    1015 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1052 K  |    1052 K  |
|       from large pool |       9    |      14    |     536 K  |     536 K  |
|       from small pool |      12    |      16    |     516 K  |     516 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 15, Average loss: 0.0132, Accuracy: 0.5520, Time consumed:3.45s

Training Epoch: 16 [128/50000]	Loss: 1.2349	LR: 0.050000
Training Epoch: 16 [256/50000]	Loss: 0.9822	LR: 0.050000
Training Epoch: 16 [384/50000]	Loss: 1.1894	LR: 0.050000
Training Epoch: 16 [512/50000]	Loss: 1.0689	LR: 0.050000
Training Epoch: 16 [640/50000]	Loss: 1.1949	LR: 0.050000
Training Epoch: 16 [768/50000]	Loss: 1.3234	LR: 0.050000
Training Epoch: 16 [896/50000]	Loss: 1.2467	LR: 0.050000
Training Epoch: 16 [1024/50000]	Loss: 1.3732	LR: 0.050000
Training Epoch: 16 [1152/50000]	Loss: 1.2712	LR: 0.050000
Training Epoch: 16 [1280/50000]	Loss: 1.1488	LR: 0.050000
Training Epoch: 16 [1408/50000]	Loss: 1.5728	LR: 0.050000
Training Epoch: 16 [1536/50000]	Loss: 1.2411	LR: 0.050000
Training Epoch: 16 [1664/50000]	Loss: 1.3092	LR: 0.050000
Training Epoch: 16 [1792/50000]	Loss: 1.1253	LR: 0.050000
Training Epoch: 16 [1920/50000]	Loss: 1.2733	LR: 0.050000
Training Epoch: 16 [2048/50000]	Loss: 1.3046	LR: 0.050000
Training Epoch: 16 [2176/50000]	Loss: 1.0148	LR: 0.050000
Training Epoch: 16 [2304/50000]	Loss: 1.0894	LR: 0.050000
Training Epoch: 16 [2432/50000]	Loss: 1.4742	LR: 0.050000
Training Epoch: 16 [2560/50000]	Loss: 1.3901	LR: 0.050000
Training Epoch: 16 [2688/50000]	Loss: 1.4402	LR: 0.050000
Training Epoch: 16 [2816/50000]	Loss: 1.0814	LR: 0.050000
Training Epoch: 16 [2944/50000]	Loss: 1.3937	LR: 0.050000
Training Epoch: 16 [3072/50000]	Loss: 1.3209	LR: 0.050000
Training Epoch: 16 [3200/50000]	Loss: 1.1920	LR: 0.050000
Training Epoch: 16 [3328/50000]	Loss: 1.3906	LR: 0.050000
Training Epoch: 16 [3456/50000]	Loss: 1.4420	LR: 0.050000
Training Epoch: 16 [3584/50000]	Loss: 1.2481	LR: 0.050000
Training Epoch: 16 [3712/50000]	Loss: 1.2822	LR: 0.050000
Training Epoch: 16 [3840/50000]	Loss: 1.1900	LR: 0.050000
Training Epoch: 16 [3968/50000]	Loss: 1.2106	LR: 0.050000
Training Epoch: 16 [4096/50000]	Loss: 1.1792	LR: 0.050000
Training Epoch: 16 [4224/50000]	Loss: 1.2833	LR: 0.050000
Training Epoch: 16 [4352/50000]	Loss: 1.2973	LR: 0.050000
Training Epoch: 16 [4480/50000]	Loss: 1.5155	LR: 0.050000
Training Epoch: 16 [4608/50000]	Loss: 1.3111	LR: 0.050000
Training Epoch: 16 [4736/50000]	Loss: 1.1618	LR: 0.050000
Training Epoch: 16 [4864/50000]	Loss: 1.2135	LR: 0.050000
Training Epoch: 16 [4992/50000]	Loss: 1.1338	LR: 0.050000
Training Epoch: 16 [5120/50000]	Loss: 1.2501	LR: 0.050000
Training Epoch: 16 [5248/50000]	Loss: 1.3444	LR: 0.050000
Training Epoch: 16 [5376/50000]	Loss: 1.2296	LR: 0.050000
Training Epoch: 16 [5504/50000]	Loss: 1.5607	LR: 0.050000
Training Epoch: 16 [5632/50000]	Loss: 1.2862	LR: 0.050000
Training Epoch: 16 [5760/50000]	Loss: 1.2109	LR: 0.050000
Training Epoch: 16 [5888/50000]	Loss: 1.2671	LR: 0.050000
Training Epoch: 16 [6016/50000]	Loss: 1.3418	LR: 0.050000
Training Epoch: 16 [6144/50000]	Loss: 1.6057	LR: 0.050000
Training Epoch: 16 [6272/50000]	Loss: 1.2695	LR: 0.050000
Training Epoch: 16 [6400/50000]	Loss: 1.3772	LR: 0.050000
Training Epoch: 16 [6528/50000]	Loss: 1.2682	LR: 0.050000
Training Epoch: 16 [6656/50000]	Loss: 1.3389	LR: 0.050000
Training Epoch: 16 [6784/50000]	Loss: 1.1217	LR: 0.050000
Training Epoch: 16 [6912/50000]	Loss: 1.6097	LR: 0.050000
Training Epoch: 16 [7040/50000]	Loss: 1.2741	LR: 0.050000
Training Epoch: 16 [7168/50000]	Loss: 1.1917	LR: 0.050000
Training Epoch: 16 [7296/50000]	Loss: 1.2256	LR: 0.050000
Training Epoch: 16 [7424/50000]	Loss: 1.2667	LR: 0.050000
Training Epoch: 16 [7552/50000]	Loss: 1.3789	LR: 0.050000
Training Epoch: 16 [7680/50000]	Loss: 1.2522	LR: 0.050000
Training Epoch: 16 [7808/50000]	Loss: 1.1979	LR: 0.050000
Training Epoch: 16 [7936/50000]	Loss: 1.4401	LR: 0.050000
Training Epoch: 16 [8064/50000]	Loss: 1.4224	LR: 0.050000
Training Epoch: 16 [8192/50000]	Loss: 1.3433	LR: 0.050000
Training Epoch: 16 [8320/50000]	Loss: 1.2061	LR: 0.050000
Training Epoch: 16 [8448/50000]	Loss: 1.3566	LR: 0.050000
Training Epoch: 16 [8576/50000]	Loss: 1.4867	LR: 0.050000
Training Epoch: 16 [8704/50000]	Loss: 1.3255	LR: 0.050000
Training Epoch: 16 [8832/50000]	Loss: 1.4579	LR: 0.050000
Training Epoch: 16 [8960/50000]	Loss: 1.4809	LR: 0.050000
Training Epoch: 16 [9088/50000]	Loss: 1.4129	LR: 0.050000
Training Epoch: 16 [9216/50000]	Loss: 1.3295	LR: 0.050000
Training Epoch: 16 [9344/50000]	Loss: 1.2152	LR: 0.050000
Training Epoch: 16 [9472/50000]	Loss: 1.4917	LR: 0.050000
Training Epoch: 16 [9600/50000]	Loss: 1.1588	LR: 0.050000
Training Epoch: 16 [9728/50000]	Loss: 1.1287	LR: 0.050000
Training Epoch: 16 [9856/50000]	Loss: 1.2675	LR: 0.050000
Training Epoch: 16 [9984/50000]	Loss: 1.4152	LR: 0.050000
Training Epoch: 16 [10112/50000]	Loss: 1.2435	LR: 0.050000
Training Epoch: 16 [10240/50000]	Loss: 1.2637	LR: 0.050000
Training Epoch: 16 [10368/50000]	Loss: 1.2932	LR: 0.050000
Training Epoch: 16 [10496/50000]	Loss: 1.2161	LR: 0.050000
Training Epoch: 16 [10624/50000]	Loss: 1.5012	LR: 0.050000
Training Epoch: 16 [10752/50000]	Loss: 1.2527	LR: 0.050000
Training Epoch: 16 [10880/50000]	Loss: 1.2708	LR: 0.050000
Training Epoch: 16 [11008/50000]	Loss: 1.3409	LR: 0.050000
Training Epoch: 16 [11136/50000]	Loss: 1.3697	LR: 0.050000
Training Epoch: 16 [11264/50000]	Loss: 1.3379	LR: 0.050000
Training Epoch: 16 [11392/50000]	Loss: 1.2245	LR: 0.050000
Training Epoch: 16 [11520/50000]	Loss: 1.3805	LR: 0.050000
Training Epoch: 16 [11648/50000]	Loss: 1.3462	LR: 0.050000
Training Epoch: 16 [11776/50000]	Loss: 1.4203	LR: 0.050000
Training Epoch: 16 [11904/50000]	Loss: 1.3937	LR: 0.050000
Training Epoch: 16 [12032/50000]	Loss: 1.0487	LR: 0.050000
Training Epoch: 16 [12160/50000]	Loss: 1.3822	LR: 0.050000
Training Epoch: 16 [12288/50000]	Loss: 1.1751	LR: 0.050000
Training Epoch: 16 [12416/50000]	Loss: 1.3051	LR: 0.050000
Training Epoch: 16 [12544/50000]	Loss: 1.3241	LR: 0.050000
Training Epoch: 16 [12672/50000]	Loss: 1.2942	LR: 0.050000
Training Epoch: 16 [12800/50000]	Loss: 1.2227	LR: 0.050000
Training Epoch: 16 [12928/50000]	Loss: 1.3778	LR: 0.050000
Training Epoch: 16 [13056/50000]	Loss: 1.0971	LR: 0.050000
Training Epoch: 16 [13184/50000]	Loss: 1.1928	LR: 0.050000
Training Epoch: 16 [13312/50000]	Loss: 1.3965	LR: 0.050000
Training Epoch: 16 [13440/50000]	Loss: 1.4292	LR: 0.050000
Training Epoch: 16 [13568/50000]	Loss: 1.2759	LR: 0.050000
Training Epoch: 16 [13696/50000]	Loss: 1.2262	LR: 0.050000
Training Epoch: 16 [13824/50000]	Loss: 1.3056	LR: 0.050000
Training Epoch: 16 [13952/50000]	Loss: 1.1913	LR: 0.050000
Training Epoch: 16 [14080/50000]	Loss: 1.3263	LR: 0.050000
Training Epoch: 16 [14208/50000]	Loss: 1.3806	LR: 0.050000
Training Epoch: 16 [14336/50000]	Loss: 1.3932	LR: 0.050000
Training Epoch: 16 [14464/50000]	Loss: 0.9627	LR: 0.050000
Training Epoch: 16 [14592/50000]	Loss: 1.2684	LR: 0.050000
Training Epoch: 16 [14720/50000]	Loss: 1.3557	LR: 0.050000
Training Epoch: 16 [14848/50000]	Loss: 1.4847	LR: 0.050000
Training Epoch: 16 [14976/50000]	Loss: 1.4582	LR: 0.050000
Training Epoch: 16 [15104/50000]	Loss: 1.1997	LR: 0.050000
Training Epoch: 16 [15232/50000]	Loss: 1.2120	LR: 0.050000
Training Epoch: 16 [15360/50000]	Loss: 1.2083	LR: 0.050000
Training Epoch: 16 [15488/50000]	Loss: 1.3496	LR: 0.050000
Training Epoch: 16 [15616/50000]	Loss: 1.3415	LR: 0.050000
Training Epoch: 16 [15744/50000]	Loss: 1.3272	LR: 0.050000
Training Epoch: 16 [15872/50000]	Loss: 1.2092	LR: 0.050000
Training Epoch: 16 [16000/50000]	Loss: 1.3289	LR: 0.050000
Training Epoch: 16 [16128/50000]	Loss: 1.5217	LR: 0.050000
Training Epoch: 16 [16256/50000]	Loss: 1.3035	LR: 0.050000
Training Epoch: 16 [16384/50000]	Loss: 1.6802	LR: 0.050000
Training Epoch: 16 [16512/50000]	Loss: 1.3368	LR: 0.050000
Training Epoch: 16 [16640/50000]	Loss: 1.1397	LR: 0.050000
Training Epoch: 16 [16768/50000]	Loss: 1.2241	LR: 0.050000
Training Epoch: 16 [16896/50000]	Loss: 1.4056	LR: 0.050000
Training Epoch: 16 [17024/50000]	Loss: 1.3514	LR: 0.050000
Training Epoch: 16 [17152/50000]	Loss: 1.2134	LR: 0.050000
Training Epoch: 16 [17280/50000]	Loss: 1.1190	LR: 0.050000
Training Epoch: 16 [17408/50000]	Loss: 1.1701	LR: 0.050000
Training Epoch: 16 [17536/50000]	Loss: 1.2301	LR: 0.050000
Training Epoch: 16 [17664/50000]	Loss: 1.5667	LR: 0.050000
Training Epoch: 16 [17792/50000]	Loss: 1.5792	LR: 0.050000
Training Epoch: 16 [17920/50000]	Loss: 1.2780	LR: 0.050000
Training Epoch: 16 [18048/50000]	Loss: 1.1071	LR: 0.050000
Training Epoch: 16 [18176/50000]	Loss: 1.3929	LR: 0.050000
Training Epoch: 16 [18304/50000]	Loss: 1.5046	LR: 0.050000
Training Epoch: 16 [18432/50000]	Loss: 1.2491	LR: 0.050000
Training Epoch: 16 [18560/50000]	Loss: 1.2890	LR: 0.050000
Training Epoch: 16 [18688/50000]	Loss: 1.5354	LR: 0.050000
Training Epoch: 16 [18816/50000]	Loss: 1.4132	LR: 0.050000
Training Epoch: 16 [18944/50000]	Loss: 1.4307	LR: 0.050000
Training Epoch: 16 [19072/50000]	Loss: 1.1421	LR: 0.050000
Training Epoch: 16 [19200/50000]	Loss: 1.1498	LR: 0.050000
Training Epoch: 16 [19328/50000]	Loss: 1.4155	LR: 0.050000
Training Epoch: 16 [19456/50000]	Loss: 1.6805	LR: 0.050000
Training Epoch: 16 [19584/50000]	Loss: 1.5808	LR: 0.050000
Training Epoch: 16 [19712/50000]	Loss: 1.5523	LR: 0.050000
Training Epoch: 16 [19840/50000]	Loss: 1.0590	LR: 0.050000
Training Epoch: 16 [19968/50000]	Loss: 1.4405	LR: 0.050000
Training Epoch: 16 [20096/50000]	Loss: 1.2978	LR: 0.050000
Training Epoch: 16 [20224/50000]	Loss: 1.3984	LR: 0.050000
Training Epoch: 16 [20352/50000]	Loss: 1.2245	LR: 0.050000
Training Epoch: 16 [20480/50000]	Loss: 1.5024	LR: 0.050000
Training Epoch: 16 [20608/50000]	Loss: 1.2432	LR: 0.050000
Training Epoch: 16 [20736/50000]	Loss: 1.1823	LR: 0.050000
Training Epoch: 16 [20864/50000]	Loss: 1.3646	LR: 0.050000
Training Epoch: 16 [20992/50000]	Loss: 1.4462	LR: 0.050000
Training Epoch: 16 [21120/50000]	Loss: 1.4067	LR: 0.050000
Training Epoch: 16 [21248/50000]	Loss: 1.3112	LR: 0.050000
Training Epoch: 16 [21376/50000]	Loss: 1.3598	LR: 0.050000
Training Epoch: 16 [21504/50000]	Loss: 1.3734	LR: 0.050000
Training Epoch: 16 [21632/50000]	Loss: 1.1999	LR: 0.050000
Training Epoch: 16 [21760/50000]	Loss: 1.3197	LR: 0.050000
Training Epoch: 16 [21888/50000]	Loss: 0.9908	LR: 0.050000
Training Epoch: 16 [22016/50000]	Loss: 1.2190	LR: 0.050000
Training Epoch: 16 [22144/50000]	Loss: 1.3160	LR: 0.050000
Training Epoch: 16 [22272/50000]	Loss: 1.2682	LR: 0.050000
Training Epoch: 16 [22400/50000]	Loss: 1.2462	LR: 0.050000
Training Epoch: 16 [22528/50000]	Loss: 1.2796	LR: 0.050000
Training Epoch: 16 [22656/50000]	Loss: 1.4133	LR: 0.050000
Training Epoch: 16 [22784/50000]	Loss: 1.3395	LR: 0.050000
Training Epoch: 16 [22912/50000]	Loss: 1.2549	LR: 0.050000
Training Epoch: 16 [23040/50000]	Loss: 1.6137	LR: 0.050000
Training Epoch: 16 [23168/50000]	Loss: 1.1040	LR: 0.050000
Training Epoch: 16 [23296/50000]	Loss: 1.3695	LR: 0.050000
Training Epoch: 16 [23424/50000]	Loss: 1.3927	LR: 0.050000
Training Epoch: 16 [23552/50000]	Loss: 1.3072	LR: 0.050000
Training Epoch: 16 [23680/50000]	Loss: 1.3755	LR: 0.050000
Training Epoch: 16 [23808/50000]	Loss: 1.2882	LR: 0.050000
Training Epoch: 16 [23936/50000]	Loss: 1.3426	LR: 0.050000
Training Epoch: 16 [24064/50000]	Loss: 1.3096	LR: 0.050000
Training Epoch: 16 [24192/50000]	Loss: 1.2675	LR: 0.050000
Training Epoch: 16 [24320/50000]	Loss: 1.3842	LR: 0.050000
Training Epoch: 16 [24448/50000]	Loss: 1.2837	LR: 0.050000
Training Epoch: 16 [24576/50000]	Loss: 1.3238	LR: 0.050000
Training Epoch: 16 [24704/50000]	Loss: 1.1555	LR: 0.050000
Training Epoch: 16 [24832/50000]	Loss: 1.4151	LR: 0.050000
Training Epoch: 16 [24960/50000]	Loss: 1.3792	LR: 0.050000
Training Epoch: 16 [25088/50000]	Loss: 1.2264	LR: 0.050000
Training Epoch: 16 [25216/50000]	Loss: 1.4051	LR: 0.050000
Training Epoch: 16 [25344/50000]	Loss: 1.3530	LR: 0.050000
Training Epoch: 16 [25472/50000]	Loss: 1.6066	LR: 0.050000
Training Epoch: 16 [25600/50000]	Loss: 1.2637	LR: 0.050000
Training Epoch: 16 [25728/50000]	Loss: 1.0088	LR: 0.050000
Training Epoch: 16 [25856/50000]	Loss: 1.2724	LR: 0.050000
Training Epoch: 16 [25984/50000]	Loss: 1.4084	LR: 0.050000
Training Epoch: 16 [26112/50000]	Loss: 1.4316	LR: 0.050000
Training Epoch: 16 [26240/50000]	Loss: 1.2813	LR: 0.050000
Training Epoch: 16 [26368/50000]	Loss: 1.4315	LR: 0.050000
Training Epoch: 16 [26496/50000]	Loss: 1.3084	LR: 0.050000
Training Epoch: 16 [26624/50000]	Loss: 1.4321	LR: 0.050000
Training Epoch: 16 [26752/50000]	Loss: 1.4922	LR: 0.050000
Training Epoch: 16 [26880/50000]	Loss: 1.4918	LR: 0.050000
Training Epoch: 16 [27008/50000]	Loss: 1.2888	LR: 0.050000
Training Epoch: 16 [27136/50000]	Loss: 1.3987	LR: 0.050000
Training Epoch: 16 [27264/50000]	Loss: 1.3378	LR: 0.050000
Training Epoch: 16 [27392/50000]	Loss: 1.2906	LR: 0.050000
Training Epoch: 16 [27520/50000]	Loss: 1.3639	LR: 0.050000
Training Epoch: 16 [27648/50000]	Loss: 1.2582	LR: 0.050000
Training Epoch: 16 [27776/50000]	Loss: 1.2882	LR: 0.050000
Training Epoch: 16 [27904/50000]	Loss: 1.3285	LR: 0.050000
Training Epoch: 16 [28032/50000]	Loss: 1.4598	LR: 0.050000
Training Epoch: 16 [28160/50000]	Loss: 1.2176	LR: 0.050000
Training Epoch: 16 [28288/50000]	Loss: 1.2331	LR: 0.050000
Training Epoch: 16 [28416/50000]	Loss: 1.2236	LR: 0.050000
Training Epoch: 16 [28544/50000]	Loss: 1.3605	LR: 0.050000
Training Epoch: 16 [28672/50000]	Loss: 1.4371	LR: 0.050000
Training Epoch: 16 [28800/50000]	Loss: 1.3554	LR: 0.050000
Training Epoch: 16 [28928/50000]	Loss: 1.3018	LR: 0.050000
Training Epoch: 16 [29056/50000]	Loss: 1.2333	LR: 0.050000
Training Epoch: 16 [29184/50000]	Loss: 1.2666	LR: 0.050000
Training Epoch: 16 [29312/50000]	Loss: 1.2283	LR: 0.050000
Training Epoch: 16 [29440/50000]	Loss: 1.2628	LR: 0.050000
Training Epoch: 16 [29568/50000]	Loss: 1.2801	LR: 0.050000
Training Epoch: 16 [29696/50000]	Loss: 1.2401	LR: 0.050000
Training Epoch: 16 [29824/50000]	Loss: 1.4383	LR: 0.050000
Training Epoch: 16 [29952/50000]	Loss: 1.1396	LR: 0.050000
Training Epoch: 16 [30080/50000]	Loss: 1.4151	LR: 0.050000
Training Epoch: 16 [30208/50000]	Loss: 1.1154	LR: 0.050000
Training Epoch: 16 [30336/50000]	Loss: 1.2075	LR: 0.050000
Training Epoch: 16 [30464/50000]	Loss: 1.3116	LR: 0.050000
Training Epoch: 16 [30592/50000]	Loss: 1.3976	LR: 0.050000
Training Epoch: 16 [30720/50000]	Loss: 1.5156	LR: 0.050000
Training Epoch: 16 [30848/50000]	Loss: 1.1524	LR: 0.050000
Training Epoch: 16 [30976/50000]	Loss: 1.2206	LR: 0.050000
Training Epoch: 16 [31104/50000]	Loss: 1.4064	LR: 0.050000
Training Epoch: 16 [31232/50000]	Loss: 1.2954	LR: 0.050000
Training Epoch: 16 [31360/50000]	Loss: 1.1864	LR: 0.050000
Training Epoch: 16 [31488/50000]	Loss: 1.4377	LR: 0.050000
Training Epoch: 16 [31616/50000]	Loss: 1.6303	LR: 0.050000
Training Epoch: 16 [31744/50000]	Loss: 1.1261	LR: 0.050000
Training Epoch: 16 [31872/50000]	Loss: 1.3623	LR: 0.050000
Training Epoch: 16 [32000/50000]	Loss: 1.1842	LR: 0.050000
Training Epoch: 16 [32128/50000]	Loss: 1.2227	LR: 0.050000
Training Epoch: 16 [32256/50000]	Loss: 1.5055	LR: 0.050000
Training Epoch: 16 [32384/50000]	Loss: 1.2360	LR: 0.050000
Training Epoch: 16 [32512/50000]	Loss: 1.4278	LR: 0.050000
Training Epoch: 16 [32640/50000]	Loss: 1.6520	LR: 0.050000
Training Epoch: 16 [32768/50000]	Loss: 1.1889	LR: 0.050000
Training Epoch: 16 [32896/50000]	Loss: 1.4325	LR: 0.050000
Training Epoch: 16 [33024/50000]	Loss: 1.2205	LR: 0.050000
Training Epoch: 16 [33152/50000]	Loss: 1.3155	LR: 0.050000
Training Epoch: 16 [33280/50000]	Loss: 1.2634	LR: 0.050000
Training Epoch: 16 [33408/50000]	Loss: 1.5939	LR: 0.050000
Training Epoch: 16 [33536/50000]	Loss: 1.1586	LR: 0.050000
Training Epoch: 16 [33664/50000]	Loss: 1.4122	LR: 0.050000
Training Epoch: 16 [33792/50000]	Loss: 1.3786	LR: 0.050000
Training Epoch: 16 [33920/50000]	Loss: 1.6206	LR: 0.050000
Training Epoch: 16 [34048/50000]	Loss: 1.2269	LR: 0.050000
Training Epoch: 16 [34176/50000]	Loss: 1.3806	LR: 0.050000
Training Epoch: 16 [34304/50000]	Loss: 1.1164	LR: 0.050000
Training Epoch: 16 [34432/50000]	Loss: 1.4275	LR: 0.050000
Training Epoch: 16 [34560/50000]	Loss: 1.2607	LR: 0.050000
Training Epoch: 16 [34688/50000]	Loss: 1.3637	LR: 0.050000
Training Epoch: 16 [34816/50000]	Loss: 1.4324	LR: 0.050000
Training Epoch: 16 [34944/50000]	Loss: 1.4077	LR: 0.050000
Training Epoch: 16 [35072/50000]	Loss: 1.2431	LR: 0.050000
Training Epoch: 16 [35200/50000]	Loss: 1.3258	LR: 0.050000
Training Epoch: 16 [35328/50000]	Loss: 1.3322	LR: 0.050000
Training Epoch: 16 [35456/50000]	Loss: 1.4085	LR: 0.050000
Training Epoch: 16 [35584/50000]	Loss: 1.3227	LR: 0.050000
Training Epoch: 16 [35712/50000]	Loss: 1.4892	LR: 0.050000
Training Epoch: 16 [35840/50000]	Loss: 1.3616	LR: 0.050000
Training Epoch: 16 [35968/50000]	Loss: 1.2774	LR: 0.050000
Training Epoch: 16 [36096/50000]	Loss: 1.3523	LR: 0.050000
Training Epoch: 16 [36224/50000]	Loss: 1.2958	LR: 0.050000
Training Epoch: 16 [36352/50000]	Loss: 1.2873	LR: 0.050000
Training Epoch: 16 [36480/50000]	Loss: 1.3162	LR: 0.050000
Training Epoch: 16 [36608/50000]	Loss: 1.4060	LR: 0.050000
Training Epoch: 16 [36736/50000]	Loss: 1.3114	LR: 0.050000
Training Epoch: 16 [36864/50000]	Loss: 1.2918	LR: 0.050000
Training Epoch: 16 [36992/50000]	Loss: 1.2298	LR: 0.050000
Training Epoch: 16 [37120/50000]	Loss: 1.3047	LR: 0.050000
Training Epoch: 16 [37248/50000]	Loss: 1.3896	LR: 0.050000
Training Epoch: 16 [37376/50000]	Loss: 1.6183	LR: 0.050000
Training Epoch: 16 [37504/50000]	Loss: 1.1841	LR: 0.050000
Training Epoch: 16 [37632/50000]	Loss: 1.3758	LR: 0.050000
Training Epoch: 16 [37760/50000]	Loss: 1.2657	LR: 0.050000
Training Epoch: 16 [37888/50000]	Loss: 1.2993	LR: 0.050000
Training Epoch: 16 [38016/50000]	Loss: 1.3072	LR: 0.050000
Training Epoch: 16 [38144/50000]	Loss: 1.2191	LR: 0.050000
Training Epoch: 16 [38272/50000]	Loss: 1.3899	LR: 0.050000
Training Epoch: 16 [38400/50000]	Loss: 1.2146	LR: 0.050000
Training Epoch: 16 [38528/50000]	Loss: 1.3237	LR: 0.050000
Training Epoch: 16 [38656/50000]	Loss: 1.4713	LR: 0.050000
Training Epoch: 16 [38784/50000]	Loss: 1.3622	LR: 0.050000
Training Epoch: 16 [38912/50000]	Loss: 1.1667	LR: 0.050000
Training Epoch: 16 [39040/50000]	Loss: 1.4727	LR: 0.050000
Training Epoch: 16 [39168/50000]	Loss: 1.4084	LR: 0.050000
Training Epoch: 16 [39296/50000]	Loss: 1.6482	LR: 0.050000
Training Epoch: 16 [39424/50000]	Loss: 1.3322	LR: 0.050000
Training Epoch: 16 [39552/50000]	Loss: 1.2902	LR: 0.050000
Training Epoch: 16 [39680/50000]	Loss: 1.2311	LR: 0.050000
Training Epoch: 16 [39808/50000]	Loss: 1.4476	LR: 0.050000
Training Epoch: 16 [39936/50000]	Loss: 1.2377	LR: 0.050000
Training Epoch: 16 [40064/50000]	Loss: 1.4022	LR: 0.050000
Training Epoch: 16 [40192/50000]	Loss: 1.2275	LR: 0.050000
Training Epoch: 16 [40320/50000]	Loss: 1.4344	LR: 0.050000
Training Epoch: 16 [40448/50000]	Loss: 1.1077	LR: 0.050000
Training Epoch: 16 [40576/50000]	Loss: 1.6110	LR: 0.050000
Training Epoch: 16 [40704/50000]	Loss: 1.3593	LR: 0.050000
Training Epoch: 16 [40832/50000]	Loss: 1.5178	LR: 0.050000
Training Epoch: 16 [40960/50000]	Loss: 1.2455	LR: 0.050000
Training Epoch: 16 [41088/50000]	Loss: 1.4264	LR: 0.050000
Training Epoch: 16 [41216/50000]	Loss: 1.4640	LR: 0.050000
Training Epoch: 16 [41344/50000]	Loss: 1.4281	LR: 0.050000
Training Epoch: 16 [41472/50000]	Loss: 1.3591	LR: 0.050000
Training Epoch: 16 [41600/50000]	Loss: 1.1740	LR: 0.050000
Training Epoch: 16 [41728/50000]	Loss: 1.1424	LR: 0.050000
Training Epoch: 16 [41856/50000]	Loss: 1.2796	LR: 0.050000
Training Epoch: 16 [41984/50000]	Loss: 1.2779	LR: 0.050000
Training Epoch: 16 [42112/50000]	Loss: 1.2303	LR: 0.050000
Training Epoch: 16 [42240/50000]	Loss: 1.1814	LR: 0.050000
Training Epoch: 16 [42368/50000]	Loss: 1.3692	LR: 0.050000
Training Epoch: 16 [42496/50000]	Loss: 1.3269	LR: 0.050000
Training Epoch: 16 [42624/50000]	Loss: 1.1771	LR: 0.050000
Training Epoch: 16 [42752/50000]	Loss: 1.3514	LR: 0.050000
Training Epoch: 16 [42880/50000]	Loss: 1.5234	LR: 0.050000
Training Epoch: 16 [43008/50000]	Loss: 1.4034	LR: 0.050000
Training Epoch: 16 [43136/50000]	Loss: 1.3484	LR: 0.050000
Training Epoch: 16 [43264/50000]	Loss: 1.4478	LR: 0.050000
Training Epoch: 16 [43392/50000]	Loss: 1.2040	LR: 0.050000
Training Epoch: 16 [43520/50000]	Loss: 1.3414	LR: 0.050000
Training Epoch: 16 [43648/50000]	Loss: 1.4630	LR: 0.050000
Training Epoch: 16 [43776/50000]	Loss: 1.2937	LR: 0.050000
Training Epoch: 16 [43904/50000]	Loss: 1.5556	LR: 0.050000
Training Epoch: 16 [44032/50000]	Loss: 1.2300	LR: 0.050000
Training Epoch: 16 [44160/50000]	Loss: 1.1592	LR: 0.050000
Training Epoch: 16 [44288/50000]	Loss: 1.1485	LR: 0.050000
Training Epoch: 16 [44416/50000]	Loss: 1.4494	LR: 0.050000
Training Epoch: 16 [44544/50000]	Loss: 1.4447	LR: 0.050000
Training Epoch: 16 [44672/50000]	Loss: 1.4391	LR: 0.050000
Training Epoch: 16 [44800/50000]	Loss: 1.3797	LR: 0.050000
Training Epoch: 16 [44928/50000]	Loss: 1.1537	LR: 0.050000
Training Epoch: 16 [45056/50000]	Loss: 1.3961	LR: 0.050000
Training Epoch: 16 [45184/50000]	Loss: 1.2280	LR: 0.050000
Training Epoch: 16 [45312/50000]	Loss: 1.4978	LR: 0.050000
Training Epoch: 16 [45440/50000]	Loss: 1.2108	LR: 0.050000
Training Epoch: 16 [45568/50000]	Loss: 1.3955	LR: 0.050000
Training Epoch: 16 [45696/50000]	Loss: 1.0776	LR: 0.050000
Training Epoch: 16 [45824/50000]	Loss: 1.2718	LR: 0.050000
Training Epoch: 16 [45952/50000]	Loss: 1.2697	LR: 0.050000
Training Epoch: 16 [46080/50000]	Loss: 1.0391	LR: 0.050000
Training Epoch: 16 [46208/50000]	Loss: 1.3579	LR: 0.050000
Training Epoch: 16 [46336/50000]	Loss: 1.0867	LR: 0.050000
Training Epoch: 16 [46464/50000]	Loss: 1.3751	LR: 0.050000
Training Epoch: 16 [46592/50000]	Loss: 1.1086	LR: 0.050000
Training Epoch: 16 [46720/50000]	Loss: 1.2462	LR: 0.050000
Training Epoch: 16 [46848/50000]	Loss: 1.5034	LR: 0.050000
Training Epoch: 16 [46976/50000]	Loss: 1.3676	LR: 0.050000
Training Epoch: 16 [47104/50000]	Loss: 1.2944	LR: 0.050000
Training Epoch: 16 [47232/50000]	Loss: 1.1961	LR: 0.050000
Training Epoch: 16 [47360/50000]	Loss: 1.3813	LR: 0.050000
Training Epoch: 16 [47488/50000]	Loss: 1.3139	LR: 0.050000
Training Epoch: 16 [47616/50000]	Loss: 1.3738	LR: 0.050000
Training Epoch: 16 [47744/50000]	Loss: 1.2960	LR: 0.050000
Training Epoch: 16 [47872/50000]	Loss: 1.2135	LR: 0.050000
Training Epoch: 16 [48000/50000]	Loss: 1.5730	LR: 0.050000
Training Epoch: 16 [48128/50000]	Loss: 1.3109	LR: 0.050000
Training Epoch: 16 [48256/50000]	Loss: 1.1418	LR: 0.050000
Training Epoch: 16 [48384/50000]	Loss: 1.3531	LR: 0.050000
Training Epoch: 16 [48512/50000]	Loss: 1.2750	LR: 0.050000
Training Epoch: 16 [48640/50000]	Loss: 1.5660	LR: 0.050000
Training Epoch: 16 [48768/50000]	Loss: 1.4047	LR: 0.050000
Training Epoch: 16 [48896/50000]	Loss: 1.6058	LR: 0.050000
Training Epoch: 16 [49024/50000]	Loss: 1.3818	LR: 0.050000
Training Epoch: 16 [49152/50000]	Loss: 1.3943	LR: 0.050000
Training Epoch: 16 [49280/50000]	Loss: 1.3252	LR: 0.050000
Training Epoch: 16 [49408/50000]	Loss: 1.1701	LR: 0.050000
Training Epoch: 16 [49536/50000]	Loss: 1.3231	LR: 0.050000
Training Epoch: 16 [49664/50000]	Loss: 1.2020	LR: 0.050000
Training Epoch: 16 [49792/50000]	Loss: 1.1642	LR: 0.050000
Training Epoch: 16 [49920/50000]	Loss: 1.3242	LR: 0.050000
Training Epoch: 16 [50000/50000]	Loss: 1.1261	LR: 0.050000
epoch 16 training time consumed: 53.89s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   58721 GB |   58721 GB |
|       from large pool |  123392 KB |    1034 MB |   58663 GB |   58663 GB |
|       from small pool |   10798 KB |      13 MB |      57 GB |      57 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   58721 GB |   58721 GB |
|       from large pool |  123392 KB |    1034 MB |   58663 GB |   58663 GB |
|       from small pool |   10798 KB |      13 MB |      57 GB |      57 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   25843 GB |   25843 GB |
|       from large pool |  155136 KB |  433088 KB |   25779 GB |   25779 GB |
|       from small pool |    1490 KB |    3494 KB |      63 GB |      63 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    2266 K  |    2266 K  |
|       from large pool |      24    |      65    |    1182 K  |    1182 K  |
|       from small pool |     231    |     274    |    1083 K  |    1083 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    2266 K  |    2266 K  |
|       from large pool |      24    |      65    |    1182 K  |    1182 K  |
|       from small pool |     231    |     274    |    1083 K  |    1083 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1122 K  |    1122 K  |
|       from large pool |       9    |      14    |     572 K  |     572 K  |
|       from small pool |      12    |      16    |     550 K  |     550 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 16, Average loss: 0.0125, Accuracy: 0.5681, Time consumed:3.45s

Training Epoch: 17 [128/50000]	Loss: 1.2893	LR: 0.050000
Training Epoch: 17 [256/50000]	Loss: 1.2417	LR: 0.050000
Training Epoch: 17 [384/50000]	Loss: 1.1970	LR: 0.050000
Training Epoch: 17 [512/50000]	Loss: 1.3948	LR: 0.050000
Training Epoch: 17 [640/50000]	Loss: 1.1607	LR: 0.050000
Training Epoch: 17 [768/50000]	Loss: 1.0941	LR: 0.050000
Training Epoch: 17 [896/50000]	Loss: 1.2451	LR: 0.050000
Training Epoch: 17 [1024/50000]	Loss: 1.2314	LR: 0.050000
Training Epoch: 17 [1152/50000]	Loss: 1.1506	LR: 0.050000
Training Epoch: 17 [1280/50000]	Loss: 1.2162	LR: 0.050000
Training Epoch: 17 [1408/50000]	Loss: 1.3290	LR: 0.050000
Training Epoch: 17 [1536/50000]	Loss: 1.2591	LR: 0.050000
Training Epoch: 17 [1664/50000]	Loss: 1.0744	LR: 0.050000
Training Epoch: 17 [1792/50000]	Loss: 1.3646	LR: 0.050000
Training Epoch: 17 [1920/50000]	Loss: 1.0880	LR: 0.050000
Training Epoch: 17 [2048/50000]	Loss: 1.3095	LR: 0.050000
Training Epoch: 17 [2176/50000]	Loss: 1.1828	LR: 0.050000
Training Epoch: 17 [2304/50000]	Loss: 1.1592	LR: 0.050000
Training Epoch: 17 [2432/50000]	Loss: 1.5150	LR: 0.050000
Training Epoch: 17 [2560/50000]	Loss: 1.3372	LR: 0.050000
Training Epoch: 17 [2688/50000]	Loss: 1.1783	LR: 0.050000
Training Epoch: 17 [2816/50000]	Loss: 1.4265	LR: 0.050000
Training Epoch: 17 [2944/50000]	Loss: 1.1256	LR: 0.050000
Training Epoch: 17 [3072/50000]	Loss: 1.2481	LR: 0.050000
Training Epoch: 17 [3200/50000]	Loss: 1.2075	LR: 0.050000
Training Epoch: 17 [3328/50000]	Loss: 1.2498	LR: 0.050000
Training Epoch: 17 [3456/50000]	Loss: 1.4315	LR: 0.050000
Training Epoch: 17 [3584/50000]	Loss: 1.4161	LR: 0.050000
Training Epoch: 17 [3712/50000]	Loss: 1.3789	LR: 0.050000
Training Epoch: 17 [3840/50000]	Loss: 1.1824	LR: 0.050000
Training Epoch: 17 [3968/50000]	Loss: 0.9919	LR: 0.050000
Training Epoch: 17 [4096/50000]	Loss: 1.0777	LR: 0.050000
Training Epoch: 17 [4224/50000]	Loss: 1.2009	LR: 0.050000
Training Epoch: 17 [4352/50000]	Loss: 1.2036	LR: 0.050000
Training Epoch: 17 [4480/50000]	Loss: 1.2120	LR: 0.050000
Training Epoch: 17 [4608/50000]	Loss: 1.3970	LR: 0.050000
Training Epoch: 17 [4736/50000]	Loss: 1.2507	LR: 0.050000
Training Epoch: 17 [4864/50000]	Loss: 1.4011	LR: 0.050000
Training Epoch: 17 [4992/50000]	Loss: 1.2229	LR: 0.050000
Training Epoch: 17 [5120/50000]	Loss: 1.3680	LR: 0.050000
Training Epoch: 17 [5248/50000]	Loss: 1.4884	LR: 0.050000
Training Epoch: 17 [5376/50000]	Loss: 1.3240	LR: 0.050000
Training Epoch: 17 [5504/50000]	Loss: 1.3838	LR: 0.050000
Training Epoch: 17 [5632/50000]	Loss: 1.2437	LR: 0.050000
Training Epoch: 17 [5760/50000]	Loss: 1.2223	LR: 0.050000
Training Epoch: 17 [5888/50000]	Loss: 1.3188	LR: 0.050000
Training Epoch: 17 [6016/50000]	Loss: 1.2311	LR: 0.050000
Training Epoch: 17 [6144/50000]	Loss: 0.9767	LR: 0.050000
Training Epoch: 17 [6272/50000]	Loss: 1.4068	LR: 0.050000
Training Epoch: 17 [6400/50000]	Loss: 0.8866	LR: 0.050000
Training Epoch: 17 [6528/50000]	Loss: 1.2027	LR: 0.050000
Training Epoch: 17 [6656/50000]	Loss: 1.3969	LR: 0.050000
Training Epoch: 17 [6784/50000]	Loss: 1.2678	LR: 0.050000
Training Epoch: 17 [6912/50000]	Loss: 1.3974	LR: 0.050000
Training Epoch: 17 [7040/50000]	Loss: 1.1734	LR: 0.050000
Training Epoch: 17 [7168/50000]	Loss: 1.4129	LR: 0.050000
Training Epoch: 17 [7296/50000]	Loss: 1.4289	LR: 0.050000
Training Epoch: 17 [7424/50000]	Loss: 1.2142	LR: 0.050000
Training Epoch: 17 [7552/50000]	Loss: 1.2817	LR: 0.050000
Training Epoch: 17 [7680/50000]	Loss: 1.1943	LR: 0.050000
Training Epoch: 17 [7808/50000]	Loss: 1.2176	LR: 0.050000
Training Epoch: 17 [7936/50000]	Loss: 1.3184	LR: 0.050000
Training Epoch: 17 [8064/50000]	Loss: 1.3545	LR: 0.050000
Training Epoch: 17 [8192/50000]	Loss: 0.9094	LR: 0.050000
Training Epoch: 17 [8320/50000]	Loss: 1.1985	LR: 0.050000
Training Epoch: 17 [8448/50000]	Loss: 1.4754	LR: 0.050000
Training Epoch: 17 [8576/50000]	Loss: 1.3050	LR: 0.050000
Training Epoch: 17 [8704/50000]	Loss: 0.9134	LR: 0.050000
Training Epoch: 17 [8832/50000]	Loss: 1.2106	LR: 0.050000
Training Epoch: 17 [8960/50000]	Loss: 1.4031	LR: 0.050000
Training Epoch: 17 [9088/50000]	Loss: 1.2903	LR: 0.050000
Training Epoch: 17 [9216/50000]	Loss: 1.3166	LR: 0.050000
Training Epoch: 17 [9344/50000]	Loss: 1.3172	LR: 0.050000
Training Epoch: 17 [9472/50000]	Loss: 1.3337	LR: 0.050000
Training Epoch: 17 [9600/50000]	Loss: 0.9846	LR: 0.050000
Training Epoch: 17 [9728/50000]	Loss: 1.5303	LR: 0.050000
Training Epoch: 17 [9856/50000]	Loss: 1.1443	LR: 0.050000
Training Epoch: 17 [9984/50000]	Loss: 1.1554	LR: 0.050000
Training Epoch: 17 [10112/50000]	Loss: 1.1898	LR: 0.050000
Training Epoch: 17 [10240/50000]	Loss: 1.1892	LR: 0.050000
Training Epoch: 17 [10368/50000]	Loss: 1.1415	LR: 0.050000
Training Epoch: 17 [10496/50000]	Loss: 1.2682	LR: 0.050000
Training Epoch: 17 [10624/50000]	Loss: 1.2687	LR: 0.050000
Training Epoch: 17 [10752/50000]	Loss: 1.4286	LR: 0.050000
Training Epoch: 17 [10880/50000]	Loss: 1.2294	LR: 0.050000
Training Epoch: 17 [11008/50000]	Loss: 1.1322	LR: 0.050000
Training Epoch: 17 [11136/50000]	Loss: 1.4056	LR: 0.050000
Training Epoch: 17 [11264/50000]	Loss: 1.0329	LR: 0.050000
Training Epoch: 17 [11392/50000]	Loss: 0.9520	LR: 0.050000
Training Epoch: 17 [11520/50000]	Loss: 1.1890	LR: 0.050000
Training Epoch: 17 [11648/50000]	Loss: 1.4509	LR: 0.050000
Training Epoch: 17 [11776/50000]	Loss: 1.2080	LR: 0.050000
Training Epoch: 17 [11904/50000]	Loss: 1.1348	LR: 0.050000
Training Epoch: 17 [12032/50000]	Loss: 1.2605	LR: 0.050000
Training Epoch: 17 [12160/50000]	Loss: 1.0966	LR: 0.050000
Training Epoch: 17 [12288/50000]	Loss: 1.2398	LR: 0.050000
Training Epoch: 17 [12416/50000]	Loss: 1.4259	LR: 0.050000
Training Epoch: 17 [12544/50000]	Loss: 1.2947	LR: 0.050000
Training Epoch: 17 [12672/50000]	Loss: 1.1888	LR: 0.050000
Training Epoch: 17 [12800/50000]	Loss: 1.2028	LR: 0.050000
Training Epoch: 17 [12928/50000]	Loss: 1.2735	LR: 0.050000
Training Epoch: 17 [13056/50000]	Loss: 1.1215	LR: 0.050000
Training Epoch: 17 [13184/50000]	Loss: 1.2130	LR: 0.050000
Training Epoch: 17 [13312/50000]	Loss: 1.2735	LR: 0.050000
Training Epoch: 17 [13440/50000]	Loss: 1.2641	LR: 0.050000
Training Epoch: 17 [13568/50000]	Loss: 1.1508	LR: 0.050000
Training Epoch: 17 [13696/50000]	Loss: 1.3243	LR: 0.050000
Training Epoch: 17 [13824/50000]	Loss: 1.3761	LR: 0.050000
Training Epoch: 17 [13952/50000]	Loss: 1.1749	LR: 0.050000
Training Epoch: 17 [14080/50000]	Loss: 1.2985	LR: 0.050000
Training Epoch: 17 [14208/50000]	Loss: 1.2401	LR: 0.050000
Training Epoch: 17 [14336/50000]	Loss: 1.1261	LR: 0.050000
Training Epoch: 17 [14464/50000]	Loss: 1.3951	LR: 0.050000
Training Epoch: 17 [14592/50000]	Loss: 1.3316	LR: 0.050000
Training Epoch: 17 [14720/50000]	Loss: 1.1657	LR: 0.050000
Training Epoch: 17 [14848/50000]	Loss: 1.2646	LR: 0.050000
Training Epoch: 17 [14976/50000]	Loss: 1.2289	LR: 0.050000
Training Epoch: 17 [15104/50000]	Loss: 1.3681	LR: 0.050000
Training Epoch: 17 [15232/50000]	Loss: 1.3025	LR: 0.050000
Training Epoch: 17 [15360/50000]	Loss: 1.3791	LR: 0.050000
Training Epoch: 17 [15488/50000]	Loss: 1.2314	LR: 0.050000
Training Epoch: 17 [15616/50000]	Loss: 1.4327	LR: 0.050000
Training Epoch: 17 [15744/50000]	Loss: 1.2715	LR: 0.050000
Training Epoch: 17 [15872/50000]	Loss: 1.5087	LR: 0.050000
Training Epoch: 17 [16000/50000]	Loss: 1.3660	LR: 0.050000
Training Epoch: 17 [16128/50000]	Loss: 1.2652	LR: 0.050000
Training Epoch: 17 [16256/50000]	Loss: 1.3247	LR: 0.050000
Training Epoch: 17 [16384/50000]	Loss: 1.2653	LR: 0.050000
Training Epoch: 17 [16512/50000]	Loss: 1.2895	LR: 0.050000
Training Epoch: 17 [16640/50000]	Loss: 1.2962	LR: 0.050000
Training Epoch: 17 [16768/50000]	Loss: 1.1288	LR: 0.050000
Training Epoch: 17 [16896/50000]	Loss: 1.3582	LR: 0.050000
Training Epoch: 17 [17024/50000]	Loss: 1.1705	LR: 0.050000
Training Epoch: 17 [17152/50000]	Loss: 1.3743	LR: 0.050000
Training Epoch: 17 [17280/50000]	Loss: 1.2723	LR: 0.050000
Training Epoch: 17 [17408/50000]	Loss: 1.1822	LR: 0.050000
Training Epoch: 17 [17536/50000]	Loss: 1.3631	LR: 0.050000
Training Epoch: 17 [17664/50000]	Loss: 1.2792	LR: 0.050000
Training Epoch: 17 [17792/50000]	Loss: 1.3476	LR: 0.050000
Training Epoch: 17 [17920/50000]	Loss: 1.2131	LR: 0.050000
Training Epoch: 17 [18048/50000]	Loss: 1.7397	LR: 0.050000
Training Epoch: 17 [18176/50000]	Loss: 1.3746	LR: 0.050000
Training Epoch: 17 [18304/50000]	Loss: 1.4373	LR: 0.050000
Training Epoch: 17 [18432/50000]	Loss: 1.1664	LR: 0.050000
Training Epoch: 17 [18560/50000]	Loss: 1.2726	LR: 0.050000
Training Epoch: 17 [18688/50000]	Loss: 1.3707	LR: 0.050000
Training Epoch: 17 [18816/50000]	Loss: 1.4486	LR: 0.050000
Training Epoch: 17 [18944/50000]	Loss: 1.3078	LR: 0.050000
Training Epoch: 17 [19072/50000]	Loss: 1.2773	LR: 0.050000
Training Epoch: 17 [19200/50000]	Loss: 1.4000	LR: 0.050000
Training Epoch: 17 [19328/50000]	Loss: 1.3145	LR: 0.050000
Training Epoch: 17 [19456/50000]	Loss: 1.5690	LR: 0.050000
Training Epoch: 17 [19584/50000]	Loss: 1.5511	LR: 0.050000
Training Epoch: 17 [19712/50000]	Loss: 1.2223	LR: 0.050000
Training Epoch: 17 [19840/50000]	Loss: 1.3397	LR: 0.050000
Training Epoch: 17 [19968/50000]	Loss: 1.0091	LR: 0.050000
Training Epoch: 17 [20096/50000]	Loss: 1.4583	LR: 0.050000
Training Epoch: 17 [20224/50000]	Loss: 1.1391	LR: 0.050000
Training Epoch: 17 [20352/50000]	Loss: 1.1625	LR: 0.050000
Training Epoch: 17 [20480/50000]	Loss: 1.3195	LR: 0.050000
Training Epoch: 17 [20608/50000]	Loss: 1.5188	LR: 0.050000
Training Epoch: 17 [20736/50000]	Loss: 1.3890	LR: 0.050000
Training Epoch: 17 [20864/50000]	Loss: 1.3425	LR: 0.050000
Training Epoch: 17 [20992/50000]	Loss: 1.0556	LR: 0.050000
Training Epoch: 17 [21120/50000]	Loss: 1.2181	LR: 0.050000
Training Epoch: 17 [21248/50000]	Loss: 1.2199	LR: 0.050000
Training Epoch: 17 [21376/50000]	Loss: 1.2593	LR: 0.050000
Training Epoch: 17 [21504/50000]	Loss: 1.3538	LR: 0.050000
Training Epoch: 17 [21632/50000]	Loss: 1.0571	LR: 0.050000
Training Epoch: 17 [21760/50000]	Loss: 1.5630	LR: 0.050000
Training Epoch: 17 [21888/50000]	Loss: 1.2980	LR: 0.050000
Training Epoch: 17 [22016/50000]	Loss: 1.2089	LR: 0.050000
Training Epoch: 17 [22144/50000]	Loss: 1.3078	LR: 0.050000
Training Epoch: 17 [22272/50000]	Loss: 1.4488	LR: 0.050000
Training Epoch: 17 [22400/50000]	Loss: 1.4438	LR: 0.050000
Training Epoch: 17 [22528/50000]	Loss: 1.0914	LR: 0.050000
Training Epoch: 17 [22656/50000]	Loss: 1.3372	LR: 0.050000
Training Epoch: 17 [22784/50000]	Loss: 1.1777	LR: 0.050000
Training Epoch: 17 [22912/50000]	Loss: 1.0672	LR: 0.050000
Training Epoch: 17 [23040/50000]	Loss: 1.3022	LR: 0.050000
Training Epoch: 17 [23168/50000]	Loss: 1.2428	LR: 0.050000
Training Epoch: 17 [23296/50000]	Loss: 1.1991	LR: 0.050000
Training Epoch: 17 [23424/50000]	Loss: 1.2957	LR: 0.050000
Training Epoch: 17 [23552/50000]	Loss: 1.0749	LR: 0.050000
Training Epoch: 17 [23680/50000]	Loss: 1.3946	LR: 0.050000
Training Epoch: 17 [23808/50000]	Loss: 1.5437	LR: 0.050000
Training Epoch: 17 [23936/50000]	Loss: 1.3696	LR: 0.050000
Training Epoch: 17 [24064/50000]	Loss: 1.0139	LR: 0.050000
Training Epoch: 17 [24192/50000]	Loss: 1.0278	LR: 0.050000
Training Epoch: 17 [24320/50000]	Loss: 1.2641	LR: 0.050000
Training Epoch: 17 [24448/50000]	Loss: 1.2407	LR: 0.050000
Training Epoch: 17 [24576/50000]	Loss: 1.1587	LR: 0.050000
Training Epoch: 17 [24704/50000]	Loss: 1.0682	LR: 0.050000
Training Epoch: 17 [24832/50000]	Loss: 1.1106	LR: 0.050000
Training Epoch: 17 [24960/50000]	Loss: 1.3591	LR: 0.050000
Training Epoch: 17 [25088/50000]	Loss: 1.3682	LR: 0.050000
Training Epoch: 17 [25216/50000]	Loss: 1.4813	LR: 0.050000
Training Epoch: 17 [25344/50000]	Loss: 1.3655	LR: 0.050000
Training Epoch: 17 [25472/50000]	Loss: 1.1805	LR: 0.050000
Training Epoch: 17 [25600/50000]	Loss: 1.1258	LR: 0.050000
Training Epoch: 17 [25728/50000]	Loss: 1.2092	LR: 0.050000
Training Epoch: 17 [25856/50000]	Loss: 1.3398	LR: 0.050000
Training Epoch: 17 [25984/50000]	Loss: 1.6534	LR: 0.050000
Training Epoch: 17 [26112/50000]	Loss: 1.4266	LR: 0.050000
Training Epoch: 17 [26240/50000]	Loss: 1.1012	LR: 0.050000
Training Epoch: 17 [26368/50000]	Loss: 1.4358	LR: 0.050000
Training Epoch: 17 [26496/50000]	Loss: 1.3166	LR: 0.050000
Training Epoch: 17 [26624/50000]	Loss: 1.2287	LR: 0.050000
Training Epoch: 17 [26752/50000]	Loss: 1.4357	LR: 0.050000
Training Epoch: 17 [26880/50000]	Loss: 1.3103	LR: 0.050000
Training Epoch: 17 [27008/50000]	Loss: 1.4598	LR: 0.050000
Training Epoch: 17 [27136/50000]	Loss: 1.2742	LR: 0.050000
Training Epoch: 17 [27264/50000]	Loss: 1.2246	LR: 0.050000
Training Epoch: 17 [27392/50000]	Loss: 1.0687	LR: 0.050000
Training Epoch: 17 [27520/50000]	Loss: 1.0086	LR: 0.050000
Training Epoch: 17 [27648/50000]	Loss: 1.5098	LR: 0.050000
Training Epoch: 17 [27776/50000]	Loss: 1.2125	LR: 0.050000
Training Epoch: 17 [27904/50000]	Loss: 1.0421	LR: 0.050000
Training Epoch: 17 [28032/50000]	Loss: 1.0875	LR: 0.050000
Training Epoch: 17 [28160/50000]	Loss: 1.2582	LR: 0.050000
Training Epoch: 17 [28288/50000]	Loss: 1.3176	LR: 0.050000
Training Epoch: 17 [28416/50000]	Loss: 1.4813	LR: 0.050000
Training Epoch: 17 [28544/50000]	Loss: 1.4984	LR: 0.050000
Training Epoch: 17 [28672/50000]	Loss: 1.4216	LR: 0.050000
Training Epoch: 17 [28800/50000]	Loss: 1.2225	LR: 0.050000
Training Epoch: 17 [28928/50000]	Loss: 1.1769	LR: 0.050000
Training Epoch: 17 [29056/50000]	Loss: 1.2040	LR: 0.050000
Training Epoch: 17 [29184/50000]	Loss: 1.1896	LR: 0.050000
Training Epoch: 17 [29312/50000]	Loss: 1.3981	LR: 0.050000
Training Epoch: 17 [29440/50000]	Loss: 1.1163	LR: 0.050000
Training Epoch: 17 [29568/50000]	Loss: 1.1882	LR: 0.050000
Training Epoch: 17 [29696/50000]	Loss: 1.2339	LR: 0.050000
Training Epoch: 17 [29824/50000]	Loss: 1.1265	LR: 0.050000
Training Epoch: 17 [29952/50000]	Loss: 1.2493	LR: 0.050000
Training Epoch: 17 [30080/50000]	Loss: 1.3355	LR: 0.050000
Training Epoch: 17 [30208/50000]	Loss: 1.2107	LR: 0.050000
Training Epoch: 17 [30336/50000]	Loss: 1.3610	LR: 0.050000
Training Epoch: 17 [30464/50000]	Loss: 1.1546	LR: 0.050000
Training Epoch: 17 [30592/50000]	Loss: 1.1416	LR: 0.050000
Training Epoch: 17 [30720/50000]	Loss: 1.2888	LR: 0.050000
Training Epoch: 17 [30848/50000]	Loss: 1.4193	LR: 0.050000
Training Epoch: 17 [30976/50000]	Loss: 1.3297	LR: 0.050000
Training Epoch: 17 [31104/50000]	Loss: 1.3791	LR: 0.050000
Training Epoch: 17 [31232/50000]	Loss: 1.2311	LR: 0.050000
Training Epoch: 17 [31360/50000]	Loss: 1.1469	LR: 0.050000
Training Epoch: 17 [31488/50000]	Loss: 1.2102	LR: 0.050000
Training Epoch: 17 [31616/50000]	Loss: 1.1542	LR: 0.050000
Training Epoch: 17 [31744/50000]	Loss: 1.0738	LR: 0.050000
Training Epoch: 17 [31872/50000]	Loss: 1.2637	LR: 0.050000
Training Epoch: 17 [32000/50000]	Loss: 1.4461	LR: 0.050000
Training Epoch: 17 [32128/50000]	Loss: 1.2687	LR: 0.050000
Training Epoch: 17 [32256/50000]	Loss: 1.4278	LR: 0.050000
Training Epoch: 17 [32384/50000]	Loss: 1.1936	LR: 0.050000
Training Epoch: 17 [32512/50000]	Loss: 1.1915	LR: 0.050000
Training Epoch: 17 [32640/50000]	Loss: 1.2817	LR: 0.050000
Training Epoch: 17 [32768/50000]	Loss: 1.3544	LR: 0.050000
Training Epoch: 17 [32896/50000]	Loss: 1.3799	LR: 0.050000
Training Epoch: 17 [33024/50000]	Loss: 1.2800	LR: 0.050000
Training Epoch: 17 [33152/50000]	Loss: 1.3972	LR: 0.050000
Training Epoch: 17 [33280/50000]	Loss: 1.2120	LR: 0.050000
Training Epoch: 17 [33408/50000]	Loss: 1.0457	LR: 0.050000
Training Epoch: 17 [33536/50000]	Loss: 1.0054	LR: 0.050000
Training Epoch: 17 [33664/50000]	Loss: 1.3381	LR: 0.050000
Training Epoch: 17 [33792/50000]	Loss: 1.2014	LR: 0.050000
Training Epoch: 17 [33920/50000]	Loss: 1.3011	LR: 0.050000
Training Epoch: 17 [34048/50000]	Loss: 1.3844	LR: 0.050000
Training Epoch: 17 [34176/50000]	Loss: 1.2853	LR: 0.050000
Training Epoch: 17 [34304/50000]	Loss: 1.3391	LR: 0.050000
Training Epoch: 17 [34432/50000]	Loss: 1.3103	LR: 0.050000
Training Epoch: 17 [34560/50000]	Loss: 1.2023	LR: 0.050000
Training Epoch: 17 [34688/50000]	Loss: 1.2910	LR: 0.050000
Training Epoch: 17 [34816/50000]	Loss: 1.3917	LR: 0.050000
Training Epoch: 17 [34944/50000]	Loss: 1.3570	LR: 0.050000
Training Epoch: 17 [35072/50000]	Loss: 1.3059	LR: 0.050000
Training Epoch: 17 [35200/50000]	Loss: 1.1625	LR: 0.050000
Training Epoch: 17 [35328/50000]	Loss: 1.4700	LR: 0.050000
Training Epoch: 17 [35456/50000]	Loss: 1.3433	LR: 0.050000
Training Epoch: 17 [35584/50000]	Loss: 1.2376	LR: 0.050000
Training Epoch: 17 [35712/50000]	Loss: 1.4406	LR: 0.050000
Training Epoch: 17 [35840/50000]	Loss: 1.4042	LR: 0.050000
Training Epoch: 17 [35968/50000]	Loss: 1.3586	LR: 0.050000
Training Epoch: 17 [36096/50000]	Loss: 1.2814	LR: 0.050000
Training Epoch: 17 [36224/50000]	Loss: 1.4047	LR: 0.050000
Training Epoch: 17 [36352/50000]	Loss: 1.2457	LR: 0.050000
Training Epoch: 17 [36480/50000]	Loss: 1.3548	LR: 0.050000
Training Epoch: 17 [36608/50000]	Loss: 1.4318	LR: 0.050000
Training Epoch: 17 [36736/50000]	Loss: 1.3333	LR: 0.050000
Training Epoch: 17 [36864/50000]	Loss: 1.1316	LR: 0.050000
Training Epoch: 17 [36992/50000]	Loss: 1.3785	LR: 0.050000
Training Epoch: 17 [37120/50000]	Loss: 1.4443	LR: 0.050000
Training Epoch: 17 [37248/50000]	Loss: 1.3205	LR: 0.050000
Training Epoch: 17 [37376/50000]	Loss: 0.9490	LR: 0.050000
Training Epoch: 17 [37504/50000]	Loss: 1.2951	LR: 0.050000
Training Epoch: 17 [37632/50000]	Loss: 1.1337	LR: 0.050000
Training Epoch: 17 [37760/50000]	Loss: 1.3576	LR: 0.050000
Training Epoch: 17 [37888/50000]	Loss: 1.2023	LR: 0.050000
Training Epoch: 17 [38016/50000]	Loss: 1.2035	LR: 0.050000
Training Epoch: 17 [38144/50000]	Loss: 1.3587	LR: 0.050000
Training Epoch: 17 [38272/50000]	Loss: 1.4034	LR: 0.050000
Training Epoch: 17 [38400/50000]	Loss: 1.0998	LR: 0.050000
Training Epoch: 17 [38528/50000]	Loss: 1.3192	LR: 0.050000
Training Epoch: 17 [38656/50000]	Loss: 1.5116	LR: 0.050000
Training Epoch: 17 [38784/50000]	Loss: 1.2267	LR: 0.050000
Training Epoch: 17 [38912/50000]	Loss: 1.4998	LR: 0.050000
Training Epoch: 17 [39040/50000]	Loss: 1.2377	LR: 0.050000
Training Epoch: 17 [39168/50000]	Loss: 1.0367	LR: 0.050000
Training Epoch: 17 [39296/50000]	Loss: 1.4174	LR: 0.050000
Training Epoch: 17 [39424/50000]	Loss: 1.3290	LR: 0.050000
Training Epoch: 17 [39552/50000]	Loss: 1.4778	LR: 0.050000
Training Epoch: 17 [39680/50000]	Loss: 1.5116	LR: 0.050000
Training Epoch: 17 [39808/50000]	Loss: 1.1990	LR: 0.050000
Training Epoch: 17 [39936/50000]	Loss: 1.1372	LR: 0.050000
Training Epoch: 17 [40064/50000]	Loss: 1.3455	LR: 0.050000
Training Epoch: 17 [40192/50000]	Loss: 1.3292	LR: 0.050000
Training Epoch: 17 [40320/50000]	Loss: 1.2160	LR: 0.050000
Training Epoch: 17 [40448/50000]	Loss: 1.6031	LR: 0.050000
Training Epoch: 17 [40576/50000]	Loss: 1.2441	LR: 0.050000
Training Epoch: 17 [40704/50000]	Loss: 1.2786	LR: 0.050000
Training Epoch: 17 [40832/50000]	Loss: 1.3897	LR: 0.050000
Training Epoch: 17 [40960/50000]	Loss: 1.2790	LR: 0.050000
Training Epoch: 17 [41088/50000]	Loss: 1.2682	LR: 0.050000
Training Epoch: 17 [41216/50000]	Loss: 1.1577	LR: 0.050000
Training Epoch: 17 [41344/50000]	Loss: 1.5025	LR: 0.050000
Training Epoch: 17 [41472/50000]	Loss: 1.5693	LR: 0.050000
Training Epoch: 17 [41600/50000]	Loss: 1.1511	LR: 0.050000
Training Epoch: 17 [41728/50000]	Loss: 1.3137	LR: 0.050000
Training Epoch: 17 [41856/50000]	Loss: 1.4050	LR: 0.050000
Training Epoch: 17 [41984/50000]	Loss: 1.2713	LR: 0.050000
Training Epoch: 17 [42112/50000]	Loss: 1.4734	LR: 0.050000
Training Epoch: 17 [42240/50000]	Loss: 1.5437	LR: 0.050000
Training Epoch: 17 [42368/50000]	Loss: 1.2645	LR: 0.050000
Training Epoch: 17 [42496/50000]	Loss: 1.4552	LR: 0.050000
Training Epoch: 17 [42624/50000]	Loss: 1.3919	LR: 0.050000
Training Epoch: 17 [42752/50000]	Loss: 1.3640	LR: 0.050000
Training Epoch: 17 [42880/50000]	Loss: 1.3180	LR: 0.050000
Training Epoch: 17 [43008/50000]	Loss: 1.3356	LR: 0.050000
Training Epoch: 17 [43136/50000]	Loss: 1.1812	LR: 0.050000
Training Epoch: 17 [43264/50000]	Loss: 1.2901	LR: 0.050000
Training Epoch: 17 [43392/50000]	Loss: 1.3413	LR: 0.050000
Training Epoch: 17 [43520/50000]	Loss: 1.3801	LR: 0.050000
Training Epoch: 17 [43648/50000]	Loss: 1.3732	LR: 0.050000
Training Epoch: 17 [43776/50000]	Loss: 1.2743	LR: 0.050000
Training Epoch: 17 [43904/50000]	Loss: 1.1824	LR: 0.050000
Training Epoch: 17 [44032/50000]	Loss: 1.2836	LR: 0.050000
Training Epoch: 17 [44160/50000]	Loss: 1.6294	LR: 0.050000
Training Epoch: 17 [44288/50000]	Loss: 1.2872	LR: 0.050000
Training Epoch: 17 [44416/50000]	Loss: 1.3241	LR: 0.050000
Training Epoch: 17 [44544/50000]	Loss: 1.3202	LR: 0.050000
Training Epoch: 17 [44672/50000]	Loss: 1.2447	LR: 0.050000
Training Epoch: 17 [44800/50000]	Loss: 1.2818	LR: 0.050000
Training Epoch: 17 [44928/50000]	Loss: 1.5448	LR: 0.050000
Training Epoch: 17 [45056/50000]	Loss: 1.4793	LR: 0.050000
Training Epoch: 17 [45184/50000]	Loss: 1.2219	LR: 0.050000
Training Epoch: 17 [45312/50000]	Loss: 1.4576	LR: 0.050000
Training Epoch: 17 [45440/50000]	Loss: 1.2886	LR: 0.050000
Training Epoch: 17 [45568/50000]	Loss: 1.3401	LR: 0.050000
Training Epoch: 17 [45696/50000]	Loss: 1.1819	LR: 0.050000
Training Epoch: 17 [45824/50000]	Loss: 1.2820	LR: 0.050000
Training Epoch: 17 [45952/50000]	Loss: 1.5887	LR: 0.050000
Training Epoch: 17 [46080/50000]	Loss: 1.3711	LR: 0.050000
Training Epoch: 17 [46208/50000]	Loss: 1.5870	LR: 0.050000
Training Epoch: 17 [46336/50000]	Loss: 1.5940	LR: 0.050000
Training Epoch: 17 [46464/50000]	Loss: 1.2529	LR: 0.050000
Training Epoch: 17 [46592/50000]	Loss: 1.3748	LR: 0.050000
Training Epoch: 17 [46720/50000]	Loss: 1.4474	LR: 0.050000
Training Epoch: 17 [46848/50000]	Loss: 1.3583	LR: 0.050000
Training Epoch: 17 [46976/50000]	Loss: 1.3317	LR: 0.050000
Training Epoch: 17 [47104/50000]	Loss: 1.5054	LR: 0.050000
Training Epoch: 17 [47232/50000]	Loss: 1.3912	LR: 0.050000
Training Epoch: 17 [47360/50000]	Loss: 1.3844	LR: 0.050000
Training Epoch: 17 [47488/50000]	Loss: 1.1903	LR: 0.050000
Training Epoch: 17 [47616/50000]	Loss: 1.3878	LR: 0.050000
Training Epoch: 17 [47744/50000]	Loss: 1.2672	LR: 0.050000
Training Epoch: 17 [47872/50000]	Loss: 1.3049	LR: 0.050000
Training Epoch: 17 [48000/50000]	Loss: 1.3115	LR: 0.050000
Training Epoch: 17 [48128/50000]	Loss: 1.4805	LR: 0.050000
Training Epoch: 17 [48256/50000]	Loss: 1.6211	LR: 0.050000
Training Epoch: 17 [48384/50000]	Loss: 1.5870	LR: 0.050000
Training Epoch: 17 [48512/50000]	Loss: 1.1587	LR: 0.050000
Training Epoch: 17 [48640/50000]	Loss: 1.3288	LR: 0.050000
Training Epoch: 17 [48768/50000]	Loss: 1.4995	LR: 0.050000
Training Epoch: 17 [48896/50000]	Loss: 1.3301	LR: 0.050000
Training Epoch: 17 [49024/50000]	Loss: 1.4899	LR: 0.050000
Training Epoch: 17 [49152/50000]	Loss: 1.2461	LR: 0.050000
Training Epoch: 17 [49280/50000]	Loss: 1.2542	LR: 0.050000
Training Epoch: 17 [49408/50000]	Loss: 1.5280	LR: 0.050000
Training Epoch: 17 [49536/50000]	Loss: 1.2086	LR: 0.050000
Training Epoch: 17 [49664/50000]	Loss: 1.1661	LR: 0.050000
Training Epoch: 17 [49792/50000]	Loss: 1.4168	LR: 0.050000
Training Epoch: 17 [49920/50000]	Loss: 1.3364	LR: 0.050000
Training Epoch: 17 [50000/50000]	Loss: 1.3752	LR: 0.050000
epoch 17 training time consumed: 53.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   62391 GB |   62391 GB |
|       from large pool |  123392 KB |    1034 MB |   62330 GB |   62329 GB |
|       from small pool |   10798 KB |      13 MB |      61 GB |      61 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   62391 GB |   62391 GB |
|       from large pool |  123392 KB |    1034 MB |   62330 GB |   62329 GB |
|       from small pool |   10798 KB |      13 MB |      61 GB |      61 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   27458 GB |   27458 GB |
|       from large pool |  155136 KB |  433088 KB |   27390 GB |   27390 GB |
|       from small pool |    1490 KB |    3494 KB |      67 GB |      67 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    2407 K  |    2407 K  |
|       from large pool |      24    |      65    |    1256 K  |    1256 K  |
|       from small pool |     231    |     274    |    1151 K  |    1150 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    2407 K  |    2407 K  |
|       from large pool |      24    |      65    |    1256 K  |    1256 K  |
|       from small pool |     231    |     274    |    1151 K  |    1150 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1192 K  |    1192 K  |
|       from large pool |       9    |      14    |     608 K  |     608 K  |
|       from small pool |      12    |      16    |     584 K  |     584 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 17, Average loss: 0.0119, Accuracy: 0.5826, Time consumed:3.49s

Training Epoch: 18 [128/50000]	Loss: 1.1147	LR: 0.050000
Training Epoch: 18 [256/50000]	Loss: 1.1256	LR: 0.050000
Training Epoch: 18 [384/50000]	Loss: 1.3509	LR: 0.050000
Training Epoch: 18 [512/50000]	Loss: 1.0643	LR: 0.050000
Training Epoch: 18 [640/50000]	Loss: 1.0979	LR: 0.050000
Training Epoch: 18 [768/50000]	Loss: 1.0703	LR: 0.050000
Training Epoch: 18 [896/50000]	Loss: 1.3493	LR: 0.050000
Training Epoch: 18 [1024/50000]	Loss: 1.4485	LR: 0.050000
Training Epoch: 18 [1152/50000]	Loss: 1.3363	LR: 0.050000
Training Epoch: 18 [1280/50000]	Loss: 1.2322	LR: 0.050000
Training Epoch: 18 [1408/50000]	Loss: 1.1717	LR: 0.050000
Training Epoch: 18 [1536/50000]	Loss: 1.2605	LR: 0.050000
Training Epoch: 18 [1664/50000]	Loss: 1.1576	LR: 0.050000
Training Epoch: 18 [1792/50000]	Loss: 1.1639	LR: 0.050000
Training Epoch: 18 [1920/50000]	Loss: 1.1915	LR: 0.050000
Training Epoch: 18 [2048/50000]	Loss: 1.1732	LR: 0.050000
Training Epoch: 18 [2176/50000]	Loss: 1.3478	LR: 0.050000
Training Epoch: 18 [2304/50000]	Loss: 1.2823	LR: 0.050000
Training Epoch: 18 [2432/50000]	Loss: 1.2283	LR: 0.050000
Training Epoch: 18 [2560/50000]	Loss: 1.1479	LR: 0.050000
Training Epoch: 18 [2688/50000]	Loss: 1.3863	LR: 0.050000
Training Epoch: 18 [2816/50000]	Loss: 1.1650	LR: 0.050000
Training Epoch: 18 [2944/50000]	Loss: 1.2524	LR: 0.050000
Training Epoch: 18 [3072/50000]	Loss: 1.0309	LR: 0.050000
Training Epoch: 18 [3200/50000]	Loss: 1.4266	LR: 0.050000
Training Epoch: 18 [3328/50000]	Loss: 1.1036	LR: 0.050000
Training Epoch: 18 [3456/50000]	Loss: 1.3532	LR: 0.050000
Training Epoch: 18 [3584/50000]	Loss: 0.9064	LR: 0.050000
Training Epoch: 18 [3712/50000]	Loss: 1.1432	LR: 0.050000
Training Epoch: 18 [3840/50000]	Loss: 1.2412	LR: 0.050000
Training Epoch: 18 [3968/50000]	Loss: 1.2197	LR: 0.050000
Training Epoch: 18 [4096/50000]	Loss: 1.0986	LR: 0.050000
Training Epoch: 18 [4224/50000]	Loss: 1.2690	LR: 0.050000
Training Epoch: 18 [4352/50000]	Loss: 0.9940	LR: 0.050000
Training Epoch: 18 [4480/50000]	Loss: 1.2742	LR: 0.050000
Training Epoch: 18 [4608/50000]	Loss: 1.3811	LR: 0.050000
Training Epoch: 18 [4736/50000]	Loss: 1.2299	LR: 0.050000
Training Epoch: 18 [4864/50000]	Loss: 1.2807	LR: 0.050000
Training Epoch: 18 [4992/50000]	Loss: 1.2043	LR: 0.050000
Training Epoch: 18 [5120/50000]	Loss: 1.3307	LR: 0.050000
Training Epoch: 18 [5248/50000]	Loss: 1.5044	LR: 0.050000
Training Epoch: 18 [5376/50000]	Loss: 1.2779	LR: 0.050000
Training Epoch: 18 [5504/50000]	Loss: 1.1393	LR: 0.050000
Training Epoch: 18 [5632/50000]	Loss: 1.1581	LR: 0.050000
Training Epoch: 18 [5760/50000]	Loss: 1.4309	LR: 0.050000
Training Epoch: 18 [5888/50000]	Loss: 1.1689	LR: 0.050000
Training Epoch: 18 [6016/50000]	Loss: 1.1642	LR: 0.050000
Training Epoch: 18 [6144/50000]	Loss: 1.0747	LR: 0.050000
Training Epoch: 18 [6272/50000]	Loss: 1.1387	LR: 0.050000
Training Epoch: 18 [6400/50000]	Loss: 1.2615	LR: 0.050000
Training Epoch: 18 [6528/50000]	Loss: 1.3201	LR: 0.050000
Training Epoch: 18 [6656/50000]	Loss: 1.1767	LR: 0.050000
Training Epoch: 18 [6784/50000]	Loss: 1.0689	LR: 0.050000
Training Epoch: 18 [6912/50000]	Loss: 1.1110	LR: 0.050000
Training Epoch: 18 [7040/50000]	Loss: 1.1194	LR: 0.050000
Training Epoch: 18 [7168/50000]	Loss: 1.4062	LR: 0.050000
Training Epoch: 18 [7296/50000]	Loss: 1.0863	LR: 0.050000
Training Epoch: 18 [7424/50000]	Loss: 1.1118	LR: 0.050000
Training Epoch: 18 [7552/50000]	Loss: 1.1254	LR: 0.050000
Training Epoch: 18 [7680/50000]	Loss: 1.1319	LR: 0.050000
Training Epoch: 18 [7808/50000]	Loss: 1.3530	LR: 0.050000
Training Epoch: 18 [7936/50000]	Loss: 1.3496	LR: 0.050000
Training Epoch: 18 [8064/50000]	Loss: 0.9865	LR: 0.050000
Training Epoch: 18 [8192/50000]	Loss: 1.3192	LR: 0.050000
Training Epoch: 18 [8320/50000]	Loss: 1.2776	LR: 0.050000
Training Epoch: 18 [8448/50000]	Loss: 1.1999	LR: 0.050000
Training Epoch: 18 [8576/50000]	Loss: 1.2958	LR: 0.050000
Training Epoch: 18 [8704/50000]	Loss: 1.1336	LR: 0.050000
Training Epoch: 18 [8832/50000]	Loss: 0.9722	LR: 0.050000
Training Epoch: 18 [8960/50000]	Loss: 1.2140	LR: 0.050000
Training Epoch: 18 [9088/50000]	Loss: 1.2047	LR: 0.050000
Training Epoch: 18 [9216/50000]	Loss: 1.3122	LR: 0.050000
Training Epoch: 18 [9344/50000]	Loss: 1.3617	LR: 0.050000
Training Epoch: 18 [9472/50000]	Loss: 1.1649	LR: 0.050000
Training Epoch: 18 [9600/50000]	Loss: 1.2940	LR: 0.050000
Training Epoch: 18 [9728/50000]	Loss: 1.0195	LR: 0.050000
Training Epoch: 18 [9856/50000]	Loss: 1.3448	LR: 0.050000
Training Epoch: 18 [9984/50000]	Loss: 1.2406	LR: 0.050000
Training Epoch: 18 [10112/50000]	Loss: 1.5019	LR: 0.050000
Training Epoch: 18 [10240/50000]	Loss: 1.2861	LR: 0.050000
Training Epoch: 18 [10368/50000]	Loss: 1.1134	LR: 0.050000
Training Epoch: 18 [10496/50000]	Loss: 1.1594	LR: 0.050000
Training Epoch: 18 [10624/50000]	Loss: 1.2286	LR: 0.050000
Training Epoch: 18 [10752/50000]	Loss: 1.2323	LR: 0.050000
Training Epoch: 18 [10880/50000]	Loss: 1.0425	LR: 0.050000
Training Epoch: 18 [11008/50000]	Loss: 0.9889	LR: 0.050000
Training Epoch: 18 [11136/50000]	Loss: 1.3226	LR: 0.050000
Training Epoch: 18 [11264/50000]	Loss: 1.5391	LR: 0.050000
Training Epoch: 18 [11392/50000]	Loss: 1.2686	LR: 0.050000
Training Epoch: 18 [11520/50000]	Loss: 1.3873	LR: 0.050000
Training Epoch: 18 [11648/50000]	Loss: 1.0065	LR: 0.050000
Training Epoch: 18 [11776/50000]	Loss: 1.2676	LR: 0.050000
Training Epoch: 18 [11904/50000]	Loss: 1.1992	LR: 0.050000
Training Epoch: 18 [12032/50000]	Loss: 1.4031	LR: 0.050000
Training Epoch: 18 [12160/50000]	Loss: 1.2082	LR: 0.050000
Training Epoch: 18 [12288/50000]	Loss: 1.3812	LR: 0.050000
Training Epoch: 18 [12416/50000]	Loss: 1.3772	LR: 0.050000
Training Epoch: 18 [12544/50000]	Loss: 1.1436	LR: 0.050000
Training Epoch: 18 [12672/50000]	Loss: 0.9477	LR: 0.050000
Training Epoch: 18 [12800/50000]	Loss: 1.2998	LR: 0.050000
Training Epoch: 18 [12928/50000]	Loss: 1.2795	LR: 0.050000
Training Epoch: 18 [13056/50000]	Loss: 1.2944	LR: 0.050000
Training Epoch: 18 [13184/50000]	Loss: 1.2930	LR: 0.050000
Training Epoch: 18 [13312/50000]	Loss: 1.3786	LR: 0.050000
Training Epoch: 18 [13440/50000]	Loss: 1.2370	LR: 0.050000
Training Epoch: 18 [13568/50000]	Loss: 1.3056	LR: 0.050000
Training Epoch: 18 [13696/50000]	Loss: 1.2155	LR: 0.050000
Training Epoch: 18 [13824/50000]	Loss: 1.0590	LR: 0.050000
Training Epoch: 18 [13952/50000]	Loss: 1.2595	LR: 0.050000
Training Epoch: 18 [14080/50000]	Loss: 1.2491	LR: 0.050000
Training Epoch: 18 [14208/50000]	Loss: 0.9899	LR: 0.050000
Training Epoch: 18 [14336/50000]	Loss: 1.3950	LR: 0.050000
Training Epoch: 18 [14464/50000]	Loss: 1.1864	LR: 0.050000
Training Epoch: 18 [14592/50000]	Loss: 1.0399	LR: 0.050000
Training Epoch: 18 [14720/50000]	Loss: 1.0469	LR: 0.050000
Training Epoch: 18 [14848/50000]	Loss: 1.2157	LR: 0.050000
Training Epoch: 18 [14976/50000]	Loss: 1.2924	LR: 0.050000
Training Epoch: 18 [15104/50000]	Loss: 1.3637	LR: 0.050000
Training Epoch: 18 [15232/50000]	Loss: 1.3409	LR: 0.050000
Training Epoch: 18 [15360/50000]	Loss: 1.2234	LR: 0.050000
Training Epoch: 18 [15488/50000]	Loss: 1.2514	LR: 0.050000
Training Epoch: 18 [15616/50000]	Loss: 1.0912	LR: 0.050000
Training Epoch: 18 [15744/50000]	Loss: 1.0253	LR: 0.050000
Training Epoch: 18 [15872/50000]	Loss: 1.2641	LR: 0.050000
Training Epoch: 18 [16000/50000]	Loss: 1.1552	LR: 0.050000
Training Epoch: 18 [16128/50000]	Loss: 1.1194	LR: 0.050000
Training Epoch: 18 [16256/50000]	Loss: 1.2211	LR: 0.050000
Training Epoch: 18 [16384/50000]	Loss: 1.1304	LR: 0.050000
Training Epoch: 18 [16512/50000]	Loss: 1.3611	LR: 0.050000
Training Epoch: 18 [16640/50000]	Loss: 1.1309	LR: 0.050000
Training Epoch: 18 [16768/50000]	Loss: 1.3382	LR: 0.050000
Training Epoch: 18 [16896/50000]	Loss: 1.3080	LR: 0.050000
Training Epoch: 18 [17024/50000]	Loss: 1.1478	LR: 0.050000
Training Epoch: 18 [17152/50000]	Loss: 1.0838	LR: 0.050000
Training Epoch: 18 [17280/50000]	Loss: 1.4342	LR: 0.050000
Training Epoch: 18 [17408/50000]	Loss: 1.3196	LR: 0.050000
Training Epoch: 18 [17536/50000]	Loss: 1.1808	LR: 0.050000
Training Epoch: 18 [17664/50000]	Loss: 1.2312	LR: 0.050000
Training Epoch: 18 [17792/50000]	Loss: 1.2884	LR: 0.050000
Training Epoch: 18 [17920/50000]	Loss: 1.3141	LR: 0.050000
Training Epoch: 18 [18048/50000]	Loss: 1.2578	LR: 0.050000
Training Epoch: 18 [18176/50000]	Loss: 1.4599	LR: 0.050000
Training Epoch: 18 [18304/50000]	Loss: 1.1503	LR: 0.050000
Training Epoch: 18 [18432/50000]	Loss: 1.3497	LR: 0.050000
Training Epoch: 18 [18560/50000]	Loss: 1.0255	LR: 0.050000
Training Epoch: 18 [18688/50000]	Loss: 1.2843	LR: 0.050000
Training Epoch: 18 [18816/50000]	Loss: 0.9379	LR: 0.050000
Training Epoch: 18 [18944/50000]	Loss: 1.2988	LR: 0.050000
Training Epoch: 18 [19072/50000]	Loss: 1.3923	LR: 0.050000
Training Epoch: 18 [19200/50000]	Loss: 1.3351	LR: 0.050000
Training Epoch: 18 [19328/50000]	Loss: 1.2724	LR: 0.050000
Training Epoch: 18 [19456/50000]	Loss: 1.3457	LR: 0.050000
Training Epoch: 18 [19584/50000]	Loss: 1.1389	LR: 0.050000
Training Epoch: 18 [19712/50000]	Loss: 1.3958	LR: 0.050000
Training Epoch: 18 [19840/50000]	Loss: 1.2635	LR: 0.050000
Training Epoch: 18 [19968/50000]	Loss: 1.1122	LR: 0.050000
Training Epoch: 18 [20096/50000]	Loss: 1.0645	LR: 0.050000
Training Epoch: 18 [20224/50000]	Loss: 1.3828	LR: 0.050000
Training Epoch: 18 [20352/50000]	Loss: 1.2404	LR: 0.050000
Training Epoch: 18 [20480/50000]	Loss: 1.3484	LR: 0.050000
Training Epoch: 18 [20608/50000]	Loss: 1.4751	LR: 0.050000
Training Epoch: 18 [20736/50000]	Loss: 1.3516	LR: 0.050000
Training Epoch: 18 [20864/50000]	Loss: 1.3698	LR: 0.050000
Training Epoch: 18 [20992/50000]	Loss: 1.3493	LR: 0.050000
Training Epoch: 18 [21120/50000]	Loss: 1.0689	LR: 0.050000
Training Epoch: 18 [21248/50000]	Loss: 1.2291	LR: 0.050000
Training Epoch: 18 [21376/50000]	Loss: 1.2047	LR: 0.050000
Training Epoch: 18 [21504/50000]	Loss: 1.1148	LR: 0.050000
Training Epoch: 18 [21632/50000]	Loss: 1.2500	LR: 0.050000
Training Epoch: 18 [21760/50000]	Loss: 1.1331	LR: 0.050000
Training Epoch: 18 [21888/50000]	Loss: 1.1619	LR: 0.050000
Training Epoch: 18 [22016/50000]	Loss: 1.3081	LR: 0.050000
Training Epoch: 18 [22144/50000]	Loss: 1.0818	LR: 0.050000
Training Epoch: 18 [22272/50000]	Loss: 1.1731	LR: 0.050000
Training Epoch: 18 [22400/50000]	Loss: 1.4121	LR: 0.050000
Training Epoch: 18 [22528/50000]	Loss: 1.3618	LR: 0.050000
Training Epoch: 18 [22656/50000]	Loss: 1.3087	LR: 0.050000
Training Epoch: 18 [22784/50000]	Loss: 1.3090	LR: 0.050000
Training Epoch: 18 [22912/50000]	Loss: 1.1745	LR: 0.050000
Training Epoch: 18 [23040/50000]	Loss: 1.2297	LR: 0.050000
Training Epoch: 18 [23168/50000]	Loss: 1.4711	LR: 0.050000
Training Epoch: 18 [23296/50000]	Loss: 1.0884	LR: 0.050000
Training Epoch: 18 [23424/50000]	Loss: 1.4801	LR: 0.050000
Training Epoch: 18 [23552/50000]	Loss: 1.5413	LR: 0.050000
Training Epoch: 18 [23680/50000]	Loss: 1.2127	LR: 0.050000
Training Epoch: 18 [23808/50000]	Loss: 1.2126	LR: 0.050000
Training Epoch: 18 [23936/50000]	Loss: 1.1438	LR: 0.050000
Training Epoch: 18 [24064/50000]	Loss: 1.2992	LR: 0.050000
Training Epoch: 18 [24192/50000]	Loss: 1.2648	LR: 0.050000
Training Epoch: 18 [24320/50000]	Loss: 1.2961	LR: 0.050000
Training Epoch: 18 [24448/50000]	Loss: 1.4080	LR: 0.050000
Training Epoch: 18 [24576/50000]	Loss: 1.2375	LR: 0.050000
Training Epoch: 18 [24704/50000]	Loss: 1.2713	LR: 0.050000
Training Epoch: 18 [24832/50000]	Loss: 1.2262	LR: 0.050000
Training Epoch: 18 [24960/50000]	Loss: 1.0815	LR: 0.050000
Training Epoch: 18 [25088/50000]	Loss: 1.3458	LR: 0.050000
Training Epoch: 18 [25216/50000]	Loss: 1.1577	LR: 0.050000
Training Epoch: 18 [25344/50000]	Loss: 1.3152	LR: 0.050000
Training Epoch: 18 [25472/50000]	Loss: 1.3794	LR: 0.050000
Training Epoch: 18 [25600/50000]	Loss: 1.1859	LR: 0.050000
Training Epoch: 18 [25728/50000]	Loss: 1.3914	LR: 0.050000
Training Epoch: 18 [25856/50000]	Loss: 1.0314	LR: 0.050000
Training Epoch: 18 [25984/50000]	Loss: 1.4467	LR: 0.050000
Training Epoch: 18 [26112/50000]	Loss: 1.3539	LR: 0.050000
Training Epoch: 18 [26240/50000]	Loss: 1.3285	LR: 0.050000
Training Epoch: 18 [26368/50000]	Loss: 1.2121	LR: 0.050000
Training Epoch: 18 [26496/50000]	Loss: 1.4541	LR: 0.050000
Training Epoch: 18 [26624/50000]	Loss: 1.2344	LR: 0.050000
Training Epoch: 18 [26752/50000]	Loss: 1.4223	LR: 0.050000
Training Epoch: 18 [26880/50000]	Loss: 1.2961	LR: 0.050000
Training Epoch: 18 [27008/50000]	Loss: 1.4964	LR: 0.050000
Training Epoch: 18 [27136/50000]	Loss: 1.2853	LR: 0.050000
Training Epoch: 18 [27264/50000]	Loss: 1.3978	LR: 0.050000
Training Epoch: 18 [27392/50000]	Loss: 1.1755	LR: 0.050000
Training Epoch: 18 [27520/50000]	Loss: 1.3011	LR: 0.050000
Training Epoch: 18 [27648/50000]	Loss: 1.4262	LR: 0.050000
Training Epoch: 18 [27776/50000]	Loss: 1.5522	LR: 0.050000
Training Epoch: 18 [27904/50000]	Loss: 1.0600	LR: 0.050000
Training Epoch: 18 [28032/50000]	Loss: 1.1870	LR: 0.050000
Training Epoch: 18 [28160/50000]	Loss: 1.4568	LR: 0.050000
Training Epoch: 18 [28288/50000]	Loss: 1.1615	LR: 0.050000
Training Epoch: 18 [28416/50000]	Loss: 1.5219	LR: 0.050000
Training Epoch: 18 [28544/50000]	Loss: 1.2399	LR: 0.050000
Training Epoch: 18 [28672/50000]	Loss: 1.0199	LR: 0.050000
Training Epoch: 18 [28800/50000]	Loss: 1.2689	LR: 0.050000
Training Epoch: 18 [28928/50000]	Loss: 1.2556	LR: 0.050000
Training Epoch: 18 [29056/50000]	Loss: 1.4146	LR: 0.050000
Training Epoch: 18 [29184/50000]	Loss: 1.1878	LR: 0.050000
Training Epoch: 18 [29312/50000]	Loss: 1.2276	LR: 0.050000
Training Epoch: 18 [29440/50000]	Loss: 1.3459	LR: 0.050000
Training Epoch: 18 [29568/50000]	Loss: 1.2655	LR: 0.050000
Training Epoch: 18 [29696/50000]	Loss: 1.2282	LR: 0.050000
Training Epoch: 18 [29824/50000]	Loss: 1.4620	LR: 0.050000
Training Epoch: 18 [29952/50000]	Loss: 1.2997	LR: 0.050000
Training Epoch: 18 [30080/50000]	Loss: 1.2869	LR: 0.050000
Training Epoch: 18 [30208/50000]	Loss: 1.1563	LR: 0.050000
Training Epoch: 18 [30336/50000]	Loss: 1.3893	LR: 0.050000
Training Epoch: 18 [30464/50000]	Loss: 1.1506	LR: 0.050000
Training Epoch: 18 [30592/50000]	Loss: 1.1551	LR: 0.050000
Training Epoch: 18 [30720/50000]	Loss: 1.3656	LR: 0.050000
Training Epoch: 18 [30848/50000]	Loss: 1.1682	LR: 0.050000
Training Epoch: 18 [30976/50000]	Loss: 1.1255	LR: 0.050000
Training Epoch: 18 [31104/50000]	Loss: 1.2191	LR: 0.050000
Training Epoch: 18 [31232/50000]	Loss: 1.0526	LR: 0.050000
Training Epoch: 18 [31360/50000]	Loss: 1.3399	LR: 0.050000
Training Epoch: 18 [31488/50000]	Loss: 1.4532	LR: 0.050000
Training Epoch: 18 [31616/50000]	Loss: 1.2901	LR: 0.050000
Training Epoch: 18 [31744/50000]	Loss: 1.2610	LR: 0.050000
Training Epoch: 18 [31872/50000]	Loss: 1.1119	LR: 0.050000
Training Epoch: 18 [32000/50000]	Loss: 1.0990	LR: 0.050000
Training Epoch: 18 [32128/50000]	Loss: 1.3355	LR: 0.050000
Training Epoch: 18 [32256/50000]	Loss: 1.0395	LR: 0.050000
Training Epoch: 18 [32384/50000]	Loss: 1.4978	LR: 0.050000
Training Epoch: 18 [32512/50000]	Loss: 1.1415	LR: 0.050000
Training Epoch: 18 [32640/50000]	Loss: 1.3323	LR: 0.050000
Training Epoch: 18 [32768/50000]	Loss: 1.5949	LR: 0.050000
Training Epoch: 18 [32896/50000]	Loss: 1.4615	LR: 0.050000
Training Epoch: 18 [33024/50000]	Loss: 1.2110	LR: 0.050000
Training Epoch: 18 [33152/50000]	Loss: 1.0750	LR: 0.050000
Training Epoch: 18 [33280/50000]	Loss: 1.4711	LR: 0.050000
Training Epoch: 18 [33408/50000]	Loss: 1.2773	LR: 0.050000
Training Epoch: 18 [33536/50000]	Loss: 1.2803	LR: 0.050000
Training Epoch: 18 [33664/50000]	Loss: 1.1022	LR: 0.050000
Training Epoch: 18 [33792/50000]	Loss: 1.4846	LR: 0.050000
Training Epoch: 18 [33920/50000]	Loss: 1.1844	LR: 0.050000
Training Epoch: 18 [34048/50000]	Loss: 1.4679	LR: 0.050000
Training Epoch: 18 [34176/50000]	Loss: 1.2591	LR: 0.050000
Training Epoch: 18 [34304/50000]	Loss: 1.2603	LR: 0.050000
Training Epoch: 18 [34432/50000]	Loss: 1.2136	LR: 0.050000
Training Epoch: 18 [34560/50000]	Loss: 1.1286	LR: 0.050000
Training Epoch: 18 [34688/50000]	Loss: 1.0706	LR: 0.050000
Training Epoch: 18 [34816/50000]	Loss: 1.2365	LR: 0.050000
Training Epoch: 18 [34944/50000]	Loss: 1.2234	LR: 0.050000
Training Epoch: 18 [35072/50000]	Loss: 1.2945	LR: 0.050000
Training Epoch: 18 [35200/50000]	Loss: 1.2783	LR: 0.050000
Training Epoch: 18 [35328/50000]	Loss: 1.1613	LR: 0.050000
Training Epoch: 18 [35456/50000]	Loss: 1.3493	LR: 0.050000
Training Epoch: 18 [35584/50000]	Loss: 1.0277	LR: 0.050000
Training Epoch: 18 [35712/50000]	Loss: 1.3235	LR: 0.050000
Training Epoch: 18 [35840/50000]	Loss: 1.5242	LR: 0.050000
Training Epoch: 18 [35968/50000]	Loss: 1.5350	LR: 0.050000
Training Epoch: 18 [36096/50000]	Loss: 1.3460	LR: 0.050000
Training Epoch: 18 [36224/50000]	Loss: 1.4300	LR: 0.050000
Training Epoch: 18 [36352/50000]	Loss: 1.3213	LR: 0.050000
Training Epoch: 18 [36480/50000]	Loss: 1.3715	LR: 0.050000
Training Epoch: 18 [36608/50000]	Loss: 1.3241	LR: 0.050000
Training Epoch: 18 [36736/50000]	Loss: 1.3126	LR: 0.050000
Training Epoch: 18 [36864/50000]	Loss: 1.3854	LR: 0.050000
Training Epoch: 18 [36992/50000]	Loss: 1.1847	LR: 0.050000
Training Epoch: 18 [37120/50000]	Loss: 1.3543	LR: 0.050000
Training Epoch: 18 [37248/50000]	Loss: 1.2332	LR: 0.050000
Training Epoch: 18 [37376/50000]	Loss: 1.3512	LR: 0.050000
Training Epoch: 18 [37504/50000]	Loss: 1.1329	LR: 0.050000
Training Epoch: 18 [37632/50000]	Loss: 1.1157	LR: 0.050000
Training Epoch: 18 [37760/50000]	Loss: 1.3128	LR: 0.050000
Training Epoch: 18 [37888/50000]	Loss: 1.0711	LR: 0.050000
Training Epoch: 18 [38016/50000]	Loss: 1.4129	LR: 0.050000
Training Epoch: 18 [38144/50000]	Loss: 1.1290	LR: 0.050000
Training Epoch: 18 [38272/50000]	Loss: 1.3143	LR: 0.050000
Training Epoch: 18 [38400/50000]	Loss: 1.4738	LR: 0.050000
Training Epoch: 18 [38528/50000]	Loss: 1.4195	LR: 0.050000
Training Epoch: 18 [38656/50000]	Loss: 1.2082	LR: 0.050000
Training Epoch: 18 [38784/50000]	Loss: 1.1257	LR: 0.050000
Training Epoch: 18 [38912/50000]	Loss: 1.3612	LR: 0.050000
Training Epoch: 18 [39040/50000]	Loss: 1.3491	LR: 0.050000
Training Epoch: 18 [39168/50000]	Loss: 1.3673	LR: 0.050000
Training Epoch: 18 [39296/50000]	Loss: 1.1709	LR: 0.050000
Training Epoch: 18 [39424/50000]	Loss: 1.4680	LR: 0.050000
Training Epoch: 18 [39552/50000]	Loss: 1.2169	LR: 0.050000
Training Epoch: 18 [39680/50000]	Loss: 1.4765	LR: 0.050000
Training Epoch: 18 [39808/50000]	Loss: 1.1131	LR: 0.050000
Training Epoch: 18 [39936/50000]	Loss: 1.1368	LR: 0.050000
Training Epoch: 18 [40064/50000]	Loss: 1.4268	LR: 0.050000
Training Epoch: 18 [40192/50000]	Loss: 1.4693	LR: 0.050000
Training Epoch: 18 [40320/50000]	Loss: 1.0803	LR: 0.050000
Training Epoch: 18 [40448/50000]	Loss: 1.1894	LR: 0.050000
Training Epoch: 18 [40576/50000]	Loss: 1.2606	LR: 0.050000
Training Epoch: 18 [40704/50000]	Loss: 1.3661	LR: 0.050000
Training Epoch: 18 [40832/50000]	Loss: 1.3002	LR: 0.050000
Training Epoch: 18 [40960/50000]	Loss: 1.3353	LR: 0.050000
Training Epoch: 18 [41088/50000]	Loss: 1.0920	LR: 0.050000
Training Epoch: 18 [41216/50000]	Loss: 1.3435	LR: 0.050000
Training Epoch: 18 [41344/50000]	Loss: 1.1825	LR: 0.050000
Training Epoch: 18 [41472/50000]	Loss: 1.1956	LR: 0.050000
Training Epoch: 18 [41600/50000]	Loss: 1.1995	LR: 0.050000
Training Epoch: 18 [41728/50000]	Loss: 1.0634	LR: 0.050000
Training Epoch: 18 [41856/50000]	Loss: 1.1929	LR: 0.050000
Training Epoch: 18 [41984/50000]	Loss: 1.3329	LR: 0.050000
Training Epoch: 18 [42112/50000]	Loss: 1.5555	LR: 0.050000
Training Epoch: 18 [42240/50000]	Loss: 1.4092	LR: 0.050000
Training Epoch: 18 [42368/50000]	Loss: 1.4426	LR: 0.050000
Training Epoch: 18 [42496/50000]	Loss: 1.3324	LR: 0.050000
Training Epoch: 18 [42624/50000]	Loss: 0.9489	LR: 0.050000
Training Epoch: 18 [42752/50000]	Loss: 1.1880	LR: 0.050000
Training Epoch: 18 [42880/50000]	Loss: 1.2611	LR: 0.050000
Training Epoch: 18 [43008/50000]	Loss: 1.3412	LR: 0.050000
Training Epoch: 18 [43136/50000]	Loss: 1.4683	LR: 0.050000
Training Epoch: 18 [43264/50000]	Loss: 1.5159	LR: 0.050000
Training Epoch: 18 [43392/50000]	Loss: 1.2059	LR: 0.050000
Training Epoch: 18 [43520/50000]	Loss: 1.2523	LR: 0.050000
Training Epoch: 18 [43648/50000]	Loss: 1.4109	LR: 0.050000
Training Epoch: 18 [43776/50000]	Loss: 1.4706	LR: 0.050000
Training Epoch: 18 [43904/50000]	Loss: 1.3569	LR: 0.050000
Training Epoch: 18 [44032/50000]	Loss: 1.2257	LR: 0.050000
Training Epoch: 18 [44160/50000]	Loss: 1.4316	LR: 0.050000
Training Epoch: 18 [44288/50000]	Loss: 1.2141	LR: 0.050000
Training Epoch: 18 [44416/50000]	Loss: 1.2070	LR: 0.050000
Training Epoch: 18 [44544/50000]	Loss: 1.1390	LR: 0.050000
Training Epoch: 18 [44672/50000]	Loss: 1.1111	LR: 0.050000
Training Epoch: 18 [44800/50000]	Loss: 1.4864	LR: 0.050000
Training Epoch: 18 [44928/50000]	Loss: 1.4037	LR: 0.050000
Training Epoch: 18 [45056/50000]	Loss: 1.2414	LR: 0.050000
Training Epoch: 18 [45184/50000]	Loss: 1.3611	LR: 0.050000
Training Epoch: 18 [45312/50000]	Loss: 1.3683	LR: 0.050000
Training Epoch: 18 [45440/50000]	Loss: 1.1723	LR: 0.050000
Training Epoch: 18 [45568/50000]	Loss: 1.0570	LR: 0.050000
Training Epoch: 18 [45696/50000]	Loss: 1.3334	LR: 0.050000
Training Epoch: 18 [45824/50000]	Loss: 1.1599	LR: 0.050000
Training Epoch: 18 [45952/50000]	Loss: 1.1665	LR: 0.050000
Training Epoch: 18 [46080/50000]	Loss: 1.0984	LR: 0.050000
Training Epoch: 18 [46208/50000]	Loss: 1.0630	LR: 0.050000
Training Epoch: 18 [46336/50000]	Loss: 1.3722	LR: 0.050000
Training Epoch: 18 [46464/50000]	Loss: 1.2418	LR: 0.050000
Training Epoch: 18 [46592/50000]	Loss: 1.3679	LR: 0.050000
Training Epoch: 18 [46720/50000]	Loss: 1.3446	LR: 0.050000
Training Epoch: 18 [46848/50000]	Loss: 1.3125	LR: 0.050000
Training Epoch: 18 [46976/50000]	Loss: 1.5195	LR: 0.050000
Training Epoch: 18 [47104/50000]	Loss: 1.2668	LR: 0.050000
Training Epoch: 18 [47232/50000]	Loss: 1.3131	LR: 0.050000
Training Epoch: 18 [47360/50000]	Loss: 1.2444	LR: 0.050000
Training Epoch: 18 [47488/50000]	Loss: 1.4233	LR: 0.050000
Training Epoch: 18 [47616/50000]	Loss: 1.4103	LR: 0.050000
Training Epoch: 18 [47744/50000]	Loss: 1.3422	LR: 0.050000
Training Epoch: 18 [47872/50000]	Loss: 1.1367	LR: 0.050000
Training Epoch: 18 [48000/50000]	Loss: 1.3883	LR: 0.050000
Training Epoch: 18 [48128/50000]	Loss: 1.4163	LR: 0.050000
Training Epoch: 18 [48256/50000]	Loss: 1.0860	LR: 0.050000
Training Epoch: 18 [48384/50000]	Loss: 1.2900	LR: 0.050000
Training Epoch: 18 [48512/50000]	Loss: 1.3515	LR: 0.050000
Training Epoch: 18 [48640/50000]	Loss: 1.3248	LR: 0.050000
Training Epoch: 18 [48768/50000]	Loss: 1.2692	LR: 0.050000
Training Epoch: 18 [48896/50000]	Loss: 1.2057	LR: 0.050000
Training Epoch: 18 [49024/50000]	Loss: 1.3424	LR: 0.050000
Training Epoch: 18 [49152/50000]	Loss: 1.2588	LR: 0.050000
Training Epoch: 18 [49280/50000]	Loss: 1.2164	LR: 0.050000
Training Epoch: 18 [49408/50000]	Loss: 1.2727	LR: 0.050000
Training Epoch: 18 [49536/50000]	Loss: 1.2733	LR: 0.050000
Training Epoch: 18 [49664/50000]	Loss: 1.4035	LR: 0.050000
Training Epoch: 18 [49792/50000]	Loss: 1.1674	LR: 0.050000
Training Epoch: 18 [49920/50000]	Loss: 1.3022	LR: 0.050000
Training Epoch: 18 [50000/50000]	Loss: 1.2767	LR: 0.050000
epoch 18 training time consumed: 54.00s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   66061 GB |   66061 GB |
|       from large pool |  123392 KB |    1034 MB |   65996 GB |   65996 GB |
|       from small pool |   10798 KB |      13 MB |      65 GB |      65 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   66061 GB |   66061 GB |
|       from large pool |  123392 KB |    1034 MB |   65996 GB |   65996 GB |
|       from small pool |   10798 KB |      13 MB |      65 GB |      65 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   29073 GB |   29073 GB |
|       from large pool |  155136 KB |  433088 KB |   29001 GB |   29001 GB |
|       from small pool |    1490 KB |    3494 KB |      71 GB |      71 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    2549 K  |    2549 K  |
|       from large pool |      24    |      65    |    1330 K  |    1330 K  |
|       from small pool |     231    |     274    |    1218 K  |    1218 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    2549 K  |    2549 K  |
|       from large pool |      24    |      65    |    1330 K  |    1330 K  |
|       from small pool |     231    |     274    |    1218 K  |    1218 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1262 K  |    1262 K  |
|       from large pool |       9    |      14    |     644 K  |     644 K  |
|       from small pool |      12    |      16    |     618 K  |     618 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 18, Average loss: 0.0129, Accuracy: 0.5607, Time consumed:3.46s

Training Epoch: 19 [128/50000]	Loss: 1.2933	LR: 0.050000
Training Epoch: 19 [256/50000]	Loss: 1.4897	LR: 0.050000
Training Epoch: 19 [384/50000]	Loss: 1.2224	LR: 0.050000
Training Epoch: 19 [512/50000]	Loss: 1.1877	LR: 0.050000
Training Epoch: 19 [640/50000]	Loss: 1.0483	LR: 0.050000
Training Epoch: 19 [768/50000]	Loss: 1.3482	LR: 0.050000
Training Epoch: 19 [896/50000]	Loss: 1.2535	LR: 0.050000
Training Epoch: 19 [1024/50000]	Loss: 0.9999	LR: 0.050000
Training Epoch: 19 [1152/50000]	Loss: 1.1855	LR: 0.050000
Training Epoch: 19 [1280/50000]	Loss: 1.2459	LR: 0.050000
Training Epoch: 19 [1408/50000]	Loss: 1.2438	LR: 0.050000
Training Epoch: 19 [1536/50000]	Loss: 1.1131	LR: 0.050000
Training Epoch: 19 [1664/50000]	Loss: 1.1640	LR: 0.050000
Training Epoch: 19 [1792/50000]	Loss: 1.1646	LR: 0.050000
Training Epoch: 19 [1920/50000]	Loss: 1.0886	LR: 0.050000
Training Epoch: 19 [2048/50000]	Loss: 1.0169	LR: 0.050000
Training Epoch: 19 [2176/50000]	Loss: 1.0344	LR: 0.050000
Training Epoch: 19 [2304/50000]	Loss: 1.2236	LR: 0.050000
Training Epoch: 19 [2432/50000]	Loss: 0.9920	LR: 0.050000
Training Epoch: 19 [2560/50000]	Loss: 1.2962	LR: 0.050000
Training Epoch: 19 [2688/50000]	Loss: 1.2256	LR: 0.050000
Training Epoch: 19 [2816/50000]	Loss: 1.2533	LR: 0.050000
Training Epoch: 19 [2944/50000]	Loss: 1.0846	LR: 0.050000
Training Epoch: 19 [3072/50000]	Loss: 1.3391	LR: 0.050000
Training Epoch: 19 [3200/50000]	Loss: 1.4156	LR: 0.050000
Training Epoch: 19 [3328/50000]	Loss: 1.3710	LR: 0.050000
Training Epoch: 19 [3456/50000]	Loss: 1.3368	LR: 0.050000
Training Epoch: 19 [3584/50000]	Loss: 1.1023	LR: 0.050000
Training Epoch: 19 [3712/50000]	Loss: 1.0333	LR: 0.050000
Training Epoch: 19 [3840/50000]	Loss: 1.0216	LR: 0.050000
Training Epoch: 19 [3968/50000]	Loss: 0.9479	LR: 0.050000
Training Epoch: 19 [4096/50000]	Loss: 1.1527	LR: 0.050000
Training Epoch: 19 [4224/50000]	Loss: 1.1978	LR: 0.050000
Training Epoch: 19 [4352/50000]	Loss: 1.2776	LR: 0.050000
Training Epoch: 19 [4480/50000]	Loss: 1.3110	LR: 0.050000
Training Epoch: 19 [4608/50000]	Loss: 1.2125	LR: 0.050000
Training Epoch: 19 [4736/50000]	Loss: 1.4141	LR: 0.050000
Training Epoch: 19 [4864/50000]	Loss: 1.2899	LR: 0.050000
Training Epoch: 19 [4992/50000]	Loss: 1.0475	LR: 0.050000
Training Epoch: 19 [5120/50000]	Loss: 1.2897	LR: 0.050000
Training Epoch: 19 [5248/50000]	Loss: 1.2433	LR: 0.050000
Training Epoch: 19 [5376/50000]	Loss: 1.1621	LR: 0.050000
Training Epoch: 19 [5504/50000]	Loss: 1.1268	LR: 0.050000
Training Epoch: 19 [5632/50000]	Loss: 1.1116	LR: 0.050000
Training Epoch: 19 [5760/50000]	Loss: 1.1072	LR: 0.050000
Training Epoch: 19 [5888/50000]	Loss: 1.4687	LR: 0.050000
Training Epoch: 19 [6016/50000]	Loss: 1.1879	LR: 0.050000
Training Epoch: 19 [6144/50000]	Loss: 1.2995	LR: 0.050000
Training Epoch: 19 [6272/50000]	Loss: 1.0750	LR: 0.050000
Training Epoch: 19 [6400/50000]	Loss: 1.2500	LR: 0.050000
Training Epoch: 19 [6528/50000]	Loss: 1.2596	LR: 0.050000
Training Epoch: 19 [6656/50000]	Loss: 1.2251	LR: 0.050000
Training Epoch: 19 [6784/50000]	Loss: 1.0375	LR: 0.050000
Training Epoch: 19 [6912/50000]	Loss: 1.0985	LR: 0.050000
Training Epoch: 19 [7040/50000]	Loss: 1.1931	LR: 0.050000
Training Epoch: 19 [7168/50000]	Loss: 1.0676	LR: 0.050000
Training Epoch: 19 [7296/50000]	Loss: 1.1904	LR: 0.050000
Training Epoch: 19 [7424/50000]	Loss: 1.1449	LR: 0.050000
Training Epoch: 19 [7552/50000]	Loss: 1.3033	LR: 0.050000
Training Epoch: 19 [7680/50000]	Loss: 0.9846	LR: 0.050000
Training Epoch: 19 [7808/50000]	Loss: 0.9762	LR: 0.050000
Training Epoch: 19 [7936/50000]	Loss: 1.1203	LR: 0.050000
Training Epoch: 19 [8064/50000]	Loss: 1.3953	LR: 0.050000
Training Epoch: 19 [8192/50000]	Loss: 1.1099	LR: 0.050000
Training Epoch: 19 [8320/50000]	Loss: 1.1610	LR: 0.050000
Training Epoch: 19 [8448/50000]	Loss: 1.1131	LR: 0.050000
Training Epoch: 19 [8576/50000]	Loss: 1.1727	LR: 0.050000
Training Epoch: 19 [8704/50000]	Loss: 1.0735	LR: 0.050000
Training Epoch: 19 [8832/50000]	Loss: 1.2608	LR: 0.050000
Training Epoch: 19 [8960/50000]	Loss: 1.3380	LR: 0.050000
Training Epoch: 19 [9088/50000]	Loss: 1.2574	LR: 0.050000
Training Epoch: 19 [9216/50000]	Loss: 1.2632	LR: 0.050000
Training Epoch: 19 [9344/50000]	Loss: 1.2792	LR: 0.050000
Training Epoch: 19 [9472/50000]	Loss: 1.1652	LR: 0.050000
Training Epoch: 19 [9600/50000]	Loss: 1.1524	LR: 0.050000
Training Epoch: 19 [9728/50000]	Loss: 1.2307	LR: 0.050000
Training Epoch: 19 [9856/50000]	Loss: 1.0462	LR: 0.050000
Training Epoch: 19 [9984/50000]	Loss: 1.4427	LR: 0.050000
Training Epoch: 19 [10112/50000]	Loss: 1.2328	LR: 0.050000
Training Epoch: 19 [10240/50000]	Loss: 1.1332	LR: 0.050000
Training Epoch: 19 [10368/50000]	Loss: 1.2828	LR: 0.050000
Training Epoch: 19 [10496/50000]	Loss: 1.2909	LR: 0.050000
Training Epoch: 19 [10624/50000]	Loss: 1.0128	LR: 0.050000
Training Epoch: 19 [10752/50000]	Loss: 0.9152	LR: 0.050000
Training Epoch: 19 [10880/50000]	Loss: 1.1055	LR: 0.050000
Training Epoch: 19 [11008/50000]	Loss: 1.0911	LR: 0.050000
Training Epoch: 19 [11136/50000]	Loss: 1.5290	LR: 0.050000
Training Epoch: 19 [11264/50000]	Loss: 1.2090	LR: 0.050000
Training Epoch: 19 [11392/50000]	Loss: 1.0487	LR: 0.050000
Training Epoch: 19 [11520/50000]	Loss: 1.0857	LR: 0.050000
Training Epoch: 19 [11648/50000]	Loss: 1.1174	LR: 0.050000
Training Epoch: 19 [11776/50000]	Loss: 0.9483	LR: 0.050000
Training Epoch: 19 [11904/50000]	Loss: 1.0997	LR: 0.050000
Training Epoch: 19 [12032/50000]	Loss: 1.2780	LR: 0.050000
Training Epoch: 19 [12160/50000]	Loss: 1.4270	LR: 0.050000
Training Epoch: 19 [12288/50000]	Loss: 1.2767	LR: 0.050000
Training Epoch: 19 [12416/50000]	Loss: 1.2710	LR: 0.050000
Training Epoch: 19 [12544/50000]	Loss: 1.2106	LR: 0.050000
Training Epoch: 19 [12672/50000]	Loss: 1.4153	LR: 0.050000
Training Epoch: 19 [12800/50000]	Loss: 1.1573	LR: 0.050000
Training Epoch: 19 [12928/50000]	Loss: 1.0063	LR: 0.050000
Training Epoch: 19 [13056/50000]	Loss: 1.1432	LR: 0.050000
Training Epoch: 19 [13184/50000]	Loss: 1.3041	LR: 0.050000
Training Epoch: 19 [13312/50000]	Loss: 1.2118	LR: 0.050000
Training Epoch: 19 [13440/50000]	Loss: 1.0750	LR: 0.050000
Training Epoch: 19 [13568/50000]	Loss: 1.6185	LR: 0.050000
Training Epoch: 19 [13696/50000]	Loss: 1.0412	LR: 0.050000
Training Epoch: 19 [13824/50000]	Loss: 1.3133	LR: 0.050000
Training Epoch: 19 [13952/50000]	Loss: 1.1818	LR: 0.050000
Training Epoch: 19 [14080/50000]	Loss: 1.3549	LR: 0.050000
Training Epoch: 19 [14208/50000]	Loss: 1.1936	LR: 0.050000
Training Epoch: 19 [14336/50000]	Loss: 1.2429	LR: 0.050000
Training Epoch: 19 [14464/50000]	Loss: 1.1515	LR: 0.050000
Training Epoch: 19 [14592/50000]	Loss: 1.2025	LR: 0.050000
Training Epoch: 19 [14720/50000]	Loss: 1.2046	LR: 0.050000
Training Epoch: 19 [14848/50000]	Loss: 1.2155	LR: 0.050000
Training Epoch: 19 [14976/50000]	Loss: 1.3398	LR: 0.050000
Training Epoch: 19 [15104/50000]	Loss: 1.3762	LR: 0.050000
Training Epoch: 19 [15232/50000]	Loss: 1.3546	LR: 0.050000
Training Epoch: 19 [15360/50000]	Loss: 1.0505	LR: 0.050000
Training Epoch: 19 [15488/50000]	Loss: 1.3313	LR: 0.050000
Training Epoch: 19 [15616/50000]	Loss: 1.2727	LR: 0.050000
Training Epoch: 19 [15744/50000]	Loss: 1.2515	LR: 0.050000
Training Epoch: 19 [15872/50000]	Loss: 1.3281	LR: 0.050000
Training Epoch: 19 [16000/50000]	Loss: 1.3854	LR: 0.050000
Training Epoch: 19 [16128/50000]	Loss: 1.3293	LR: 0.050000
Training Epoch: 19 [16256/50000]	Loss: 1.0917	LR: 0.050000
Training Epoch: 19 [16384/50000]	Loss: 1.2758	LR: 0.050000
Training Epoch: 19 [16512/50000]	Loss: 1.0940	LR: 0.050000
Training Epoch: 19 [16640/50000]	Loss: 1.0676	LR: 0.050000
Training Epoch: 19 [16768/50000]	Loss: 1.2971	LR: 0.050000
Training Epoch: 19 [16896/50000]	Loss: 1.2232	LR: 0.050000
Training Epoch: 19 [17024/50000]	Loss: 1.1301	LR: 0.050000
Training Epoch: 19 [17152/50000]	Loss: 1.2225	LR: 0.050000
Training Epoch: 19 [17280/50000]	Loss: 1.3642	LR: 0.050000
Training Epoch: 19 [17408/50000]	Loss: 1.2421	LR: 0.050000
Training Epoch: 19 [17536/50000]	Loss: 1.1889	LR: 0.050000
Training Epoch: 19 [17664/50000]	Loss: 1.1858	LR: 0.050000
Training Epoch: 19 [17792/50000]	Loss: 1.0957	LR: 0.050000
Training Epoch: 19 [17920/50000]	Loss: 1.2321	LR: 0.050000
Training Epoch: 19 [18048/50000]	Loss: 1.0518	LR: 0.050000
Training Epoch: 19 [18176/50000]	Loss: 1.1676	LR: 0.050000
Training Epoch: 19 [18304/50000]	Loss: 1.3680	LR: 0.050000
Training Epoch: 19 [18432/50000]	Loss: 1.3612	LR: 0.050000
Training Epoch: 19 [18560/50000]	Loss: 1.5399	LR: 0.050000
Training Epoch: 19 [18688/50000]	Loss: 1.3013	LR: 0.050000
Training Epoch: 19 [18816/50000]	Loss: 1.2291	LR: 0.050000
Training Epoch: 19 [18944/50000]	Loss: 1.1911	LR: 0.050000
Training Epoch: 19 [19072/50000]	Loss: 1.1278	LR: 0.050000
Training Epoch: 19 [19200/50000]	Loss: 1.2280	LR: 0.050000
Training Epoch: 19 [19328/50000]	Loss: 1.3509	LR: 0.050000
Training Epoch: 19 [19456/50000]	Loss: 1.2367	LR: 0.050000
Training Epoch: 19 [19584/50000]	Loss: 1.2751	LR: 0.050000
Training Epoch: 19 [19712/50000]	Loss: 1.2084	LR: 0.050000
Training Epoch: 19 [19840/50000]	Loss: 1.1686	LR: 0.050000
Training Epoch: 19 [19968/50000]	Loss: 1.2163	LR: 0.050000
Training Epoch: 19 [20096/50000]	Loss: 1.2161	LR: 0.050000
Training Epoch: 19 [20224/50000]	Loss: 1.2370	LR: 0.050000
Training Epoch: 19 [20352/50000]	Loss: 1.1205	LR: 0.050000
Training Epoch: 19 [20480/50000]	Loss: 1.1963	LR: 0.050000
Training Epoch: 19 [20608/50000]	Loss: 1.2032	LR: 0.050000
Training Epoch: 19 [20736/50000]	Loss: 1.6201	LR: 0.050000
Training Epoch: 19 [20864/50000]	Loss: 0.9632	LR: 0.050000
Training Epoch: 19 [20992/50000]	Loss: 1.2388	LR: 0.050000
Training Epoch: 19 [21120/50000]	Loss: 1.2745	LR: 0.050000
Training Epoch: 19 [21248/50000]	Loss: 1.1908	LR: 0.050000
Training Epoch: 19 [21376/50000]	Loss: 1.5088	LR: 0.050000
Training Epoch: 19 [21504/50000]	Loss: 1.1400	LR: 0.050000
Training Epoch: 19 [21632/50000]	Loss: 1.3640	LR: 0.050000
Training Epoch: 19 [21760/50000]	Loss: 1.2359	LR: 0.050000
Training Epoch: 19 [21888/50000]	Loss: 1.2121	LR: 0.050000
Training Epoch: 19 [22016/50000]	Loss: 1.4173	LR: 0.050000
Training Epoch: 19 [22144/50000]	Loss: 1.3615	LR: 0.050000
Training Epoch: 19 [22272/50000]	Loss: 1.2014	LR: 0.050000
Training Epoch: 19 [22400/50000]	Loss: 1.2250	LR: 0.050000
Training Epoch: 19 [22528/50000]	Loss: 1.2672	LR: 0.050000
Training Epoch: 19 [22656/50000]	Loss: 1.0843	LR: 0.050000
Training Epoch: 19 [22784/50000]	Loss: 1.2960	LR: 0.050000
Training Epoch: 19 [22912/50000]	Loss: 1.4587	LR: 0.050000
Training Epoch: 19 [23040/50000]	Loss: 1.2771	LR: 0.050000
Training Epoch: 19 [23168/50000]	Loss: 1.0856	LR: 0.050000
Training Epoch: 19 [23296/50000]	Loss: 1.5477	LR: 0.050000
Training Epoch: 19 [23424/50000]	Loss: 1.2549	LR: 0.050000
Training Epoch: 19 [23552/50000]	Loss: 1.3356	LR: 0.050000
Training Epoch: 19 [23680/50000]	Loss: 1.3235	LR: 0.050000
Training Epoch: 19 [23808/50000]	Loss: 1.0392	LR: 0.050000
Training Epoch: 19 [23936/50000]	Loss: 1.2709	LR: 0.050000
Training Epoch: 19 [24064/50000]	Loss: 0.9586	LR: 0.050000
Training Epoch: 19 [24192/50000]	Loss: 1.1915	LR: 0.050000
Training Epoch: 19 [24320/50000]	Loss: 1.3683	LR: 0.050000
Training Epoch: 19 [24448/50000]	Loss: 1.1481	LR: 0.050000
Training Epoch: 19 [24576/50000]	Loss: 1.2447	LR: 0.050000
Training Epoch: 19 [24704/50000]	Loss: 1.1657	LR: 0.050000
Training Epoch: 19 [24832/50000]	Loss: 1.1468	LR: 0.050000
Training Epoch: 19 [24960/50000]	Loss: 1.1701	LR: 0.050000
Training Epoch: 19 [25088/50000]	Loss: 1.1974	LR: 0.050000
Training Epoch: 19 [25216/50000]	Loss: 1.2069	LR: 0.050000
Training Epoch: 19 [25344/50000]	Loss: 1.4053	LR: 0.050000
Training Epoch: 19 [25472/50000]	Loss: 1.3413	LR: 0.050000
Training Epoch: 19 [25600/50000]	Loss: 1.2981	LR: 0.050000
Training Epoch: 19 [25728/50000]	Loss: 1.2783	LR: 0.050000
Training Epoch: 19 [25856/50000]	Loss: 1.3927	LR: 0.050000
Training Epoch: 19 [25984/50000]	Loss: 1.0210	LR: 0.050000
Training Epoch: 19 [26112/50000]	Loss: 1.0993	LR: 0.050000
Training Epoch: 19 [26240/50000]	Loss: 1.2432	LR: 0.050000
Training Epoch: 19 [26368/50000]	Loss: 1.2589	LR: 0.050000
Training Epoch: 19 [26496/50000]	Loss: 1.1806	LR: 0.050000
Training Epoch: 19 [26624/50000]	Loss: 1.2936	LR: 0.050000
Training Epoch: 19 [26752/50000]	Loss: 0.7569	LR: 0.050000
Training Epoch: 19 [26880/50000]	Loss: 1.3346	LR: 0.050000
Training Epoch: 19 [27008/50000]	Loss: 1.1203	LR: 0.050000
Training Epoch: 19 [27136/50000]	Loss: 1.3632	LR: 0.050000
Training Epoch: 19 [27264/50000]	Loss: 1.0984	LR: 0.050000
Training Epoch: 19 [27392/50000]	Loss: 1.1444	LR: 0.050000
Training Epoch: 19 [27520/50000]	Loss: 1.4084	LR: 0.050000
Training Epoch: 19 [27648/50000]	Loss: 1.3095	LR: 0.050000
Training Epoch: 19 [27776/50000]	Loss: 1.2624	LR: 0.050000
Training Epoch: 19 [27904/50000]	Loss: 1.3190	LR: 0.050000
Training Epoch: 19 [28032/50000]	Loss: 0.9679	LR: 0.050000
Training Epoch: 19 [28160/50000]	Loss: 1.2541	LR: 0.050000
Training Epoch: 19 [28288/50000]	Loss: 1.3202	LR: 0.050000
Training Epoch: 19 [28416/50000]	Loss: 1.3607	LR: 0.050000
Training Epoch: 19 [28544/50000]	Loss: 1.2390	LR: 0.050000
Training Epoch: 19 [28672/50000]	Loss: 1.0424	LR: 0.050000
Training Epoch: 19 [28800/50000]	Loss: 1.2054	LR: 0.050000
Training Epoch: 19 [28928/50000]	Loss: 1.0951	LR: 0.050000
Training Epoch: 19 [29056/50000]	Loss: 1.2266	LR: 0.050000
Training Epoch: 19 [29184/50000]	Loss: 1.0388	LR: 0.050000
Training Epoch: 19 [29312/50000]	Loss: 1.1919	LR: 0.050000
Training Epoch: 19 [29440/50000]	Loss: 1.0971	LR: 0.050000
Training Epoch: 19 [29568/50000]	Loss: 1.3463	LR: 0.050000
Training Epoch: 19 [29696/50000]	Loss: 1.4068	LR: 0.050000
Training Epoch: 19 [29824/50000]	Loss: 1.3140	LR: 0.050000
Training Epoch: 19 [29952/50000]	Loss: 1.4089	LR: 0.050000
Training Epoch: 19 [30080/50000]	Loss: 1.1497	LR: 0.050000
Training Epoch: 19 [30208/50000]	Loss: 1.1546	LR: 0.050000
Training Epoch: 19 [30336/50000]	Loss: 1.1158	LR: 0.050000
Training Epoch: 19 [30464/50000]	Loss: 1.3122	LR: 0.050000
Training Epoch: 19 [30592/50000]	Loss: 1.2796	LR: 0.050000
Training Epoch: 19 [30720/50000]	Loss: 1.3686	LR: 0.050000
Training Epoch: 19 [30848/50000]	Loss: 1.1038	LR: 0.050000
Training Epoch: 19 [30976/50000]	Loss: 1.2198	LR: 0.050000
Training Epoch: 19 [31104/50000]	Loss: 1.2576	LR: 0.050000
Training Epoch: 19 [31232/50000]	Loss: 1.1889	LR: 0.050000
Training Epoch: 19 [31360/50000]	Loss: 1.3612	LR: 0.050000
Training Epoch: 19 [31488/50000]	Loss: 1.2623	LR: 0.050000
Training Epoch: 19 [31616/50000]	Loss: 1.3082	LR: 0.050000
Training Epoch: 19 [31744/50000]	Loss: 1.2137	LR: 0.050000
Training Epoch: 19 [31872/50000]	Loss: 1.3835	LR: 0.050000
Training Epoch: 19 [32000/50000]	Loss: 1.2260	LR: 0.050000
Training Epoch: 19 [32128/50000]	Loss: 1.0051	LR: 0.050000
Training Epoch: 19 [32256/50000]	Loss: 1.1829	LR: 0.050000
Training Epoch: 19 [32384/50000]	Loss: 1.2250	LR: 0.050000
Training Epoch: 19 [32512/50000]	Loss: 1.1677	LR: 0.050000
Training Epoch: 19 [32640/50000]	Loss: 1.2630	LR: 0.050000
Training Epoch: 19 [32768/50000]	Loss: 1.1582	LR: 0.050000
Training Epoch: 19 [32896/50000]	Loss: 1.3260	LR: 0.050000
Training Epoch: 19 [33024/50000]	Loss: 1.1438	LR: 0.050000
Training Epoch: 19 [33152/50000]	Loss: 1.3225	LR: 0.050000
Training Epoch: 19 [33280/50000]	Loss: 1.1795	LR: 0.050000
Training Epoch: 19 [33408/50000]	Loss: 1.4333	LR: 0.050000
Training Epoch: 19 [33536/50000]	Loss: 1.0633	LR: 0.050000
Training Epoch: 19 [33664/50000]	Loss: 1.3716	LR: 0.050000
Training Epoch: 19 [33792/50000]	Loss: 1.1430	LR: 0.050000
Training Epoch: 19 [33920/50000]	Loss: 1.0735	LR: 0.050000
Training Epoch: 19 [34048/50000]	Loss: 1.3319	LR: 0.050000
Training Epoch: 19 [34176/50000]	Loss: 1.2561	LR: 0.050000
Training Epoch: 19 [34304/50000]	Loss: 1.1390	LR: 0.050000
Training Epoch: 19 [34432/50000]	Loss: 1.0595	LR: 0.050000
Training Epoch: 19 [34560/50000]	Loss: 1.0398	LR: 0.050000
Training Epoch: 19 [34688/50000]	Loss: 1.2112	LR: 0.050000
Training Epoch: 19 [34816/50000]	Loss: 1.4308	LR: 0.050000
Training Epoch: 19 [34944/50000]	Loss: 1.3859	LR: 0.050000
Training Epoch: 19 [35072/50000]	Loss: 1.4898	LR: 0.050000
Training Epoch: 19 [35200/50000]	Loss: 1.0584	LR: 0.050000
Training Epoch: 19 [35328/50000]	Loss: 1.1044	LR: 0.050000
Training Epoch: 19 [35456/50000]	Loss: 1.1901	LR: 0.050000
Training Epoch: 19 [35584/50000]	Loss: 1.2748	LR: 0.050000
Training Epoch: 19 [35712/50000]	Loss: 1.4559	LR: 0.050000
Training Epoch: 19 [35840/50000]	Loss: 1.0939	LR: 0.050000
Training Epoch: 19 [35968/50000]	Loss: 1.3303	LR: 0.050000
Training Epoch: 19 [36096/50000]	Loss: 1.4470	LR: 0.050000
Training Epoch: 19 [36224/50000]	Loss: 1.2990	LR: 0.050000
Training Epoch: 19 [36352/50000]	Loss: 1.2738	LR: 0.050000
Training Epoch: 19 [36480/50000]	Loss: 1.2729	LR: 0.050000
Training Epoch: 19 [36608/50000]	Loss: 1.3095	LR: 0.050000
Training Epoch: 19 [36736/50000]	Loss: 1.1691	LR: 0.050000
Training Epoch: 19 [36864/50000]	Loss: 1.2821	LR: 0.050000
Training Epoch: 19 [36992/50000]	Loss: 1.2353	LR: 0.050000
Training Epoch: 19 [37120/50000]	Loss: 1.3091	LR: 0.050000
Training Epoch: 19 [37248/50000]	Loss: 1.1786	LR: 0.050000
Training Epoch: 19 [37376/50000]	Loss: 1.2044	LR: 0.050000
Training Epoch: 19 [37504/50000]	Loss: 1.5792	LR: 0.050000
Training Epoch: 19 [37632/50000]	Loss: 1.2609	LR: 0.050000
Training Epoch: 19 [37760/50000]	Loss: 1.2779	LR: 0.050000
Training Epoch: 19 [37888/50000]	Loss: 1.0991	LR: 0.050000
Training Epoch: 19 [38016/50000]	Loss: 1.2243	LR: 0.050000
Training Epoch: 19 [38144/50000]	Loss: 1.3341	LR: 0.050000
Training Epoch: 19 [38272/50000]	Loss: 1.2308	LR: 0.050000
Training Epoch: 19 [38400/50000]	Loss: 1.2051	LR: 0.050000
Training Epoch: 19 [38528/50000]	Loss: 1.4768	LR: 0.050000
Training Epoch: 19 [38656/50000]	Loss: 1.2219	LR: 0.050000
Training Epoch: 19 [38784/50000]	Loss: 1.2310	LR: 0.050000
Training Epoch: 19 [38912/50000]	Loss: 1.2631	LR: 0.050000
Training Epoch: 19 [39040/50000]	Loss: 1.1116	LR: 0.050000
Training Epoch: 19 [39168/50000]	Loss: 1.1819	LR: 0.050000
Training Epoch: 19 [39296/50000]	Loss: 1.2933	LR: 0.050000
Training Epoch: 19 [39424/50000]	Loss: 1.1703	LR: 0.050000
Training Epoch: 19 [39552/50000]	Loss: 1.3005	LR: 0.050000
Training Epoch: 19 [39680/50000]	Loss: 1.0140	LR: 0.050000
Training Epoch: 19 [39808/50000]	Loss: 1.2213	LR: 0.050000
Training Epoch: 19 [39936/50000]	Loss: 1.3344	LR: 0.050000
Training Epoch: 19 [40064/50000]	Loss: 1.2650	LR: 0.050000
Training Epoch: 19 [40192/50000]	Loss: 1.3509	LR: 0.050000
Training Epoch: 19 [40320/50000]	Loss: 1.4680	LR: 0.050000
Training Epoch: 19 [40448/50000]	Loss: 1.0767	LR: 0.050000
Training Epoch: 19 [40576/50000]	Loss: 1.4513	LR: 0.050000
Training Epoch: 19 [40704/50000]	Loss: 1.2678	LR: 0.050000
Training Epoch: 19 [40832/50000]	Loss: 1.1583	LR: 0.050000
Training Epoch: 19 [40960/50000]	Loss: 1.1489	LR: 0.050000
Training Epoch: 19 [41088/50000]	Loss: 1.1064	LR: 0.050000
Training Epoch: 19 [41216/50000]	Loss: 1.1903	LR: 0.050000
Training Epoch: 19 [41344/50000]	Loss: 1.4271	LR: 0.050000
Training Epoch: 19 [41472/50000]	Loss: 1.3249	LR: 0.050000
Training Epoch: 19 [41600/50000]	Loss: 1.3698	LR: 0.050000
Training Epoch: 19 [41728/50000]	Loss: 1.3693	LR: 0.050000
Training Epoch: 19 [41856/50000]	Loss: 1.2191	LR: 0.050000
Training Epoch: 19 [41984/50000]	Loss: 1.3959	LR: 0.050000
Training Epoch: 19 [42112/50000]	Loss: 1.1288	LR: 0.050000
Training Epoch: 19 [42240/50000]	Loss: 1.1123	LR: 0.050000
Training Epoch: 19 [42368/50000]	Loss: 1.2883	LR: 0.050000
Training Epoch: 19 [42496/50000]	Loss: 1.1249	LR: 0.050000
Training Epoch: 19 [42624/50000]	Loss: 1.2756	LR: 0.050000
Training Epoch: 19 [42752/50000]	Loss: 1.0930	LR: 0.050000
Training Epoch: 19 [42880/50000]	Loss: 1.2477	LR: 0.050000
Training Epoch: 19 [43008/50000]	Loss: 1.3386	LR: 0.050000
Training Epoch: 19 [43136/50000]	Loss: 1.1791	LR: 0.050000
Training Epoch: 19 [43264/50000]	Loss: 1.2403	LR: 0.050000
Training Epoch: 19 [43392/50000]	Loss: 1.1507	LR: 0.050000
Training Epoch: 19 [43520/50000]	Loss: 1.4166	LR: 0.050000
Training Epoch: 19 [43648/50000]	Loss: 1.4657	LR: 0.050000
Training Epoch: 19 [43776/50000]	Loss: 1.1111	LR: 0.050000
Training Epoch: 19 [43904/50000]	Loss: 1.1067	LR: 0.050000
Training Epoch: 19 [44032/50000]	Loss: 1.3385	LR: 0.050000
Training Epoch: 19 [44160/50000]	Loss: 1.3249	LR: 0.050000
Training Epoch: 19 [44288/50000]	Loss: 1.3844	LR: 0.050000
Training Epoch: 19 [44416/50000]	Loss: 1.1490	LR: 0.050000
Training Epoch: 19 [44544/50000]	Loss: 1.1536	LR: 0.050000
Training Epoch: 19 [44672/50000]	Loss: 1.6073	LR: 0.050000
Training Epoch: 19 [44800/50000]	Loss: 1.4647	LR: 0.050000
Training Epoch: 19 [44928/50000]	Loss: 1.4134	LR: 0.050000
Training Epoch: 19 [45056/50000]	Loss: 1.0447	LR: 0.050000
Training Epoch: 19 [45184/50000]	Loss: 1.2849	LR: 0.050000
Training Epoch: 19 [45312/50000]	Loss: 1.4546	LR: 0.050000
Training Epoch: 19 [45440/50000]	Loss: 1.1846	LR: 0.050000
Training Epoch: 19 [45568/50000]	Loss: 1.3273	LR: 0.050000
Training Epoch: 19 [45696/50000]	Loss: 1.0589	LR: 0.050000
Training Epoch: 19 [45824/50000]	Loss: 1.4465	LR: 0.050000
Training Epoch: 19 [45952/50000]	Loss: 1.2767	LR: 0.050000
Training Epoch: 19 [46080/50000]	Loss: 1.1858	LR: 0.050000
Training Epoch: 19 [46208/50000]	Loss: 1.2457	LR: 0.050000
Training Epoch: 19 [46336/50000]	Loss: 1.3544	LR: 0.050000
Training Epoch: 19 [46464/50000]	Loss: 1.3371	LR: 0.050000
Training Epoch: 19 [46592/50000]	Loss: 1.5720	LR: 0.050000
Training Epoch: 19 [46720/50000]	Loss: 1.2319	LR: 0.050000
Training Epoch: 19 [46848/50000]	Loss: 1.1755	LR: 0.050000
Training Epoch: 19 [46976/50000]	Loss: 1.2421	LR: 0.050000
Training Epoch: 19 [47104/50000]	Loss: 1.2132	LR: 0.050000
Training Epoch: 19 [47232/50000]	Loss: 1.4436	LR: 0.050000
Training Epoch: 19 [47360/50000]	Loss: 1.1605	LR: 0.050000
Training Epoch: 19 [47488/50000]	Loss: 1.0784	LR: 0.050000
Training Epoch: 19 [47616/50000]	Loss: 1.1433	LR: 0.050000
Training Epoch: 19 [47744/50000]	Loss: 1.0098	LR: 0.050000
Training Epoch: 19 [47872/50000]	Loss: 1.2951	LR: 0.050000
Training Epoch: 19 [48000/50000]	Loss: 1.0767	LR: 0.050000
Training Epoch: 19 [48128/50000]	Loss: 1.2036	LR: 0.050000
Training Epoch: 19 [48256/50000]	Loss: 1.2997	LR: 0.050000
Training Epoch: 19 [48384/50000]	Loss: 1.2042	LR: 0.050000
Training Epoch: 19 [48512/50000]	Loss: 1.3747	LR: 0.050000
Training Epoch: 19 [48640/50000]	Loss: 1.2354	LR: 0.050000
Training Epoch: 19 [48768/50000]	Loss: 1.3558	LR: 0.050000
Training Epoch: 19 [48896/50000]	Loss: 1.1581	LR: 0.050000
Training Epoch: 19 [49024/50000]	Loss: 1.3300	LR: 0.050000
Training Epoch: 19 [49152/50000]	Loss: 1.1739	LR: 0.050000
Training Epoch: 19 [49280/50000]	Loss: 1.3769	LR: 0.050000
Training Epoch: 19 [49408/50000]	Loss: 1.2599	LR: 0.050000
Training Epoch: 19 [49536/50000]	Loss: 1.4026	LR: 0.050000
Training Epoch: 19 [49664/50000]	Loss: 1.2135	LR: 0.050000
Training Epoch: 19 [49792/50000]	Loss: 1.2507	LR: 0.050000
Training Epoch: 19 [49920/50000]	Loss: 1.2205	LR: 0.050000
Training Epoch: 19 [50000/50000]	Loss: 1.4479	LR: 0.050000
epoch 19 training time consumed: 53.92s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   69731 GB |   69731 GB |
|       from large pool |  123392 KB |    1034 MB |   69662 GB |   69662 GB |
|       from small pool |   10798 KB |      13 MB |      68 GB |      68 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   69731 GB |   69731 GB |
|       from large pool |  123392 KB |    1034 MB |   69662 GB |   69662 GB |
|       from small pool |   10798 KB |      13 MB |      68 GB |      68 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   30688 GB |   30688 GB |
|       from large pool |  155136 KB |  433088 KB |   30612 GB |   30612 GB |
|       from small pool |    1490 KB |    3494 KB |      75 GB |      75 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    2691 K  |    2690 K  |
|       from large pool |      24    |      65    |    1404 K  |    1404 K  |
|       from small pool |     231    |     274    |    1286 K  |    1286 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    2691 K  |    2690 K  |
|       from large pool |      24    |      65    |    1404 K  |    1404 K  |
|       from small pool |     231    |     274    |    1286 K  |    1286 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1332 K  |    1332 K  |
|       from large pool |       9    |      14    |     679 K  |     679 K  |
|       from small pool |      12    |      16    |     652 K  |     652 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 19, Average loss: 0.0119, Accuracy: 0.5895, Time consumed:3.45s

Training Epoch: 20 [128/50000]	Loss: 1.4655	LR: 0.050000
Training Epoch: 20 [256/50000]	Loss: 1.1527	LR: 0.050000
Training Epoch: 20 [384/50000]	Loss: 1.2697	LR: 0.050000
Training Epoch: 20 [512/50000]	Loss: 1.0302	LR: 0.050000
Training Epoch: 20 [640/50000]	Loss: 0.9911	LR: 0.050000
Training Epoch: 20 [768/50000]	Loss: 1.1246	LR: 0.050000
Training Epoch: 20 [896/50000]	Loss: 1.2234	LR: 0.050000
Training Epoch: 20 [1024/50000]	Loss: 1.0720	LR: 0.050000
Training Epoch: 20 [1152/50000]	Loss: 1.2521	LR: 0.050000
Training Epoch: 20 [1280/50000]	Loss: 1.2225	LR: 0.050000
Training Epoch: 20 [1408/50000]	Loss: 1.2715	LR: 0.050000
Training Epoch: 20 [1536/50000]	Loss: 1.0470	LR: 0.050000
Training Epoch: 20 [1664/50000]	Loss: 1.2704	LR: 0.050000
Training Epoch: 20 [1792/50000]	Loss: 1.1109	LR: 0.050000
Training Epoch: 20 [1920/50000]	Loss: 0.9513	LR: 0.050000
Training Epoch: 20 [2048/50000]	Loss: 1.2822	LR: 0.050000
Training Epoch: 20 [2176/50000]	Loss: 1.1953	LR: 0.050000
Training Epoch: 20 [2304/50000]	Loss: 0.9593	LR: 0.050000
Training Epoch: 20 [2432/50000]	Loss: 0.9711	LR: 0.050000
Training Epoch: 20 [2560/50000]	Loss: 1.0884	LR: 0.050000
Training Epoch: 20 [2688/50000]	Loss: 0.8994	LR: 0.050000
Training Epoch: 20 [2816/50000]	Loss: 1.0614	LR: 0.050000
Training Epoch: 20 [2944/50000]	Loss: 1.1737	LR: 0.050000
Training Epoch: 20 [3072/50000]	Loss: 1.2115	LR: 0.050000
Training Epoch: 20 [3200/50000]	Loss: 1.0233	LR: 0.050000
Training Epoch: 20 [3328/50000]	Loss: 1.1408	LR: 0.050000
Training Epoch: 20 [3456/50000]	Loss: 0.9941	LR: 0.050000
Training Epoch: 20 [3584/50000]	Loss: 1.1102	LR: 0.050000
Training Epoch: 20 [3712/50000]	Loss: 0.9907	LR: 0.050000
Training Epoch: 20 [3840/50000]	Loss: 1.1059	LR: 0.050000
Training Epoch: 20 [3968/50000]	Loss: 0.9170	LR: 0.050000
Training Epoch: 20 [4096/50000]	Loss: 1.0949	LR: 0.050000
Training Epoch: 20 [4224/50000]	Loss: 1.3089	LR: 0.050000
Training Epoch: 20 [4352/50000]	Loss: 0.9395	LR: 0.050000
Training Epoch: 20 [4480/50000]	Loss: 0.9300	LR: 0.050000
Training Epoch: 20 [4608/50000]	Loss: 0.9103	LR: 0.050000
Training Epoch: 20 [4736/50000]	Loss: 1.1510	LR: 0.050000
Training Epoch: 20 [4864/50000]	Loss: 1.2189	LR: 0.050000
Training Epoch: 20 [4992/50000]	Loss: 0.9364	LR: 0.050000
Training Epoch: 20 [5120/50000]	Loss: 1.1402	LR: 0.050000
Training Epoch: 20 [5248/50000]	Loss: 1.1860	LR: 0.050000
Training Epoch: 20 [5376/50000]	Loss: 1.2295	LR: 0.050000
Training Epoch: 20 [5504/50000]	Loss: 1.1616	LR: 0.050000
Training Epoch: 20 [5632/50000]	Loss: 1.2894	LR: 0.050000
Training Epoch: 20 [5760/50000]	Loss: 0.9755	LR: 0.050000
Training Epoch: 20 [5888/50000]	Loss: 1.2486	LR: 0.050000
Training Epoch: 20 [6016/50000]	Loss: 1.2004	LR: 0.050000
Training Epoch: 20 [6144/50000]	Loss: 1.0958	LR: 0.050000
Training Epoch: 20 [6272/50000]	Loss: 1.0505	LR: 0.050000
Training Epoch: 20 [6400/50000]	Loss: 1.2235	LR: 0.050000
Training Epoch: 20 [6528/50000]	Loss: 1.0766	LR: 0.050000
Training Epoch: 20 [6656/50000]	Loss: 1.1687	LR: 0.050000
Training Epoch: 20 [6784/50000]	Loss: 1.0196	LR: 0.050000
Training Epoch: 20 [6912/50000]	Loss: 1.2458	LR: 0.050000
Training Epoch: 20 [7040/50000]	Loss: 1.1650	LR: 0.050000
Training Epoch: 20 [7168/50000]	Loss: 1.1862	LR: 0.050000
Training Epoch: 20 [7296/50000]	Loss: 1.3448	LR: 0.050000
Training Epoch: 20 [7424/50000]	Loss: 1.2893	LR: 0.050000
Training Epoch: 20 [7552/50000]	Loss: 1.3397	LR: 0.050000
Training Epoch: 20 [7680/50000]	Loss: 0.9648	LR: 0.050000
Training Epoch: 20 [7808/50000]	Loss: 1.1294	LR: 0.050000
Training Epoch: 20 [7936/50000]	Loss: 1.1216	LR: 0.050000
Training Epoch: 20 [8064/50000]	Loss: 1.1569	LR: 0.050000
Training Epoch: 20 [8192/50000]	Loss: 1.1027	LR: 0.050000
Training Epoch: 20 [8320/50000]	Loss: 1.1344	LR: 0.050000
Training Epoch: 20 [8448/50000]	Loss: 1.2164	LR: 0.050000
Training Epoch: 20 [8576/50000]	Loss: 1.0662	LR: 0.050000
Training Epoch: 20 [8704/50000]	Loss: 0.9711	LR: 0.050000
Training Epoch: 20 [8832/50000]	Loss: 1.2364	LR: 0.050000
Training Epoch: 20 [8960/50000]	Loss: 1.1104	LR: 0.050000
Training Epoch: 20 [9088/50000]	Loss: 1.0829	LR: 0.050000
Training Epoch: 20 [9216/50000]	Loss: 1.3036	LR: 0.050000
Training Epoch: 20 [9344/50000]	Loss: 1.0755	LR: 0.050000
Training Epoch: 20 [9472/50000]	Loss: 1.0626	LR: 0.050000
Training Epoch: 20 [9600/50000]	Loss: 1.3099	LR: 0.050000
Training Epoch: 20 [9728/50000]	Loss: 1.0920	LR: 0.050000
Training Epoch: 20 [9856/50000]	Loss: 1.1990	LR: 0.050000
Training Epoch: 20 [9984/50000]	Loss: 1.2338	LR: 0.050000
Training Epoch: 20 [10112/50000]	Loss: 1.2016	LR: 0.050000
Training Epoch: 20 [10240/50000]	Loss: 1.2242	LR: 0.050000
Training Epoch: 20 [10368/50000]	Loss: 1.0275	LR: 0.050000
Training Epoch: 20 [10496/50000]	Loss: 1.1798	LR: 0.050000
Training Epoch: 20 [10624/50000]	Loss: 1.0698	LR: 0.050000
Training Epoch: 20 [10752/50000]	Loss: 1.0260	LR: 0.050000
Training Epoch: 20 [10880/50000]	Loss: 1.0694	LR: 0.050000
Training Epoch: 20 [11008/50000]	Loss: 1.2743	LR: 0.050000
Training Epoch: 20 [11136/50000]	Loss: 1.0417	LR: 0.050000
Training Epoch: 20 [11264/50000]	Loss: 1.0521	LR: 0.050000
Training Epoch: 20 [11392/50000]	Loss: 1.0430	LR: 0.050000
Training Epoch: 20 [11520/50000]	Loss: 1.1757	LR: 0.050000
Training Epoch: 20 [11648/50000]	Loss: 1.1243	LR: 0.050000
Training Epoch: 20 [11776/50000]	Loss: 1.1821	LR: 0.050000
Training Epoch: 20 [11904/50000]	Loss: 1.4118	LR: 0.050000
Training Epoch: 20 [12032/50000]	Loss: 1.1039	LR: 0.050000
Training Epoch: 20 [12160/50000]	Loss: 1.1018	LR: 0.050000
Training Epoch: 20 [12288/50000]	Loss: 1.1479	LR: 0.050000
Training Epoch: 20 [12416/50000]	Loss: 1.2455	LR: 0.050000
Training Epoch: 20 [12544/50000]	Loss: 1.1116	LR: 0.050000
Training Epoch: 20 [12672/50000]	Loss: 1.1189	LR: 0.050000
Training Epoch: 20 [12800/50000]	Loss: 1.0193	LR: 0.050000
Training Epoch: 20 [12928/50000]	Loss: 1.0495	LR: 0.050000
Training Epoch: 20 [13056/50000]	Loss: 1.2447	LR: 0.050000
Training Epoch: 20 [13184/50000]	Loss: 0.9424	LR: 0.050000
Training Epoch: 20 [13312/50000]	Loss: 1.4456	LR: 0.050000
Training Epoch: 20 [13440/50000]	Loss: 1.0388	LR: 0.050000
Training Epoch: 20 [13568/50000]	Loss: 0.9738	LR: 0.050000
Training Epoch: 20 [13696/50000]	Loss: 1.2581	LR: 0.050000
Training Epoch: 20 [13824/50000]	Loss: 1.0604	LR: 0.050000
Training Epoch: 20 [13952/50000]	Loss: 1.2764	LR: 0.050000
Training Epoch: 20 [14080/50000]	Loss: 1.2983	LR: 0.050000
Training Epoch: 20 [14208/50000]	Loss: 1.1372	LR: 0.050000
Training Epoch: 20 [14336/50000]	Loss: 1.4275	LR: 0.050000
Training Epoch: 20 [14464/50000]	Loss: 1.1417	LR: 0.050000
Training Epoch: 20 [14592/50000]	Loss: 1.4194	LR: 0.050000
Training Epoch: 20 [14720/50000]	Loss: 0.9988	LR: 0.050000
Training Epoch: 20 [14848/50000]	Loss: 1.2244	LR: 0.050000
Training Epoch: 20 [14976/50000]	Loss: 1.2229	LR: 0.050000
Training Epoch: 20 [15104/50000]	Loss: 0.9791	LR: 0.050000
Training Epoch: 20 [15232/50000]	Loss: 0.9717	LR: 0.050000
Training Epoch: 20 [15360/50000]	Loss: 1.0959	LR: 0.050000
Training Epoch: 20 [15488/50000]	Loss: 1.3112	LR: 0.050000
Training Epoch: 20 [15616/50000]	Loss: 1.1726	LR: 0.050000
Training Epoch: 20 [15744/50000]	Loss: 1.1177	LR: 0.050000
Training Epoch: 20 [15872/50000]	Loss: 1.3573	LR: 0.050000
Training Epoch: 20 [16000/50000]	Loss: 1.4127	LR: 0.050000
Training Epoch: 20 [16128/50000]	Loss: 0.8439	LR: 0.050000
Training Epoch: 20 [16256/50000]	Loss: 1.1532	LR: 0.050000
Training Epoch: 20 [16384/50000]	Loss: 1.0895	LR: 0.050000
Training Epoch: 20 [16512/50000]	Loss: 0.9838	LR: 0.050000
Training Epoch: 20 [16640/50000]	Loss: 1.1312	LR: 0.050000
Training Epoch: 20 [16768/50000]	Loss: 1.4632	LR: 0.050000
Training Epoch: 20 [16896/50000]	Loss: 1.0644	LR: 0.050000
Training Epoch: 20 [17024/50000]	Loss: 1.5170	LR: 0.050000
Training Epoch: 20 [17152/50000]	Loss: 1.2608	LR: 0.050000
Training Epoch: 20 [17280/50000]	Loss: 1.2052	LR: 0.050000
Training Epoch: 20 [17408/50000]	Loss: 1.0744	LR: 0.050000
Training Epoch: 20 [17536/50000]	Loss: 1.0914	LR: 0.050000
Training Epoch: 20 [17664/50000]	Loss: 1.2384	LR: 0.050000
Training Epoch: 20 [17792/50000]	Loss: 1.4277	LR: 0.050000
Training Epoch: 20 [17920/50000]	Loss: 1.1777	LR: 0.050000
Training Epoch: 20 [18048/50000]	Loss: 1.2184	LR: 0.050000
Training Epoch: 20 [18176/50000]	Loss: 1.3540	LR: 0.050000
Training Epoch: 20 [18304/50000]	Loss: 1.0016	LR: 0.050000
Training Epoch: 20 [18432/50000]	Loss: 1.3856	LR: 0.050000
Training Epoch: 20 [18560/50000]	Loss: 1.4418	LR: 0.050000
Training Epoch: 20 [18688/50000]	Loss: 1.4955	LR: 0.050000
Training Epoch: 20 [18816/50000]	Loss: 1.2377	LR: 0.050000
Training Epoch: 20 [18944/50000]	Loss: 1.1804	LR: 0.050000
Training Epoch: 20 [19072/50000]	Loss: 1.0776	LR: 0.050000
Training Epoch: 20 [19200/50000]	Loss: 1.1820	LR: 0.050000
Training Epoch: 20 [19328/50000]	Loss: 1.1474	LR: 0.050000
Training Epoch: 20 [19456/50000]	Loss: 1.2027	LR: 0.050000
Training Epoch: 20 [19584/50000]	Loss: 1.1041	LR: 0.050000
Training Epoch: 20 [19712/50000]	Loss: 1.1890	LR: 0.050000
Training Epoch: 20 [19840/50000]	Loss: 1.2194	LR: 0.050000
Training Epoch: 20 [19968/50000]	Loss: 1.2189	LR: 0.050000
Training Epoch: 20 [20096/50000]	Loss: 1.2537	LR: 0.050000
Training Epoch: 20 [20224/50000]	Loss: 1.2091	LR: 0.050000
Training Epoch: 20 [20352/50000]	Loss: 1.1846	LR: 0.050000
Training Epoch: 20 [20480/50000]	Loss: 1.2556	LR: 0.050000
Training Epoch: 20 [20608/50000]	Loss: 1.3100	LR: 0.050000
Training Epoch: 20 [20736/50000]	Loss: 1.0231	LR: 0.050000
Training Epoch: 20 [20864/50000]	Loss: 1.3334	LR: 0.050000
Training Epoch: 20 [20992/50000]	Loss: 1.4315	LR: 0.050000
Training Epoch: 20 [21120/50000]	Loss: 1.3381	LR: 0.050000
Training Epoch: 20 [21248/50000]	Loss: 1.2176	LR: 0.050000
Training Epoch: 20 [21376/50000]	Loss: 1.2544	LR: 0.050000
Training Epoch: 20 [21504/50000]	Loss: 1.3787	LR: 0.050000
Training Epoch: 20 [21632/50000]	Loss: 1.0590	LR: 0.050000
Training Epoch: 20 [21760/50000]	Loss: 1.0289	LR: 0.050000
Training Epoch: 20 [21888/50000]	Loss: 1.1136	LR: 0.050000
Training Epoch: 20 [22016/50000]	Loss: 1.2531	LR: 0.050000
Training Epoch: 20 [22144/50000]	Loss: 1.4384	LR: 0.050000
Training Epoch: 20 [22272/50000]	Loss: 1.5680	LR: 0.050000
Training Epoch: 20 [22400/50000]	Loss: 1.2455	LR: 0.050000
Training Epoch: 20 [22528/50000]	Loss: 1.1751	LR: 0.050000
Training Epoch: 20 [22656/50000]	Loss: 1.0802	LR: 0.050000
Training Epoch: 20 [22784/50000]	Loss: 1.1699	LR: 0.050000
Training Epoch: 20 [22912/50000]	Loss: 1.1990	LR: 0.050000
Training Epoch: 20 [23040/50000]	Loss: 1.0903	LR: 0.050000
Training Epoch: 20 [23168/50000]	Loss: 1.0119	LR: 0.050000
Training Epoch: 20 [23296/50000]	Loss: 1.2716	LR: 0.050000
Training Epoch: 20 [23424/50000]	Loss: 1.2474	LR: 0.050000
Training Epoch: 20 [23552/50000]	Loss: 1.2594	LR: 0.050000
Training Epoch: 20 [23680/50000]	Loss: 1.4010	LR: 0.050000
Training Epoch: 20 [23808/50000]	Loss: 1.2043	LR: 0.050000
Training Epoch: 20 [23936/50000]	Loss: 1.0102	LR: 0.050000
Training Epoch: 20 [24064/50000]	Loss: 1.2392	LR: 0.050000
Training Epoch: 20 [24192/50000]	Loss: 1.3161	LR: 0.050000
Training Epoch: 20 [24320/50000]	Loss: 1.5255	LR: 0.050000
Training Epoch: 20 [24448/50000]	Loss: 1.4866	LR: 0.050000
Training Epoch: 20 [24576/50000]	Loss: 1.2472	LR: 0.050000
Training Epoch: 20 [24704/50000]	Loss: 1.2164	LR: 0.050000
Training Epoch: 20 [24832/50000]	Loss: 1.1355	LR: 0.050000
Training Epoch: 20 [24960/50000]	Loss: 1.3948	LR: 0.050000
Training Epoch: 20 [25088/50000]	Loss: 1.1294	LR: 0.050000
Training Epoch: 20 [25216/50000]	Loss: 1.3520	LR: 0.050000
Training Epoch: 20 [25344/50000]	Loss: 1.5181	LR: 0.050000
Training Epoch: 20 [25472/50000]	Loss: 1.1725	LR: 0.050000
Training Epoch: 20 [25600/50000]	Loss: 1.1308	LR: 0.050000
Training Epoch: 20 [25728/50000]	Loss: 1.2946	LR: 0.050000
Training Epoch: 20 [25856/50000]	Loss: 1.0818	LR: 0.050000
Training Epoch: 20 [25984/50000]	Loss: 1.1383	LR: 0.050000
Training Epoch: 20 [26112/50000]	Loss: 1.1498	LR: 0.050000
Training Epoch: 20 [26240/50000]	Loss: 1.4220	LR: 0.050000
Training Epoch: 20 [26368/50000]	Loss: 1.3240	LR: 0.050000
Training Epoch: 20 [26496/50000]	Loss: 1.3848	LR: 0.050000
Training Epoch: 20 [26624/50000]	Loss: 1.3441	LR: 0.050000
Training Epoch: 20 [26752/50000]	Loss: 1.2312	LR: 0.050000
Training Epoch: 20 [26880/50000]	Loss: 1.1015	LR: 0.050000
Training Epoch: 20 [27008/50000]	Loss: 1.3712	LR: 0.050000
Training Epoch: 20 [27136/50000]	Loss: 1.4909	LR: 0.050000
Training Epoch: 20 [27264/50000]	Loss: 1.0551	LR: 0.050000
Training Epoch: 20 [27392/50000]	Loss: 1.0425	LR: 0.050000
Training Epoch: 20 [27520/50000]	Loss: 1.2512	LR: 0.050000
Training Epoch: 20 [27648/50000]	Loss: 1.3829	LR: 0.050000
Training Epoch: 20 [27776/50000]	Loss: 1.2091	LR: 0.050000
Training Epoch: 20 [27904/50000]	Loss: 1.1381	LR: 0.050000
Training Epoch: 20 [28032/50000]	Loss: 1.2718	LR: 0.050000
Training Epoch: 20 [28160/50000]	Loss: 1.3111	LR: 0.050000
Training Epoch: 20 [28288/50000]	Loss: 1.0523	LR: 0.050000
Training Epoch: 20 [28416/50000]	Loss: 1.1760	LR: 0.050000
Training Epoch: 20 [28544/50000]	Loss: 1.0776	LR: 0.050000
Training Epoch: 20 [28672/50000]	Loss: 1.2673	LR: 0.050000
Training Epoch: 20 [28800/50000]	Loss: 1.2032	LR: 0.050000
Training Epoch: 20 [28928/50000]	Loss: 1.4803	LR: 0.050000
Training Epoch: 20 [29056/50000]	Loss: 1.0392	LR: 0.050000
Training Epoch: 20 [29184/50000]	Loss: 0.9216	LR: 0.050000
Training Epoch: 20 [29312/50000]	Loss: 1.1793	LR: 0.050000
Training Epoch: 20 [29440/50000]	Loss: 1.3302	LR: 0.050000
Training Epoch: 20 [29568/50000]	Loss: 1.3832	LR: 0.050000
Training Epoch: 20 [29696/50000]	Loss: 1.1742	LR: 0.050000
Training Epoch: 20 [29824/50000]	Loss: 1.3686	LR: 0.050000
Training Epoch: 20 [29952/50000]	Loss: 1.1929	LR: 0.050000
Training Epoch: 20 [30080/50000]	Loss: 1.2901	LR: 0.050000
Training Epoch: 20 [30208/50000]	Loss: 1.2594	LR: 0.050000
Training Epoch: 20 [30336/50000]	Loss: 1.0743	LR: 0.050000
Training Epoch: 20 [30464/50000]	Loss: 1.3083	LR: 0.050000
Training Epoch: 20 [30592/50000]	Loss: 1.0968	LR: 0.050000
Training Epoch: 20 [30720/50000]	Loss: 1.1702	LR: 0.050000
Training Epoch: 20 [30848/50000]	Loss: 1.3110	LR: 0.050000
Training Epoch: 20 [30976/50000]	Loss: 1.2601	LR: 0.050000
Training Epoch: 20 [31104/50000]	Loss: 1.3991	LR: 0.050000
Training Epoch: 20 [31232/50000]	Loss: 1.2665	LR: 0.050000
Training Epoch: 20 [31360/50000]	Loss: 1.4158	LR: 0.050000
Training Epoch: 20 [31488/50000]	Loss: 0.9233	LR: 0.050000
Training Epoch: 20 [31616/50000]	Loss: 1.2248	LR: 0.050000
Training Epoch: 20 [31744/50000]	Loss: 1.5454	LR: 0.050000
Training Epoch: 20 [31872/50000]	Loss: 1.1804	LR: 0.050000
Training Epoch: 20 [32000/50000]	Loss: 0.9125	LR: 0.050000
Training Epoch: 20 [32128/50000]	Loss: 1.2376	LR: 0.050000
Training Epoch: 20 [32256/50000]	Loss: 1.4007	LR: 0.050000
Training Epoch: 20 [32384/50000]	Loss: 1.2617	LR: 0.050000
Training Epoch: 20 [32512/50000]	Loss: 1.0849	LR: 0.050000
Training Epoch: 20 [32640/50000]	Loss: 1.3461	LR: 0.050000
Training Epoch: 20 [32768/50000]	Loss: 1.3020	LR: 0.050000
Training Epoch: 20 [32896/50000]	Loss: 1.5147	LR: 0.050000
Training Epoch: 20 [33024/50000]	Loss: 1.2013	LR: 0.050000
Training Epoch: 20 [33152/50000]	Loss: 1.3281	LR: 0.050000
Training Epoch: 20 [33280/50000]	Loss: 1.3568	LR: 0.050000
Training Epoch: 20 [33408/50000]	Loss: 1.2065	LR: 0.050000
Training Epoch: 20 [33536/50000]	Loss: 0.9970	LR: 0.050000
Training Epoch: 20 [33664/50000]	Loss: 1.1950	LR: 0.050000
Training Epoch: 20 [33792/50000]	Loss: 1.4820	LR: 0.050000
Training Epoch: 20 [33920/50000]	Loss: 1.3148	LR: 0.050000
Training Epoch: 20 [34048/50000]	Loss: 1.4721	LR: 0.050000
Training Epoch: 20 [34176/50000]	Loss: 1.2974	LR: 0.050000
Training Epoch: 20 [34304/50000]	Loss: 1.3092	LR: 0.050000
Training Epoch: 20 [34432/50000]	Loss: 1.3904	LR: 0.050000
Training Epoch: 20 [34560/50000]	Loss: 1.0993	LR: 0.050000
Training Epoch: 20 [34688/50000]	Loss: 1.5191	LR: 0.050000
Training Epoch: 20 [34816/50000]	Loss: 1.4323	LR: 0.050000
Training Epoch: 20 [34944/50000]	Loss: 1.1883	LR: 0.050000
Training Epoch: 20 [35072/50000]	Loss: 1.3002	LR: 0.050000
Training Epoch: 20 [35200/50000]	Loss: 1.3052	LR: 0.050000
Training Epoch: 20 [35328/50000]	Loss: 1.4869	LR: 0.050000
Training Epoch: 20 [35456/50000]	Loss: 1.1851	LR: 0.050000
Training Epoch: 20 [35584/50000]	Loss: 1.4874	LR: 0.050000
Training Epoch: 20 [35712/50000]	Loss: 1.1571	LR: 0.050000
Training Epoch: 20 [35840/50000]	Loss: 1.0637	LR: 0.050000
Training Epoch: 20 [35968/50000]	Loss: 0.8765	LR: 0.050000
Training Epoch: 20 [36096/50000]	Loss: 1.3790	LR: 0.050000
Training Epoch: 20 [36224/50000]	Loss: 1.2280	LR: 0.050000
Training Epoch: 20 [36352/50000]	Loss: 1.3916	LR: 0.050000
Training Epoch: 20 [36480/50000]	Loss: 1.4230	LR: 0.050000
Training Epoch: 20 [36608/50000]	Loss: 1.3198	LR: 0.050000
Training Epoch: 20 [36736/50000]	Loss: 1.1014	LR: 0.050000
Training Epoch: 20 [36864/50000]	Loss: 1.2053	LR: 0.050000
Training Epoch: 20 [36992/50000]	Loss: 1.1527	LR: 0.050000
Training Epoch: 20 [37120/50000]	Loss: 1.2016	LR: 0.050000
Training Epoch: 20 [37248/50000]	Loss: 1.3343	LR: 0.050000
Training Epoch: 20 [37376/50000]	Loss: 1.2361	LR: 0.050000
Training Epoch: 20 [37504/50000]	Loss: 1.2195	LR: 0.050000
Training Epoch: 20 [37632/50000]	Loss: 1.2388	LR: 0.050000
Training Epoch: 20 [37760/50000]	Loss: 1.3109	LR: 0.050000
Training Epoch: 20 [37888/50000]	Loss: 1.2954	LR: 0.050000
Training Epoch: 20 [38016/50000]	Loss: 1.2401	LR: 0.050000
Training Epoch: 20 [38144/50000]	Loss: 1.3805	LR: 0.050000
Training Epoch: 20 [38272/50000]	Loss: 1.2086	LR: 0.050000
Training Epoch: 20 [38400/50000]	Loss: 1.2287	LR: 0.050000
Training Epoch: 20 [38528/50000]	Loss: 1.3425	LR: 0.050000
Training Epoch: 20 [38656/50000]	Loss: 1.0683	LR: 0.050000
Training Epoch: 20 [38784/50000]	Loss: 1.3782	LR: 0.050000
Training Epoch: 20 [38912/50000]	Loss: 1.3863	LR: 0.050000
Training Epoch: 20 [39040/50000]	Loss: 1.1826	LR: 0.050000
Training Epoch: 20 [39168/50000]	Loss: 1.1478	LR: 0.050000
Training Epoch: 20 [39296/50000]	Loss: 1.2840	LR: 0.050000
Training Epoch: 20 [39424/50000]	Loss: 1.2878	LR: 0.050000
Training Epoch: 20 [39552/50000]	Loss: 1.3780	LR: 0.050000
Training Epoch: 20 [39680/50000]	Loss: 1.3375	LR: 0.050000
Training Epoch: 20 [39808/50000]	Loss: 1.4752	LR: 0.050000
Training Epoch: 20 [39936/50000]	Loss: 1.1737	LR: 0.050000
Training Epoch: 20 [40064/50000]	Loss: 1.3233	LR: 0.050000
Training Epoch: 20 [40192/50000]	Loss: 1.2869	LR: 0.050000
Training Epoch: 20 [40320/50000]	Loss: 1.2665	LR: 0.050000
Training Epoch: 20 [40448/50000]	Loss: 1.3042	LR: 0.050000
Training Epoch: 20 [40576/50000]	Loss: 1.2446	LR: 0.050000
Training Epoch: 20 [40704/50000]	Loss: 0.8836	LR: 0.050000
Training Epoch: 20 [40832/50000]	Loss: 1.4346	LR: 0.050000
Training Epoch: 20 [40960/50000]	Loss: 1.3841	LR: 0.050000
Training Epoch: 20 [41088/50000]	Loss: 1.1731	LR: 0.050000
Training Epoch: 20 [41216/50000]	Loss: 1.0916	LR: 0.050000
Training Epoch: 20 [41344/50000]	Loss: 1.2053	LR: 0.050000
Training Epoch: 20 [41472/50000]	Loss: 1.1790	LR: 0.050000
Training Epoch: 20 [41600/50000]	Loss: 1.1532	LR: 0.050000
Training Epoch: 20 [41728/50000]	Loss: 1.4273	LR: 0.050000
Training Epoch: 20 [41856/50000]	Loss: 1.3407	LR: 0.050000
Training Epoch: 20 [41984/50000]	Loss: 1.1625	LR: 0.050000
Training Epoch: 20 [42112/50000]	Loss: 1.1805	LR: 0.050000
Training Epoch: 20 [42240/50000]	Loss: 1.3727	LR: 0.050000
Training Epoch: 20 [42368/50000]	Loss: 1.1684	LR: 0.050000
Training Epoch: 20 [42496/50000]	Loss: 1.5160	LR: 0.050000
Training Epoch: 20 [42624/50000]	Loss: 1.5009	LR: 0.050000
Training Epoch: 20 [42752/50000]	Loss: 1.3202	LR: 0.050000
Training Epoch: 20 [42880/50000]	Loss: 1.2012	LR: 0.050000
Training Epoch: 20 [43008/50000]	Loss: 1.2411	LR: 0.050000
Training Epoch: 20 [43136/50000]	Loss: 1.0992	LR: 0.050000
Training Epoch: 20 [43264/50000]	Loss: 1.2235	LR: 0.050000
Training Epoch: 20 [43392/50000]	Loss: 1.4385	LR: 0.050000
Training Epoch: 20 [43520/50000]	Loss: 1.2781	LR: 0.050000
Training Epoch: 20 [43648/50000]	Loss: 1.2405	LR: 0.050000
Training Epoch: 20 [43776/50000]	Loss: 1.4855	LR: 0.050000
Training Epoch: 20 [43904/50000]	Loss: 1.2850	LR: 0.050000
Training Epoch: 20 [44032/50000]	Loss: 1.3061	LR: 0.050000
Training Epoch: 20 [44160/50000]	Loss: 1.1510	LR: 0.050000
Training Epoch: 20 [44288/50000]	Loss: 1.2620	LR: 0.050000
Training Epoch: 20 [44416/50000]	Loss: 1.1982	LR: 0.050000
Training Epoch: 20 [44544/50000]	Loss: 1.2093	LR: 0.050000
Training Epoch: 20 [44672/50000]	Loss: 1.2013	LR: 0.050000
Training Epoch: 20 [44800/50000]	Loss: 1.1141	LR: 0.050000
Training Epoch: 20 [44928/50000]	Loss: 1.0248	LR: 0.050000
Training Epoch: 20 [45056/50000]	Loss: 1.1034	LR: 0.050000
Training Epoch: 20 [45184/50000]	Loss: 1.1531	LR: 0.050000
Training Epoch: 20 [45312/50000]	Loss: 1.4201	LR: 0.050000
Training Epoch: 20 [45440/50000]	Loss: 1.0952	LR: 0.050000
Training Epoch: 20 [45568/50000]	Loss: 1.1882	LR: 0.050000
Training Epoch: 20 [45696/50000]	Loss: 1.2401	LR: 0.050000
Training Epoch: 20 [45824/50000]	Loss: 1.1944	LR: 0.050000
Training Epoch: 20 [45952/50000]	Loss: 1.1994	LR: 0.050000
Training Epoch: 20 [46080/50000]	Loss: 1.2259	LR: 0.050000
Training Epoch: 20 [46208/50000]	Loss: 0.9719	LR: 0.050000
Training Epoch: 20 [46336/50000]	Loss: 1.3916	LR: 0.050000
Training Epoch: 20 [46464/50000]	Loss: 1.3770	LR: 0.050000
Training Epoch: 20 [46592/50000]	Loss: 1.1112	LR: 0.050000
Training Epoch: 20 [46720/50000]	Loss: 1.1421	LR: 0.050000
Training Epoch: 20 [46848/50000]	Loss: 1.1191	LR: 0.050000
Training Epoch: 20 [46976/50000]	Loss: 1.1232	LR: 0.050000
Training Epoch: 20 [47104/50000]	Loss: 1.1295	LR: 0.050000
Training Epoch: 20 [47232/50000]	Loss: 1.2046	LR: 0.050000
Training Epoch: 20 [47360/50000]	Loss: 1.1362	LR: 0.050000
Training Epoch: 20 [47488/50000]	Loss: 1.4473	LR: 0.050000
Training Epoch: 20 [47616/50000]	Loss: 1.4853	LR: 0.050000
Training Epoch: 20 [47744/50000]	Loss: 1.2280	LR: 0.050000
Training Epoch: 20 [47872/50000]	Loss: 1.4181	LR: 0.050000
Training Epoch: 20 [48000/50000]	Loss: 1.0647	LR: 0.050000
Training Epoch: 20 [48128/50000]	Loss: 1.4910	LR: 0.050000
Training Epoch: 20 [48256/50000]	Loss: 1.2024	LR: 0.050000
Training Epoch: 20 [48384/50000]	Loss: 1.2067	LR: 0.050000
Training Epoch: 20 [48512/50000]	Loss: 1.3089	LR: 0.050000
Training Epoch: 20 [48640/50000]	Loss: 1.2183	LR: 0.050000
Training Epoch: 20 [48768/50000]	Loss: 1.4067	LR: 0.050000
Training Epoch: 20 [48896/50000]	Loss: 1.2453	LR: 0.050000
Training Epoch: 20 [49024/50000]	Loss: 1.1803	LR: 0.050000
Training Epoch: 20 [49152/50000]	Loss: 1.4750	LR: 0.050000
Training Epoch: 20 [49280/50000]	Loss: 1.1374	LR: 0.050000
Training Epoch: 20 [49408/50000]	Loss: 1.3680	LR: 0.050000
Training Epoch: 20 [49536/50000]	Loss: 1.0956	LR: 0.050000
Training Epoch: 20 [49664/50000]	Loss: 1.1800	LR: 0.050000
Training Epoch: 20 [49792/50000]	Loss: 1.2947	LR: 0.050000
Training Epoch: 20 [49920/50000]	Loss: 1.4789	LR: 0.050000
Training Epoch: 20 [50000/50000]	Loss: 1.2277	LR: 0.050000
epoch 20 training time consumed: 53.90s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   73401 GB |   73401 GB |
|       from large pool |  123392 KB |    1034 MB |   73329 GB |   73329 GB |
|       from small pool |   10798 KB |      13 MB |      72 GB |      72 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   73401 GB |   73401 GB |
|       from large pool |  123392 KB |    1034 MB |   73329 GB |   73329 GB |
|       from small pool |   10798 KB |      13 MB |      72 GB |      72 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   32303 GB |   32303 GB |
|       from large pool |  155136 KB |  433088 KB |   32223 GB |   32223 GB |
|       from small pool |    1490 KB |    3494 KB |      79 GB |      79 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    2832 K  |    2832 K  |
|       from large pool |      24    |      65    |    1478 K  |    1478 K  |
|       from small pool |     231    |     274    |    1354 K  |    1354 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    2832 K  |    2832 K  |
|       from large pool |      24    |      65    |    1478 K  |    1478 K  |
|       from small pool |     231    |     274    |    1354 K  |    1354 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1402 K  |    1402 K  |
|       from large pool |       9    |      14    |     715 K  |     715 K  |
|       from small pool |      12    |      16    |     686 K  |     686 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 20, Average loss: 0.0129, Accuracy: 0.5650, Time consumed:3.46s

Training Epoch: 21 [128/50000]	Loss: 0.9611	LR: 0.050000
Training Epoch: 21 [256/50000]	Loss: 1.2005	LR: 0.050000
Training Epoch: 21 [384/50000]	Loss: 1.0135	LR: 0.050000
Training Epoch: 21 [512/50000]	Loss: 1.1268	LR: 0.050000
Training Epoch: 21 [640/50000]	Loss: 1.0906	LR: 0.050000
Training Epoch: 21 [768/50000]	Loss: 1.2280	LR: 0.050000
Training Epoch: 21 [896/50000]	Loss: 1.2975	LR: 0.050000
Training Epoch: 21 [1024/50000]	Loss: 1.0562	LR: 0.050000
Training Epoch: 21 [1152/50000]	Loss: 0.9223	LR: 0.050000
Training Epoch: 21 [1280/50000]	Loss: 1.0658	LR: 0.050000
Training Epoch: 21 [1408/50000]	Loss: 0.9707	LR: 0.050000
Training Epoch: 21 [1536/50000]	Loss: 0.9882	LR: 0.050000
Training Epoch: 21 [1664/50000]	Loss: 1.2577	LR: 0.050000
Training Epoch: 21 [1792/50000]	Loss: 1.0539	LR: 0.050000
Training Epoch: 21 [1920/50000]	Loss: 1.3092	LR: 0.050000
Training Epoch: 21 [2048/50000]	Loss: 1.1014	LR: 0.050000
Training Epoch: 21 [2176/50000]	Loss: 1.1245	LR: 0.050000
Training Epoch: 21 [2304/50000]	Loss: 1.0604	LR: 0.050000
Training Epoch: 21 [2432/50000]	Loss: 1.2500	LR: 0.050000
Training Epoch: 21 [2560/50000]	Loss: 1.2091	LR: 0.050000
Training Epoch: 21 [2688/50000]	Loss: 1.1548	LR: 0.050000
Training Epoch: 21 [2816/50000]	Loss: 1.1309	LR: 0.050000
Training Epoch: 21 [2944/50000]	Loss: 1.1517	LR: 0.050000
Training Epoch: 21 [3072/50000]	Loss: 1.4906	LR: 0.050000
Training Epoch: 21 [3200/50000]	Loss: 1.1085	LR: 0.050000
Training Epoch: 21 [3328/50000]	Loss: 1.1932	LR: 0.050000
Training Epoch: 21 [3456/50000]	Loss: 0.9497	LR: 0.050000
Training Epoch: 21 [3584/50000]	Loss: 1.1761	LR: 0.050000
Training Epoch: 21 [3712/50000]	Loss: 1.0840	LR: 0.050000
Training Epoch: 21 [3840/50000]	Loss: 1.1975	LR: 0.050000
Training Epoch: 21 [3968/50000]	Loss: 1.0290	LR: 0.050000
Training Epoch: 21 [4096/50000]	Loss: 1.1418	LR: 0.050000
Training Epoch: 21 [4224/50000]	Loss: 1.1105	LR: 0.050000
Training Epoch: 21 [4352/50000]	Loss: 1.1687	LR: 0.050000
Training Epoch: 21 [4480/50000]	Loss: 0.9061	LR: 0.050000
Training Epoch: 21 [4608/50000]	Loss: 1.2560	LR: 0.050000
Training Epoch: 21 [4736/50000]	Loss: 1.0775	LR: 0.050000
Training Epoch: 21 [4864/50000]	Loss: 1.1617	LR: 0.050000
Training Epoch: 21 [4992/50000]	Loss: 1.1760	LR: 0.050000
Training Epoch: 21 [5120/50000]	Loss: 1.0079	LR: 0.050000
Training Epoch: 21 [5248/50000]	Loss: 1.1689	LR: 0.050000
Training Epoch: 21 [5376/50000]	Loss: 1.1792	LR: 0.050000
Training Epoch: 21 [5504/50000]	Loss: 1.3261	LR: 0.050000
Training Epoch: 21 [5632/50000]	Loss: 1.2333	LR: 0.050000
Training Epoch: 21 [5760/50000]	Loss: 1.0136	LR: 0.050000
Training Epoch: 21 [5888/50000]	Loss: 1.0912	LR: 0.050000
Training Epoch: 21 [6016/50000]	Loss: 1.3298	LR: 0.050000
Training Epoch: 21 [6144/50000]	Loss: 0.9717	LR: 0.050000
Training Epoch: 21 [6272/50000]	Loss: 1.2649	LR: 0.050000
Training Epoch: 21 [6400/50000]	Loss: 1.1859	LR: 0.050000
Training Epoch: 21 [6528/50000]	Loss: 1.1485	LR: 0.050000
Training Epoch: 21 [6656/50000]	Loss: 1.1204	LR: 0.050000
Training Epoch: 21 [6784/50000]	Loss: 0.9748	LR: 0.050000
Training Epoch: 21 [6912/50000]	Loss: 1.1619	LR: 0.050000
Training Epoch: 21 [7040/50000]	Loss: 1.0385	LR: 0.050000
Training Epoch: 21 [7168/50000]	Loss: 1.1425	LR: 0.050000
Training Epoch: 21 [7296/50000]	Loss: 1.1387	LR: 0.050000
Training Epoch: 21 [7424/50000]	Loss: 1.2873	LR: 0.050000
Training Epoch: 21 [7552/50000]	Loss: 1.0042	LR: 0.050000
Training Epoch: 21 [7680/50000]	Loss: 1.2978	LR: 0.050000
Training Epoch: 21 [7808/50000]	Loss: 1.0944	LR: 0.050000
Training Epoch: 21 [7936/50000]	Loss: 1.0312	LR: 0.050000
Training Epoch: 21 [8064/50000]	Loss: 1.2971	LR: 0.050000
Training Epoch: 21 [8192/50000]	Loss: 1.2861	LR: 0.050000
Training Epoch: 21 [8320/50000]	Loss: 1.3234	LR: 0.050000
Training Epoch: 21 [8448/50000]	Loss: 1.1160	LR: 0.050000
Training Epoch: 21 [8576/50000]	Loss: 1.1457	LR: 0.050000
Training Epoch: 21 [8704/50000]	Loss: 0.8039	LR: 0.050000
Training Epoch: 21 [8832/50000]	Loss: 1.1754	LR: 0.050000
Training Epoch: 21 [8960/50000]	Loss: 1.0391	LR: 0.050000
Training Epoch: 21 [9088/50000]	Loss: 1.3723	LR: 0.050000
Training Epoch: 21 [9216/50000]	Loss: 1.1311	LR: 0.050000
Training Epoch: 21 [9344/50000]	Loss: 0.9030	LR: 0.050000
Training Epoch: 21 [9472/50000]	Loss: 1.0657	LR: 0.050000
Training Epoch: 21 [9600/50000]	Loss: 1.0987	LR: 0.050000
Training Epoch: 21 [9728/50000]	Loss: 1.0108	LR: 0.050000
Training Epoch: 21 [9856/50000]	Loss: 1.1562	LR: 0.050000
Training Epoch: 21 [9984/50000]	Loss: 1.2171	LR: 0.050000
Training Epoch: 21 [10112/50000]	Loss: 1.1258	LR: 0.050000
Training Epoch: 21 [10240/50000]	Loss: 1.2509	LR: 0.050000
Training Epoch: 21 [10368/50000]	Loss: 1.3011	LR: 0.050000
Training Epoch: 21 [10496/50000]	Loss: 1.2621	LR: 0.050000
Training Epoch: 21 [10624/50000]	Loss: 1.3222	LR: 0.050000
Training Epoch: 21 [10752/50000]	Loss: 1.0632	LR: 0.050000
Training Epoch: 21 [10880/50000]	Loss: 1.3169	LR: 0.050000
Training Epoch: 21 [11008/50000]	Loss: 1.1115	LR: 0.050000
Training Epoch: 21 [11136/50000]	Loss: 1.1925	LR: 0.050000
Training Epoch: 21 [11264/50000]	Loss: 1.2388	LR: 0.050000
Training Epoch: 21 [11392/50000]	Loss: 1.1943	LR: 0.050000
Training Epoch: 21 [11520/50000]	Loss: 1.2471	LR: 0.050000
Training Epoch: 21 [11648/50000]	Loss: 1.0446	LR: 0.050000
Training Epoch: 21 [11776/50000]	Loss: 0.9559	LR: 0.050000
Training Epoch: 21 [11904/50000]	Loss: 1.1982	LR: 0.050000
Training Epoch: 21 [12032/50000]	Loss: 1.2228	LR: 0.050000
Training Epoch: 21 [12160/50000]	Loss: 1.2541	LR: 0.050000
Training Epoch: 21 [12288/50000]	Loss: 1.0266	LR: 0.050000
Training Epoch: 21 [12416/50000]	Loss: 0.9297	LR: 0.050000
Training Epoch: 21 [12544/50000]	Loss: 1.1497	LR: 0.050000
Training Epoch: 21 [12672/50000]	Loss: 1.2459	LR: 0.050000
Training Epoch: 21 [12800/50000]	Loss: 1.1494	LR: 0.050000
Training Epoch: 21 [12928/50000]	Loss: 1.1085	LR: 0.050000
Training Epoch: 21 [13056/50000]	Loss: 1.2229	LR: 0.050000
Training Epoch: 21 [13184/50000]	Loss: 0.9991	LR: 0.050000
Training Epoch: 21 [13312/50000]	Loss: 1.3424	LR: 0.050000
Training Epoch: 21 [13440/50000]	Loss: 1.0014	LR: 0.050000
Training Epoch: 21 [13568/50000]	Loss: 1.1318	LR: 0.050000
Training Epoch: 21 [13696/50000]	Loss: 1.2353	LR: 0.050000
Training Epoch: 21 [13824/50000]	Loss: 1.1682	LR: 0.050000
Training Epoch: 21 [13952/50000]	Loss: 1.0576	LR: 0.050000
Training Epoch: 21 [14080/50000]	Loss: 0.9955	LR: 0.050000
Training Epoch: 21 [14208/50000]	Loss: 0.9868	LR: 0.050000
Training Epoch: 21 [14336/50000]	Loss: 1.0936	LR: 0.050000
Training Epoch: 21 [14464/50000]	Loss: 1.0887	LR: 0.050000
Training Epoch: 21 [14592/50000]	Loss: 1.1417	LR: 0.050000
Training Epoch: 21 [14720/50000]	Loss: 1.1874	LR: 0.050000
Training Epoch: 21 [14848/50000]	Loss: 1.0890	LR: 0.050000
Training Epoch: 21 [14976/50000]	Loss: 1.4348	LR: 0.050000
Training Epoch: 21 [15104/50000]	Loss: 1.0614	LR: 0.050000
Training Epoch: 21 [15232/50000]	Loss: 1.1874	LR: 0.050000
Training Epoch: 21 [15360/50000]	Loss: 1.1643	LR: 0.050000
Training Epoch: 21 [15488/50000]	Loss: 1.1759	LR: 0.050000
Training Epoch: 21 [15616/50000]	Loss: 1.0803	LR: 0.050000
Training Epoch: 21 [15744/50000]	Loss: 1.0848	LR: 0.050000
Training Epoch: 21 [15872/50000]	Loss: 0.9379	LR: 0.050000
Training Epoch: 21 [16000/50000]	Loss: 1.3114	LR: 0.050000
Training Epoch: 21 [16128/50000]	Loss: 1.0009	LR: 0.050000
Training Epoch: 21 [16256/50000]	Loss: 1.1664	LR: 0.050000
Training Epoch: 21 [16384/50000]	Loss: 0.9216	LR: 0.050000
Training Epoch: 21 [16512/50000]	Loss: 0.8928	LR: 0.050000
Training Epoch: 21 [16640/50000]	Loss: 1.2011	LR: 0.050000
Training Epoch: 21 [16768/50000]	Loss: 1.0687	LR: 0.050000
Training Epoch: 21 [16896/50000]	Loss: 1.2490	LR: 0.050000
Training Epoch: 21 [17024/50000]	Loss: 1.2708	LR: 0.050000
Training Epoch: 21 [17152/50000]	Loss: 1.3344	LR: 0.050000
Training Epoch: 21 [17280/50000]	Loss: 1.0052	LR: 0.050000
Training Epoch: 21 [17408/50000]	Loss: 1.3192	LR: 0.050000
Training Epoch: 21 [17536/50000]	Loss: 1.2435	LR: 0.050000
Training Epoch: 21 [17664/50000]	Loss: 1.1544	LR: 0.050000
Training Epoch: 21 [17792/50000]	Loss: 1.1269	LR: 0.050000
Training Epoch: 21 [17920/50000]	Loss: 1.1282	LR: 0.050000
Training Epoch: 21 [18048/50000]	Loss: 1.2947	LR: 0.050000
Training Epoch: 21 [18176/50000]	Loss: 0.8863	LR: 0.050000
Training Epoch: 21 [18304/50000]	Loss: 1.1443	LR: 0.050000
Training Epoch: 21 [18432/50000]	Loss: 1.0835	LR: 0.050000
Training Epoch: 21 [18560/50000]	Loss: 1.2991	LR: 0.050000
Training Epoch: 21 [18688/50000]	Loss: 1.2353	LR: 0.050000
Training Epoch: 21 [18816/50000]	Loss: 1.2331	LR: 0.050000
Training Epoch: 21 [18944/50000]	Loss: 1.3211	LR: 0.050000
Training Epoch: 21 [19072/50000]	Loss: 1.3811	LR: 0.050000
Training Epoch: 21 [19200/50000]	Loss: 1.1201	LR: 0.050000
Training Epoch: 21 [19328/50000]	Loss: 1.1011	LR: 0.050000
Training Epoch: 21 [19456/50000]	Loss: 1.2433	LR: 0.050000
Training Epoch: 21 [19584/50000]	Loss: 0.9556	LR: 0.050000
Training Epoch: 21 [19712/50000]	Loss: 1.2488	LR: 0.050000
Training Epoch: 21 [19840/50000]	Loss: 1.1415	LR: 0.050000
Training Epoch: 21 [19968/50000]	Loss: 0.9270	LR: 0.050000
Training Epoch: 21 [20096/50000]	Loss: 1.3037	LR: 0.050000
Training Epoch: 21 [20224/50000]	Loss: 1.2108	LR: 0.050000
Training Epoch: 21 [20352/50000]	Loss: 1.2941	LR: 0.050000
Training Epoch: 21 [20480/50000]	Loss: 1.2277	LR: 0.050000
Training Epoch: 21 [20608/50000]	Loss: 1.2108	LR: 0.050000
Training Epoch: 21 [20736/50000]	Loss: 1.2134	LR: 0.050000
Training Epoch: 21 [20864/50000]	Loss: 1.1325	LR: 0.050000
Training Epoch: 21 [20992/50000]	Loss: 1.2335	LR: 0.050000
Training Epoch: 21 [21120/50000]	Loss: 1.0249	LR: 0.050000
Training Epoch: 21 [21248/50000]	Loss: 1.1548	LR: 0.050000
Training Epoch: 21 [21376/50000]	Loss: 1.0870	LR: 0.050000
Training Epoch: 21 [21504/50000]	Loss: 1.3731	LR: 0.050000
Training Epoch: 21 [21632/50000]	Loss: 1.1221	LR: 0.050000
Training Epoch: 21 [21760/50000]	Loss: 1.1177	LR: 0.050000
Training Epoch: 21 [21888/50000]	Loss: 1.2667	LR: 0.050000
Training Epoch: 21 [22016/50000]	Loss: 1.2306	LR: 0.050000
Training Epoch: 21 [22144/50000]	Loss: 1.2891	LR: 0.050000
Training Epoch: 21 [22272/50000]	Loss: 1.4372	LR: 0.050000
Training Epoch: 21 [22400/50000]	Loss: 1.1560	LR: 0.050000
Training Epoch: 21 [22528/50000]	Loss: 1.3384	LR: 0.050000
Training Epoch: 21 [22656/50000]	Loss: 1.3521	LR: 0.050000
Training Epoch: 21 [22784/50000]	Loss: 1.2261	LR: 0.050000
Training Epoch: 21 [22912/50000]	Loss: 1.5095	LR: 0.050000
Training Epoch: 21 [23040/50000]	Loss: 1.1399	LR: 0.050000
Training Epoch: 21 [23168/50000]	Loss: 1.0347	LR: 0.050000
Training Epoch: 21 [23296/50000]	Loss: 1.0460	LR: 0.050000
Training Epoch: 21 [23424/50000]	Loss: 1.3756	LR: 0.050000
Training Epoch: 21 [23552/50000]	Loss: 1.1196	LR: 0.050000
Training Epoch: 21 [23680/50000]	Loss: 1.2650	LR: 0.050000
Training Epoch: 21 [23808/50000]	Loss: 1.1174	LR: 0.050000
Training Epoch: 21 [23936/50000]	Loss: 1.0646	LR: 0.050000
Training Epoch: 21 [24064/50000]	Loss: 0.9954	LR: 0.050000
Training Epoch: 21 [24192/50000]	Loss: 1.3415	LR: 0.050000
Training Epoch: 21 [24320/50000]	Loss: 1.3538	LR: 0.050000
Training Epoch: 21 [24448/50000]	Loss: 0.9057	LR: 0.050000
Training Epoch: 21 [24576/50000]	Loss: 0.9646	LR: 0.050000
Training Epoch: 21 [24704/50000]	Loss: 1.0857	LR: 0.050000
Training Epoch: 21 [24832/50000]	Loss: 1.1734	LR: 0.050000
Training Epoch: 21 [24960/50000]	Loss: 1.1844	LR: 0.050000
Training Epoch: 21 [25088/50000]	Loss: 0.9348	LR: 0.050000
Training Epoch: 21 [25216/50000]	Loss: 1.6092	LR: 0.050000
Training Epoch: 21 [25344/50000]	Loss: 1.1250	LR: 0.050000
Training Epoch: 21 [25472/50000]	Loss: 1.1983	LR: 0.050000
Training Epoch: 21 [25600/50000]	Loss: 1.0581	LR: 0.050000
Training Epoch: 21 [25728/50000]	Loss: 1.0058	LR: 0.050000
Training Epoch: 21 [25856/50000]	Loss: 1.2793	LR: 0.050000
Training Epoch: 21 [25984/50000]	Loss: 1.2216	LR: 0.050000
Training Epoch: 21 [26112/50000]	Loss: 1.1281	LR: 0.050000
Training Epoch: 21 [26240/50000]	Loss: 1.1241	LR: 0.050000
Training Epoch: 21 [26368/50000]	Loss: 1.0798	LR: 0.050000
Training Epoch: 21 [26496/50000]	Loss: 1.1842	LR: 0.050000
Training Epoch: 21 [26624/50000]	Loss: 1.0023	LR: 0.050000
Training Epoch: 21 [26752/50000]	Loss: 1.0485	LR: 0.050000
Training Epoch: 21 [26880/50000]	Loss: 1.3738	LR: 0.050000
Training Epoch: 21 [27008/50000]	Loss: 1.2365	LR: 0.050000
Training Epoch: 21 [27136/50000]	Loss: 1.0806	LR: 0.050000
Training Epoch: 21 [27264/50000]	Loss: 1.1251	LR: 0.050000
Training Epoch: 21 [27392/50000]	Loss: 1.2097	LR: 0.050000
Training Epoch: 21 [27520/50000]	Loss: 1.2614	LR: 0.050000
Training Epoch: 21 [27648/50000]	Loss: 1.2063	LR: 0.050000
Training Epoch: 21 [27776/50000]	Loss: 1.1804	LR: 0.050000
Training Epoch: 21 [27904/50000]	Loss: 1.0430	LR: 0.050000
Training Epoch: 21 [28032/50000]	Loss: 1.4219	LR: 0.050000
Training Epoch: 21 [28160/50000]	Loss: 1.3598	LR: 0.050000
Training Epoch: 21 [28288/50000]	Loss: 1.3320	LR: 0.050000
Training Epoch: 21 [28416/50000]	Loss: 1.1449	LR: 0.050000
Training Epoch: 21 [28544/50000]	Loss: 1.3467	LR: 0.050000
Training Epoch: 21 [28672/50000]	Loss: 1.3767	LR: 0.050000
Training Epoch: 21 [28800/50000]	Loss: 1.2140	LR: 0.050000
Training Epoch: 21 [28928/50000]	Loss: 1.1743	LR: 0.050000
Training Epoch: 21 [29056/50000]	Loss: 1.4147	LR: 0.050000
Training Epoch: 21 [29184/50000]	Loss: 1.1088	LR: 0.050000
Training Epoch: 21 [29312/50000]	Loss: 1.1468	LR: 0.050000
Training Epoch: 21 [29440/50000]	Loss: 1.2153	LR: 0.050000
Training Epoch: 21 [29568/50000]	Loss: 1.1252	LR: 0.050000
Training Epoch: 21 [29696/50000]	Loss: 1.3367	LR: 0.050000
Training Epoch: 21 [29824/50000]	Loss: 1.0683	LR: 0.050000
Training Epoch: 21 [29952/50000]	Loss: 1.3702	LR: 0.050000
Training Epoch: 21 [30080/50000]	Loss: 1.1731	LR: 0.050000
Training Epoch: 21 [30208/50000]	Loss: 1.2008	LR: 0.050000
Training Epoch: 21 [30336/50000]	Loss: 1.0824	LR: 0.050000
Training Epoch: 21 [30464/50000]	Loss: 1.1641	LR: 0.050000
Training Epoch: 21 [30592/50000]	Loss: 1.2369	LR: 0.050000
Training Epoch: 21 [30720/50000]	Loss: 1.1620	LR: 0.050000
Training Epoch: 21 [30848/50000]	Loss: 1.0999	LR: 0.050000
Training Epoch: 21 [30976/50000]	Loss: 1.2298	LR: 0.050000
Training Epoch: 21 [31104/50000]	Loss: 1.3190	LR: 0.050000
Training Epoch: 21 [31232/50000]	Loss: 1.1339	LR: 0.050000
Training Epoch: 21 [31360/50000]	Loss: 1.2464	LR: 0.050000
Training Epoch: 21 [31488/50000]	Loss: 1.3374	LR: 0.050000
Training Epoch: 21 [31616/50000]	Loss: 1.3364	LR: 0.050000
Training Epoch: 21 [31744/50000]	Loss: 1.3412	LR: 0.050000
Training Epoch: 21 [31872/50000]	Loss: 1.1618	LR: 0.050000
Training Epoch: 21 [32000/50000]	Loss: 0.8702	LR: 0.050000
Training Epoch: 21 [32128/50000]	Loss: 1.6030	LR: 0.050000
Training Epoch: 21 [32256/50000]	Loss: 1.2570	LR: 0.050000
Training Epoch: 21 [32384/50000]	Loss: 1.2909	LR: 0.050000
Training Epoch: 21 [32512/50000]	Loss: 1.3496	LR: 0.050000
Training Epoch: 21 [32640/50000]	Loss: 1.2675	LR: 0.050000
Training Epoch: 21 [32768/50000]	Loss: 1.1663	LR: 0.050000
Training Epoch: 21 [32896/50000]	Loss: 1.2690	LR: 0.050000
Training Epoch: 21 [33024/50000]	Loss: 1.4124	LR: 0.050000
Training Epoch: 21 [33152/50000]	Loss: 1.1423	LR: 0.050000
Training Epoch: 21 [33280/50000]	Loss: 0.9555	LR: 0.050000
Training Epoch: 21 [33408/50000]	Loss: 1.4106	LR: 0.050000
Training Epoch: 21 [33536/50000]	Loss: 1.0202	LR: 0.050000
Training Epoch: 21 [33664/50000]	Loss: 1.0508	LR: 0.050000
Training Epoch: 21 [33792/50000]	Loss: 1.4013	LR: 0.050000
Training Epoch: 21 [33920/50000]	Loss: 1.2462	LR: 0.050000
Training Epoch: 21 [34048/50000]	Loss: 1.1842	LR: 0.050000
Training Epoch: 21 [34176/50000]	Loss: 1.0953	LR: 0.050000
Training Epoch: 21 [34304/50000]	Loss: 1.1877	LR: 0.050000
Training Epoch: 21 [34432/50000]	Loss: 1.0471	LR: 0.050000
Training Epoch: 21 [34560/50000]	Loss: 1.0466	LR: 0.050000
Training Epoch: 21 [34688/50000]	Loss: 1.3135	LR: 0.050000
Training Epoch: 21 [34816/50000]	Loss: 1.2768	LR: 0.050000
Training Epoch: 21 [34944/50000]	Loss: 1.3932	LR: 0.050000
Training Epoch: 21 [35072/50000]	Loss: 1.2460	LR: 0.050000
Training Epoch: 21 [35200/50000]	Loss: 1.1110	LR: 0.050000
Training Epoch: 21 [35328/50000]	Loss: 1.3109	LR: 0.050000
Training Epoch: 21 [35456/50000]	Loss: 1.0329	LR: 0.050000
Training Epoch: 21 [35584/50000]	Loss: 1.1957	LR: 0.050000
Training Epoch: 21 [35712/50000]	Loss: 1.1833	LR: 0.050000
Training Epoch: 21 [35840/50000]	Loss: 1.3028	LR: 0.050000
Training Epoch: 21 [35968/50000]	Loss: 1.2595	LR: 0.050000
Training Epoch: 21 [36096/50000]	Loss: 1.0847	LR: 0.050000
Training Epoch: 21 [36224/50000]	Loss: 1.2620	LR: 0.050000
Training Epoch: 21 [36352/50000]	Loss: 1.1748	LR: 0.050000
Training Epoch: 21 [36480/50000]	Loss: 1.5672	LR: 0.050000
Training Epoch: 21 [36608/50000]	Loss: 0.9164	LR: 0.050000
Training Epoch: 21 [36736/50000]	Loss: 1.3758	LR: 0.050000
Training Epoch: 21 [36864/50000]	Loss: 1.3292	LR: 0.050000
Training Epoch: 21 [36992/50000]	Loss: 1.2516	LR: 0.050000
Training Epoch: 21 [37120/50000]	Loss: 1.1483	LR: 0.050000
Training Epoch: 21 [37248/50000]	Loss: 1.1307	LR: 0.050000
Training Epoch: 21 [37376/50000]	Loss: 1.2802	LR: 0.050000
Training Epoch: 21 [37504/50000]	Loss: 1.2018	LR: 0.050000
Training Epoch: 21 [37632/50000]	Loss: 1.0868	LR: 0.050000
Training Epoch: 21 [37760/50000]	Loss: 1.2175	LR: 0.050000
Training Epoch: 21 [37888/50000]	Loss: 1.0046	LR: 0.050000
Training Epoch: 21 [38016/50000]	Loss: 0.9909	LR: 0.050000
Training Epoch: 21 [38144/50000]	Loss: 1.3513	LR: 0.050000
Training Epoch: 21 [38272/50000]	Loss: 1.1866	LR: 0.050000
Training Epoch: 21 [38400/50000]	Loss: 1.2219	LR: 0.050000
Training Epoch: 21 [38528/50000]	Loss: 1.4050	LR: 0.050000
Training Epoch: 21 [38656/50000]	Loss: 1.4851	LR: 0.050000
Training Epoch: 21 [38784/50000]	Loss: 1.1441	LR: 0.050000
Training Epoch: 21 [38912/50000]	Loss: 0.9949	LR: 0.050000
Training Epoch: 21 [39040/50000]	Loss: 1.3319	LR: 0.050000
Training Epoch: 21 [39168/50000]	Loss: 1.3983	LR: 0.050000
Training Epoch: 21 [39296/50000]	Loss: 1.2823	LR: 0.050000
Training Epoch: 21 [39424/50000]	Loss: 1.2227	LR: 0.050000
Training Epoch: 21 [39552/50000]	Loss: 1.4517	LR: 0.050000
Training Epoch: 21 [39680/50000]	Loss: 1.1477	LR: 0.050000
Training Epoch: 21 [39808/50000]	Loss: 1.0918	LR: 0.050000
Training Epoch: 21 [39936/50000]	Loss: 1.3753	LR: 0.050000
Training Epoch: 21 [40064/50000]	Loss: 1.3765	LR: 0.050000
Training Epoch: 21 [40192/50000]	Loss: 1.2579	LR: 0.050000
Training Epoch: 21 [40320/50000]	Loss: 1.3719	LR: 0.050000
Training Epoch: 21 [40448/50000]	Loss: 1.2004	LR: 0.050000
Training Epoch: 21 [40576/50000]	Loss: 1.2911	LR: 0.050000
Training Epoch: 21 [40704/50000]	Loss: 1.1948	LR: 0.050000
Training Epoch: 21 [40832/50000]	Loss: 1.0446	LR: 0.050000
Training Epoch: 21 [40960/50000]	Loss: 1.2375	LR: 0.050000
Training Epoch: 21 [41088/50000]	Loss: 1.0086	LR: 0.050000
Training Epoch: 21 [41216/50000]	Loss: 1.1833	LR: 0.050000
Training Epoch: 21 [41344/50000]	Loss: 1.2014	LR: 0.050000
Training Epoch: 21 [41472/50000]	Loss: 1.1628	LR: 0.050000
Training Epoch: 21 [41600/50000]	Loss: 1.2186	LR: 0.050000
Training Epoch: 21 [41728/50000]	Loss: 1.3947	LR: 0.050000
Training Epoch: 21 [41856/50000]	Loss: 1.1771	LR: 0.050000
Training Epoch: 21 [41984/50000]	Loss: 1.4928	LR: 0.050000
Training Epoch: 21 [42112/50000]	Loss: 1.3645	LR: 0.050000
Training Epoch: 21 [42240/50000]	Loss: 1.2110	LR: 0.050000
Training Epoch: 21 [42368/50000]	Loss: 1.3258	LR: 0.050000
Training Epoch: 21 [42496/50000]	Loss: 1.3086	LR: 0.050000
Training Epoch: 21 [42624/50000]	Loss: 1.1863	LR: 0.050000
Training Epoch: 21 [42752/50000]	Loss: 1.0891	LR: 0.050000
Training Epoch: 21 [42880/50000]	Loss: 1.1947	LR: 0.050000
Training Epoch: 21 [43008/50000]	Loss: 1.2003	LR: 0.050000
Training Epoch: 21 [43136/50000]	Loss: 1.1435	LR: 0.050000
Training Epoch: 21 [43264/50000]	Loss: 1.2592	LR: 0.050000
Training Epoch: 21 [43392/50000]	Loss: 1.1003	LR: 0.050000
Training Epoch: 21 [43520/50000]	Loss: 1.1653	LR: 0.050000
Training Epoch: 21 [43648/50000]	Loss: 1.2245	LR: 0.050000
Training Epoch: 21 [43776/50000]	Loss: 1.3282	LR: 0.050000
Training Epoch: 21 [43904/50000]	Loss: 1.0950	LR: 0.050000
Training Epoch: 21 [44032/50000]	Loss: 1.1370	LR: 0.050000
Training Epoch: 21 [44160/50000]	Loss: 1.1323	LR: 0.050000
Training Epoch: 21 [44288/50000]	Loss: 1.3567	LR: 0.050000
Training Epoch: 21 [44416/50000]	Loss: 1.3566	LR: 0.050000
Training Epoch: 21 [44544/50000]	Loss: 1.2432	LR: 0.050000
Training Epoch: 21 [44672/50000]	Loss: 1.2508	LR: 0.050000
Training Epoch: 21 [44800/50000]	Loss: 1.1930	LR: 0.050000
Training Epoch: 21 [44928/50000]	Loss: 1.1700	LR: 0.050000
Training Epoch: 21 [45056/50000]	Loss: 1.1243	LR: 0.050000
Training Epoch: 21 [45184/50000]	Loss: 1.0952	LR: 0.050000
Training Epoch: 21 [45312/50000]	Loss: 1.1080	LR: 0.050000
Training Epoch: 21 [45440/50000]	Loss: 1.2062	LR: 0.050000
Training Epoch: 21 [45568/50000]	Loss: 1.2050	LR: 0.050000
Training Epoch: 21 [45696/50000]	Loss: 1.2365	LR: 0.050000
Training Epoch: 21 [45824/50000]	Loss: 0.9583	LR: 0.050000
Training Epoch: 21 [45952/50000]	Loss: 1.1553	LR: 0.050000
Training Epoch: 21 [46080/50000]	Loss: 1.5216	LR: 0.050000
Training Epoch: 21 [46208/50000]	Loss: 1.2603	LR: 0.050000
Training Epoch: 21 [46336/50000]	Loss: 1.2410	LR: 0.050000
Training Epoch: 21 [46464/50000]	Loss: 1.1782	LR: 0.050000
Training Epoch: 21 [46592/50000]	Loss: 1.3415	LR: 0.050000
Training Epoch: 21 [46720/50000]	Loss: 0.9939	LR: 0.050000
Training Epoch: 21 [46848/50000]	Loss: 1.2314	LR: 0.050000
Training Epoch: 21 [46976/50000]	Loss: 1.1087	LR: 0.050000
Training Epoch: 21 [47104/50000]	Loss: 1.2693	LR: 0.050000
Training Epoch: 21 [47232/50000]	Loss: 1.4488	LR: 0.050000
Training Epoch: 21 [47360/50000]	Loss: 1.0354	LR: 0.050000
Training Epoch: 21 [47488/50000]	Loss: 1.5215	LR: 0.050000
Training Epoch: 21 [47616/50000]	Loss: 1.3202	LR: 0.050000
Training Epoch: 21 [47744/50000]	Loss: 1.0110	LR: 0.050000
Training Epoch: 21 [47872/50000]	Loss: 1.2610	LR: 0.050000
Training Epoch: 21 [48000/50000]	Loss: 1.2326	LR: 0.050000
Training Epoch: 21 [48128/50000]	Loss: 1.3449	LR: 0.050000
Training Epoch: 21 [48256/50000]	Loss: 1.2712	LR: 0.050000
Training Epoch: 21 [48384/50000]	Loss: 1.2549	LR: 0.050000
Training Epoch: 21 [48512/50000]	Loss: 1.2447	LR: 0.050000
Training Epoch: 21 [48640/50000]	Loss: 1.4097	LR: 0.050000
Training Epoch: 21 [48768/50000]	Loss: 1.3995	LR: 0.050000
Training Epoch: 21 [48896/50000]	Loss: 1.0773	LR: 0.050000
Training Epoch: 21 [49024/50000]	Loss: 1.1137	LR: 0.050000
Training Epoch: 21 [49152/50000]	Loss: 1.4435	LR: 0.050000
Training Epoch: 21 [49280/50000]	Loss: 1.0336	LR: 0.050000
Training Epoch: 21 [49408/50000]	Loss: 1.2114	LR: 0.050000
Training Epoch: 21 [49536/50000]	Loss: 1.1676	LR: 0.050000
Training Epoch: 21 [49664/50000]	Loss: 1.3213	LR: 0.050000
Training Epoch: 21 [49792/50000]	Loss: 1.2620	LR: 0.050000
Training Epoch: 21 [49920/50000]	Loss: 1.4175	LR: 0.050000
Training Epoch: 21 [50000/50000]	Loss: 1.3872	LR: 0.050000
epoch 21 training time consumed: 53.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   77071 GB |   77071 GB |
|       from large pool |  123392 KB |    1034 MB |   76995 GB |   76995 GB |
|       from small pool |   10798 KB |      13 MB |      75 GB |      75 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   77071 GB |   77071 GB |
|       from large pool |  123392 KB |    1034 MB |   76995 GB |   76995 GB |
|       from small pool |   10798 KB |      13 MB |      75 GB |      75 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   33918 GB |   33918 GB |
|       from large pool |  155136 KB |  433088 KB |   33834 GB |   33834 GB |
|       from small pool |    1490 KB |    3494 KB |      83 GB |      83 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    2974 K  |    2974 K  |
|       from large pool |      24    |      65    |    1552 K  |    1552 K  |
|       from small pool |     231    |     274    |    1421 K  |    1421 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    2974 K  |    2974 K  |
|       from large pool |      24    |      65    |    1552 K  |    1552 K  |
|       from small pool |     231    |     274    |    1421 K  |    1421 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1472 K  |    1472 K  |
|       from large pool |       9    |      14    |     751 K  |     751 K  |
|       from small pool |      12    |      16    |     721 K  |     721 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 21, Average loss: 0.0126, Accuracy: 0.5769, Time consumed:3.44s

Training Epoch: 22 [128/50000]	Loss: 1.1396	LR: 0.050000
Training Epoch: 22 [256/50000]	Loss: 0.8726	LR: 0.050000
Training Epoch: 22 [384/50000]	Loss: 1.1189	LR: 0.050000
Training Epoch: 22 [512/50000]	Loss: 0.9663	LR: 0.050000
Training Epoch: 22 [640/50000]	Loss: 1.0214	LR: 0.050000
Training Epoch: 22 [768/50000]	Loss: 1.1887	LR: 0.050000
Training Epoch: 22 [896/50000]	Loss: 0.8376	LR: 0.050000
Training Epoch: 22 [1024/50000]	Loss: 1.1351	LR: 0.050000
Training Epoch: 22 [1152/50000]	Loss: 1.3485	LR: 0.050000
Training Epoch: 22 [1280/50000]	Loss: 0.9853	LR: 0.050000
Training Epoch: 22 [1408/50000]	Loss: 1.0557	LR: 0.050000
Training Epoch: 22 [1536/50000]	Loss: 1.0164	LR: 0.050000
Training Epoch: 22 [1664/50000]	Loss: 1.1514	LR: 0.050000
Training Epoch: 22 [1792/50000]	Loss: 1.0907	LR: 0.050000
Training Epoch: 22 [1920/50000]	Loss: 1.0428	LR: 0.050000
Training Epoch: 22 [2048/50000]	Loss: 1.1561	LR: 0.050000
Training Epoch: 22 [2176/50000]	Loss: 1.1401	LR: 0.050000
Training Epoch: 22 [2304/50000]	Loss: 1.2608	LR: 0.050000
Training Epoch: 22 [2432/50000]	Loss: 0.8945	LR: 0.050000
Training Epoch: 22 [2560/50000]	Loss: 1.2291	LR: 0.050000
Training Epoch: 22 [2688/50000]	Loss: 1.1708	LR: 0.050000
Training Epoch: 22 [2816/50000]	Loss: 1.2427	LR: 0.050000
Training Epoch: 22 [2944/50000]	Loss: 1.0538	LR: 0.050000
Training Epoch: 22 [3072/50000]	Loss: 0.9461	LR: 0.050000
Training Epoch: 22 [3200/50000]	Loss: 0.9681	LR: 0.050000
Training Epoch: 22 [3328/50000]	Loss: 1.1456	LR: 0.050000
Training Epoch: 22 [3456/50000]	Loss: 1.2283	LR: 0.050000
Training Epoch: 22 [3584/50000]	Loss: 1.2549	LR: 0.050000
Training Epoch: 22 [3712/50000]	Loss: 1.2837	LR: 0.050000
Training Epoch: 22 [3840/50000]	Loss: 1.1143	LR: 0.050000
Training Epoch: 22 [3968/50000]	Loss: 1.0922	LR: 0.050000
Training Epoch: 22 [4096/50000]	Loss: 1.0630	LR: 0.050000
Training Epoch: 22 [4224/50000]	Loss: 1.1013	LR: 0.050000
Training Epoch: 22 [4352/50000]	Loss: 1.1591	LR: 0.050000
Training Epoch: 22 [4480/50000]	Loss: 1.3111	LR: 0.050000
Training Epoch: 22 [4608/50000]	Loss: 1.1215	LR: 0.050000
Training Epoch: 22 [4736/50000]	Loss: 1.0953	LR: 0.050000
Training Epoch: 22 [4864/50000]	Loss: 1.0036	LR: 0.050000
Training Epoch: 22 [4992/50000]	Loss: 1.0838	LR: 0.050000
Training Epoch: 22 [5120/50000]	Loss: 0.9141	LR: 0.050000
Training Epoch: 22 [5248/50000]	Loss: 1.0082	LR: 0.050000
Training Epoch: 22 [5376/50000]	Loss: 1.1073	LR: 0.050000
Training Epoch: 22 [5504/50000]	Loss: 1.1510	LR: 0.050000
Training Epoch: 22 [5632/50000]	Loss: 0.8650	LR: 0.050000
Training Epoch: 22 [5760/50000]	Loss: 1.0466	LR: 0.050000
Training Epoch: 22 [5888/50000]	Loss: 1.1253	LR: 0.050000
Training Epoch: 22 [6016/50000]	Loss: 1.0946	LR: 0.050000
Training Epoch: 22 [6144/50000]	Loss: 1.1912	LR: 0.050000
Training Epoch: 22 [6272/50000]	Loss: 0.9518	LR: 0.050000
Training Epoch: 22 [6400/50000]	Loss: 1.0562	LR: 0.050000
Training Epoch: 22 [6528/50000]	Loss: 1.1306	LR: 0.050000
Training Epoch: 22 [6656/50000]	Loss: 1.0079	LR: 0.050000
Training Epoch: 22 [6784/50000]	Loss: 1.2090	LR: 0.050000
Training Epoch: 22 [6912/50000]	Loss: 1.1026	LR: 0.050000
Training Epoch: 22 [7040/50000]	Loss: 1.0886	LR: 0.050000
Training Epoch: 22 [7168/50000]	Loss: 1.1231	LR: 0.050000
Training Epoch: 22 [7296/50000]	Loss: 1.1844	LR: 0.050000
Training Epoch: 22 [7424/50000]	Loss: 1.1666	LR: 0.050000
Training Epoch: 22 [7552/50000]	Loss: 1.1738	LR: 0.050000
Training Epoch: 22 [7680/50000]	Loss: 0.9723	LR: 0.050000
Training Epoch: 22 [7808/50000]	Loss: 0.8629	LR: 0.050000
Training Epoch: 22 [7936/50000]	Loss: 1.1139	LR: 0.050000
Training Epoch: 22 [8064/50000]	Loss: 1.1123	LR: 0.050000
Training Epoch: 22 [8192/50000]	Loss: 1.1667	LR: 0.050000
Training Epoch: 22 [8320/50000]	Loss: 1.1405	LR: 0.050000
Training Epoch: 22 [8448/50000]	Loss: 1.0365	LR: 0.050000
Training Epoch: 22 [8576/50000]	Loss: 1.0198	LR: 0.050000
Training Epoch: 22 [8704/50000]	Loss: 1.0596	LR: 0.050000
Training Epoch: 22 [8832/50000]	Loss: 0.9968	LR: 0.050000
Training Epoch: 22 [8960/50000]	Loss: 1.0334	LR: 0.050000
Training Epoch: 22 [9088/50000]	Loss: 1.2856	LR: 0.050000
Training Epoch: 22 [9216/50000]	Loss: 1.0219	LR: 0.050000
Training Epoch: 22 [9344/50000]	Loss: 1.1165	LR: 0.050000
Training Epoch: 22 [9472/50000]	Loss: 1.0715	LR: 0.050000
Training Epoch: 22 [9600/50000]	Loss: 1.2858	LR: 0.050000
Training Epoch: 22 [9728/50000]	Loss: 1.1651	LR: 0.050000
Training Epoch: 22 [9856/50000]	Loss: 1.0032	LR: 0.050000
Training Epoch: 22 [9984/50000]	Loss: 0.7965	LR: 0.050000
Training Epoch: 22 [10112/50000]	Loss: 1.0647	LR: 0.050000
Training Epoch: 22 [10240/50000]	Loss: 0.9360	LR: 0.050000
Training Epoch: 22 [10368/50000]	Loss: 1.1406	LR: 0.050000
Training Epoch: 22 [10496/50000]	Loss: 0.9533	LR: 0.050000
Training Epoch: 22 [10624/50000]	Loss: 1.3336	LR: 0.050000
Training Epoch: 22 [10752/50000]	Loss: 1.1240	LR: 0.050000
Training Epoch: 22 [10880/50000]	Loss: 1.2146	LR: 0.050000
Training Epoch: 22 [11008/50000]	Loss: 1.1647	LR: 0.050000
Training Epoch: 22 [11136/50000]	Loss: 1.0972	LR: 0.050000
Training Epoch: 22 [11264/50000]	Loss: 0.8705	LR: 0.050000
Training Epoch: 22 [11392/50000]	Loss: 1.0219	LR: 0.050000
Training Epoch: 22 [11520/50000]	Loss: 1.3342	LR: 0.050000
Training Epoch: 22 [11648/50000]	Loss: 1.1342	LR: 0.050000
Training Epoch: 22 [11776/50000]	Loss: 1.2850	LR: 0.050000
Training Epoch: 22 [11904/50000]	Loss: 1.0805	LR: 0.050000
Training Epoch: 22 [12032/50000]	Loss: 1.1940	LR: 0.050000
Training Epoch: 22 [12160/50000]	Loss: 1.2554	LR: 0.050000
Training Epoch: 22 [12288/50000]	Loss: 1.0129	LR: 0.050000
Training Epoch: 22 [12416/50000]	Loss: 1.0305	LR: 0.050000
Training Epoch: 22 [12544/50000]	Loss: 1.1341	LR: 0.050000
Training Epoch: 22 [12672/50000]	Loss: 0.9641	LR: 0.050000
Training Epoch: 22 [12800/50000]	Loss: 1.4485	LR: 0.050000
Training Epoch: 22 [12928/50000]	Loss: 1.0999	LR: 0.050000
Training Epoch: 22 [13056/50000]	Loss: 1.1293	LR: 0.050000
Training Epoch: 22 [13184/50000]	Loss: 1.0302	LR: 0.050000
Training Epoch: 22 [13312/50000]	Loss: 1.1788	LR: 0.050000
Training Epoch: 22 [13440/50000]	Loss: 1.2527	LR: 0.050000
Training Epoch: 22 [13568/50000]	Loss: 1.2047	LR: 0.050000
Training Epoch: 22 [13696/50000]	Loss: 1.1261	LR: 0.050000
Training Epoch: 22 [13824/50000]	Loss: 1.2395	LR: 0.050000
Training Epoch: 22 [13952/50000]	Loss: 1.2924	LR: 0.050000
Training Epoch: 22 [14080/50000]	Loss: 1.2540	LR: 0.050000
Training Epoch: 22 [14208/50000]	Loss: 1.2005	LR: 0.050000
Training Epoch: 22 [14336/50000]	Loss: 1.3691	LR: 0.050000
Training Epoch: 22 [14464/50000]	Loss: 1.1041	LR: 0.050000
Training Epoch: 22 [14592/50000]	Loss: 1.2275	LR: 0.050000
Training Epoch: 22 [14720/50000]	Loss: 1.1161	LR: 0.050000
Training Epoch: 22 [14848/50000]	Loss: 1.1219	LR: 0.050000
Training Epoch: 22 [14976/50000]	Loss: 1.1415	LR: 0.050000
Training Epoch: 22 [15104/50000]	Loss: 1.1678	LR: 0.050000
Training Epoch: 22 [15232/50000]	Loss: 0.9678	LR: 0.050000
Training Epoch: 22 [15360/50000]	Loss: 1.2234	LR: 0.050000
Training Epoch: 22 [15488/50000]	Loss: 1.1599	LR: 0.050000
Training Epoch: 22 [15616/50000]	Loss: 1.0264	LR: 0.050000
Training Epoch: 22 [15744/50000]	Loss: 0.9803	LR: 0.050000
Training Epoch: 22 [15872/50000]	Loss: 1.4090	LR: 0.050000
Training Epoch: 22 [16000/50000]	Loss: 1.1836	LR: 0.050000
Training Epoch: 22 [16128/50000]	Loss: 1.0845	LR: 0.050000
Training Epoch: 22 [16256/50000]	Loss: 1.0319	LR: 0.050000
Training Epoch: 22 [16384/50000]	Loss: 1.0574	LR: 0.050000
Training Epoch: 22 [16512/50000]	Loss: 1.1138	LR: 0.050000
Training Epoch: 22 [16640/50000]	Loss: 1.3008	LR: 0.050000
Training Epoch: 22 [16768/50000]	Loss: 1.3238	LR: 0.050000
Training Epoch: 22 [16896/50000]	Loss: 1.1021	LR: 0.050000
Training Epoch: 22 [17024/50000]	Loss: 1.2685	LR: 0.050000
Training Epoch: 22 [17152/50000]	Loss: 1.1742	LR: 0.050000
Training Epoch: 22 [17280/50000]	Loss: 1.0854	LR: 0.050000
Training Epoch: 22 [17408/50000]	Loss: 1.1490	LR: 0.050000
Training Epoch: 22 [17536/50000]	Loss: 1.2005	LR: 0.050000
Training Epoch: 22 [17664/50000]	Loss: 1.2061	LR: 0.050000
Training Epoch: 22 [17792/50000]	Loss: 1.1638	LR: 0.050000
Training Epoch: 22 [17920/50000]	Loss: 1.0094	LR: 0.050000
Training Epoch: 22 [18048/50000]	Loss: 1.2048	LR: 0.050000
Training Epoch: 22 [18176/50000]	Loss: 1.0953	LR: 0.050000
Training Epoch: 22 [18304/50000]	Loss: 1.4299	LR: 0.050000
Training Epoch: 22 [18432/50000]	Loss: 1.3420	LR: 0.050000
Training Epoch: 22 [18560/50000]	Loss: 1.0272	LR: 0.050000
Training Epoch: 22 [18688/50000]	Loss: 0.9963	LR: 0.050000
Training Epoch: 22 [18816/50000]	Loss: 1.1361	LR: 0.050000
Training Epoch: 22 [18944/50000]	Loss: 1.0933	LR: 0.050000
Training Epoch: 22 [19072/50000]	Loss: 1.2575	LR: 0.050000
Training Epoch: 22 [19200/50000]	Loss: 1.1123	LR: 0.050000
Training Epoch: 22 [19328/50000]	Loss: 1.1165	LR: 0.050000
Training Epoch: 22 [19456/50000]	Loss: 0.9633	LR: 0.050000
Training Epoch: 22 [19584/50000]	Loss: 1.0515	LR: 0.050000
Training Epoch: 22 [19712/50000]	Loss: 1.0953	LR: 0.050000
Training Epoch: 22 [19840/50000]	Loss: 1.3286	LR: 0.050000
Training Epoch: 22 [19968/50000]	Loss: 1.1571	LR: 0.050000
Training Epoch: 22 [20096/50000]	Loss: 1.1375	LR: 0.050000
Training Epoch: 22 [20224/50000]	Loss: 1.4524	LR: 0.050000
Training Epoch: 22 [20352/50000]	Loss: 1.1908	LR: 0.050000
Training Epoch: 22 [20480/50000]	Loss: 1.3261	LR: 0.050000
Training Epoch: 22 [20608/50000]	Loss: 0.9058	LR: 0.050000
Training Epoch: 22 [20736/50000]	Loss: 1.1874	LR: 0.050000
Training Epoch: 22 [20864/50000]	Loss: 1.1887	LR: 0.050000
Training Epoch: 22 [20992/50000]	Loss: 1.0102	LR: 0.050000
Training Epoch: 22 [21120/50000]	Loss: 1.3166	LR: 0.050000
Training Epoch: 22 [21248/50000]	Loss: 0.9693	LR: 0.050000
Training Epoch: 22 [21376/50000]	Loss: 1.1097	LR: 0.050000
Training Epoch: 22 [21504/50000]	Loss: 1.1184	LR: 0.050000
Training Epoch: 22 [21632/50000]	Loss: 1.2981	LR: 0.050000
Training Epoch: 22 [21760/50000]	Loss: 1.2206	LR: 0.050000
Training Epoch: 22 [21888/50000]	Loss: 1.2870	LR: 0.050000
Training Epoch: 22 [22016/50000]	Loss: 1.2796	LR: 0.050000
Training Epoch: 22 [22144/50000]	Loss: 1.0292	LR: 0.050000
Training Epoch: 22 [22272/50000]	Loss: 1.1064	LR: 0.050000
Training Epoch: 22 [22400/50000]	Loss: 1.3195	LR: 0.050000
Training Epoch: 22 [22528/50000]	Loss: 1.4656	LR: 0.050000
Training Epoch: 22 [22656/50000]	Loss: 1.4638	LR: 0.050000
Training Epoch: 22 [22784/50000]	Loss: 1.0407	LR: 0.050000
Training Epoch: 22 [22912/50000]	Loss: 1.1376	LR: 0.050000
Training Epoch: 22 [23040/50000]	Loss: 1.1024	LR: 0.050000
Training Epoch: 22 [23168/50000]	Loss: 1.2351	LR: 0.050000
Training Epoch: 22 [23296/50000]	Loss: 1.1406	LR: 0.050000
Training Epoch: 22 [23424/50000]	Loss: 1.0595	LR: 0.050000
Training Epoch: 22 [23552/50000]	Loss: 1.0533	LR: 0.050000
Training Epoch: 22 [23680/50000]	Loss: 1.1894	LR: 0.050000
Training Epoch: 22 [23808/50000]	Loss: 1.0690	LR: 0.050000
Training Epoch: 22 [23936/50000]	Loss: 1.3199	LR: 0.050000
Training Epoch: 22 [24064/50000]	Loss: 1.1432	LR: 0.050000
Training Epoch: 22 [24192/50000]	Loss: 1.2668	LR: 0.050000
Training Epoch: 22 [24320/50000]	Loss: 1.0824	LR: 0.050000
Training Epoch: 22 [24448/50000]	Loss: 1.1139	LR: 0.050000
Training Epoch: 22 [24576/50000]	Loss: 1.2250	LR: 0.050000
Training Epoch: 22 [24704/50000]	Loss: 1.2398	LR: 0.050000
Training Epoch: 22 [24832/50000]	Loss: 1.0749	LR: 0.050000
Training Epoch: 22 [24960/50000]	Loss: 1.0985	LR: 0.050000
Training Epoch: 22 [25088/50000]	Loss: 1.1326	LR: 0.050000
Training Epoch: 22 [25216/50000]	Loss: 1.1501	LR: 0.050000
Training Epoch: 22 [25344/50000]	Loss: 1.2485	LR: 0.050000
Training Epoch: 22 [25472/50000]	Loss: 1.1819	LR: 0.050000
Training Epoch: 22 [25600/50000]	Loss: 1.1086	LR: 0.050000
Training Epoch: 22 [25728/50000]	Loss: 1.2038	LR: 0.050000
Training Epoch: 22 [25856/50000]	Loss: 1.2456	LR: 0.050000
Training Epoch: 22 [25984/50000]	Loss: 1.1459	LR: 0.050000
Training Epoch: 22 [26112/50000]	Loss: 1.2713	LR: 0.050000
Training Epoch: 22 [26240/50000]	Loss: 1.1618	LR: 0.050000
Training Epoch: 22 [26368/50000]	Loss: 1.2608	LR: 0.050000
Training Epoch: 22 [26496/50000]	Loss: 1.1332	LR: 0.050000
Training Epoch: 22 [26624/50000]	Loss: 1.1619	LR: 0.050000
Training Epoch: 22 [26752/50000]	Loss: 1.2068	LR: 0.050000
Training Epoch: 22 [26880/50000]	Loss: 1.2432	LR: 0.050000
Training Epoch: 22 [27008/50000]	Loss: 1.3889	LR: 0.050000
Training Epoch: 22 [27136/50000]	Loss: 1.3224	LR: 0.050000
Training Epoch: 22 [27264/50000]	Loss: 1.2034	LR: 0.050000
Training Epoch: 22 [27392/50000]	Loss: 1.0411	LR: 0.050000
Training Epoch: 22 [27520/50000]	Loss: 1.2168	LR: 0.050000
Training Epoch: 22 [27648/50000]	Loss: 1.2823	LR: 0.050000
Training Epoch: 22 [27776/50000]	Loss: 1.0727	LR: 0.050000
Training Epoch: 22 [27904/50000]	Loss: 1.3501	LR: 0.050000
Training Epoch: 22 [28032/50000]	Loss: 1.0735	LR: 0.050000
Training Epoch: 22 [28160/50000]	Loss: 1.2827	LR: 0.050000
Training Epoch: 22 [28288/50000]	Loss: 1.1376	LR: 0.050000
Training Epoch: 22 [28416/50000]	Loss: 1.1205	LR: 0.050000
Training Epoch: 22 [28544/50000]	Loss: 1.3018	LR: 0.050000
Training Epoch: 22 [28672/50000]	Loss: 1.0810	LR: 0.050000
Training Epoch: 22 [28800/50000]	Loss: 1.3093	LR: 0.050000
Training Epoch: 22 [28928/50000]	Loss: 1.3173	LR: 0.050000
Training Epoch: 22 [29056/50000]	Loss: 0.9418	LR: 0.050000
Training Epoch: 22 [29184/50000]	Loss: 1.1171	LR: 0.050000
Training Epoch: 22 [29312/50000]	Loss: 1.0422	LR: 0.050000
Training Epoch: 22 [29440/50000]	Loss: 1.3003	LR: 0.050000
Training Epoch: 22 [29568/50000]	Loss: 1.3790	LR: 0.050000
Training Epoch: 22 [29696/50000]	Loss: 1.0998	LR: 0.050000
Training Epoch: 22 [29824/50000]	Loss: 1.0382	LR: 0.050000
Training Epoch: 22 [29952/50000]	Loss: 1.2308	LR: 0.050000
Training Epoch: 22 [30080/50000]	Loss: 1.4193	LR: 0.050000
Training Epoch: 22 [30208/50000]	Loss: 1.1800	LR: 0.050000
Training Epoch: 22 [30336/50000]	Loss: 1.1425	LR: 0.050000
Training Epoch: 22 [30464/50000]	Loss: 1.0527	LR: 0.050000
Training Epoch: 22 [30592/50000]	Loss: 0.9926	LR: 0.050000
Training Epoch: 22 [30720/50000]	Loss: 1.2914	LR: 0.050000
Training Epoch: 22 [30848/50000]	Loss: 1.0726	LR: 0.050000
Training Epoch: 22 [30976/50000]	Loss: 1.3354	LR: 0.050000
Training Epoch: 22 [31104/50000]	Loss: 1.1519	LR: 0.050000
Training Epoch: 22 [31232/50000]	Loss: 1.0579	LR: 0.050000
Training Epoch: 22 [31360/50000]	Loss: 1.2108	LR: 0.050000
Training Epoch: 22 [31488/50000]	Loss: 0.9969	LR: 0.050000
Training Epoch: 22 [31616/50000]	Loss: 1.2065	LR: 0.050000
Training Epoch: 22 [31744/50000]	Loss: 1.2239	LR: 0.050000
Training Epoch: 22 [31872/50000]	Loss: 1.1673	LR: 0.050000
Training Epoch: 22 [32000/50000]	Loss: 1.3865	LR: 0.050000
Training Epoch: 22 [32128/50000]	Loss: 0.8790	LR: 0.050000
Training Epoch: 22 [32256/50000]	Loss: 1.0384	LR: 0.050000
Training Epoch: 22 [32384/50000]	Loss: 1.2649	LR: 0.050000
Training Epoch: 22 [32512/50000]	Loss: 1.1508	LR: 0.050000
Training Epoch: 22 [32640/50000]	Loss: 1.2268	LR: 0.050000
Training Epoch: 22 [32768/50000]	Loss: 1.2293	LR: 0.050000
Training Epoch: 22 [32896/50000]	Loss: 1.0649	LR: 0.050000
Training Epoch: 22 [33024/50000]	Loss: 1.1425	LR: 0.050000
Training Epoch: 22 [33152/50000]	Loss: 0.8909	LR: 0.050000
Training Epoch: 22 [33280/50000]	Loss: 1.0987	LR: 0.050000
Training Epoch: 22 [33408/50000]	Loss: 0.9495	LR: 0.050000
Training Epoch: 22 [33536/50000]	Loss: 1.3425	LR: 0.050000
Training Epoch: 22 [33664/50000]	Loss: 1.0437	LR: 0.050000
Training Epoch: 22 [33792/50000]	Loss: 1.0442	LR: 0.050000
Training Epoch: 22 [33920/50000]	Loss: 1.2136	LR: 0.050000
Training Epoch: 22 [34048/50000]	Loss: 1.1788	LR: 0.050000
Training Epoch: 22 [34176/50000]	Loss: 1.2436	LR: 0.050000
Training Epoch: 22 [34304/50000]	Loss: 1.1012	LR: 0.050000
Training Epoch: 22 [34432/50000]	Loss: 1.0735	LR: 0.050000
Training Epoch: 22 [34560/50000]	Loss: 1.1527	LR: 0.050000
Training Epoch: 22 [34688/50000]	Loss: 1.1298	LR: 0.050000
Training Epoch: 22 [34816/50000]	Loss: 1.5243	LR: 0.050000
Training Epoch: 22 [34944/50000]	Loss: 1.0992	LR: 0.050000
Training Epoch: 22 [35072/50000]	Loss: 1.1609	LR: 0.050000
Training Epoch: 22 [35200/50000]	Loss: 1.0666	LR: 0.050000
Training Epoch: 22 [35328/50000]	Loss: 1.2148	LR: 0.050000
Training Epoch: 22 [35456/50000]	Loss: 1.0332	LR: 0.050000
Training Epoch: 22 [35584/50000]	Loss: 1.3312	LR: 0.050000
Training Epoch: 22 [35712/50000]	Loss: 1.2521	LR: 0.050000
Training Epoch: 22 [35840/50000]	Loss: 1.0183	LR: 0.050000
Training Epoch: 22 [35968/50000]	Loss: 1.0733	LR: 0.050000
Training Epoch: 22 [36096/50000]	Loss: 1.2013	LR: 0.050000
Training Epoch: 22 [36224/50000]	Loss: 1.1403	LR: 0.050000
Training Epoch: 22 [36352/50000]	Loss: 0.9700	LR: 0.050000
Training Epoch: 22 [36480/50000]	Loss: 1.2206	LR: 0.050000
Training Epoch: 22 [36608/50000]	Loss: 1.3475	LR: 0.050000
Training Epoch: 22 [36736/50000]	Loss: 1.0703	LR: 0.050000
Training Epoch: 22 [36864/50000]	Loss: 1.1720	LR: 0.050000
Training Epoch: 22 [36992/50000]	Loss: 1.1204	LR: 0.050000
Training Epoch: 22 [37120/50000]	Loss: 1.1983	LR: 0.050000
Training Epoch: 22 [37248/50000]	Loss: 1.2855	LR: 0.050000
Training Epoch: 22 [37376/50000]	Loss: 1.1709	LR: 0.050000
Training Epoch: 22 [37504/50000]	Loss: 1.2424	LR: 0.050000
Training Epoch: 22 [37632/50000]	Loss: 0.9976	LR: 0.050000
Training Epoch: 22 [37760/50000]	Loss: 1.3112	LR: 0.050000
Training Epoch: 22 [37888/50000]	Loss: 1.1947	LR: 0.050000
Training Epoch: 22 [38016/50000]	Loss: 1.1717	LR: 0.050000
Training Epoch: 22 [38144/50000]	Loss: 1.0010	LR: 0.050000
Training Epoch: 22 [38272/50000]	Loss: 1.3569	LR: 0.050000
Training Epoch: 22 [38400/50000]	Loss: 1.0789	LR: 0.050000
Training Epoch: 22 [38528/50000]	Loss: 0.9524	LR: 0.050000
Training Epoch: 22 [38656/50000]	Loss: 1.2975	LR: 0.050000
Training Epoch: 22 [38784/50000]	Loss: 1.1748	LR: 0.050000
Training Epoch: 22 [38912/50000]	Loss: 1.1174	LR: 0.050000
Training Epoch: 22 [39040/50000]	Loss: 1.4784	LR: 0.050000
Training Epoch: 22 [39168/50000]	Loss: 1.3714	LR: 0.050000
Training Epoch: 22 [39296/50000]	Loss: 1.2071	LR: 0.050000
Training Epoch: 22 [39424/50000]	Loss: 1.1446	LR: 0.050000
Training Epoch: 22 [39552/50000]	Loss: 1.1697	LR: 0.050000
Training Epoch: 22 [39680/50000]	Loss: 1.5311	LR: 0.050000
Training Epoch: 22 [39808/50000]	Loss: 1.3485	LR: 0.050000
Training Epoch: 22 [39936/50000]	Loss: 1.3544	LR: 0.050000
Training Epoch: 22 [40064/50000]	Loss: 1.0812	LR: 0.050000
Training Epoch: 22 [40192/50000]	Loss: 1.3201	LR: 0.050000
Training Epoch: 22 [40320/50000]	Loss: 1.0295	LR: 0.050000
Training Epoch: 22 [40448/50000]	Loss: 1.5507	LR: 0.050000
Training Epoch: 22 [40576/50000]	Loss: 1.3037	LR: 0.050000
Training Epoch: 22 [40704/50000]	Loss: 1.1300	LR: 0.050000
Training Epoch: 22 [40832/50000]	Loss: 1.4061	LR: 0.050000
Training Epoch: 22 [40960/50000]	Loss: 1.0791	LR: 0.050000
Training Epoch: 22 [41088/50000]	Loss: 1.2510	LR: 0.050000
Training Epoch: 22 [41216/50000]	Loss: 1.3149	LR: 0.050000
Training Epoch: 22 [41344/50000]	Loss: 1.0593	LR: 0.050000
Training Epoch: 22 [41472/50000]	Loss: 1.2596	LR: 0.050000
Training Epoch: 22 [41600/50000]	Loss: 1.1516	LR: 0.050000
Training Epoch: 22 [41728/50000]	Loss: 1.1299	LR: 0.050000
Training Epoch: 22 [41856/50000]	Loss: 1.1908	LR: 0.050000
Training Epoch: 22 [41984/50000]	Loss: 1.2597	LR: 0.050000
Training Epoch: 22 [42112/50000]	Loss: 1.2753	LR: 0.050000
Training Epoch: 22 [42240/50000]	Loss: 1.3255	LR: 0.050000
Training Epoch: 22 [42368/50000]	Loss: 1.2460	LR: 0.050000
Training Epoch: 22 [42496/50000]	Loss: 1.3571	LR: 0.050000
Training Epoch: 22 [42624/50000]	Loss: 1.2709	LR: 0.050000
Training Epoch: 22 [42752/50000]	Loss: 1.2288	LR: 0.050000
Training Epoch: 22 [42880/50000]	Loss: 1.0967	LR: 0.050000
Training Epoch: 22 [43008/50000]	Loss: 1.0720	LR: 0.050000
Training Epoch: 22 [43136/50000]	Loss: 1.2931	LR: 0.050000
Training Epoch: 22 [43264/50000]	Loss: 1.1892	LR: 0.050000
Training Epoch: 22 [43392/50000]	Loss: 1.2306	LR: 0.050000
Training Epoch: 22 [43520/50000]	Loss: 1.1699	LR: 0.050000
Training Epoch: 22 [43648/50000]	Loss: 1.3993	LR: 0.050000
Training Epoch: 22 [43776/50000]	Loss: 1.1535	LR: 0.050000
Training Epoch: 22 [43904/50000]	Loss: 1.0450	LR: 0.050000
Training Epoch: 22 [44032/50000]	Loss: 1.1590	LR: 0.050000
Training Epoch: 22 [44160/50000]	Loss: 1.1999	LR: 0.050000
Training Epoch: 22 [44288/50000]	Loss: 1.2137	LR: 0.050000
Training Epoch: 22 [44416/50000]	Loss: 1.2123	LR: 0.050000
Training Epoch: 22 [44544/50000]	Loss: 1.2943	LR: 0.050000
Training Epoch: 22 [44672/50000]	Loss: 1.1965	LR: 0.050000
Training Epoch: 22 [44800/50000]	Loss: 1.5240	LR: 0.050000
Training Epoch: 22 [44928/50000]	Loss: 1.0105	LR: 0.050000
Training Epoch: 22 [45056/50000]	Loss: 1.3140	LR: 0.050000
Training Epoch: 22 [45184/50000]	Loss: 1.3022	LR: 0.050000
Training Epoch: 22 [45312/50000]	Loss: 1.2046	LR: 0.050000
Training Epoch: 22 [45440/50000]	Loss: 1.0660	LR: 0.050000
Training Epoch: 22 [45568/50000]	Loss: 1.3058	LR: 0.050000
Training Epoch: 22 [45696/50000]	Loss: 1.1150	LR: 0.050000
Training Epoch: 22 [45824/50000]	Loss: 1.0230	LR: 0.050000
Training Epoch: 22 [45952/50000]	Loss: 1.1073	LR: 0.050000
Training Epoch: 22 [46080/50000]	Loss: 1.2622	LR: 0.050000
Training Epoch: 22 [46208/50000]	Loss: 1.1582	LR: 0.050000
Training Epoch: 22 [46336/50000]	Loss: 1.4204	LR: 0.050000
Training Epoch: 22 [46464/50000]	Loss: 1.3721	LR: 0.050000
Training Epoch: 22 [46592/50000]	Loss: 1.2188	LR: 0.050000
Training Epoch: 22 [46720/50000]	Loss: 1.3623	LR: 0.050000
Training Epoch: 22 [46848/50000]	Loss: 1.3395	LR: 0.050000
Training Epoch: 22 [46976/50000]	Loss: 1.3060	LR: 0.050000
Training Epoch: 22 [47104/50000]	Loss: 1.4523	LR: 0.050000
Training Epoch: 22 [47232/50000]	Loss: 1.1841	LR: 0.050000
Training Epoch: 22 [47360/50000]	Loss: 1.1037	LR: 0.050000
Training Epoch: 22 [47488/50000]	Loss: 1.3366	LR: 0.050000
Training Epoch: 22 [47616/50000]	Loss: 1.3170	LR: 0.050000
Training Epoch: 22 [47744/50000]	Loss: 0.9635	LR: 0.050000
Training Epoch: 22 [47872/50000]	Loss: 1.3732	LR: 0.050000
Training Epoch: 22 [48000/50000]	Loss: 1.2575	LR: 0.050000
Training Epoch: 22 [48128/50000]	Loss: 1.1978	LR: 0.050000
Training Epoch: 22 [48256/50000]	Loss: 1.3787	LR: 0.050000
Training Epoch: 22 [48384/50000]	Loss: 1.0252	LR: 0.050000
Training Epoch: 22 [48512/50000]	Loss: 1.1402	LR: 0.050000
Training Epoch: 22 [48640/50000]	Loss: 1.3297	LR: 0.050000
Training Epoch: 22 [48768/50000]	Loss: 1.0176	LR: 0.050000
Training Epoch: 22 [48896/50000]	Loss: 1.1880	LR: 0.050000
Training Epoch: 22 [49024/50000]	Loss: 1.3122	LR: 0.050000
Training Epoch: 22 [49152/50000]	Loss: 1.1321	LR: 0.050000
Training Epoch: 22 [49280/50000]	Loss: 1.5098	LR: 0.050000
Training Epoch: 22 [49408/50000]	Loss: 1.0707	LR: 0.050000
Training Epoch: 22 [49536/50000]	Loss: 1.1820	LR: 0.050000
Training Epoch: 22 [49664/50000]	Loss: 1.1719	LR: 0.050000
Training Epoch: 22 [49792/50000]	Loss: 1.3152	LR: 0.050000
Training Epoch: 22 [49920/50000]	Loss: 1.1841	LR: 0.050000
Training Epoch: 22 [50000/50000]	Loss: 1.2467	LR: 0.050000
epoch 22 training time consumed: 53.87s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   80741 GB |   80741 GB |
|       from large pool |  123392 KB |    1034 MB |   80662 GB |   80662 GB |
|       from small pool |   10798 KB |      13 MB |      79 GB |      79 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   80741 GB |   80741 GB |
|       from large pool |  123392 KB |    1034 MB |   80662 GB |   80662 GB |
|       from small pool |   10798 KB |      13 MB |      79 GB |      79 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   35533 GB |   35533 GB |
|       from large pool |  155136 KB |  433088 KB |   35445 GB |   35445 GB |
|       from small pool |    1490 KB |    3494 KB |      87 GB |      87 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    3115 K  |    3115 K  |
|       from large pool |      24    |      65    |    1626 K  |    1626 K  |
|       from small pool |     231    |     274    |    1489 K  |    1489 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    3115 K  |    3115 K  |
|       from large pool |      24    |      65    |    1626 K  |    1626 K  |
|       from small pool |     231    |     274    |    1489 K  |    1489 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1542 K  |    1542 K  |
|       from large pool |       9    |      14    |     787 K  |     787 K  |
|       from small pool |      12    |      16    |     755 K  |     755 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 22, Average loss: 0.0128, Accuracy: 0.5775, Time consumed:3.46s

Training Epoch: 23 [128/50000]	Loss: 1.1762	LR: 0.050000
Training Epoch: 23 [256/50000]	Loss: 1.0417	LR: 0.050000
Training Epoch: 23 [384/50000]	Loss: 1.2812	LR: 0.050000
Training Epoch: 23 [512/50000]	Loss: 1.2192	LR: 0.050000
Training Epoch: 23 [640/50000]	Loss: 1.0597	LR: 0.050000
Training Epoch: 23 [768/50000]	Loss: 1.0413	LR: 0.050000
Training Epoch: 23 [896/50000]	Loss: 1.1511	LR: 0.050000
Training Epoch: 23 [1024/50000]	Loss: 1.0926	LR: 0.050000
Training Epoch: 23 [1152/50000]	Loss: 0.8999	LR: 0.050000
Training Epoch: 23 [1280/50000]	Loss: 1.1134	LR: 0.050000
Training Epoch: 23 [1408/50000]	Loss: 1.0299	LR: 0.050000
Training Epoch: 23 [1536/50000]	Loss: 1.0946	LR: 0.050000
Training Epoch: 23 [1664/50000]	Loss: 1.0960	LR: 0.050000
Training Epoch: 23 [1792/50000]	Loss: 0.9990	LR: 0.050000
Training Epoch: 23 [1920/50000]	Loss: 1.0766	LR: 0.050000
Training Epoch: 23 [2048/50000]	Loss: 1.0278	LR: 0.050000
Training Epoch: 23 [2176/50000]	Loss: 1.1252	LR: 0.050000
Training Epoch: 23 [2304/50000]	Loss: 1.1196	LR: 0.050000
Training Epoch: 23 [2432/50000]	Loss: 1.1241	LR: 0.050000
Training Epoch: 23 [2560/50000]	Loss: 0.9404	LR: 0.050000
Training Epoch: 23 [2688/50000]	Loss: 1.3076	LR: 0.050000
Training Epoch: 23 [2816/50000]	Loss: 1.0572	LR: 0.050000
Training Epoch: 23 [2944/50000]	Loss: 1.0779	LR: 0.050000
Training Epoch: 23 [3072/50000]	Loss: 1.0622	LR: 0.050000
Training Epoch: 23 [3200/50000]	Loss: 0.9914	LR: 0.050000
Training Epoch: 23 [3328/50000]	Loss: 1.0274	LR: 0.050000
Training Epoch: 23 [3456/50000]	Loss: 0.9110	LR: 0.050000
Training Epoch: 23 [3584/50000]	Loss: 0.9128	LR: 0.050000
Training Epoch: 23 [3712/50000]	Loss: 1.3228	LR: 0.050000
Training Epoch: 23 [3840/50000]	Loss: 0.7800	LR: 0.050000
Training Epoch: 23 [3968/50000]	Loss: 1.1606	LR: 0.050000
Training Epoch: 23 [4096/50000]	Loss: 0.9607	LR: 0.050000
Training Epoch: 23 [4224/50000]	Loss: 1.1038	LR: 0.050000
Training Epoch: 23 [4352/50000]	Loss: 1.1281	LR: 0.050000
Training Epoch: 23 [4480/50000]	Loss: 1.0085	LR: 0.050000
Training Epoch: 23 [4608/50000]	Loss: 1.0349	LR: 0.050000
Training Epoch: 23 [4736/50000]	Loss: 1.0137	LR: 0.050000
Training Epoch: 23 [4864/50000]	Loss: 1.0425	LR: 0.050000
Training Epoch: 23 [4992/50000]	Loss: 1.0782	LR: 0.050000
Training Epoch: 23 [5120/50000]	Loss: 1.1164	LR: 0.050000
Training Epoch: 23 [5248/50000]	Loss: 1.1385	LR: 0.050000
Training Epoch: 23 [5376/50000]	Loss: 1.1112	LR: 0.050000
Training Epoch: 23 [5504/50000]	Loss: 1.1106	LR: 0.050000
Training Epoch: 23 [5632/50000]	Loss: 1.2747	LR: 0.050000
Training Epoch: 23 [5760/50000]	Loss: 1.2759	LR: 0.050000
Training Epoch: 23 [5888/50000]	Loss: 1.2053	LR: 0.050000
Training Epoch: 23 [6016/50000]	Loss: 1.1335	LR: 0.050000
Training Epoch: 23 [6144/50000]	Loss: 0.9790	LR: 0.050000
Training Epoch: 23 [6272/50000]	Loss: 0.7653	LR: 0.050000
Training Epoch: 23 [6400/50000]	Loss: 1.0931	LR: 0.050000
Training Epoch: 23 [6528/50000]	Loss: 1.3219	LR: 0.050000
Training Epoch: 23 [6656/50000]	Loss: 1.2652	LR: 0.050000
Training Epoch: 23 [6784/50000]	Loss: 1.1281	LR: 0.050000
Training Epoch: 23 [6912/50000]	Loss: 1.0458	LR: 0.050000
Training Epoch: 23 [7040/50000]	Loss: 1.0067	LR: 0.050000
Training Epoch: 23 [7168/50000]	Loss: 0.9447	LR: 0.050000
Training Epoch: 23 [7296/50000]	Loss: 1.1830	LR: 0.050000
Training Epoch: 23 [7424/50000]	Loss: 1.1069	LR: 0.050000
Training Epoch: 23 [7552/50000]	Loss: 1.3071	LR: 0.050000
Training Epoch: 23 [7680/50000]	Loss: 1.0392	LR: 0.050000
Training Epoch: 23 [7808/50000]	Loss: 0.8910	LR: 0.050000
Training Epoch: 23 [7936/50000]	Loss: 1.2855	LR: 0.050000
Training Epoch: 23 [8064/50000]	Loss: 1.0659	LR: 0.050000
Training Epoch: 23 [8192/50000]	Loss: 1.1489	LR: 0.050000
Training Epoch: 23 [8320/50000]	Loss: 1.1840	LR: 0.050000
Training Epoch: 23 [8448/50000]	Loss: 1.1490	LR: 0.050000
Training Epoch: 23 [8576/50000]	Loss: 1.0208	LR: 0.050000
Training Epoch: 23 [8704/50000]	Loss: 1.1108	LR: 0.050000
Training Epoch: 23 [8832/50000]	Loss: 1.0368	LR: 0.050000
Training Epoch: 23 [8960/50000]	Loss: 1.1222	LR: 0.050000
Training Epoch: 23 [9088/50000]	Loss: 0.9149	LR: 0.050000
Training Epoch: 23 [9216/50000]	Loss: 1.2042	LR: 0.050000
Training Epoch: 23 [9344/50000]	Loss: 0.9440	LR: 0.050000
Training Epoch: 23 [9472/50000]	Loss: 1.2112	LR: 0.050000
Training Epoch: 23 [9600/50000]	Loss: 1.1321	LR: 0.050000
Training Epoch: 23 [9728/50000]	Loss: 1.1744	LR: 0.050000
Training Epoch: 23 [9856/50000]	Loss: 1.0128	LR: 0.050000
Training Epoch: 23 [9984/50000]	Loss: 1.0363	LR: 0.050000
Training Epoch: 23 [10112/50000]	Loss: 1.1744	LR: 0.050000
Training Epoch: 23 [10240/50000]	Loss: 1.2428	LR: 0.050000
Training Epoch: 23 [10368/50000]	Loss: 1.0850	LR: 0.050000
Training Epoch: 23 [10496/50000]	Loss: 1.3385	LR: 0.050000
Training Epoch: 23 [10624/50000]	Loss: 1.2184	LR: 0.050000
Training Epoch: 23 [10752/50000]	Loss: 0.9613	LR: 0.050000
Training Epoch: 23 [10880/50000]	Loss: 1.1003	LR: 0.050000
Training Epoch: 23 [11008/50000]	Loss: 0.8796	LR: 0.050000
Training Epoch: 23 [11136/50000]	Loss: 1.5439	LR: 0.050000
Training Epoch: 23 [11264/50000]	Loss: 1.0045	LR: 0.050000
Training Epoch: 23 [11392/50000]	Loss: 1.2464	LR: 0.050000
Training Epoch: 23 [11520/50000]	Loss: 1.0490	LR: 0.050000
Training Epoch: 23 [11648/50000]	Loss: 1.2130	LR: 0.050000
Training Epoch: 23 [11776/50000]	Loss: 1.2980	LR: 0.050000
Training Epoch: 23 [11904/50000]	Loss: 1.0749	LR: 0.050000
Training Epoch: 23 [12032/50000]	Loss: 1.1199	LR: 0.050000
Training Epoch: 23 [12160/50000]	Loss: 1.0071	LR: 0.050000
Training Epoch: 23 [12288/50000]	Loss: 1.2050	LR: 0.050000
Training Epoch: 23 [12416/50000]	Loss: 1.1999	LR: 0.050000
Training Epoch: 23 [12544/50000]	Loss: 1.1750	LR: 0.050000
Training Epoch: 23 [12672/50000]	Loss: 1.3093	LR: 0.050000
Training Epoch: 23 [12800/50000]	Loss: 1.1398	LR: 0.050000
Training Epoch: 23 [12928/50000]	Loss: 1.2868	LR: 0.050000
Training Epoch: 23 [13056/50000]	Loss: 1.0390	LR: 0.050000
Training Epoch: 23 [13184/50000]	Loss: 1.1228	LR: 0.050000
Training Epoch: 23 [13312/50000]	Loss: 1.0959	LR: 0.050000
Training Epoch: 23 [13440/50000]	Loss: 1.0622	LR: 0.050000
Training Epoch: 23 [13568/50000]	Loss: 1.2685	LR: 0.050000
Training Epoch: 23 [13696/50000]	Loss: 1.1612	LR: 0.050000
Training Epoch: 23 [13824/50000]	Loss: 1.3319	LR: 0.050000
Training Epoch: 23 [13952/50000]	Loss: 1.0713	LR: 0.050000
Training Epoch: 23 [14080/50000]	Loss: 1.0012	LR: 0.050000
Training Epoch: 23 [14208/50000]	Loss: 1.0031	LR: 0.050000
Training Epoch: 23 [14336/50000]	Loss: 1.0858	LR: 0.050000
Training Epoch: 23 [14464/50000]	Loss: 1.0737	LR: 0.050000
Training Epoch: 23 [14592/50000]	Loss: 1.1170	LR: 0.050000
Training Epoch: 23 [14720/50000]	Loss: 1.1733	LR: 0.050000
Training Epoch: 23 [14848/50000]	Loss: 1.3274	LR: 0.050000
Training Epoch: 23 [14976/50000]	Loss: 1.2533	LR: 0.050000
Training Epoch: 23 [15104/50000]	Loss: 1.1274	LR: 0.050000
Training Epoch: 23 [15232/50000]	Loss: 1.0253	LR: 0.050000
Training Epoch: 23 [15360/50000]	Loss: 1.3543	LR: 0.050000
Training Epoch: 23 [15488/50000]	Loss: 1.0575	LR: 0.050000
Training Epoch: 23 [15616/50000]	Loss: 0.8260	LR: 0.050000
Training Epoch: 23 [15744/50000]	Loss: 1.0499	LR: 0.050000
Training Epoch: 23 [15872/50000]	Loss: 1.0236	LR: 0.050000
Training Epoch: 23 [16000/50000]	Loss: 0.9770	LR: 0.050000
Training Epoch: 23 [16128/50000]	Loss: 1.1641	LR: 0.050000
Training Epoch: 23 [16256/50000]	Loss: 1.1393	LR: 0.050000
Training Epoch: 23 [16384/50000]	Loss: 1.3271	LR: 0.050000
Training Epoch: 23 [16512/50000]	Loss: 1.1654	LR: 0.050000
Training Epoch: 23 [16640/50000]	Loss: 1.0325	LR: 0.050000
Training Epoch: 23 [16768/50000]	Loss: 0.9374	LR: 0.050000
Training Epoch: 23 [16896/50000]	Loss: 1.1270	LR: 0.050000
Training Epoch: 23 [17024/50000]	Loss: 1.1908	LR: 0.050000
Training Epoch: 23 [17152/50000]	Loss: 0.9531	LR: 0.050000
Training Epoch: 23 [17280/50000]	Loss: 1.2029	LR: 0.050000
Training Epoch: 23 [17408/50000]	Loss: 1.1137	LR: 0.050000
Training Epoch: 23 [17536/50000]	Loss: 1.3015	LR: 0.050000
Training Epoch: 23 [17664/50000]	Loss: 1.2796	LR: 0.050000
Training Epoch: 23 [17792/50000]	Loss: 1.2244	LR: 0.050000
Training Epoch: 23 [17920/50000]	Loss: 0.9704	LR: 0.050000
Training Epoch: 23 [18048/50000]	Loss: 1.1675	LR: 0.050000
Training Epoch: 23 [18176/50000]	Loss: 1.0906	LR: 0.050000
Training Epoch: 23 [18304/50000]	Loss: 0.9090	LR: 0.050000
Training Epoch: 23 [18432/50000]	Loss: 0.9675	LR: 0.050000
Training Epoch: 23 [18560/50000]	Loss: 1.0355	LR: 0.050000
Training Epoch: 23 [18688/50000]	Loss: 1.0426	LR: 0.050000
Training Epoch: 23 [18816/50000]	Loss: 1.2047	LR: 0.050000
Training Epoch: 23 [18944/50000]	Loss: 1.0608	LR: 0.050000
Training Epoch: 23 [19072/50000]	Loss: 1.2794	LR: 0.050000
Training Epoch: 23 [19200/50000]	Loss: 1.1555	LR: 0.050000
Training Epoch: 23 [19328/50000]	Loss: 1.1896	LR: 0.050000
Training Epoch: 23 [19456/50000]	Loss: 1.1854	LR: 0.050000
Training Epoch: 23 [19584/50000]	Loss: 1.2281	LR: 0.050000
Training Epoch: 23 [19712/50000]	Loss: 1.2231	LR: 0.050000
Training Epoch: 23 [19840/50000]	Loss: 1.2771	LR: 0.050000
Training Epoch: 23 [19968/50000]	Loss: 1.0180	LR: 0.050000
Training Epoch: 23 [20096/50000]	Loss: 1.0405	LR: 0.050000
Training Epoch: 23 [20224/50000]	Loss: 1.1375	LR: 0.050000
Training Epoch: 23 [20352/50000]	Loss: 1.0513	LR: 0.050000
Training Epoch: 23 [20480/50000]	Loss: 1.2856	LR: 0.050000
Training Epoch: 23 [20608/50000]	Loss: 0.9582	LR: 0.050000
Training Epoch: 23 [20736/50000]	Loss: 1.1016	LR: 0.050000
Training Epoch: 23 [20864/50000]	Loss: 1.4294	LR: 0.050000
Training Epoch: 23 [20992/50000]	Loss: 1.0892	LR: 0.050000
Training Epoch: 23 [21120/50000]	Loss: 1.0110	LR: 0.050000
Training Epoch: 23 [21248/50000]	Loss: 1.0413	LR: 0.050000
Training Epoch: 23 [21376/50000]	Loss: 1.1706	LR: 0.050000
Training Epoch: 23 [21504/50000]	Loss: 1.0308	LR: 0.050000
Training Epoch: 23 [21632/50000]	Loss: 1.1247	LR: 0.050000
Training Epoch: 23 [21760/50000]	Loss: 1.2526	LR: 0.050000
Training Epoch: 23 [21888/50000]	Loss: 1.1857	LR: 0.050000
Training Epoch: 23 [22016/50000]	Loss: 1.2326	LR: 0.050000
Training Epoch: 23 [22144/50000]	Loss: 0.9453	LR: 0.050000
Training Epoch: 23 [22272/50000]	Loss: 1.0751	LR: 0.050000
Training Epoch: 23 [22400/50000]	Loss: 1.3151	LR: 0.050000
Training Epoch: 23 [22528/50000]	Loss: 1.0824	LR: 0.050000
Training Epoch: 23 [22656/50000]	Loss: 1.3193	LR: 0.050000
Training Epoch: 23 [22784/50000]	Loss: 1.2869	LR: 0.050000
Training Epoch: 23 [22912/50000]	Loss: 1.1561	LR: 0.050000
Training Epoch: 23 [23040/50000]	Loss: 1.2359	LR: 0.050000
Training Epoch: 23 [23168/50000]	Loss: 1.0000	LR: 0.050000
Training Epoch: 23 [23296/50000]	Loss: 1.0059	LR: 0.050000
Training Epoch: 23 [23424/50000]	Loss: 1.2769	LR: 0.050000
Training Epoch: 23 [23552/50000]	Loss: 1.0734	LR: 0.050000
Training Epoch: 23 [23680/50000]	Loss: 1.0717	LR: 0.050000
Training Epoch: 23 [23808/50000]	Loss: 0.9433	LR: 0.050000
Training Epoch: 23 [23936/50000]	Loss: 1.3277	LR: 0.050000
Training Epoch: 23 [24064/50000]	Loss: 1.1431	LR: 0.050000
Training Epoch: 23 [24192/50000]	Loss: 1.2093	LR: 0.050000
Training Epoch: 23 [24320/50000]	Loss: 0.9689	LR: 0.050000
Training Epoch: 23 [24448/50000]	Loss: 1.2027	LR: 0.050000
Training Epoch: 23 [24576/50000]	Loss: 1.2007	LR: 0.050000
Training Epoch: 23 [24704/50000]	Loss: 0.8703	LR: 0.050000
Training Epoch: 23 [24832/50000]	Loss: 1.4678	LR: 0.050000
Training Epoch: 23 [24960/50000]	Loss: 1.2592	LR: 0.050000
Training Epoch: 23 [25088/50000]	Loss: 1.1728	LR: 0.050000
Training Epoch: 23 [25216/50000]	Loss: 1.0108	LR: 0.050000
Training Epoch: 23 [25344/50000]	Loss: 1.0386	LR: 0.050000
Training Epoch: 23 [25472/50000]	Loss: 1.4691	LR: 0.050000
Training Epoch: 23 [25600/50000]	Loss: 1.0600	LR: 0.050000
Training Epoch: 23 [25728/50000]	Loss: 1.1650	LR: 0.050000
Training Epoch: 23 [25856/50000]	Loss: 1.1007	LR: 0.050000
Training Epoch: 23 [25984/50000]	Loss: 1.0310	LR: 0.050000
Training Epoch: 23 [26112/50000]	Loss: 1.4837	LR: 0.050000
Training Epoch: 23 [26240/50000]	Loss: 1.2990	LR: 0.050000
Training Epoch: 23 [26368/50000]	Loss: 1.0105	LR: 0.050000
Training Epoch: 23 [26496/50000]	Loss: 1.2983	LR: 0.050000
Training Epoch: 23 [26624/50000]	Loss: 1.3252	LR: 0.050000
Training Epoch: 23 [26752/50000]	Loss: 1.0479	LR: 0.050000
Training Epoch: 23 [26880/50000]	Loss: 1.4476	LR: 0.050000
Training Epoch: 23 [27008/50000]	Loss: 1.0333	LR: 0.050000
Training Epoch: 23 [27136/50000]	Loss: 1.1398	LR: 0.050000
Training Epoch: 23 [27264/50000]	Loss: 1.0753	LR: 0.050000
Training Epoch: 23 [27392/50000]	Loss: 1.1169	LR: 0.050000
Training Epoch: 23 [27520/50000]	Loss: 1.0305	LR: 0.050000
Training Epoch: 23 [27648/50000]	Loss: 1.1656	LR: 0.050000
Training Epoch: 23 [27776/50000]	Loss: 1.0316	LR: 0.050000
Training Epoch: 23 [27904/50000]	Loss: 1.3015	LR: 0.050000
Training Epoch: 23 [28032/50000]	Loss: 1.1489	LR: 0.050000
Training Epoch: 23 [28160/50000]	Loss: 1.3434	LR: 0.050000
Training Epoch: 23 [28288/50000]	Loss: 1.3481	LR: 0.050000
Training Epoch: 23 [28416/50000]	Loss: 1.2989	LR: 0.050000
Training Epoch: 23 [28544/50000]	Loss: 1.1700	LR: 0.050000
Training Epoch: 23 [28672/50000]	Loss: 1.0978	LR: 0.050000
Training Epoch: 23 [28800/50000]	Loss: 1.2149	LR: 0.050000
Training Epoch: 23 [28928/50000]	Loss: 1.2019	LR: 0.050000
Training Epoch: 23 [29056/50000]	Loss: 1.0526	LR: 0.050000
Training Epoch: 23 [29184/50000]	Loss: 1.2358	LR: 0.050000
Training Epoch: 23 [29312/50000]	Loss: 1.1234	LR: 0.050000
Training Epoch: 23 [29440/50000]	Loss: 1.3062	LR: 0.050000
Training Epoch: 23 [29568/50000]	Loss: 1.3480	LR: 0.050000
Training Epoch: 23 [29696/50000]	Loss: 1.0959	LR: 0.050000
Training Epoch: 23 [29824/50000]	Loss: 1.2560	LR: 0.050000
Training Epoch: 23 [29952/50000]	Loss: 1.1309	LR: 0.050000
Training Epoch: 23 [30080/50000]	Loss: 1.3559	LR: 0.050000
Training Epoch: 23 [30208/50000]	Loss: 1.1603	LR: 0.050000
Training Epoch: 23 [30336/50000]	Loss: 1.0921	LR: 0.050000
Training Epoch: 23 [30464/50000]	Loss: 1.0297	LR: 0.050000
Training Epoch: 23 [30592/50000]	Loss: 1.1396	LR: 0.050000
Training Epoch: 23 [30720/50000]	Loss: 1.2367	LR: 0.050000
Training Epoch: 23 [30848/50000]	Loss: 1.2982	LR: 0.050000
Training Epoch: 23 [30976/50000]	Loss: 1.2097	LR: 0.050000
Training Epoch: 23 [31104/50000]	Loss: 1.1968	LR: 0.050000
Training Epoch: 23 [31232/50000]	Loss: 0.9016	LR: 0.050000
Training Epoch: 23 [31360/50000]	Loss: 1.0300	LR: 0.050000
Training Epoch: 23 [31488/50000]	Loss: 1.3774	LR: 0.050000
Training Epoch: 23 [31616/50000]	Loss: 0.9711	LR: 0.050000
Training Epoch: 23 [31744/50000]	Loss: 1.2390	LR: 0.050000
Training Epoch: 23 [31872/50000]	Loss: 1.0771	LR: 0.050000
Training Epoch: 23 [32000/50000]	Loss: 1.3257	LR: 0.050000
Training Epoch: 23 [32128/50000]	Loss: 1.0990	LR: 0.050000
Training Epoch: 23 [32256/50000]	Loss: 0.9644	LR: 0.050000
Training Epoch: 23 [32384/50000]	Loss: 0.9936	LR: 0.050000
Training Epoch: 23 [32512/50000]	Loss: 1.3495	LR: 0.050000
Training Epoch: 23 [32640/50000]	Loss: 1.2927	LR: 0.050000
Training Epoch: 23 [32768/50000]	Loss: 1.3744	LR: 0.050000
Training Epoch: 23 [32896/50000]	Loss: 1.1476	LR: 0.050000
Training Epoch: 23 [33024/50000]	Loss: 1.4013	LR: 0.050000
Training Epoch: 23 [33152/50000]	Loss: 1.2797	LR: 0.050000
Training Epoch: 23 [33280/50000]	Loss: 1.4086	LR: 0.050000
Training Epoch: 23 [33408/50000]	Loss: 1.0851	LR: 0.050000
Training Epoch: 23 [33536/50000]	Loss: 1.1433	LR: 0.050000
Training Epoch: 23 [33664/50000]	Loss: 1.2610	LR: 0.050000
Training Epoch: 23 [33792/50000]	Loss: 1.3327	LR: 0.050000
Training Epoch: 23 [33920/50000]	Loss: 1.0491	LR: 0.050000
Training Epoch: 23 [34048/50000]	Loss: 1.1507	LR: 0.050000
Training Epoch: 23 [34176/50000]	Loss: 1.2953	LR: 0.050000
Training Epoch: 23 [34304/50000]	Loss: 1.0002	LR: 0.050000
Training Epoch: 23 [34432/50000]	Loss: 1.2529	LR: 0.050000
Training Epoch: 23 [34560/50000]	Loss: 1.0406	LR: 0.050000
Training Epoch: 23 [34688/50000]	Loss: 1.2547	LR: 0.050000
Training Epoch: 23 [34816/50000]	Loss: 1.4301	LR: 0.050000
Training Epoch: 23 [34944/50000]	Loss: 1.0765	LR: 0.050000
Training Epoch: 23 [35072/50000]	Loss: 1.0306	LR: 0.050000
Training Epoch: 23 [35200/50000]	Loss: 1.1558	LR: 0.050000
Training Epoch: 23 [35328/50000]	Loss: 1.1067	LR: 0.050000
Training Epoch: 23 [35456/50000]	Loss: 1.0729	LR: 0.050000
Training Epoch: 23 [35584/50000]	Loss: 1.0842	LR: 0.050000
Training Epoch: 23 [35712/50000]	Loss: 1.2669	LR: 0.050000
Training Epoch: 23 [35840/50000]	Loss: 1.3080	LR: 0.050000
Training Epoch: 23 [35968/50000]	Loss: 1.3035	LR: 0.050000
Training Epoch: 23 [36096/50000]	Loss: 1.2373	LR: 0.050000
Training Epoch: 23 [36224/50000]	Loss: 1.0728	LR: 0.050000
Training Epoch: 23 [36352/50000]	Loss: 1.2566	LR: 0.050000
Training Epoch: 23 [36480/50000]	Loss: 1.2911	LR: 0.050000
Training Epoch: 23 [36608/50000]	Loss: 1.3129	LR: 0.050000
Training Epoch: 23 [36736/50000]	Loss: 1.0773	LR: 0.050000
Training Epoch: 23 [36864/50000]	Loss: 1.3614	LR: 0.050000
Training Epoch: 23 [36992/50000]	Loss: 1.1395	LR: 0.050000
Training Epoch: 23 [37120/50000]	Loss: 1.2443	LR: 0.050000
Training Epoch: 23 [37248/50000]	Loss: 1.0116	LR: 0.050000
Training Epoch: 23 [37376/50000]	Loss: 1.1766	LR: 0.050000
Training Epoch: 23 [37504/50000]	Loss: 1.4145	LR: 0.050000
Training Epoch: 23 [37632/50000]	Loss: 1.0147	LR: 0.050000
Training Epoch: 23 [37760/50000]	Loss: 1.1053	LR: 0.050000
Training Epoch: 23 [37888/50000]	Loss: 1.3296	LR: 0.050000
Training Epoch: 23 [38016/50000]	Loss: 1.3541	LR: 0.050000
Training Epoch: 23 [38144/50000]	Loss: 1.3640	LR: 0.050000
Training Epoch: 23 [38272/50000]	Loss: 1.3556	LR: 0.050000
Training Epoch: 23 [38400/50000]	Loss: 1.0860	LR: 0.050000
Training Epoch: 23 [38528/50000]	Loss: 1.1787	LR: 0.050000
Training Epoch: 23 [38656/50000]	Loss: 0.9698	LR: 0.050000
Training Epoch: 23 [38784/50000]	Loss: 1.0610	LR: 0.050000
Training Epoch: 23 [38912/50000]	Loss: 1.0840	LR: 0.050000
Training Epoch: 23 [39040/50000]	Loss: 1.1657	LR: 0.050000
Training Epoch: 23 [39168/50000]	Loss: 1.3595	LR: 0.050000
Training Epoch: 23 [39296/50000]	Loss: 0.9672	LR: 0.050000
Training Epoch: 23 [39424/50000]	Loss: 1.2249	LR: 0.050000
Training Epoch: 23 [39552/50000]	Loss: 1.1048	LR: 0.050000
Training Epoch: 23 [39680/50000]	Loss: 1.0789	LR: 0.050000
Training Epoch: 23 [39808/50000]	Loss: 1.1375	LR: 0.050000
Training Epoch: 23 [39936/50000]	Loss: 1.3061	LR: 0.050000
Training Epoch: 23 [40064/50000]	Loss: 1.1622	LR: 0.050000
Training Epoch: 23 [40192/50000]	Loss: 1.2196	LR: 0.050000
Training Epoch: 23 [40320/50000]	Loss: 1.2370	LR: 0.050000
Training Epoch: 23 [40448/50000]	Loss: 1.4600	LR: 0.050000
Training Epoch: 23 [40576/50000]	Loss: 1.1457	LR: 0.050000
Training Epoch: 23 [40704/50000]	Loss: 1.4063	LR: 0.050000
Training Epoch: 23 [40832/50000]	Loss: 1.2544	LR: 0.050000
Training Epoch: 23 [40960/50000]	Loss: 0.9900	LR: 0.050000
Training Epoch: 23 [41088/50000]	Loss: 0.9643	LR: 0.050000
Training Epoch: 23 [41216/50000]	Loss: 1.4421	LR: 0.050000
Training Epoch: 23 [41344/50000]	Loss: 1.1252	LR: 0.050000
Training Epoch: 23 [41472/50000]	Loss: 1.0840	LR: 0.050000
Training Epoch: 23 [41600/50000]	Loss: 0.9361	LR: 0.050000
Training Epoch: 23 [41728/50000]	Loss: 1.2118	LR: 0.050000
Training Epoch: 23 [41856/50000]	Loss: 1.1092	LR: 0.050000
Training Epoch: 23 [41984/50000]	Loss: 1.1077	LR: 0.050000
Training Epoch: 23 [42112/50000]	Loss: 1.0719	LR: 0.050000
Training Epoch: 23 [42240/50000]	Loss: 1.0779	LR: 0.050000
Training Epoch: 23 [42368/50000]	Loss: 1.2615	LR: 0.050000
Training Epoch: 23 [42496/50000]	Loss: 1.0610	LR: 0.050000
Training Epoch: 23 [42624/50000]	Loss: 1.2305	LR: 0.050000
Training Epoch: 23 [42752/50000]	Loss: 1.2641	LR: 0.050000
Training Epoch: 23 [42880/50000]	Loss: 1.1530	LR: 0.050000
Training Epoch: 23 [43008/50000]	Loss: 1.1575	LR: 0.050000
Training Epoch: 23 [43136/50000]	Loss: 1.1936	LR: 0.050000
Training Epoch: 23 [43264/50000]	Loss: 1.3322	LR: 0.050000
Training Epoch: 23 [43392/50000]	Loss: 1.5318	LR: 0.050000
Training Epoch: 23 [43520/50000]	Loss: 1.0276	LR: 0.050000
Training Epoch: 23 [43648/50000]	Loss: 1.1384	LR: 0.050000
Training Epoch: 23 [43776/50000]	Loss: 1.0497	LR: 0.050000
Training Epoch: 23 [43904/50000]	Loss: 1.3488	LR: 0.050000
Training Epoch: 23 [44032/50000]	Loss: 1.1397	LR: 0.050000
Training Epoch: 23 [44160/50000]	Loss: 1.3186	LR: 0.050000
Training Epoch: 23 [44288/50000]	Loss: 1.4134	LR: 0.050000
Training Epoch: 23 [44416/50000]	Loss: 1.1116	LR: 0.050000
Training Epoch: 23 [44544/50000]	Loss: 1.2516	LR: 0.050000
Training Epoch: 23 [44672/50000]	Loss: 0.9424	LR: 0.050000
Training Epoch: 23 [44800/50000]	Loss: 1.3517	LR: 0.050000
Training Epoch: 23 [44928/50000]	Loss: 1.2428	LR: 0.050000
Training Epoch: 23 [45056/50000]	Loss: 1.1959	LR: 0.050000
Training Epoch: 23 [45184/50000]	Loss: 1.1742	LR: 0.050000
Training Epoch: 23 [45312/50000]	Loss: 1.1866	LR: 0.050000
Training Epoch: 23 [45440/50000]	Loss: 1.4427	LR: 0.050000
Training Epoch: 23 [45568/50000]	Loss: 1.3254	LR: 0.050000
Training Epoch: 23 [45696/50000]	Loss: 1.2863	LR: 0.050000
Training Epoch: 23 [45824/50000]	Loss: 1.3566	LR: 0.050000
Training Epoch: 23 [45952/50000]	Loss: 1.2251	LR: 0.050000
Training Epoch: 23 [46080/50000]	Loss: 1.3081	LR: 0.050000
Training Epoch: 23 [46208/50000]	Loss: 0.9948	LR: 0.050000
Training Epoch: 23 [46336/50000]	Loss: 1.2786	LR: 0.050000
Training Epoch: 23 [46464/50000]	Loss: 1.0811	LR: 0.050000
Training Epoch: 23 [46592/50000]	Loss: 1.1063	LR: 0.050000
Training Epoch: 23 [46720/50000]	Loss: 1.0333	LR: 0.050000
Training Epoch: 23 [46848/50000]	Loss: 0.9731	LR: 0.050000
Training Epoch: 23 [46976/50000]	Loss: 1.1513	LR: 0.050000
Training Epoch: 23 [47104/50000]	Loss: 0.9816	LR: 0.050000
Training Epoch: 23 [47232/50000]	Loss: 0.7836	LR: 0.050000
Training Epoch: 23 [47360/50000]	Loss: 1.0880	LR: 0.050000
Training Epoch: 23 [47488/50000]	Loss: 1.1469	LR: 0.050000
Training Epoch: 23 [47616/50000]	Loss: 1.2122	LR: 0.050000
Training Epoch: 23 [47744/50000]	Loss: 0.9846	LR: 0.050000
Training Epoch: 23 [47872/50000]	Loss: 1.4892	LR: 0.050000
Training Epoch: 23 [48000/50000]	Loss: 1.1654	LR: 0.050000
Training Epoch: 23 [48128/50000]	Loss: 1.1364	LR: 0.050000
Training Epoch: 23 [48256/50000]	Loss: 1.0778	LR: 0.050000
Training Epoch: 23 [48384/50000]	Loss: 1.1006	LR: 0.050000
Training Epoch: 23 [48512/50000]	Loss: 1.3932	LR: 0.050000
Training Epoch: 23 [48640/50000]	Loss: 1.2464	LR: 0.050000
Training Epoch: 23 [48768/50000]	Loss: 1.2114	LR: 0.050000
Training Epoch: 23 [48896/50000]	Loss: 1.2121	LR: 0.050000
Training Epoch: 23 [49024/50000]	Loss: 1.3743	LR: 0.050000
Training Epoch: 23 [49152/50000]	Loss: 0.9563	LR: 0.050000
Training Epoch: 23 [49280/50000]	Loss: 1.2324	LR: 0.050000
Training Epoch: 23 [49408/50000]	Loss: 1.1377	LR: 0.050000
Training Epoch: 23 [49536/50000]	Loss: 1.3975	LR: 0.050000
Training Epoch: 23 [49664/50000]	Loss: 1.1096	LR: 0.050000
Training Epoch: 23 [49792/50000]	Loss: 1.2673	LR: 0.050000
Training Epoch: 23 [49920/50000]	Loss: 1.3303	LR: 0.050000
Training Epoch: 23 [50000/50000]	Loss: 1.1953	LR: 0.050000
epoch 23 training time consumed: 53.92s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   84411 GB |   84411 GB |
|       from large pool |  123392 KB |    1034 MB |   84328 GB |   84328 GB |
|       from small pool |   10798 KB |      13 MB |      83 GB |      83 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   84411 GB |   84411 GB |
|       from large pool |  123392 KB |    1034 MB |   84328 GB |   84328 GB |
|       from small pool |   10798 KB |      13 MB |      83 GB |      83 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   37148 GB |   37148 GB |
|       from large pool |  155136 KB |  433088 KB |   37056 GB |   37056 GB |
|       from small pool |    1490 KB |    3494 KB |      91 GB |      91 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    3257 K  |    3257 K  |
|       from large pool |      24    |      65    |    1700 K  |    1700 K  |
|       from small pool |     231    |     274    |    1557 K  |    1557 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    3257 K  |    3257 K  |
|       from large pool |      24    |      65    |    1700 K  |    1700 K  |
|       from small pool |     231    |     274    |    1557 K  |    1557 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1612 K  |    1612 K  |
|       from large pool |       9    |      14    |     822 K  |     822 K  |
|       from small pool |      12    |      16    |     789 K  |     789 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 23, Average loss: 0.0118, Accuracy: 0.5998, Time consumed:3.48s

Training Epoch: 24 [128/50000]	Loss: 1.1069	LR: 0.050000
Training Epoch: 24 [256/50000]	Loss: 1.1019	LR: 0.050000
Training Epoch: 24 [384/50000]	Loss: 1.1058	LR: 0.050000
Training Epoch: 24 [512/50000]	Loss: 0.9782	LR: 0.050000
Training Epoch: 24 [640/50000]	Loss: 1.0297	LR: 0.050000
Training Epoch: 24 [768/50000]	Loss: 0.9339	LR: 0.050000
Training Epoch: 24 [896/50000]	Loss: 1.1955	LR: 0.050000
Training Epoch: 24 [1024/50000]	Loss: 1.1231	LR: 0.050000
Training Epoch: 24 [1152/50000]	Loss: 1.2017	LR: 0.050000
Training Epoch: 24 [1280/50000]	Loss: 1.0947	LR: 0.050000
Training Epoch: 24 [1408/50000]	Loss: 0.8534	LR: 0.050000
Training Epoch: 24 [1536/50000]	Loss: 1.0765	LR: 0.050000
Training Epoch: 24 [1664/50000]	Loss: 1.0780	LR: 0.050000
Training Epoch: 24 [1792/50000]	Loss: 1.2802	LR: 0.050000
Training Epoch: 24 [1920/50000]	Loss: 1.0524	LR: 0.050000
Training Epoch: 24 [2048/50000]	Loss: 1.1669	LR: 0.050000
Training Epoch: 24 [2176/50000]	Loss: 1.1705	LR: 0.050000
Training Epoch: 24 [2304/50000]	Loss: 0.9844	LR: 0.050000
Training Epoch: 24 [2432/50000]	Loss: 1.0132	LR: 0.050000
Training Epoch: 24 [2560/50000]	Loss: 1.0376	LR: 0.050000
Training Epoch: 24 [2688/50000]	Loss: 0.9655	LR: 0.050000
Training Epoch: 24 [2816/50000]	Loss: 1.1338	LR: 0.050000
Training Epoch: 24 [2944/50000]	Loss: 0.8576	LR: 0.050000
Training Epoch: 24 [3072/50000]	Loss: 1.2492	LR: 0.050000
Training Epoch: 24 [3200/50000]	Loss: 1.1307	LR: 0.050000
Training Epoch: 24 [3328/50000]	Loss: 1.1080	LR: 0.050000
Training Epoch: 24 [3456/50000]	Loss: 1.1587	LR: 0.050000
Training Epoch: 24 [3584/50000]	Loss: 0.9580	LR: 0.050000
Training Epoch: 24 [3712/50000]	Loss: 1.1002	LR: 0.050000
Training Epoch: 24 [3840/50000]	Loss: 1.1895	LR: 0.050000
Training Epoch: 24 [3968/50000]	Loss: 1.0499	LR: 0.050000
Training Epoch: 24 [4096/50000]	Loss: 1.2174	LR: 0.050000
Training Epoch: 24 [4224/50000]	Loss: 1.0784	LR: 0.050000
Training Epoch: 24 [4352/50000]	Loss: 0.9994	LR: 0.050000
Training Epoch: 24 [4480/50000]	Loss: 0.9260	LR: 0.050000
Training Epoch: 24 [4608/50000]	Loss: 1.1677	LR: 0.050000
Training Epoch: 24 [4736/50000]	Loss: 1.2098	LR: 0.050000
Training Epoch: 24 [4864/50000]	Loss: 0.9857	LR: 0.050000
Training Epoch: 24 [4992/50000]	Loss: 1.1123	LR: 0.050000
Training Epoch: 24 [5120/50000]	Loss: 1.2285	LR: 0.050000
Training Epoch: 24 [5248/50000]	Loss: 1.1451	LR: 0.050000
Training Epoch: 24 [5376/50000]	Loss: 0.9225	LR: 0.050000
Training Epoch: 24 [5504/50000]	Loss: 1.0045	LR: 0.050000
Training Epoch: 24 [5632/50000]	Loss: 1.3864	LR: 0.050000
Training Epoch: 24 [5760/50000]	Loss: 1.0645	LR: 0.050000
Training Epoch: 24 [5888/50000]	Loss: 1.0321	LR: 0.050000
Training Epoch: 24 [6016/50000]	Loss: 0.9823	LR: 0.050000
Training Epoch: 24 [6144/50000]	Loss: 1.1226	LR: 0.050000
Training Epoch: 24 [6272/50000]	Loss: 1.0624	LR: 0.050000
Training Epoch: 24 [6400/50000]	Loss: 0.8494	LR: 0.050000
Training Epoch: 24 [6528/50000]	Loss: 1.0608	LR: 0.050000
Training Epoch: 24 [6656/50000]	Loss: 1.2758	LR: 0.050000
Training Epoch: 24 [6784/50000]	Loss: 1.2786	LR: 0.050000
Training Epoch: 24 [6912/50000]	Loss: 1.2170	LR: 0.050000
Training Epoch: 24 [7040/50000]	Loss: 1.0265	LR: 0.050000
Training Epoch: 24 [7168/50000]	Loss: 0.9781	LR: 0.050000
Training Epoch: 24 [7296/50000]	Loss: 1.1781	LR: 0.050000
Training Epoch: 24 [7424/50000]	Loss: 1.0743	LR: 0.050000
Training Epoch: 24 [7552/50000]	Loss: 1.1905	LR: 0.050000
Training Epoch: 24 [7680/50000]	Loss: 0.9592	LR: 0.050000
Training Epoch: 24 [7808/50000]	Loss: 0.8581	LR: 0.050000
Training Epoch: 24 [7936/50000]	Loss: 0.9426	LR: 0.050000
Training Epoch: 24 [8064/50000]	Loss: 1.2438	LR: 0.050000
Training Epoch: 24 [8192/50000]	Loss: 0.8508	LR: 0.050000
Training Epoch: 24 [8320/50000]	Loss: 0.9943	LR: 0.050000
Training Epoch: 24 [8448/50000]	Loss: 1.1292	LR: 0.050000
Training Epoch: 24 [8576/50000]	Loss: 1.0531	LR: 0.050000
Training Epoch: 24 [8704/50000]	Loss: 1.0296	LR: 0.050000
Training Epoch: 24 [8832/50000]	Loss: 1.0336	LR: 0.050000
Training Epoch: 24 [8960/50000]	Loss: 1.0590	LR: 0.050000
Training Epoch: 24 [9088/50000]	Loss: 1.0012	LR: 0.050000
Training Epoch: 24 [9216/50000]	Loss: 1.0132	LR: 0.050000
Training Epoch: 24 [9344/50000]	Loss: 1.1456	LR: 0.050000
Training Epoch: 24 [9472/50000]	Loss: 1.1004	LR: 0.050000
Training Epoch: 24 [9600/50000]	Loss: 1.0234	LR: 0.050000
Training Epoch: 24 [9728/50000]	Loss: 0.9301	LR: 0.050000
Training Epoch: 24 [9856/50000]	Loss: 1.0278	LR: 0.050000
Training Epoch: 24 [9984/50000]	Loss: 1.2806	LR: 0.050000
Training Epoch: 24 [10112/50000]	Loss: 1.1027	LR: 0.050000
Training Epoch: 24 [10240/50000]	Loss: 1.0385	LR: 0.050000
Training Epoch: 24 [10368/50000]	Loss: 1.0151	LR: 0.050000
Training Epoch: 24 [10496/50000]	Loss: 1.0507	LR: 0.050000
Training Epoch: 24 [10624/50000]	Loss: 1.0775	LR: 0.050000
Training Epoch: 24 [10752/50000]	Loss: 1.1220	LR: 0.050000
Training Epoch: 24 [10880/50000]	Loss: 1.1283	LR: 0.050000
Training Epoch: 24 [11008/50000]	Loss: 1.1073	LR: 0.050000
Training Epoch: 24 [11136/50000]	Loss: 0.9727	LR: 0.050000
Training Epoch: 24 [11264/50000]	Loss: 0.9860	LR: 0.050000
Training Epoch: 24 [11392/50000]	Loss: 0.8974	LR: 0.050000
Training Epoch: 24 [11520/50000]	Loss: 0.8980	LR: 0.050000
Training Epoch: 24 [11648/50000]	Loss: 0.9563	LR: 0.050000
Training Epoch: 24 [11776/50000]	Loss: 0.9701	LR: 0.050000
Training Epoch: 24 [11904/50000]	Loss: 1.2427	LR: 0.050000
Training Epoch: 24 [12032/50000]	Loss: 1.0969	LR: 0.050000
Training Epoch: 24 [12160/50000]	Loss: 1.1498	LR: 0.050000
Training Epoch: 24 [12288/50000]	Loss: 1.0208	LR: 0.050000
Training Epoch: 24 [12416/50000]	Loss: 0.9159	LR: 0.050000
Training Epoch: 24 [12544/50000]	Loss: 1.1801	LR: 0.050000
Training Epoch: 24 [12672/50000]	Loss: 1.0059	LR: 0.050000
Training Epoch: 24 [12800/50000]	Loss: 0.8999	LR: 0.050000
Training Epoch: 24 [12928/50000]	Loss: 1.0321	LR: 0.050000
Training Epoch: 24 [13056/50000]	Loss: 1.2868	LR: 0.050000
Training Epoch: 24 [13184/50000]	Loss: 1.2026	LR: 0.050000
Training Epoch: 24 [13312/50000]	Loss: 1.1394	LR: 0.050000
Training Epoch: 24 [13440/50000]	Loss: 1.1226	LR: 0.050000
Training Epoch: 24 [13568/50000]	Loss: 1.1368	LR: 0.050000
Training Epoch: 24 [13696/50000]	Loss: 1.0012	LR: 0.050000
Training Epoch: 24 [13824/50000]	Loss: 1.0943	LR: 0.050000
Training Epoch: 24 [13952/50000]	Loss: 1.1310	LR: 0.050000
Training Epoch: 24 [14080/50000]	Loss: 1.0331	LR: 0.050000
Training Epoch: 24 [14208/50000]	Loss: 1.1376	LR: 0.050000
Training Epoch: 24 [14336/50000]	Loss: 1.0869	LR: 0.050000
Training Epoch: 24 [14464/50000]	Loss: 1.0746	LR: 0.050000
Training Epoch: 24 [14592/50000]	Loss: 1.1523	LR: 0.050000
Training Epoch: 24 [14720/50000]	Loss: 1.2998	LR: 0.050000
Training Epoch: 24 [14848/50000]	Loss: 1.0397	LR: 0.050000
Training Epoch: 24 [14976/50000]	Loss: 1.0121	LR: 0.050000
Training Epoch: 24 [15104/50000]	Loss: 1.0732	LR: 0.050000
Training Epoch: 24 [15232/50000]	Loss: 1.1142	LR: 0.050000
Training Epoch: 24 [15360/50000]	Loss: 1.1230	LR: 0.050000
Training Epoch: 24 [15488/50000]	Loss: 1.1290	LR: 0.050000
Training Epoch: 24 [15616/50000]	Loss: 1.0220	LR: 0.050000
Training Epoch: 24 [15744/50000]	Loss: 1.2190	LR: 0.050000
Training Epoch: 24 [15872/50000]	Loss: 1.1775	LR: 0.050000
Training Epoch: 24 [16000/50000]	Loss: 1.0141	LR: 0.050000
Training Epoch: 24 [16128/50000]	Loss: 0.9112	LR: 0.050000
Training Epoch: 24 [16256/50000]	Loss: 1.0062	LR: 0.050000
Training Epoch: 24 [16384/50000]	Loss: 1.2303	LR: 0.050000
Training Epoch: 24 [16512/50000]	Loss: 0.9883	LR: 0.050000
Training Epoch: 24 [16640/50000]	Loss: 1.1942	LR: 0.050000
Training Epoch: 24 [16768/50000]	Loss: 1.1499	LR: 0.050000
Training Epoch: 24 [16896/50000]	Loss: 1.0549	LR: 0.050000
Training Epoch: 24 [17024/50000]	Loss: 0.9620	LR: 0.050000
Training Epoch: 24 [17152/50000]	Loss: 1.2013	LR: 0.050000
Training Epoch: 24 [17280/50000]	Loss: 1.2820	LR: 0.050000
Training Epoch: 24 [17408/50000]	Loss: 1.0454	LR: 0.050000
Training Epoch: 24 [17536/50000]	Loss: 1.1831	LR: 0.050000
Training Epoch: 24 [17664/50000]	Loss: 1.1754	LR: 0.050000
Training Epoch: 24 [17792/50000]	Loss: 1.2472	LR: 0.050000
Training Epoch: 24 [17920/50000]	Loss: 1.0978	LR: 0.050000
Training Epoch: 24 [18048/50000]	Loss: 1.2111	LR: 0.050000
Training Epoch: 24 [18176/50000]	Loss: 0.9715	LR: 0.050000
Training Epoch: 24 [18304/50000]	Loss: 1.1041	LR: 0.050000
Training Epoch: 24 [18432/50000]	Loss: 1.0031	LR: 0.050000
Training Epoch: 24 [18560/50000]	Loss: 0.7909	LR: 0.050000
Training Epoch: 24 [18688/50000]	Loss: 1.1471	LR: 0.050000
Training Epoch: 24 [18816/50000]	Loss: 1.0304	LR: 0.050000
Training Epoch: 24 [18944/50000]	Loss: 1.0833	LR: 0.050000
Training Epoch: 24 [19072/50000]	Loss: 1.2407	LR: 0.050000
Training Epoch: 24 [19200/50000]	Loss: 1.1440	LR: 0.050000
Training Epoch: 24 [19328/50000]	Loss: 1.0083	LR: 0.050000
Training Epoch: 24 [19456/50000]	Loss: 1.1756	LR: 0.050000
Training Epoch: 24 [19584/50000]	Loss: 1.1535	LR: 0.050000
Training Epoch: 24 [19712/50000]	Loss: 0.9672	LR: 0.050000
Training Epoch: 24 [19840/50000]	Loss: 1.1600	LR: 0.050000
Training Epoch: 24 [19968/50000]	Loss: 1.1610	LR: 0.050000
Training Epoch: 24 [20096/50000]	Loss: 1.1568	LR: 0.050000
Training Epoch: 24 [20224/50000]	Loss: 1.2189	LR: 0.050000
Training Epoch: 24 [20352/50000]	Loss: 1.1104	LR: 0.050000
Training Epoch: 24 [20480/50000]	Loss: 1.1156	LR: 0.050000
Training Epoch: 24 [20608/50000]	Loss: 1.0713	LR: 0.050000
Training Epoch: 24 [20736/50000]	Loss: 1.4022	LR: 0.050000
Training Epoch: 24 [20864/50000]	Loss: 1.0799	LR: 0.050000
Training Epoch: 24 [20992/50000]	Loss: 1.3418	LR: 0.050000
Training Epoch: 24 [21120/50000]	Loss: 1.1882	LR: 0.050000
Training Epoch: 24 [21248/50000]	Loss: 1.0928	LR: 0.050000
Training Epoch: 24 [21376/50000]	Loss: 1.1202	LR: 0.050000
Training Epoch: 24 [21504/50000]	Loss: 1.1185	LR: 0.050000
Training Epoch: 24 [21632/50000]	Loss: 1.4758	LR: 0.050000
Training Epoch: 24 [21760/50000]	Loss: 1.1574	LR: 0.050000
Training Epoch: 24 [21888/50000]	Loss: 1.0202	LR: 0.050000
Training Epoch: 24 [22016/50000]	Loss: 1.1847	LR: 0.050000
Training Epoch: 24 [22144/50000]	Loss: 1.4358	LR: 0.050000
Training Epoch: 24 [22272/50000]	Loss: 1.2785	LR: 0.050000
Training Epoch: 24 [22400/50000]	Loss: 1.1617	LR: 0.050000
Training Epoch: 24 [22528/50000]	Loss: 1.1423	LR: 0.050000
Training Epoch: 24 [22656/50000]	Loss: 1.1064	LR: 0.050000
Training Epoch: 24 [22784/50000]	Loss: 1.0303	LR: 0.050000
Training Epoch: 24 [22912/50000]	Loss: 1.2183	LR: 0.050000
Training Epoch: 24 [23040/50000]	Loss: 1.1573	LR: 0.050000
Training Epoch: 24 [23168/50000]	Loss: 1.2074	LR: 0.050000
Training Epoch: 24 [23296/50000]	Loss: 1.2594	LR: 0.050000
Training Epoch: 24 [23424/50000]	Loss: 1.1454	LR: 0.050000
Training Epoch: 24 [23552/50000]	Loss: 1.3311	LR: 0.050000
Training Epoch: 24 [23680/50000]	Loss: 1.1303	LR: 0.050000
Training Epoch: 24 [23808/50000]	Loss: 1.2301	LR: 0.050000
Training Epoch: 24 [23936/50000]	Loss: 1.0493	LR: 0.050000
Training Epoch: 24 [24064/50000]	Loss: 1.2308	LR: 0.050000
Training Epoch: 24 [24192/50000]	Loss: 1.0195	LR: 0.050000
Training Epoch: 24 [24320/50000]	Loss: 1.2144	LR: 0.050000
Training Epoch: 24 [24448/50000]	Loss: 1.0164	LR: 0.050000
Training Epoch: 24 [24576/50000]	Loss: 0.8819	LR: 0.050000
Training Epoch: 24 [24704/50000]	Loss: 1.0561	LR: 0.050000
Training Epoch: 24 [24832/50000]	Loss: 1.1505	LR: 0.050000
Training Epoch: 24 [24960/50000]	Loss: 1.1724	LR: 0.050000
Training Epoch: 24 [25088/50000]	Loss: 1.1564	LR: 0.050000
Training Epoch: 24 [25216/50000]	Loss: 1.1109	LR: 0.050000
Training Epoch: 24 [25344/50000]	Loss: 1.2724	LR: 0.050000
Training Epoch: 24 [25472/50000]	Loss: 1.1090	LR: 0.050000
Training Epoch: 24 [25600/50000]	Loss: 1.1170	LR: 0.050000
Training Epoch: 24 [25728/50000]	Loss: 1.2191	LR: 0.050000
Training Epoch: 24 [25856/50000]	Loss: 1.0832	LR: 0.050000
Training Epoch: 24 [25984/50000]	Loss: 1.1563	LR: 0.050000
Training Epoch: 24 [26112/50000]	Loss: 1.0877	LR: 0.050000
Training Epoch: 24 [26240/50000]	Loss: 1.1602	LR: 0.050000
Training Epoch: 24 [26368/50000]	Loss: 1.4016	LR: 0.050000
Training Epoch: 24 [26496/50000]	Loss: 1.2323	LR: 0.050000
Training Epoch: 24 [26624/50000]	Loss: 1.1973	LR: 0.050000
Training Epoch: 24 [26752/50000]	Loss: 1.1045	LR: 0.050000
Training Epoch: 24 [26880/50000]	Loss: 1.0988	LR: 0.050000
Training Epoch: 24 [27008/50000]	Loss: 1.1608	LR: 0.050000
Training Epoch: 24 [27136/50000]	Loss: 1.2266	LR: 0.050000
Training Epoch: 24 [27264/50000]	Loss: 0.9261	LR: 0.050000
Training Epoch: 24 [27392/50000]	Loss: 1.1650	LR: 0.050000
Training Epoch: 24 [27520/50000]	Loss: 1.0898	LR: 0.050000
Training Epoch: 24 [27648/50000]	Loss: 1.2121	LR: 0.050000
Training Epoch: 24 [27776/50000]	Loss: 1.3046	LR: 0.050000
Training Epoch: 24 [27904/50000]	Loss: 1.2734	LR: 0.050000
Training Epoch: 24 [28032/50000]	Loss: 1.0525	LR: 0.050000
Training Epoch: 24 [28160/50000]	Loss: 1.1306	LR: 0.050000
Training Epoch: 24 [28288/50000]	Loss: 1.1369	LR: 0.050000
Training Epoch: 24 [28416/50000]	Loss: 1.0597	LR: 0.050000
Training Epoch: 24 [28544/50000]	Loss: 1.0940	LR: 0.050000
Training Epoch: 24 [28672/50000]	Loss: 1.1830	LR: 0.050000
Training Epoch: 24 [28800/50000]	Loss: 1.3255	LR: 0.050000
Training Epoch: 24 [28928/50000]	Loss: 1.1137	LR: 0.050000
Training Epoch: 24 [29056/50000]	Loss: 0.8620	LR: 0.050000
Training Epoch: 24 [29184/50000]	Loss: 1.1878	LR: 0.050000
Training Epoch: 24 [29312/50000]	Loss: 1.0369	LR: 0.050000
Training Epoch: 24 [29440/50000]	Loss: 1.0964	LR: 0.050000
Training Epoch: 24 [29568/50000]	Loss: 1.3160	LR: 0.050000
Training Epoch: 24 [29696/50000]	Loss: 1.1151	LR: 0.050000
Training Epoch: 24 [29824/50000]	Loss: 1.0592	LR: 0.050000
Training Epoch: 24 [29952/50000]	Loss: 1.2980	LR: 0.050000
Training Epoch: 24 [30080/50000]	Loss: 1.2593	LR: 0.050000
Training Epoch: 24 [30208/50000]	Loss: 1.2362	LR: 0.050000
Training Epoch: 24 [30336/50000]	Loss: 0.9942	LR: 0.050000
Training Epoch: 24 [30464/50000]	Loss: 1.1469	LR: 0.050000
Training Epoch: 24 [30592/50000]	Loss: 1.1489	LR: 0.050000
Training Epoch: 24 [30720/50000]	Loss: 0.9391	LR: 0.050000
Training Epoch: 24 [30848/50000]	Loss: 1.0681	LR: 0.050000
Training Epoch: 24 [30976/50000]	Loss: 0.9317	LR: 0.050000
Training Epoch: 24 [31104/50000]	Loss: 1.3279	LR: 0.050000
Training Epoch: 24 [31232/50000]	Loss: 0.8732	LR: 0.050000
Training Epoch: 24 [31360/50000]	Loss: 1.2933	LR: 0.050000
Training Epoch: 24 [31488/50000]	Loss: 1.1445	LR: 0.050000
Training Epoch: 24 [31616/50000]	Loss: 1.2179	LR: 0.050000
Training Epoch: 24 [31744/50000]	Loss: 1.0475	LR: 0.050000
Training Epoch: 24 [31872/50000]	Loss: 1.0728	LR: 0.050000
Training Epoch: 24 [32000/50000]	Loss: 1.0101	LR: 0.050000
Training Epoch: 24 [32128/50000]	Loss: 1.0904	LR: 0.050000
Training Epoch: 24 [32256/50000]	Loss: 0.9663	LR: 0.050000
Training Epoch: 24 [32384/50000]	Loss: 0.9844	LR: 0.050000
Training Epoch: 24 [32512/50000]	Loss: 1.1616	LR: 0.050000
Training Epoch: 24 [32640/50000]	Loss: 0.9235	LR: 0.050000
Training Epoch: 24 [32768/50000]	Loss: 1.0073	LR: 0.050000
Training Epoch: 24 [32896/50000]	Loss: 1.0652	LR: 0.050000
Training Epoch: 24 [33024/50000]	Loss: 1.1558	LR: 0.050000
Training Epoch: 24 [33152/50000]	Loss: 1.0292	LR: 0.050000
Training Epoch: 24 [33280/50000]	Loss: 1.1575	LR: 0.050000
Training Epoch: 24 [33408/50000]	Loss: 1.1430	LR: 0.050000
Training Epoch: 24 [33536/50000]	Loss: 1.1839	LR: 0.050000
Training Epoch: 24 [33664/50000]	Loss: 1.1531	LR: 0.050000
Training Epoch: 24 [33792/50000]	Loss: 1.0146	LR: 0.050000
Training Epoch: 24 [33920/50000]	Loss: 1.2052	LR: 0.050000
Training Epoch: 24 [34048/50000]	Loss: 1.1730	LR: 0.050000
Training Epoch: 24 [34176/50000]	Loss: 1.2566	LR: 0.050000
Training Epoch: 24 [34304/50000]	Loss: 0.8101	LR: 0.050000
Training Epoch: 24 [34432/50000]	Loss: 1.1379	LR: 0.050000
Training Epoch: 24 [34560/50000]	Loss: 0.9640	LR: 0.050000
Training Epoch: 24 [34688/50000]	Loss: 1.1119	LR: 0.050000
Training Epoch: 24 [34816/50000]	Loss: 1.0870	LR: 0.050000
Training Epoch: 24 [34944/50000]	Loss: 1.0597	LR: 0.050000
Training Epoch: 24 [35072/50000]	Loss: 1.0935	LR: 0.050000
Training Epoch: 24 [35200/50000]	Loss: 1.4673	LR: 0.050000
Training Epoch: 24 [35328/50000]	Loss: 1.2282	LR: 0.050000
Training Epoch: 24 [35456/50000]	Loss: 1.0706	LR: 0.050000
Training Epoch: 24 [35584/50000]	Loss: 1.1674	LR: 0.050000
Training Epoch: 24 [35712/50000]	Loss: 1.0686	LR: 0.050000
Training Epoch: 24 [35840/50000]	Loss: 1.0154	LR: 0.050000
Training Epoch: 24 [35968/50000]	Loss: 0.8653	LR: 0.050000
Training Epoch: 24 [36096/50000]	Loss: 1.1940	LR: 0.050000
Training Epoch: 24 [36224/50000]	Loss: 1.2450	LR: 0.050000
Training Epoch: 24 [36352/50000]	Loss: 1.1905	LR: 0.050000
Training Epoch: 24 [36480/50000]	Loss: 1.0307	LR: 0.050000
Training Epoch: 24 [36608/50000]	Loss: 1.0855	LR: 0.050000
Training Epoch: 24 [36736/50000]	Loss: 1.3450	LR: 0.050000
Training Epoch: 24 [36864/50000]	Loss: 1.1359	LR: 0.050000
Training Epoch: 24 [36992/50000]	Loss: 1.0700	LR: 0.050000
Training Epoch: 24 [37120/50000]	Loss: 1.1435	LR: 0.050000
Training Epoch: 24 [37248/50000]	Loss: 1.0972	LR: 0.050000
Training Epoch: 24 [37376/50000]	Loss: 1.0738	LR: 0.050000
Training Epoch: 24 [37504/50000]	Loss: 1.1922	LR: 0.050000
Training Epoch: 24 [37632/50000]	Loss: 1.1768	LR: 0.050000
Training Epoch: 24 [37760/50000]	Loss: 1.0151	LR: 0.050000
Training Epoch: 24 [37888/50000]	Loss: 1.1446	LR: 0.050000
Training Epoch: 24 [38016/50000]	Loss: 0.9831	LR: 0.050000
Training Epoch: 24 [38144/50000]	Loss: 1.0053	LR: 0.050000
Training Epoch: 24 [38272/50000]	Loss: 1.1731	LR: 0.050000
Training Epoch: 24 [38400/50000]	Loss: 1.2308	LR: 0.050000
Training Epoch: 24 [38528/50000]	Loss: 1.0530	LR: 0.050000
Training Epoch: 24 [38656/50000]	Loss: 1.2900	LR: 0.050000
Training Epoch: 24 [38784/50000]	Loss: 1.2161	LR: 0.050000
Training Epoch: 24 [38912/50000]	Loss: 1.0756	LR: 0.050000
Training Epoch: 24 [39040/50000]	Loss: 1.1398	LR: 0.050000
Training Epoch: 24 [39168/50000]	Loss: 1.1414	LR: 0.050000
Training Epoch: 24 [39296/50000]	Loss: 1.3256	LR: 0.050000
Training Epoch: 24 [39424/50000]	Loss: 1.2151	LR: 0.050000
Training Epoch: 24 [39552/50000]	Loss: 1.0640	LR: 0.050000
Training Epoch: 24 [39680/50000]	Loss: 0.9983	LR: 0.050000
Training Epoch: 24 [39808/50000]	Loss: 1.1305	LR: 0.050000
Training Epoch: 24 [39936/50000]	Loss: 1.2194	LR: 0.050000
Training Epoch: 24 [40064/50000]	Loss: 0.9994	LR: 0.050000
Training Epoch: 24 [40192/50000]	Loss: 0.9995	LR: 0.050000
Training Epoch: 24 [40320/50000]	Loss: 0.9128	LR: 0.050000
Training Epoch: 24 [40448/50000]	Loss: 1.1173	LR: 0.050000
Training Epoch: 24 [40576/50000]	Loss: 0.9321	LR: 0.050000
Training Epoch: 24 [40704/50000]	Loss: 1.0942	LR: 0.050000
Training Epoch: 24 [40832/50000]	Loss: 1.1403	LR: 0.050000
Training Epoch: 24 [40960/50000]	Loss: 1.1600	LR: 0.050000
Training Epoch: 24 [41088/50000]	Loss: 1.3543	LR: 0.050000
Training Epoch: 24 [41216/50000]	Loss: 1.4113	LR: 0.050000
Training Epoch: 24 [41344/50000]	Loss: 1.1400	LR: 0.050000
Training Epoch: 24 [41472/50000]	Loss: 1.2277	LR: 0.050000
Training Epoch: 24 [41600/50000]	Loss: 1.0917	LR: 0.050000
Training Epoch: 24 [41728/50000]	Loss: 1.2246	LR: 0.050000
Training Epoch: 24 [41856/50000]	Loss: 1.1456	LR: 0.050000
Training Epoch: 24 [41984/50000]	Loss: 1.1598	LR: 0.050000
Training Epoch: 24 [42112/50000]	Loss: 1.0824	LR: 0.050000
Training Epoch: 24 [42240/50000]	Loss: 1.2061	LR: 0.050000
Training Epoch: 24 [42368/50000]	Loss: 0.9793	LR: 0.050000
Training Epoch: 24 [42496/50000]	Loss: 1.1098	LR: 0.050000
Training Epoch: 24 [42624/50000]	Loss: 1.2704	LR: 0.050000
Training Epoch: 24 [42752/50000]	Loss: 1.0138	LR: 0.050000
Training Epoch: 24 [42880/50000]	Loss: 1.0927	LR: 0.050000
Training Epoch: 24 [43008/50000]	Loss: 0.9806	LR: 0.050000
Training Epoch: 24 [43136/50000]	Loss: 1.1859	LR: 0.050000
Training Epoch: 24 [43264/50000]	Loss: 1.2687	LR: 0.050000
Training Epoch: 24 [43392/50000]	Loss: 1.0936	LR: 0.050000
Training Epoch: 24 [43520/50000]	Loss: 0.9504	LR: 0.050000
Training Epoch: 24 [43648/50000]	Loss: 1.0383	LR: 0.050000
Training Epoch: 24 [43776/50000]	Loss: 1.2728	LR: 0.050000
Training Epoch: 24 [43904/50000]	Loss: 1.2409	LR: 0.050000
Training Epoch: 24 [44032/50000]	Loss: 0.9779	LR: 0.050000
Training Epoch: 24 [44160/50000]	Loss: 1.3621	LR: 0.050000
Training Epoch: 24 [44288/50000]	Loss: 1.1310	LR: 0.050000
Training Epoch: 24 [44416/50000]	Loss: 1.1977	LR: 0.050000
Training Epoch: 24 [44544/50000]	Loss: 1.0904	LR: 0.050000
Training Epoch: 24 [44672/50000]	Loss: 1.1165	LR: 0.050000
Training Epoch: 24 [44800/50000]	Loss: 1.2859	LR: 0.050000
Training Epoch: 24 [44928/50000]	Loss: 1.3047	LR: 0.050000
Training Epoch: 24 [45056/50000]	Loss: 1.1821	LR: 0.050000
Training Epoch: 24 [45184/50000]	Loss: 0.8922	LR: 0.050000
Training Epoch: 24 [45312/50000]	Loss: 1.2224	LR: 0.050000
Training Epoch: 24 [45440/50000]	Loss: 1.1613	LR: 0.050000
Training Epoch: 24 [45568/50000]	Loss: 1.0684	LR: 0.050000
Training Epoch: 24 [45696/50000]	Loss: 1.5185	LR: 0.050000
Training Epoch: 24 [45824/50000]	Loss: 1.2273	LR: 0.050000
Training Epoch: 24 [45952/50000]	Loss: 1.3992	LR: 0.050000
Training Epoch: 24 [46080/50000]	Loss: 1.3168	LR: 0.050000
Training Epoch: 24 [46208/50000]	Loss: 1.3001	LR: 0.050000
Training Epoch: 24 [46336/50000]	Loss: 1.2618	LR: 0.050000
Training Epoch: 24 [46464/50000]	Loss: 1.1137	LR: 0.050000
Training Epoch: 24 [46592/50000]	Loss: 1.3955	LR: 0.050000
Training Epoch: 24 [46720/50000]	Loss: 1.0748	LR: 0.050000
Training Epoch: 24 [46848/50000]	Loss: 1.2864	LR: 0.050000
Training Epoch: 24 [46976/50000]	Loss: 1.2121	LR: 0.050000
Training Epoch: 24 [47104/50000]	Loss: 1.0161	LR: 0.050000
Training Epoch: 24 [47232/50000]	Loss: 1.1083	LR: 0.050000
Training Epoch: 24 [47360/50000]	Loss: 1.2650	LR: 0.050000
Training Epoch: 24 [47488/50000]	Loss: 1.2238	LR: 0.050000
Training Epoch: 24 [47616/50000]	Loss: 1.1087	LR: 0.050000
Training Epoch: 24 [47744/50000]	Loss: 1.3098	LR: 0.050000
Training Epoch: 24 [47872/50000]	Loss: 1.2729	LR: 0.050000
Training Epoch: 24 [48000/50000]	Loss: 1.0720	LR: 0.050000
Training Epoch: 24 [48128/50000]	Loss: 1.3091	LR: 0.050000
Training Epoch: 24 [48256/50000]	Loss: 1.2793	LR: 0.050000
Training Epoch: 24 [48384/50000]	Loss: 1.1255	LR: 0.050000
Training Epoch: 24 [48512/50000]	Loss: 1.4035	LR: 0.050000
Training Epoch: 24 [48640/50000]	Loss: 1.3041	LR: 0.050000
Training Epoch: 24 [48768/50000]	Loss: 1.2727	LR: 0.050000
Training Epoch: 24 [48896/50000]	Loss: 1.2646	LR: 0.050000
Training Epoch: 24 [49024/50000]	Loss: 1.3124	LR: 0.050000
Training Epoch: 24 [49152/50000]	Loss: 1.2903	LR: 0.050000
Training Epoch: 24 [49280/50000]	Loss: 1.0013	LR: 0.050000
Training Epoch: 24 [49408/50000]	Loss: 1.2445	LR: 0.050000
Training Epoch: 24 [49536/50000]	Loss: 1.2558	LR: 0.050000
Training Epoch: 24 [49664/50000]	Loss: 1.3205	LR: 0.050000
Training Epoch: 24 [49792/50000]	Loss: 1.1413	LR: 0.050000
Training Epoch: 24 [49920/50000]	Loss: 1.1792	LR: 0.050000
Training Epoch: 24 [50000/50000]	Loss: 1.3280	LR: 0.050000
epoch 24 training time consumed: 54.09s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   88081 GB |   88081 GB |
|       from large pool |  123392 KB |    1034 MB |   87995 GB |   87994 GB |
|       from small pool |   10798 KB |      13 MB |      86 GB |      86 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   88081 GB |   88081 GB |
|       from large pool |  123392 KB |    1034 MB |   87995 GB |   87994 GB |
|       from small pool |   10798 KB |      13 MB |      86 GB |      86 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   38763 GB |   38763 GB |
|       from large pool |  155136 KB |  433088 KB |   38667 GB |   38667 GB |
|       from small pool |    1490 KB |    3494 KB |      95 GB |      95 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    3399 K  |    3398 K  |
|       from large pool |      24    |      65    |    1774 K  |    1774 K  |
|       from small pool |     231    |     274    |    1625 K  |    1624 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    3399 K  |    3398 K  |
|       from large pool |      24    |      65    |    1774 K  |    1774 K  |
|       from small pool |     231    |     274    |    1625 K  |    1624 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1682 K  |    1682 K  |
|       from large pool |       9    |      14    |     858 K  |     858 K  |
|       from small pool |      12    |      16    |     823 K  |     823 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 24, Average loss: 0.0121, Accuracy: 0.5856, Time consumed:3.48s

Training Epoch: 25 [128/50000]	Loss: 1.1131	LR: 0.050000
Training Epoch: 25 [256/50000]	Loss: 1.1625	LR: 0.050000
Training Epoch: 25 [384/50000]	Loss: 1.0514	LR: 0.050000
Training Epoch: 25 [512/50000]	Loss: 0.9201	LR: 0.050000
Training Epoch: 25 [640/50000]	Loss: 0.8362	LR: 0.050000
Training Epoch: 25 [768/50000]	Loss: 0.9426	LR: 0.050000
Training Epoch: 25 [896/50000]	Loss: 0.9548	LR: 0.050000
Training Epoch: 25 [1024/50000]	Loss: 0.9329	LR: 0.050000
Training Epoch: 25 [1152/50000]	Loss: 0.9198	LR: 0.050000
Training Epoch: 25 [1280/50000]	Loss: 1.0765	LR: 0.050000
Training Epoch: 25 [1408/50000]	Loss: 1.1450	LR: 0.050000
Training Epoch: 25 [1536/50000]	Loss: 1.1127	LR: 0.050000
Training Epoch: 25 [1664/50000]	Loss: 1.0544	LR: 0.050000
Training Epoch: 25 [1792/50000]	Loss: 0.9647	LR: 0.050000
Training Epoch: 25 [1920/50000]	Loss: 0.8292	LR: 0.050000
Training Epoch: 25 [2048/50000]	Loss: 1.2491	LR: 0.050000
Training Epoch: 25 [2176/50000]	Loss: 1.0232	LR: 0.050000
Training Epoch: 25 [2304/50000]	Loss: 1.0205	LR: 0.050000
Training Epoch: 25 [2432/50000]	Loss: 1.0885	LR: 0.050000
Training Epoch: 25 [2560/50000]	Loss: 1.0473	LR: 0.050000
Training Epoch: 25 [2688/50000]	Loss: 0.9425	LR: 0.050000
Training Epoch: 25 [2816/50000]	Loss: 0.8691	LR: 0.050000
Training Epoch: 25 [2944/50000]	Loss: 0.9035	LR: 0.050000
Training Epoch: 25 [3072/50000]	Loss: 1.1519	LR: 0.050000
Training Epoch: 25 [3200/50000]	Loss: 1.2279	LR: 0.050000
Training Epoch: 25 [3328/50000]	Loss: 1.1198	LR: 0.050000
Training Epoch: 25 [3456/50000]	Loss: 0.8184	LR: 0.050000
Training Epoch: 25 [3584/50000]	Loss: 0.8920	LR: 0.050000
Training Epoch: 25 [3712/50000]	Loss: 1.1905	LR: 0.050000
Training Epoch: 25 [3840/50000]	Loss: 1.0339	LR: 0.050000
Training Epoch: 25 [3968/50000]	Loss: 1.0108	LR: 0.050000
Training Epoch: 25 [4096/50000]	Loss: 0.9866	LR: 0.050000
Training Epoch: 25 [4224/50000]	Loss: 0.8226	LR: 0.050000
Training Epoch: 25 [4352/50000]	Loss: 1.0050	LR: 0.050000
Training Epoch: 25 [4480/50000]	Loss: 1.1249	LR: 0.050000
Training Epoch: 25 [4608/50000]	Loss: 0.9850	LR: 0.050000
Training Epoch: 25 [4736/50000]	Loss: 1.0298	LR: 0.050000
Training Epoch: 25 [4864/50000]	Loss: 0.9619	LR: 0.050000
Training Epoch: 25 [4992/50000]	Loss: 0.9639	LR: 0.050000
Training Epoch: 25 [5120/50000]	Loss: 0.9672	LR: 0.050000
Training Epoch: 25 [5248/50000]	Loss: 0.8276	LR: 0.050000
Training Epoch: 25 [5376/50000]	Loss: 0.9158	LR: 0.050000
Training Epoch: 25 [5504/50000]	Loss: 0.9837	LR: 0.050000
Training Epoch: 25 [5632/50000]	Loss: 0.8943	LR: 0.050000
Training Epoch: 25 [5760/50000]	Loss: 0.8686	LR: 0.050000
Training Epoch: 25 [5888/50000]	Loss: 1.1775	LR: 0.050000
Training Epoch: 25 [6016/50000]	Loss: 1.1579	LR: 0.050000
Training Epoch: 25 [6144/50000]	Loss: 1.1318	LR: 0.050000
Training Epoch: 25 [6272/50000]	Loss: 0.8965	LR: 0.050000
Training Epoch: 25 [6400/50000]	Loss: 1.0776	LR: 0.050000
Training Epoch: 25 [6528/50000]	Loss: 0.8332	LR: 0.050000
Training Epoch: 25 [6656/50000]	Loss: 0.9040	LR: 0.050000
Training Epoch: 25 [6784/50000]	Loss: 1.1007	LR: 0.050000
Training Epoch: 25 [6912/50000]	Loss: 1.0038	LR: 0.050000
Training Epoch: 25 [7040/50000]	Loss: 1.1553	LR: 0.050000
Training Epoch: 25 [7168/50000]	Loss: 1.2902	LR: 0.050000
Training Epoch: 25 [7296/50000]	Loss: 1.0805	LR: 0.050000
Training Epoch: 25 [7424/50000]	Loss: 1.0504	LR: 0.050000
Training Epoch: 25 [7552/50000]	Loss: 0.8390	LR: 0.050000
Training Epoch: 25 [7680/50000]	Loss: 1.2030	LR: 0.050000
Training Epoch: 25 [7808/50000]	Loss: 0.9945	LR: 0.050000
Training Epoch: 25 [7936/50000]	Loss: 1.0675	LR: 0.050000
Training Epoch: 25 [8064/50000]	Loss: 1.2025	LR: 0.050000
Training Epoch: 25 [8192/50000]	Loss: 1.1765	LR: 0.050000
Training Epoch: 25 [8320/50000]	Loss: 1.0273	LR: 0.050000
Training Epoch: 25 [8448/50000]	Loss: 1.3965	LR: 0.050000
Training Epoch: 25 [8576/50000]	Loss: 0.8438	LR: 0.050000
Training Epoch: 25 [8704/50000]	Loss: 1.2044	LR: 0.050000
Training Epoch: 25 [8832/50000]	Loss: 0.9687	LR: 0.050000
Training Epoch: 25 [8960/50000]	Loss: 1.0505	LR: 0.050000
Training Epoch: 25 [9088/50000]	Loss: 0.9407	LR: 0.050000
Training Epoch: 25 [9216/50000]	Loss: 1.1911	LR: 0.050000
Training Epoch: 25 [9344/50000]	Loss: 1.1703	LR: 0.050000
Training Epoch: 25 [9472/50000]	Loss: 0.9914	LR: 0.050000
Training Epoch: 25 [9600/50000]	Loss: 1.1015	LR: 0.050000
Training Epoch: 25 [9728/50000]	Loss: 1.4232	LR: 0.050000
Training Epoch: 25 [9856/50000]	Loss: 1.1014	LR: 0.050000
Training Epoch: 25 [9984/50000]	Loss: 1.2288	LR: 0.050000
Training Epoch: 25 [10112/50000]	Loss: 1.0318	LR: 0.050000
Training Epoch: 25 [10240/50000]	Loss: 1.2773	LR: 0.050000
Training Epoch: 25 [10368/50000]	Loss: 1.1084	LR: 0.050000
Training Epoch: 25 [10496/50000]	Loss: 1.0474	LR: 0.050000
Training Epoch: 25 [10624/50000]	Loss: 1.1780	LR: 0.050000
Training Epoch: 25 [10752/50000]	Loss: 1.1873	LR: 0.050000
Training Epoch: 25 [10880/50000]	Loss: 0.9759	LR: 0.050000
Training Epoch: 25 [11008/50000]	Loss: 0.8453	LR: 0.050000
Training Epoch: 25 [11136/50000]	Loss: 0.9289	LR: 0.050000
Training Epoch: 25 [11264/50000]	Loss: 1.1424	LR: 0.050000
Training Epoch: 25 [11392/50000]	Loss: 1.1615	LR: 0.050000
Training Epoch: 25 [11520/50000]	Loss: 1.1144	LR: 0.050000
Training Epoch: 25 [11648/50000]	Loss: 1.0808	LR: 0.050000
Training Epoch: 25 [11776/50000]	Loss: 0.9118	LR: 0.050000
Training Epoch: 25 [11904/50000]	Loss: 1.1901	LR: 0.050000
Training Epoch: 25 [12032/50000]	Loss: 1.1827	LR: 0.050000
Training Epoch: 25 [12160/50000]	Loss: 1.0755	LR: 0.050000
Training Epoch: 25 [12288/50000]	Loss: 0.9889	LR: 0.050000
Training Epoch: 25 [12416/50000]	Loss: 1.1994	LR: 0.050000
Training Epoch: 25 [12544/50000]	Loss: 1.0603	LR: 0.050000
Training Epoch: 25 [12672/50000]	Loss: 1.1179	LR: 0.050000
Training Epoch: 25 [12800/50000]	Loss: 0.9581	LR: 0.050000
Training Epoch: 25 [12928/50000]	Loss: 1.0794	LR: 0.050000
Training Epoch: 25 [13056/50000]	Loss: 1.0535	LR: 0.050000
Training Epoch: 25 [13184/50000]	Loss: 1.1469	LR: 0.050000
Training Epoch: 25 [13312/50000]	Loss: 0.8987	LR: 0.050000
Training Epoch: 25 [13440/50000]	Loss: 1.2249	LR: 0.050000
Training Epoch: 25 [13568/50000]	Loss: 0.9294	LR: 0.050000
Training Epoch: 25 [13696/50000]	Loss: 1.0386	LR: 0.050000
Training Epoch: 25 [13824/50000]	Loss: 1.2430	LR: 0.050000
Training Epoch: 25 [13952/50000]	Loss: 0.9783	LR: 0.050000
Training Epoch: 25 [14080/50000]	Loss: 1.1525	LR: 0.050000
Training Epoch: 25 [14208/50000]	Loss: 0.8923	LR: 0.050000
Training Epoch: 25 [14336/50000]	Loss: 0.8683	LR: 0.050000
Training Epoch: 25 [14464/50000]	Loss: 1.1696	LR: 0.050000
Training Epoch: 25 [14592/50000]	Loss: 1.1124	LR: 0.050000
Training Epoch: 25 [14720/50000]	Loss: 1.1156	LR: 0.050000
Training Epoch: 25 [14848/50000]	Loss: 1.0548	LR: 0.050000
Training Epoch: 25 [14976/50000]	Loss: 1.2174	LR: 0.050000
Training Epoch: 25 [15104/50000]	Loss: 0.9522	LR: 0.050000
Training Epoch: 25 [15232/50000]	Loss: 1.0731	LR: 0.050000
Training Epoch: 25 [15360/50000]	Loss: 1.1735	LR: 0.050000
Training Epoch: 25 [15488/50000]	Loss: 0.9634	LR: 0.050000
Training Epoch: 25 [15616/50000]	Loss: 1.1517	LR: 0.050000
Training Epoch: 25 [15744/50000]	Loss: 0.9705	LR: 0.050000
Training Epoch: 25 [15872/50000]	Loss: 1.1582	LR: 0.050000
Training Epoch: 25 [16000/50000]	Loss: 1.2835	LR: 0.050000
Training Epoch: 25 [16128/50000]	Loss: 1.2903	LR: 0.050000
Training Epoch: 25 [16256/50000]	Loss: 1.0256	LR: 0.050000
Training Epoch: 25 [16384/50000]	Loss: 1.2085	LR: 0.050000
Training Epoch: 25 [16512/50000]	Loss: 0.9858	LR: 0.050000
Training Epoch: 25 [16640/50000]	Loss: 1.1914	LR: 0.050000
Training Epoch: 25 [16768/50000]	Loss: 1.1564	LR: 0.050000
Training Epoch: 25 [16896/50000]	Loss: 1.1676	LR: 0.050000
Training Epoch: 25 [17024/50000]	Loss: 1.2063	LR: 0.050000
Training Epoch: 25 [17152/50000]	Loss: 1.0711	LR: 0.050000
Training Epoch: 25 [17280/50000]	Loss: 0.9274	LR: 0.050000
Training Epoch: 25 [17408/50000]	Loss: 0.9735	LR: 0.050000
Training Epoch: 25 [17536/50000]	Loss: 1.0529	LR: 0.050000
Training Epoch: 25 [17664/50000]	Loss: 1.1649	LR: 0.050000
Training Epoch: 25 [17792/50000]	Loss: 1.3238	LR: 0.050000
Training Epoch: 25 [17920/50000]	Loss: 1.0591	LR: 0.050000
Training Epoch: 25 [18048/50000]	Loss: 0.9292	LR: 0.050000
Training Epoch: 25 [18176/50000]	Loss: 1.2472	LR: 0.050000
Training Epoch: 25 [18304/50000]	Loss: 1.1404	LR: 0.050000
Training Epoch: 25 [18432/50000]	Loss: 1.1202	LR: 0.050000
Training Epoch: 25 [18560/50000]	Loss: 1.2234	LR: 0.050000
Training Epoch: 25 [18688/50000]	Loss: 1.1301	LR: 0.050000
Training Epoch: 25 [18816/50000]	Loss: 1.0215	LR: 0.050000
Training Epoch: 25 [18944/50000]	Loss: 1.1740	LR: 0.050000
Training Epoch: 25 [19072/50000]	Loss: 1.2287	LR: 0.050000
Training Epoch: 25 [19200/50000]	Loss: 1.2161	LR: 0.050000
Training Epoch: 25 [19328/50000]	Loss: 1.0518	LR: 0.050000
Training Epoch: 25 [19456/50000]	Loss: 0.9475	LR: 0.050000
Training Epoch: 25 [19584/50000]	Loss: 1.0708	LR: 0.050000
Training Epoch: 25 [19712/50000]	Loss: 1.3366	LR: 0.050000
Training Epoch: 25 [19840/50000]	Loss: 0.9395	LR: 0.050000
Training Epoch: 25 [19968/50000]	Loss: 1.1423	LR: 0.050000
Training Epoch: 25 [20096/50000]	Loss: 0.9252	LR: 0.050000
Training Epoch: 25 [20224/50000]	Loss: 1.1763	LR: 0.050000
Training Epoch: 25 [20352/50000]	Loss: 1.2947	LR: 0.050000
Training Epoch: 25 [20480/50000]	Loss: 1.0959	LR: 0.050000
Training Epoch: 25 [20608/50000]	Loss: 1.2652	LR: 0.050000
Training Epoch: 25 [20736/50000]	Loss: 0.9677	LR: 0.050000
Training Epoch: 25 [20864/50000]	Loss: 1.0782	LR: 0.050000
Training Epoch: 25 [20992/50000]	Loss: 1.0916	LR: 0.050000
Training Epoch: 25 [21120/50000]	Loss: 1.1824	LR: 0.050000
Training Epoch: 25 [21248/50000]	Loss: 1.1374	LR: 0.050000
Training Epoch: 25 [21376/50000]	Loss: 1.1166	LR: 0.050000
Training Epoch: 25 [21504/50000]	Loss: 1.2611	LR: 0.050000
Training Epoch: 25 [21632/50000]	Loss: 0.9734	LR: 0.050000
Training Epoch: 25 [21760/50000]	Loss: 1.2052	LR: 0.050000
Training Epoch: 25 [21888/50000]	Loss: 1.1419	LR: 0.050000
Training Epoch: 25 [22016/50000]	Loss: 1.1868	LR: 0.050000
Training Epoch: 25 [22144/50000]	Loss: 0.9824	LR: 0.050000
Training Epoch: 25 [22272/50000]	Loss: 1.2880	LR: 0.050000
Training Epoch: 25 [22400/50000]	Loss: 1.0499	LR: 0.050000
Training Epoch: 25 [22528/50000]	Loss: 1.1405	LR: 0.050000
Training Epoch: 25 [22656/50000]	Loss: 1.0966	LR: 0.050000
Training Epoch: 25 [22784/50000]	Loss: 1.0924	LR: 0.050000
Training Epoch: 25 [22912/50000]	Loss: 1.2712	LR: 0.050000
Training Epoch: 25 [23040/50000]	Loss: 1.1766	LR: 0.050000
Training Epoch: 25 [23168/50000]	Loss: 1.2063	LR: 0.050000
Training Epoch: 25 [23296/50000]	Loss: 1.0727	LR: 0.050000
Training Epoch: 25 [23424/50000]	Loss: 1.1745	LR: 0.050000
Training Epoch: 25 [23552/50000]	Loss: 0.9702	LR: 0.050000
Training Epoch: 25 [23680/50000]	Loss: 1.2527	LR: 0.050000
Training Epoch: 25 [23808/50000]	Loss: 1.0761	LR: 0.050000
Training Epoch: 25 [23936/50000]	Loss: 1.0904	LR: 0.050000
Training Epoch: 25 [24064/50000]	Loss: 0.9462	LR: 0.050000
Training Epoch: 25 [24192/50000]	Loss: 1.0261	LR: 0.050000
Training Epoch: 25 [24320/50000]	Loss: 1.1520	LR: 0.050000
Training Epoch: 25 [24448/50000]	Loss: 1.2408	LR: 0.050000
Training Epoch: 25 [24576/50000]	Loss: 1.3778	LR: 0.050000
Training Epoch: 25 [24704/50000]	Loss: 1.0949	LR: 0.050000
Training Epoch: 25 [24832/50000]	Loss: 1.0407	LR: 0.050000
Training Epoch: 25 [24960/50000]	Loss: 1.2026	LR: 0.050000
Training Epoch: 25 [25088/50000]	Loss: 1.2330	LR: 0.050000
Training Epoch: 25 [25216/50000]	Loss: 1.4660	LR: 0.050000
Training Epoch: 25 [25344/50000]	Loss: 1.0766	LR: 0.050000
Training Epoch: 25 [25472/50000]	Loss: 1.2226	LR: 0.050000
Training Epoch: 25 [25600/50000]	Loss: 1.1975	LR: 0.050000
Training Epoch: 25 [25728/50000]	Loss: 1.2902	LR: 0.050000
Training Epoch: 25 [25856/50000]	Loss: 1.2719	LR: 0.050000
Training Epoch: 25 [25984/50000]	Loss: 1.4075	LR: 0.050000
Training Epoch: 25 [26112/50000]	Loss: 1.3530	LR: 0.050000
Training Epoch: 25 [26240/50000]	Loss: 1.2053	LR: 0.050000
Training Epoch: 25 [26368/50000]	Loss: 1.1074	LR: 0.050000
Training Epoch: 25 [26496/50000]	Loss: 0.9688	LR: 0.050000
Training Epoch: 25 [26624/50000]	Loss: 1.0264	LR: 0.050000
Training Epoch: 25 [26752/50000]	Loss: 1.1073	LR: 0.050000
Training Epoch: 25 [26880/50000]	Loss: 1.1916	LR: 0.050000
Training Epoch: 25 [27008/50000]	Loss: 1.1368	LR: 0.050000
Training Epoch: 25 [27136/50000]	Loss: 1.1009	LR: 0.050000
Training Epoch: 25 [27264/50000]	Loss: 1.2804	LR: 0.050000
Training Epoch: 25 [27392/50000]	Loss: 1.0645	LR: 0.050000
Training Epoch: 25 [27520/50000]	Loss: 1.1928	LR: 0.050000
Training Epoch: 25 [27648/50000]	Loss: 1.2587	LR: 0.050000
Training Epoch: 25 [27776/50000]	Loss: 0.8576	LR: 0.050000
Training Epoch: 25 [27904/50000]	Loss: 1.0278	LR: 0.050000
Training Epoch: 25 [28032/50000]	Loss: 1.2440	LR: 0.050000
Training Epoch: 25 [28160/50000]	Loss: 0.9378	LR: 0.050000
Training Epoch: 25 [28288/50000]	Loss: 0.9274	LR: 0.050000
Training Epoch: 25 [28416/50000]	Loss: 1.1238	LR: 0.050000
Training Epoch: 25 [28544/50000]	Loss: 0.9248	LR: 0.050000
Training Epoch: 25 [28672/50000]	Loss: 1.2393	LR: 0.050000
Training Epoch: 25 [28800/50000]	Loss: 1.3596	LR: 0.050000
Training Epoch: 25 [28928/50000]	Loss: 1.1059	LR: 0.050000
Training Epoch: 25 [29056/50000]	Loss: 1.0457	LR: 0.050000
Training Epoch: 25 [29184/50000]	Loss: 1.0515	LR: 0.050000
Training Epoch: 25 [29312/50000]	Loss: 1.0878	LR: 0.050000
Training Epoch: 25 [29440/50000]	Loss: 1.1108	LR: 0.050000
Training Epoch: 25 [29568/50000]	Loss: 1.1098	LR: 0.050000
Training Epoch: 25 [29696/50000]	Loss: 1.1608	LR: 0.050000
Training Epoch: 25 [29824/50000]	Loss: 1.1721	LR: 0.050000
Training Epoch: 25 [29952/50000]	Loss: 0.9496	LR: 0.050000
Training Epoch: 25 [30080/50000]	Loss: 1.5228	LR: 0.050000
Training Epoch: 25 [30208/50000]	Loss: 1.1517	LR: 0.050000
Training Epoch: 25 [30336/50000]	Loss: 1.1772	LR: 0.050000
Training Epoch: 25 [30464/50000]	Loss: 1.3273	LR: 0.050000
Training Epoch: 25 [30592/50000]	Loss: 1.0899	LR: 0.050000
Training Epoch: 25 [30720/50000]	Loss: 0.9881	LR: 0.050000
Training Epoch: 25 [30848/50000]	Loss: 0.9926	LR: 0.050000
Training Epoch: 25 [30976/50000]	Loss: 0.9319	LR: 0.050000
Training Epoch: 25 [31104/50000]	Loss: 0.9107	LR: 0.050000
Training Epoch: 25 [31232/50000]	Loss: 1.0422	LR: 0.050000
Training Epoch: 25 [31360/50000]	Loss: 1.1791	LR: 0.050000
Training Epoch: 25 [31488/50000]	Loss: 1.1629	LR: 0.050000
Training Epoch: 25 [31616/50000]	Loss: 1.0698	LR: 0.050000
Training Epoch: 25 [31744/50000]	Loss: 1.0394	LR: 0.050000
Training Epoch: 25 [31872/50000]	Loss: 1.2298	LR: 0.050000
Training Epoch: 25 [32000/50000]	Loss: 0.9772	LR: 0.050000
Training Epoch: 25 [32128/50000]	Loss: 1.0944	LR: 0.050000
Training Epoch: 25 [32256/50000]	Loss: 1.2015	LR: 0.050000
Training Epoch: 25 [32384/50000]	Loss: 1.0896	LR: 0.050000
Training Epoch: 25 [32512/50000]	Loss: 1.4717	LR: 0.050000
Training Epoch: 25 [32640/50000]	Loss: 1.2238	LR: 0.050000
Training Epoch: 25 [32768/50000]	Loss: 1.1808	LR: 0.050000
Training Epoch: 25 [32896/50000]	Loss: 1.0091	LR: 0.050000
Training Epoch: 25 [33024/50000]	Loss: 1.2743	LR: 0.050000
Training Epoch: 25 [33152/50000]	Loss: 0.9933	LR: 0.050000
Training Epoch: 25 [33280/50000]	Loss: 1.0800	LR: 0.050000
Training Epoch: 25 [33408/50000]	Loss: 1.1889	LR: 0.050000
Training Epoch: 25 [33536/50000]	Loss: 1.2680	LR: 0.050000
Training Epoch: 25 [33664/50000]	Loss: 1.1347	LR: 0.050000
Training Epoch: 25 [33792/50000]	Loss: 1.0479	LR: 0.050000
Training Epoch: 25 [33920/50000]	Loss: 1.2146	LR: 0.050000
Training Epoch: 25 [34048/50000]	Loss: 1.1933	LR: 0.050000
Training Epoch: 25 [34176/50000]	Loss: 1.1718	LR: 0.050000
Training Epoch: 25 [34304/50000]	Loss: 1.1500	LR: 0.050000
Training Epoch: 25 [34432/50000]	Loss: 1.1935	LR: 0.050000
Training Epoch: 25 [34560/50000]	Loss: 1.2016	LR: 0.050000
Training Epoch: 25 [34688/50000]	Loss: 1.2402	LR: 0.050000
Training Epoch: 25 [34816/50000]	Loss: 1.0134	LR: 0.050000
Training Epoch: 25 [34944/50000]	Loss: 1.2141	LR: 0.050000
Training Epoch: 25 [35072/50000]	Loss: 1.1825	LR: 0.050000
Training Epoch: 25 [35200/50000]	Loss: 0.9762	LR: 0.050000
Training Epoch: 25 [35328/50000]	Loss: 1.0256	LR: 0.050000
Training Epoch: 25 [35456/50000]	Loss: 1.2567	LR: 0.050000
Training Epoch: 25 [35584/50000]	Loss: 1.2544	LR: 0.050000
Training Epoch: 25 [35712/50000]	Loss: 0.9432	LR: 0.050000
Training Epoch: 25 [35840/50000]	Loss: 0.9286	LR: 0.050000
Training Epoch: 25 [35968/50000]	Loss: 1.0147	LR: 0.050000
Training Epoch: 25 [36096/50000]	Loss: 1.2041	LR: 0.050000
Training Epoch: 25 [36224/50000]	Loss: 1.2421	LR: 0.050000
Training Epoch: 25 [36352/50000]	Loss: 0.9819	LR: 0.050000
Training Epoch: 25 [36480/50000]	Loss: 1.1356	LR: 0.050000
Training Epoch: 25 [36608/50000]	Loss: 1.0942	LR: 0.050000
Training Epoch: 25 [36736/50000]	Loss: 0.9427	LR: 0.050000
Training Epoch: 25 [36864/50000]	Loss: 1.3017	LR: 0.050000
Training Epoch: 25 [36992/50000]	Loss: 1.2068	LR: 0.050000
Training Epoch: 25 [37120/50000]	Loss: 0.8125	LR: 0.050000
Training Epoch: 25 [37248/50000]	Loss: 1.2270	LR: 0.050000
Training Epoch: 25 [37376/50000]	Loss: 1.1508	LR: 0.050000
Training Epoch: 25 [37504/50000]	Loss: 1.0162	LR: 0.050000
Training Epoch: 25 [37632/50000]	Loss: 1.1820	LR: 0.050000
Training Epoch: 25 [37760/50000]	Loss: 1.1130	LR: 0.050000
Training Epoch: 25 [37888/50000]	Loss: 1.1524	LR: 0.050000
Training Epoch: 25 [38016/50000]	Loss: 1.1380	LR: 0.050000
Training Epoch: 25 [38144/50000]	Loss: 1.0172	LR: 0.050000
Training Epoch: 25 [38272/50000]	Loss: 1.1752	LR: 0.050000
Training Epoch: 25 [38400/50000]	Loss: 1.0257	LR: 0.050000
Training Epoch: 25 [38528/50000]	Loss: 1.1279	LR: 0.050000
Training Epoch: 25 [38656/50000]	Loss: 1.0581	LR: 0.050000
Training Epoch: 25 [38784/50000]	Loss: 0.9592	LR: 0.050000
Training Epoch: 25 [38912/50000]	Loss: 1.0719	LR: 0.050000
Training Epoch: 25 [39040/50000]	Loss: 1.4665	LR: 0.050000
Training Epoch: 25 [39168/50000]	Loss: 0.9383	LR: 0.050000
Training Epoch: 25 [39296/50000]	Loss: 0.8011	LR: 0.050000
Training Epoch: 25 [39424/50000]	Loss: 1.0762	LR: 0.050000
Training Epoch: 25 [39552/50000]	Loss: 1.2327	LR: 0.050000
Training Epoch: 25 [39680/50000]	Loss: 1.1671	LR: 0.050000
Training Epoch: 25 [39808/50000]	Loss: 1.1257	LR: 0.050000
Training Epoch: 25 [39936/50000]	Loss: 0.9728	LR: 0.050000
Training Epoch: 25 [40064/50000]	Loss: 1.2943	LR: 0.050000
Training Epoch: 25 [40192/50000]	Loss: 1.1357	LR: 0.050000
Training Epoch: 25 [40320/50000]	Loss: 1.1993	LR: 0.050000
Training Epoch: 25 [40448/50000]	Loss: 1.2990	LR: 0.050000
Training Epoch: 25 [40576/50000]	Loss: 1.1081	LR: 0.050000
Training Epoch: 25 [40704/50000]	Loss: 1.2269	LR: 0.050000
Training Epoch: 25 [40832/50000]	Loss: 1.3435	LR: 0.050000
Training Epoch: 25 [40960/50000]	Loss: 1.1361	LR: 0.050000
Training Epoch: 25 [41088/50000]	Loss: 1.2473	LR: 0.050000
Training Epoch: 25 [41216/50000]	Loss: 1.0832	LR: 0.050000
Training Epoch: 25 [41344/50000]	Loss: 1.1736	LR: 0.050000
Training Epoch: 25 [41472/50000]	Loss: 1.0824	LR: 0.050000
Training Epoch: 25 [41600/50000]	Loss: 1.2550	LR: 0.050000
Training Epoch: 25 [41728/50000]	Loss: 1.2114	LR: 0.050000
Training Epoch: 25 [41856/50000]	Loss: 0.9514	LR: 0.050000
Training Epoch: 25 [41984/50000]	Loss: 1.0705	LR: 0.050000
Training Epoch: 25 [42112/50000]	Loss: 0.9883	LR: 0.050000
Training Epoch: 25 [42240/50000]	Loss: 1.1565	LR: 0.050000
Training Epoch: 25 [42368/50000]	Loss: 1.1857	LR: 0.050000
Training Epoch: 25 [42496/50000]	Loss: 1.0673	LR: 0.050000
Training Epoch: 25 [42624/50000]	Loss: 1.5630	LR: 0.050000
Training Epoch: 25 [42752/50000]	Loss: 1.2109	LR: 0.050000
Training Epoch: 25 [42880/50000]	Loss: 1.0989	LR: 0.050000
Training Epoch: 25 [43008/50000]	Loss: 1.1540	LR: 0.050000
Training Epoch: 25 [43136/50000]	Loss: 1.1230	LR: 0.050000
Training Epoch: 25 [43264/50000]	Loss: 1.2906	LR: 0.050000
Training Epoch: 25 [43392/50000]	Loss: 1.1535	LR: 0.050000
Training Epoch: 25 [43520/50000]	Loss: 1.3075	LR: 0.050000
Training Epoch: 25 [43648/50000]	Loss: 1.2094	LR: 0.050000
Training Epoch: 25 [43776/50000]	Loss: 1.1739	LR: 0.050000
Training Epoch: 25 [43904/50000]	Loss: 1.1327	LR: 0.050000
Training Epoch: 25 [44032/50000]	Loss: 1.0874	LR: 0.050000
Training Epoch: 25 [44160/50000]	Loss: 1.2192	LR: 0.050000
Training Epoch: 25 [44288/50000]	Loss: 1.2308	LR: 0.050000
Training Epoch: 25 [44416/50000]	Loss: 1.2131	LR: 0.050000
Training Epoch: 25 [44544/50000]	Loss: 1.1507	LR: 0.050000
Training Epoch: 25 [44672/50000]	Loss: 1.0604	LR: 0.050000
Training Epoch: 25 [44800/50000]	Loss: 1.3427	LR: 0.050000
Training Epoch: 25 [44928/50000]	Loss: 1.0340	LR: 0.050000
Training Epoch: 25 [45056/50000]	Loss: 1.0368	LR: 0.050000
Training Epoch: 25 [45184/50000]	Loss: 1.0627	LR: 0.050000
Training Epoch: 25 [45312/50000]	Loss: 1.1589	LR: 0.050000
Training Epoch: 25 [45440/50000]	Loss: 1.1898	LR: 0.050000
Training Epoch: 25 [45568/50000]	Loss: 0.9442	LR: 0.050000
Training Epoch: 25 [45696/50000]	Loss: 1.2885	LR: 0.050000
Training Epoch: 25 [45824/50000]	Loss: 1.1276	LR: 0.050000
Training Epoch: 25 [45952/50000]	Loss: 1.1116	LR: 0.050000
Training Epoch: 25 [46080/50000]	Loss: 1.1933	LR: 0.050000
Training Epoch: 25 [46208/50000]	Loss: 0.9997	LR: 0.050000
Training Epoch: 25 [46336/50000]	Loss: 1.0853	LR: 0.050000
Training Epoch: 25 [46464/50000]	Loss: 1.0644	LR: 0.050000
Training Epoch: 25 [46592/50000]	Loss: 1.2819	LR: 0.050000
Training Epoch: 25 [46720/50000]	Loss: 1.0953	LR: 0.050000
Training Epoch: 25 [46848/50000]	Loss: 1.1052	LR: 0.050000
Training Epoch: 25 [46976/50000]	Loss: 1.3220	LR: 0.050000
Training Epoch: 25 [47104/50000]	Loss: 1.2578	LR: 0.050000
Training Epoch: 25 [47232/50000]	Loss: 1.2067	LR: 0.050000
Training Epoch: 25 [47360/50000]	Loss: 1.2165	LR: 0.050000
Training Epoch: 25 [47488/50000]	Loss: 1.1463	LR: 0.050000
Training Epoch: 25 [47616/50000]	Loss: 1.1517	LR: 0.050000
Training Epoch: 25 [47744/50000]	Loss: 1.1542	LR: 0.050000
Training Epoch: 25 [47872/50000]	Loss: 1.2639	LR: 0.050000
Training Epoch: 25 [48000/50000]	Loss: 1.2243	LR: 0.050000
Training Epoch: 25 [48128/50000]	Loss: 1.1726	LR: 0.050000
Training Epoch: 25 [48256/50000]	Loss: 1.0862	LR: 0.050000
Training Epoch: 25 [48384/50000]	Loss: 1.0261	LR: 0.050000
Training Epoch: 25 [48512/50000]	Loss: 1.2042	LR: 0.050000
Training Epoch: 25 [48640/50000]	Loss: 1.1970	LR: 0.050000
Training Epoch: 25 [48768/50000]	Loss: 1.2777	LR: 0.050000
Training Epoch: 25 [48896/50000]	Loss: 1.2380	LR: 0.050000
Training Epoch: 25 [49024/50000]	Loss: 1.0190	LR: 0.050000
Training Epoch: 25 [49152/50000]	Loss: 1.0724	LR: 0.050000
Training Epoch: 25 [49280/50000]	Loss: 1.4283	LR: 0.050000
Training Epoch: 25 [49408/50000]	Loss: 1.4421	LR: 0.050000
Training Epoch: 25 [49536/50000]	Loss: 1.0078	LR: 0.050000
Training Epoch: 25 [49664/50000]	Loss: 1.1255	LR: 0.050000
Training Epoch: 25 [49792/50000]	Loss: 1.1596	LR: 0.050000
Training Epoch: 25 [49920/50000]	Loss: 1.0987	LR: 0.050000
Training Epoch: 25 [50000/50000]	Loss: 1.0474	LR: 0.050000
epoch 25 training time consumed: 53.98s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   91751 GB |   91751 GB |
|       from large pool |  123392 KB |    1034 MB |   91661 GB |   91661 GB |
|       from small pool |   10798 KB |      13 MB |      90 GB |      90 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   91751 GB |   91751 GB |
|       from large pool |  123392 KB |    1034 MB |   91661 GB |   91661 GB |
|       from small pool |   10798 KB |      13 MB |      90 GB |      90 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   40378 GB |   40378 GB |
|       from large pool |  155136 KB |  433088 KB |   40278 GB |   40278 GB |
|       from small pool |    1490 KB |    3494 KB |      99 GB |      99 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    3540 K  |    3540 K  |
|       from large pool |      24    |      65    |    1848 K  |    1848 K  |
|       from small pool |     231    |     274    |    1692 K  |    1692 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    3540 K  |    3540 K  |
|       from large pool |      24    |      65    |    1848 K  |    1848 K  |
|       from small pool |     231    |     274    |    1692 K  |    1692 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1752 K  |    1752 K  |
|       from large pool |       9    |      14    |     894 K  |     894 K  |
|       from small pool |      12    |      16    |     857 K  |     857 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 25, Average loss: 0.0125, Accuracy: 0.5784, Time consumed:3.46s

Training Epoch: 26 [128/50000]	Loss: 0.8688	LR: 0.050000
Training Epoch: 26 [256/50000]	Loss: 0.8426	LR: 0.050000
Training Epoch: 26 [384/50000]	Loss: 1.1314	LR: 0.050000
Training Epoch: 26 [512/50000]	Loss: 0.9433	LR: 0.050000
Training Epoch: 26 [640/50000]	Loss: 1.0430	LR: 0.050000
Training Epoch: 26 [768/50000]	Loss: 0.9961	LR: 0.050000
Training Epoch: 26 [896/50000]	Loss: 1.2049	LR: 0.050000
Training Epoch: 26 [1024/50000]	Loss: 0.9902	LR: 0.050000
Training Epoch: 26 [1152/50000]	Loss: 1.0690	LR: 0.050000
Training Epoch: 26 [1280/50000]	Loss: 1.0593	LR: 0.050000
Training Epoch: 26 [1408/50000]	Loss: 0.9107	LR: 0.050000
Training Epoch: 26 [1536/50000]	Loss: 0.8101	LR: 0.050000
Training Epoch: 26 [1664/50000]	Loss: 0.9594	LR: 0.050000
Training Epoch: 26 [1792/50000]	Loss: 1.0655	LR: 0.050000
Training Epoch: 26 [1920/50000]	Loss: 0.9013	LR: 0.050000
Training Epoch: 26 [2048/50000]	Loss: 1.1561	LR: 0.050000
Training Epoch: 26 [2176/50000]	Loss: 0.7102	LR: 0.050000
Training Epoch: 26 [2304/50000]	Loss: 1.1644	LR: 0.050000
Training Epoch: 26 [2432/50000]	Loss: 0.9783	LR: 0.050000
Training Epoch: 26 [2560/50000]	Loss: 0.9889	LR: 0.050000
Training Epoch: 26 [2688/50000]	Loss: 1.0086	LR: 0.050000
Training Epoch: 26 [2816/50000]	Loss: 0.7475	LR: 0.050000
Training Epoch: 26 [2944/50000]	Loss: 1.0251	LR: 0.050000
Training Epoch: 26 [3072/50000]	Loss: 0.7202	LR: 0.050000
Training Epoch: 26 [3200/50000]	Loss: 0.8960	LR: 0.050000
Training Epoch: 26 [3328/50000]	Loss: 1.0302	LR: 0.050000
Training Epoch: 26 [3456/50000]	Loss: 1.0758	LR: 0.050000
Training Epoch: 26 [3584/50000]	Loss: 0.9597	LR: 0.050000
Training Epoch: 26 [3712/50000]	Loss: 0.8643	LR: 0.050000
Training Epoch: 26 [3840/50000]	Loss: 1.1536	LR: 0.050000
Training Epoch: 26 [3968/50000]	Loss: 1.1226	LR: 0.050000
Training Epoch: 26 [4096/50000]	Loss: 1.1105	LR: 0.050000
Training Epoch: 26 [4224/50000]	Loss: 0.9990	LR: 0.050000
Training Epoch: 26 [4352/50000]	Loss: 1.0942	LR: 0.050000
Training Epoch: 26 [4480/50000]	Loss: 0.8975	LR: 0.050000
Training Epoch: 26 [4608/50000]	Loss: 1.0250	LR: 0.050000
Training Epoch: 26 [4736/50000]	Loss: 1.0107	LR: 0.050000
Training Epoch: 26 [4864/50000]	Loss: 1.0325	LR: 0.050000
Training Epoch: 26 [4992/50000]	Loss: 0.8690	LR: 0.050000
Training Epoch: 26 [5120/50000]	Loss: 1.1178	LR: 0.050000
Training Epoch: 26 [5248/50000]	Loss: 0.9411	LR: 0.050000
Training Epoch: 26 [5376/50000]	Loss: 1.2014	LR: 0.050000
Training Epoch: 26 [5504/50000]	Loss: 1.0768	LR: 0.050000
Training Epoch: 26 [5632/50000]	Loss: 1.0182	LR: 0.050000
Training Epoch: 26 [5760/50000]	Loss: 1.0529	LR: 0.050000
Training Epoch: 26 [5888/50000]	Loss: 1.0628	LR: 0.050000
Training Epoch: 26 [6016/50000]	Loss: 1.1236	LR: 0.050000
Training Epoch: 26 [6144/50000]	Loss: 1.1070	LR: 0.050000
Training Epoch: 26 [6272/50000]	Loss: 1.2572	LR: 0.050000
Training Epoch: 26 [6400/50000]	Loss: 1.1303	LR: 0.050000
Training Epoch: 26 [6528/50000]	Loss: 1.0830	LR: 0.050000
Training Epoch: 26 [6656/50000]	Loss: 1.0790	LR: 0.050000
Training Epoch: 26 [6784/50000]	Loss: 1.1473	LR: 0.050000
Training Epoch: 26 [6912/50000]	Loss: 0.9226	LR: 0.050000
Training Epoch: 26 [7040/50000]	Loss: 1.2688	LR: 0.050000
Training Epoch: 26 [7168/50000]	Loss: 1.0924	LR: 0.050000
Training Epoch: 26 [7296/50000]	Loss: 1.1549	LR: 0.050000
Training Epoch: 26 [7424/50000]	Loss: 1.3870	LR: 0.050000
Training Epoch: 26 [7552/50000]	Loss: 1.1053	LR: 0.050000
Training Epoch: 26 [7680/50000]	Loss: 1.4316	LR: 0.050000
Training Epoch: 26 [7808/50000]	Loss: 1.1755	LR: 0.050000
Training Epoch: 26 [7936/50000]	Loss: 1.0618	LR: 0.050000
Training Epoch: 26 [8064/50000]	Loss: 0.9298	LR: 0.050000
Training Epoch: 26 [8192/50000]	Loss: 1.1130	LR: 0.050000
Training Epoch: 26 [8320/50000]	Loss: 1.1303	LR: 0.050000
Training Epoch: 26 [8448/50000]	Loss: 0.9832	LR: 0.050000
Training Epoch: 26 [8576/50000]	Loss: 1.0033	LR: 0.050000
Training Epoch: 26 [8704/50000]	Loss: 1.1092	LR: 0.050000
Training Epoch: 26 [8832/50000]	Loss: 0.9086	LR: 0.050000
Training Epoch: 26 [8960/50000]	Loss: 1.0688	LR: 0.050000
Training Epoch: 26 [9088/50000]	Loss: 1.2971	LR: 0.050000
Training Epoch: 26 [9216/50000]	Loss: 0.9575	LR: 0.050000
Training Epoch: 26 [9344/50000]	Loss: 1.1965	LR: 0.050000
Training Epoch: 26 [9472/50000]	Loss: 1.2499	LR: 0.050000
Training Epoch: 26 [9600/50000]	Loss: 0.8636	LR: 0.050000
Training Epoch: 26 [9728/50000]	Loss: 1.1515	LR: 0.050000
Training Epoch: 26 [9856/50000]	Loss: 0.9858	LR: 0.050000
Training Epoch: 26 [9984/50000]	Loss: 0.9457	LR: 0.050000
Training Epoch: 26 [10112/50000]	Loss: 0.9934	LR: 0.050000
Training Epoch: 26 [10240/50000]	Loss: 0.8868	LR: 0.050000
Training Epoch: 26 [10368/50000]	Loss: 0.9239	LR: 0.050000
Training Epoch: 26 [10496/50000]	Loss: 1.1772	LR: 0.050000
Training Epoch: 26 [10624/50000]	Loss: 1.2341	LR: 0.050000
Training Epoch: 26 [10752/50000]	Loss: 1.1968	LR: 0.050000
Training Epoch: 26 [10880/50000]	Loss: 1.1248	LR: 0.050000
Training Epoch: 26 [11008/50000]	Loss: 1.0697	LR: 0.050000
Training Epoch: 26 [11136/50000]	Loss: 0.9976	LR: 0.050000
Training Epoch: 26 [11264/50000]	Loss: 1.3118	LR: 0.050000
Training Epoch: 26 [11392/50000]	Loss: 1.2537	LR: 0.050000
Training Epoch: 26 [11520/50000]	Loss: 0.9054	LR: 0.050000
Training Epoch: 26 [11648/50000]	Loss: 1.1658	LR: 0.050000
Training Epoch: 26 [11776/50000]	Loss: 1.1262	LR: 0.050000
Training Epoch: 26 [11904/50000]	Loss: 1.3051	LR: 0.050000
Training Epoch: 26 [12032/50000]	Loss: 0.9613	LR: 0.050000
Training Epoch: 26 [12160/50000]	Loss: 0.9961	LR: 0.050000
Training Epoch: 26 [12288/50000]	Loss: 1.0774	LR: 0.050000
Training Epoch: 26 [12416/50000]	Loss: 1.2316	LR: 0.050000
Training Epoch: 26 [12544/50000]	Loss: 0.9929	LR: 0.050000
Training Epoch: 26 [12672/50000]	Loss: 0.9906	LR: 0.050000
Training Epoch: 26 [12800/50000]	Loss: 1.1388	LR: 0.050000
Training Epoch: 26 [12928/50000]	Loss: 1.1883	LR: 0.050000
Training Epoch: 26 [13056/50000]	Loss: 1.0054	LR: 0.050000
Training Epoch: 26 [13184/50000]	Loss: 1.2170	LR: 0.050000
Training Epoch: 26 [13312/50000]	Loss: 1.0876	LR: 0.050000
Training Epoch: 26 [13440/50000]	Loss: 0.8477	LR: 0.050000
Training Epoch: 26 [13568/50000]	Loss: 1.2547	LR: 0.050000
Training Epoch: 26 [13696/50000]	Loss: 1.0430	LR: 0.050000
Training Epoch: 26 [13824/50000]	Loss: 1.0113	LR: 0.050000
Training Epoch: 26 [13952/50000]	Loss: 0.8300	LR: 0.050000
Training Epoch: 26 [14080/50000]	Loss: 1.0612	LR: 0.050000
Training Epoch: 26 [14208/50000]	Loss: 1.0061	LR: 0.050000
Training Epoch: 26 [14336/50000]	Loss: 0.9715	LR: 0.050000
Training Epoch: 26 [14464/50000]	Loss: 0.9476	LR: 0.050000
Training Epoch: 26 [14592/50000]	Loss: 0.8617	LR: 0.050000
Training Epoch: 26 [14720/50000]	Loss: 1.3635	LR: 0.050000
Training Epoch: 26 [14848/50000]	Loss: 1.0112	LR: 0.050000
Training Epoch: 26 [14976/50000]	Loss: 1.0931	LR: 0.050000
Training Epoch: 26 [15104/50000]	Loss: 0.9469	LR: 0.050000
Training Epoch: 26 [15232/50000]	Loss: 1.0747	LR: 0.050000
Training Epoch: 26 [15360/50000]	Loss: 1.0403	LR: 0.050000
Training Epoch: 26 [15488/50000]	Loss: 1.1197	LR: 0.050000
Training Epoch: 26 [15616/50000]	Loss: 0.9929	LR: 0.050000
Training Epoch: 26 [15744/50000]	Loss: 0.9632	LR: 0.050000
Training Epoch: 26 [15872/50000]	Loss: 0.7816	LR: 0.050000
Training Epoch: 26 [16000/50000]	Loss: 0.9103	LR: 0.050000
Training Epoch: 26 [16128/50000]	Loss: 0.8756	LR: 0.050000
Training Epoch: 26 [16256/50000]	Loss: 0.8965	LR: 0.050000
Training Epoch: 26 [16384/50000]	Loss: 0.8909	LR: 0.050000
Training Epoch: 26 [16512/50000]	Loss: 1.0247	LR: 0.050000
Training Epoch: 26 [16640/50000]	Loss: 0.9712	LR: 0.050000
Training Epoch: 26 [16768/50000]	Loss: 1.1052	LR: 0.050000
Training Epoch: 26 [16896/50000]	Loss: 1.1680	LR: 0.050000
Training Epoch: 26 [17024/50000]	Loss: 1.0226	LR: 0.050000
Training Epoch: 26 [17152/50000]	Loss: 1.1150	LR: 0.050000
Training Epoch: 26 [17280/50000]	Loss: 1.0048	LR: 0.050000
Training Epoch: 26 [17408/50000]	Loss: 1.0205	LR: 0.050000
Training Epoch: 26 [17536/50000]	Loss: 1.2839	LR: 0.050000
Training Epoch: 26 [17664/50000]	Loss: 1.0621	LR: 0.050000
Training Epoch: 26 [17792/50000]	Loss: 1.0347	LR: 0.050000
Training Epoch: 26 [17920/50000]	Loss: 1.2755	LR: 0.050000
Training Epoch: 26 [18048/50000]	Loss: 1.1462	LR: 0.050000
Training Epoch: 26 [18176/50000]	Loss: 1.2176	LR: 0.050000
Training Epoch: 26 [18304/50000]	Loss: 1.2512	LR: 0.050000
Training Epoch: 26 [18432/50000]	Loss: 1.1513	LR: 0.050000
Training Epoch: 26 [18560/50000]	Loss: 1.1393	LR: 0.050000
Training Epoch: 26 [18688/50000]	Loss: 1.1692	LR: 0.050000
Training Epoch: 26 [18816/50000]	Loss: 0.9873	LR: 0.050000
Training Epoch: 26 [18944/50000]	Loss: 1.0494	LR: 0.050000
Training Epoch: 26 [19072/50000]	Loss: 1.0544	LR: 0.050000
Training Epoch: 26 [19200/50000]	Loss: 1.2147	LR: 0.050000
Training Epoch: 26 [19328/50000]	Loss: 0.9496	LR: 0.050000
Training Epoch: 26 [19456/50000]	Loss: 1.1076	LR: 0.050000
Training Epoch: 26 [19584/50000]	Loss: 1.1305	LR: 0.050000
Training Epoch: 26 [19712/50000]	Loss: 1.1170	LR: 0.050000
Training Epoch: 26 [19840/50000]	Loss: 1.3316	LR: 0.050000
Training Epoch: 26 [19968/50000]	Loss: 1.0006	LR: 0.050000
Training Epoch: 26 [20096/50000]	Loss: 1.3207	LR: 0.050000
Training Epoch: 26 [20224/50000]	Loss: 1.1882	LR: 0.050000
Training Epoch: 26 [20352/50000]	Loss: 0.9485	LR: 0.050000
Training Epoch: 26 [20480/50000]	Loss: 1.1644	LR: 0.050000
Training Epoch: 26 [20608/50000]	Loss: 0.9866	LR: 0.050000
Training Epoch: 26 [20736/50000]	Loss: 1.0672	LR: 0.050000
Training Epoch: 26 [20864/50000]	Loss: 0.9907	LR: 0.050000
Training Epoch: 26 [20992/50000]	Loss: 1.0091	LR: 0.050000
Training Epoch: 26 [21120/50000]	Loss: 1.1646	LR: 0.050000
Training Epoch: 26 [21248/50000]	Loss: 1.0466	LR: 0.050000
Training Epoch: 26 [21376/50000]	Loss: 1.0876	LR: 0.050000
Training Epoch: 26 [21504/50000]	Loss: 1.2309	LR: 0.050000
Training Epoch: 26 [21632/50000]	Loss: 1.0142	LR: 0.050000
Training Epoch: 26 [21760/50000]	Loss: 0.8035	LR: 0.050000
Training Epoch: 26 [21888/50000]	Loss: 1.2885	LR: 0.050000
Training Epoch: 26 [22016/50000]	Loss: 1.1601	LR: 0.050000
Training Epoch: 26 [22144/50000]	Loss: 1.4136	LR: 0.050000
Training Epoch: 26 [22272/50000]	Loss: 1.1036	LR: 0.050000
Training Epoch: 26 [22400/50000]	Loss: 1.0694	LR: 0.050000
Training Epoch: 26 [22528/50000]	Loss: 1.3890	LR: 0.050000
Training Epoch: 26 [22656/50000]	Loss: 1.0141	LR: 0.050000
Training Epoch: 26 [22784/50000]	Loss: 1.0314	LR: 0.050000
Training Epoch: 26 [22912/50000]	Loss: 1.2412	LR: 0.050000
Training Epoch: 26 [23040/50000]	Loss: 1.0739	LR: 0.050000
Training Epoch: 26 [23168/50000]	Loss: 1.3047	LR: 0.050000
Training Epoch: 26 [23296/50000]	Loss: 1.0648	LR: 0.050000
Training Epoch: 26 [23424/50000]	Loss: 1.0752	LR: 0.050000
Training Epoch: 26 [23552/50000]	Loss: 1.1753	LR: 0.050000
Training Epoch: 26 [23680/50000]	Loss: 1.0423	LR: 0.050000
Training Epoch: 26 [23808/50000]	Loss: 1.1943	LR: 0.050000
Training Epoch: 26 [23936/50000]	Loss: 1.2359	LR: 0.050000
Training Epoch: 26 [24064/50000]	Loss: 0.9697	LR: 0.050000
Training Epoch: 26 [24192/50000]	Loss: 1.0509	LR: 0.050000
Training Epoch: 26 [24320/50000]	Loss: 0.9851	LR: 0.050000
Training Epoch: 26 [24448/50000]	Loss: 1.2353	LR: 0.050000
Training Epoch: 26 [24576/50000]	Loss: 1.3559	LR: 0.050000
Training Epoch: 26 [24704/50000]	Loss: 1.1201	LR: 0.050000
Training Epoch: 26 [24832/50000]	Loss: 1.2587	LR: 0.050000
Training Epoch: 26 [24960/50000]	Loss: 1.4355	LR: 0.050000
Training Epoch: 26 [25088/50000]	Loss: 1.1628	LR: 0.050000
Training Epoch: 26 [25216/50000]	Loss: 1.2174	LR: 0.050000
Training Epoch: 26 [25344/50000]	Loss: 1.1215	LR: 0.050000
Training Epoch: 26 [25472/50000]	Loss: 1.0508	LR: 0.050000
Training Epoch: 26 [25600/50000]	Loss: 1.0289	LR: 0.050000
Training Epoch: 26 [25728/50000]	Loss: 1.5524	LR: 0.050000
Training Epoch: 26 [25856/50000]	Loss: 1.2563	LR: 0.050000
Training Epoch: 26 [25984/50000]	Loss: 1.0845	LR: 0.050000
Training Epoch: 26 [26112/50000]	Loss: 1.0144	LR: 0.050000
Training Epoch: 26 [26240/50000]	Loss: 0.9714	LR: 0.050000
Training Epoch: 26 [26368/50000]	Loss: 1.1101	LR: 0.050000
Training Epoch: 26 [26496/50000]	Loss: 1.2248	LR: 0.050000
Training Epoch: 26 [26624/50000]	Loss: 1.2608	LR: 0.050000
Training Epoch: 26 [26752/50000]	Loss: 0.9092	LR: 0.050000
Training Epoch: 26 [26880/50000]	Loss: 0.9269	LR: 0.050000
Training Epoch: 26 [27008/50000]	Loss: 1.0955	LR: 0.050000
Training Epoch: 26 [27136/50000]	Loss: 1.0952	LR: 0.050000
Training Epoch: 26 [27264/50000]	Loss: 1.2815	LR: 0.050000
Training Epoch: 26 [27392/50000]	Loss: 1.2142	LR: 0.050000
Training Epoch: 26 [27520/50000]	Loss: 1.0034	LR: 0.050000
Training Epoch: 26 [27648/50000]	Loss: 0.8536	LR: 0.050000
Training Epoch: 26 [27776/50000]	Loss: 1.1944	LR: 0.050000
Training Epoch: 26 [27904/50000]	Loss: 0.9610	LR: 0.050000
Training Epoch: 26 [28032/50000]	Loss: 0.9807	LR: 0.050000
Training Epoch: 26 [28160/50000]	Loss: 0.9930	LR: 0.050000
Training Epoch: 26 [28288/50000]	Loss: 1.0001	LR: 0.050000
Training Epoch: 26 [28416/50000]	Loss: 0.9063	LR: 0.050000
Training Epoch: 26 [28544/50000]	Loss: 1.0842	LR: 0.050000
Training Epoch: 26 [28672/50000]	Loss: 1.0189	LR: 0.050000
Training Epoch: 26 [28800/50000]	Loss: 1.0503	LR: 0.050000
Training Epoch: 26 [28928/50000]	Loss: 0.9278	LR: 0.050000
Training Epoch: 26 [29056/50000]	Loss: 0.9105	LR: 0.050000
Training Epoch: 26 [29184/50000]	Loss: 1.0900	LR: 0.050000
Training Epoch: 26 [29312/50000]	Loss: 0.9566	LR: 0.050000
Training Epoch: 26 [29440/50000]	Loss: 1.2232	LR: 0.050000
Training Epoch: 26 [29568/50000]	Loss: 1.1977	LR: 0.050000
Training Epoch: 26 [29696/50000]	Loss: 1.1230	LR: 0.050000
Training Epoch: 26 [29824/50000]	Loss: 1.1769	LR: 0.050000
Training Epoch: 26 [29952/50000]	Loss: 1.1617	LR: 0.050000
Training Epoch: 26 [30080/50000]	Loss: 0.9563	LR: 0.050000
Training Epoch: 26 [30208/50000]	Loss: 0.9496	LR: 0.050000
Training Epoch: 26 [30336/50000]	Loss: 1.2496	LR: 0.050000
Training Epoch: 26 [30464/50000]	Loss: 1.1218	LR: 0.050000
Training Epoch: 26 [30592/50000]	Loss: 0.9966	LR: 0.050000
Training Epoch: 26 [30720/50000]	Loss: 0.9437	LR: 0.050000
Training Epoch: 26 [30848/50000]	Loss: 1.1140	LR: 0.050000
Training Epoch: 26 [30976/50000]	Loss: 1.3264	LR: 0.050000
Training Epoch: 26 [31104/50000]	Loss: 1.3670	LR: 0.050000
Training Epoch: 26 [31232/50000]	Loss: 0.9759	LR: 0.050000
Training Epoch: 26 [31360/50000]	Loss: 0.9399	LR: 0.050000
Training Epoch: 26 [31488/50000]	Loss: 0.9971	LR: 0.050000
Training Epoch: 26 [31616/50000]	Loss: 1.3833	LR: 0.050000
Training Epoch: 26 [31744/50000]	Loss: 1.1341	LR: 0.050000
Training Epoch: 26 [31872/50000]	Loss: 1.2109	LR: 0.050000
Training Epoch: 26 [32000/50000]	Loss: 1.1192	LR: 0.050000
Training Epoch: 26 [32128/50000]	Loss: 0.9876	LR: 0.050000
Training Epoch: 26 [32256/50000]	Loss: 0.9918	LR: 0.050000
Training Epoch: 26 [32384/50000]	Loss: 1.1035	LR: 0.050000
Training Epoch: 26 [32512/50000]	Loss: 0.9196	LR: 0.050000
Training Epoch: 26 [32640/50000]	Loss: 1.1759	LR: 0.050000
Training Epoch: 26 [32768/50000]	Loss: 1.0388	LR: 0.050000
Training Epoch: 26 [32896/50000]	Loss: 0.9247	LR: 0.050000
Training Epoch: 26 [33024/50000]	Loss: 0.9825	LR: 0.050000
Training Epoch: 26 [33152/50000]	Loss: 0.8832	LR: 0.050000
Training Epoch: 26 [33280/50000]	Loss: 1.1672	LR: 0.050000
Training Epoch: 26 [33408/50000]	Loss: 0.9411	LR: 0.050000
Training Epoch: 26 [33536/50000]	Loss: 0.8848	LR: 0.050000
Training Epoch: 26 [33664/50000]	Loss: 1.1082	LR: 0.050000
Training Epoch: 26 [33792/50000]	Loss: 1.0418	LR: 0.050000
Training Epoch: 26 [33920/50000]	Loss: 1.0458	LR: 0.050000
Training Epoch: 26 [34048/50000]	Loss: 1.0885	LR: 0.050000
Training Epoch: 26 [34176/50000]	Loss: 1.1826	LR: 0.050000
Training Epoch: 26 [34304/50000]	Loss: 1.0469	LR: 0.050000
Training Epoch: 26 [34432/50000]	Loss: 0.9077	LR: 0.050000
Training Epoch: 26 [34560/50000]	Loss: 1.3338	LR: 0.050000
Training Epoch: 26 [34688/50000]	Loss: 1.0350	LR: 0.050000
Training Epoch: 26 [34816/50000]	Loss: 0.9401	LR: 0.050000
Training Epoch: 26 [34944/50000]	Loss: 1.2276	LR: 0.050000
Training Epoch: 26 [35072/50000]	Loss: 1.2921	LR: 0.050000
Training Epoch: 26 [35200/50000]	Loss: 1.0945	LR: 0.050000
Training Epoch: 26 [35328/50000]	Loss: 1.1164	LR: 0.050000
Training Epoch: 26 [35456/50000]	Loss: 1.0795	LR: 0.050000
Training Epoch: 26 [35584/50000]	Loss: 1.2051	LR: 0.050000
Training Epoch: 26 [35712/50000]	Loss: 1.0371	LR: 0.050000
Training Epoch: 26 [35840/50000]	Loss: 1.1097	LR: 0.050000
Training Epoch: 26 [35968/50000]	Loss: 1.1632	LR: 0.050000
Training Epoch: 26 [36096/50000]	Loss: 1.1607	LR: 0.050000
Training Epoch: 26 [36224/50000]	Loss: 1.1226	LR: 0.050000
Training Epoch: 26 [36352/50000]	Loss: 1.0647	LR: 0.050000
Training Epoch: 26 [36480/50000]	Loss: 1.2935	LR: 0.050000
Training Epoch: 26 [36608/50000]	Loss: 1.0665	LR: 0.050000
Training Epoch: 26 [36736/50000]	Loss: 0.9723	LR: 0.050000
Training Epoch: 26 [36864/50000]	Loss: 0.9550	LR: 0.050000
Training Epoch: 26 [36992/50000]	Loss: 0.9785	LR: 0.050000
Training Epoch: 26 [37120/50000]	Loss: 1.0648	LR: 0.050000
Training Epoch: 26 [37248/50000]	Loss: 1.1801	LR: 0.050000
Training Epoch: 26 [37376/50000]	Loss: 1.1125	LR: 0.050000
Training Epoch: 26 [37504/50000]	Loss: 1.0362	LR: 0.050000
Training Epoch: 26 [37632/50000]	Loss: 1.2803	LR: 0.050000
Training Epoch: 26 [37760/50000]	Loss: 1.0690	LR: 0.050000
Training Epoch: 26 [37888/50000]	Loss: 1.0253	LR: 0.050000
Training Epoch: 26 [38016/50000]	Loss: 1.2459	LR: 0.050000
Training Epoch: 26 [38144/50000]	Loss: 1.1473	LR: 0.050000
Training Epoch: 26 [38272/50000]	Loss: 1.2766	LR: 0.050000
Training Epoch: 26 [38400/50000]	Loss: 1.1484	LR: 0.050000
Training Epoch: 26 [38528/50000]	Loss: 1.1272	LR: 0.050000
Training Epoch: 26 [38656/50000]	Loss: 0.9805	LR: 0.050000
Training Epoch: 26 [38784/50000]	Loss: 0.9215	LR: 0.050000
Training Epoch: 26 [38912/50000]	Loss: 0.9693	LR: 0.050000
Training Epoch: 26 [39040/50000]	Loss: 1.1751	LR: 0.050000
Training Epoch: 26 [39168/50000]	Loss: 0.9774	LR: 0.050000
Training Epoch: 26 [39296/50000]	Loss: 1.1905	LR: 0.050000
Training Epoch: 26 [39424/50000]	Loss: 1.0107	LR: 0.050000
Training Epoch: 26 [39552/50000]	Loss: 1.0493	LR: 0.050000
Training Epoch: 26 [39680/50000]	Loss: 1.2883	LR: 0.050000
Training Epoch: 26 [39808/50000]	Loss: 1.0127	LR: 0.050000
Training Epoch: 26 [39936/50000]	Loss: 1.1225	LR: 0.050000
Training Epoch: 26 [40064/50000]	Loss: 1.2423	LR: 0.050000
Training Epoch: 26 [40192/50000]	Loss: 1.0758	LR: 0.050000
Training Epoch: 26 [40320/50000]	Loss: 1.0006	LR: 0.050000
Training Epoch: 26 [40448/50000]	Loss: 1.2184	LR: 0.050000
Training Epoch: 26 [40576/50000]	Loss: 1.1277	LR: 0.050000
Training Epoch: 26 [40704/50000]	Loss: 1.1892	LR: 0.050000
Training Epoch: 26 [40832/50000]	Loss: 1.1614	LR: 0.050000
Training Epoch: 26 [40960/50000]	Loss: 0.7886	LR: 0.050000
Training Epoch: 26 [41088/50000]	Loss: 0.9610	LR: 0.050000
Training Epoch: 26 [41216/50000]	Loss: 1.3647	LR: 0.050000
Training Epoch: 26 [41344/50000]	Loss: 1.2539	LR: 0.050000
Training Epoch: 26 [41472/50000]	Loss: 1.2282	LR: 0.050000
Training Epoch: 26 [41600/50000]	Loss: 1.2874	LR: 0.050000
Training Epoch: 26 [41728/50000]	Loss: 1.0846	LR: 0.050000
Training Epoch: 26 [41856/50000]	Loss: 1.0848	LR: 0.050000
Training Epoch: 26 [41984/50000]	Loss: 1.0882	LR: 0.050000
Training Epoch: 26 [42112/50000]	Loss: 1.0195	LR: 0.050000
Training Epoch: 26 [42240/50000]	Loss: 1.2692	LR: 0.050000
Training Epoch: 26 [42368/50000]	Loss: 1.3102	LR: 0.050000
Training Epoch: 26 [42496/50000]	Loss: 1.0916	LR: 0.050000
Training Epoch: 26 [42624/50000]	Loss: 1.2216	LR: 0.050000
Training Epoch: 26 [42752/50000]	Loss: 1.1499	LR: 0.050000
Training Epoch: 26 [42880/50000]	Loss: 0.9784	LR: 0.050000
Training Epoch: 26 [43008/50000]	Loss: 1.1745	LR: 0.050000
Training Epoch: 26 [43136/50000]	Loss: 1.1755	LR: 0.050000
Training Epoch: 26 [43264/50000]	Loss: 1.0823	LR: 0.050000
Training Epoch: 26 [43392/50000]	Loss: 1.1156	LR: 0.050000
Training Epoch: 26 [43520/50000]	Loss: 1.0373	LR: 0.050000
Training Epoch: 26 [43648/50000]	Loss: 1.3461	LR: 0.050000
Training Epoch: 26 [43776/50000]	Loss: 1.1957	LR: 0.050000
Training Epoch: 26 [43904/50000]	Loss: 1.2009	LR: 0.050000
Training Epoch: 26 [44032/50000]	Loss: 1.0529	LR: 0.050000
Training Epoch: 26 [44160/50000]	Loss: 1.0778	LR: 0.050000
Training Epoch: 26 [44288/50000]	Loss: 1.0893	LR: 0.050000
Training Epoch: 26 [44416/50000]	Loss: 1.2958	LR: 0.050000
Training Epoch: 26 [44544/50000]	Loss: 1.0601	LR: 0.050000
Training Epoch: 26 [44672/50000]	Loss: 0.9751	LR: 0.050000
Training Epoch: 26 [44800/50000]	Loss: 1.2071	LR: 0.050000
Training Epoch: 26 [44928/50000]	Loss: 1.0108	LR: 0.050000
Training Epoch: 26 [45056/50000]	Loss: 0.9662	LR: 0.050000
Training Epoch: 26 [45184/50000]	Loss: 1.1402	LR: 0.050000
Training Epoch: 26 [45312/50000]	Loss: 1.1528	LR: 0.050000
Training Epoch: 26 [45440/50000]	Loss: 1.2572	LR: 0.050000
Training Epoch: 26 [45568/50000]	Loss: 1.0172	LR: 0.050000
Training Epoch: 26 [45696/50000]	Loss: 1.1560	LR: 0.050000
Training Epoch: 26 [45824/50000]	Loss: 1.1172	LR: 0.050000
Training Epoch: 26 [45952/50000]	Loss: 1.2509	LR: 0.050000
Training Epoch: 26 [46080/50000]	Loss: 1.1139	LR: 0.050000
Training Epoch: 26 [46208/50000]	Loss: 1.1301	LR: 0.050000
Training Epoch: 26 [46336/50000]	Loss: 1.0872	LR: 0.050000
Training Epoch: 26 [46464/50000]	Loss: 1.3371	LR: 0.050000
Training Epoch: 26 [46592/50000]	Loss: 1.3230	LR: 0.050000
Training Epoch: 26 [46720/50000]	Loss: 1.0865	LR: 0.050000
Training Epoch: 26 [46848/50000]	Loss: 0.9348	LR: 0.050000
Training Epoch: 26 [46976/50000]	Loss: 1.4520	LR: 0.050000
Training Epoch: 26 [47104/50000]	Loss: 1.1003	LR: 0.050000
Training Epoch: 26 [47232/50000]	Loss: 1.2012	LR: 0.050000
Training Epoch: 26 [47360/50000]	Loss: 0.8680	LR: 0.050000
Training Epoch: 26 [47488/50000]	Loss: 1.0031	LR: 0.050000
Training Epoch: 26 [47616/50000]	Loss: 1.2707	LR: 0.050000
Training Epoch: 26 [47744/50000]	Loss: 1.2097	LR: 0.050000
Training Epoch: 26 [47872/50000]	Loss: 1.2596	LR: 0.050000
Training Epoch: 26 [48000/50000]	Loss: 1.1635	LR: 0.050000
Training Epoch: 26 [48128/50000]	Loss: 1.0755	LR: 0.050000
Training Epoch: 26 [48256/50000]	Loss: 1.2360	LR: 0.050000
Training Epoch: 26 [48384/50000]	Loss: 1.1343	LR: 0.050000
Training Epoch: 26 [48512/50000]	Loss: 1.4092	LR: 0.050000
Training Epoch: 26 [48640/50000]	Loss: 1.0446	LR: 0.050000
Training Epoch: 26 [48768/50000]	Loss: 1.0989	LR: 0.050000
Training Epoch: 26 [48896/50000]	Loss: 1.1661	LR: 0.050000
Training Epoch: 26 [49024/50000]	Loss: 1.3038	LR: 0.050000
Training Epoch: 26 [49152/50000]	Loss: 1.0072	LR: 0.050000
Training Epoch: 26 [49280/50000]	Loss: 1.0679	LR: 0.050000
Training Epoch: 26 [49408/50000]	Loss: 1.1105	LR: 0.050000
Training Epoch: 26 [49536/50000]	Loss: 1.1829	LR: 0.050000
Training Epoch: 26 [49664/50000]	Loss: 1.1730	LR: 0.050000
Training Epoch: 26 [49792/50000]	Loss: 1.0347	LR: 0.050000
Training Epoch: 26 [49920/50000]	Loss: 0.9169	LR: 0.050000
Training Epoch: 26 [50000/50000]	Loss: 1.2164	LR: 0.050000
epoch 26 training time consumed: 53.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   95422 GB |   95421 GB |
|       from large pool |  123392 KB |    1034 MB |   95327 GB |   95327 GB |
|       from small pool |   10798 KB |      13 MB |      94 GB |      94 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   95422 GB |   95421 GB |
|       from large pool |  123392 KB |    1034 MB |   95327 GB |   95327 GB |
|       from small pool |   10798 KB |      13 MB |      94 GB |      94 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   41993 GB |   41993 GB |
|       from large pool |  155136 KB |  433088 KB |   41889 GB |   41889 GB |
|       from small pool |    1490 KB |    3494 KB |     103 GB |     103 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    3682 K  |    3682 K  |
|       from large pool |      24    |      65    |    1921 K  |    1921 K  |
|       from small pool |     231    |     274    |    1760 K  |    1760 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    3682 K  |    3682 K  |
|       from large pool |      24    |      65    |    1921 K  |    1921 K  |
|       from small pool |     231    |     274    |    1760 K  |    1760 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1821 K  |    1821 K  |
|       from large pool |       9    |      14    |     930 K  |     930 K  |
|       from small pool |      12    |      16    |     891 K  |     891 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 26, Average loss: 0.0117, Accuracy: 0.6105, Time consumed:3.45s

Training Epoch: 27 [128/50000]	Loss: 0.9501	LR: 0.050000
Training Epoch: 27 [256/50000]	Loss: 1.3492	LR: 0.050000
Training Epoch: 27 [384/50000]	Loss: 1.0942	LR: 0.050000
Training Epoch: 27 [512/50000]	Loss: 0.8129	LR: 0.050000
Training Epoch: 27 [640/50000]	Loss: 1.0580	LR: 0.050000
Training Epoch: 27 [768/50000]	Loss: 1.3174	LR: 0.050000
Training Epoch: 27 [896/50000]	Loss: 0.9504	LR: 0.050000
Training Epoch: 27 [1024/50000]	Loss: 1.2150	LR: 0.050000
Training Epoch: 27 [1152/50000]	Loss: 0.9383	LR: 0.050000
Training Epoch: 27 [1280/50000]	Loss: 1.0816	LR: 0.050000
Training Epoch: 27 [1408/50000]	Loss: 1.0697	LR: 0.050000
Training Epoch: 27 [1536/50000]	Loss: 0.9788	LR: 0.050000
Training Epoch: 27 [1664/50000]	Loss: 0.9604	LR: 0.050000
Training Epoch: 27 [1792/50000]	Loss: 1.0304	LR: 0.050000
Training Epoch: 27 [1920/50000]	Loss: 1.1911	LR: 0.050000
Training Epoch: 27 [2048/50000]	Loss: 1.0580	LR: 0.050000
Training Epoch: 27 [2176/50000]	Loss: 0.8494	LR: 0.050000
Training Epoch: 27 [2304/50000]	Loss: 0.9053	LR: 0.050000
Training Epoch: 27 [2432/50000]	Loss: 1.0100	LR: 0.050000
Training Epoch: 27 [2560/50000]	Loss: 1.0827	LR: 0.050000
Training Epoch: 27 [2688/50000]	Loss: 1.0331	LR: 0.050000
Training Epoch: 27 [2816/50000]	Loss: 0.9540	LR: 0.050000
Training Epoch: 27 [2944/50000]	Loss: 0.9612	LR: 0.050000
Training Epoch: 27 [3072/50000]	Loss: 1.0982	LR: 0.050000
Training Epoch: 27 [3200/50000]	Loss: 0.9144	LR: 0.050000
Training Epoch: 27 [3328/50000]	Loss: 1.1443	LR: 0.050000
Training Epoch: 27 [3456/50000]	Loss: 1.0261	LR: 0.050000
Training Epoch: 27 [3584/50000]	Loss: 0.9360	LR: 0.050000
Training Epoch: 27 [3712/50000]	Loss: 1.0563	LR: 0.050000
Training Epoch: 27 [3840/50000]	Loss: 0.9684	LR: 0.050000
Training Epoch: 27 [3968/50000]	Loss: 1.2481	LR: 0.050000
Training Epoch: 27 [4096/50000]	Loss: 1.3390	LR: 0.050000
Training Epoch: 27 [4224/50000]	Loss: 1.1184	LR: 0.050000
Training Epoch: 27 [4352/50000]	Loss: 1.0816	LR: 0.050000
Training Epoch: 27 [4480/50000]	Loss: 1.0928	LR: 0.050000
Training Epoch: 27 [4608/50000]	Loss: 1.0204	LR: 0.050000
Training Epoch: 27 [4736/50000]	Loss: 0.9414	LR: 0.050000
Training Epoch: 27 [4864/50000]	Loss: 0.9020	LR: 0.050000
Training Epoch: 27 [4992/50000]	Loss: 1.0019	LR: 0.050000
Training Epoch: 27 [5120/50000]	Loss: 0.9094	LR: 0.050000
Training Epoch: 27 [5248/50000]	Loss: 1.1552	LR: 0.050000
Training Epoch: 27 [5376/50000]	Loss: 0.9219	LR: 0.050000
Training Epoch: 27 [5504/50000]	Loss: 1.0012	LR: 0.050000
Training Epoch: 27 [5632/50000]	Loss: 0.7485	LR: 0.050000
Training Epoch: 27 [5760/50000]	Loss: 0.9217	LR: 0.050000
Training Epoch: 27 [5888/50000]	Loss: 0.7951	LR: 0.050000
Training Epoch: 27 [6016/50000]	Loss: 0.8161	LR: 0.050000
Training Epoch: 27 [6144/50000]	Loss: 1.0113	LR: 0.050000
Training Epoch: 27 [6272/50000]	Loss: 0.9817	LR: 0.050000
Training Epoch: 27 [6400/50000]	Loss: 0.8053	LR: 0.050000
Training Epoch: 27 [6528/50000]	Loss: 0.9192	LR: 0.050000
Training Epoch: 27 [6656/50000]	Loss: 0.9555	LR: 0.050000
Training Epoch: 27 [6784/50000]	Loss: 0.9568	LR: 0.050000
Training Epoch: 27 [6912/50000]	Loss: 0.9883	LR: 0.050000
Training Epoch: 27 [7040/50000]	Loss: 0.8554	LR: 0.050000
Training Epoch: 27 [7168/50000]	Loss: 1.0542	LR: 0.050000
Training Epoch: 27 [7296/50000]	Loss: 0.9128	LR: 0.050000
Training Epoch: 27 [7424/50000]	Loss: 0.9777	LR: 0.050000
Training Epoch: 27 [7552/50000]	Loss: 0.9445	LR: 0.050000
Training Epoch: 27 [7680/50000]	Loss: 0.9961	LR: 0.050000
Training Epoch: 27 [7808/50000]	Loss: 0.9673	LR: 0.050000
Training Epoch: 27 [7936/50000]	Loss: 1.1217	LR: 0.050000
Training Epoch: 27 [8064/50000]	Loss: 1.0118	LR: 0.050000
Training Epoch: 27 [8192/50000]	Loss: 1.0075	LR: 0.050000
Training Epoch: 27 [8320/50000]	Loss: 1.0969	LR: 0.050000
Training Epoch: 27 [8448/50000]	Loss: 1.0110	LR: 0.050000
Training Epoch: 27 [8576/50000]	Loss: 1.0059	LR: 0.050000
Training Epoch: 27 [8704/50000]	Loss: 1.0641	LR: 0.050000
Training Epoch: 27 [8832/50000]	Loss: 1.1665	LR: 0.050000
Training Epoch: 27 [8960/50000]	Loss: 0.9374	LR: 0.050000
Training Epoch: 27 [9088/50000]	Loss: 0.9867	LR: 0.050000
Training Epoch: 27 [9216/50000]	Loss: 1.1052	LR: 0.050000
Training Epoch: 27 [9344/50000]	Loss: 0.8805	LR: 0.050000
Training Epoch: 27 [9472/50000]	Loss: 0.8836	LR: 0.050000
Training Epoch: 27 [9600/50000]	Loss: 1.1760	LR: 0.050000
Training Epoch: 27 [9728/50000]	Loss: 1.1035	LR: 0.050000
Training Epoch: 27 [9856/50000]	Loss: 0.7247	LR: 0.050000
Training Epoch: 27 [9984/50000]	Loss: 1.0338	LR: 0.050000
Training Epoch: 27 [10112/50000]	Loss: 0.9945	LR: 0.050000
Training Epoch: 27 [10240/50000]	Loss: 1.2223	LR: 0.050000
Training Epoch: 27 [10368/50000]	Loss: 1.2262	LR: 0.050000
Training Epoch: 27 [10496/50000]	Loss: 1.0325	LR: 0.050000
Training Epoch: 27 [10624/50000]	Loss: 1.3357	LR: 0.050000
Training Epoch: 27 [10752/50000]	Loss: 0.9419	LR: 0.050000
Training Epoch: 27 [10880/50000]	Loss: 1.0896	LR: 0.050000
Training Epoch: 27 [11008/50000]	Loss: 0.9008	LR: 0.050000
Training Epoch: 27 [11136/50000]	Loss: 1.1893	LR: 0.050000
Training Epoch: 27 [11264/50000]	Loss: 1.2327	LR: 0.050000
Training Epoch: 27 [11392/50000]	Loss: 1.0757	LR: 0.050000
Training Epoch: 27 [11520/50000]	Loss: 0.9854	LR: 0.050000
Training Epoch: 27 [11648/50000]	Loss: 1.0665	LR: 0.050000
Training Epoch: 27 [11776/50000]	Loss: 1.1590	LR: 0.050000
Training Epoch: 27 [11904/50000]	Loss: 0.7845	LR: 0.050000
Training Epoch: 27 [12032/50000]	Loss: 0.9216	LR: 0.050000
Training Epoch: 27 [12160/50000]	Loss: 1.1096	LR: 0.050000
Training Epoch: 27 [12288/50000]	Loss: 0.9631	LR: 0.050000
Training Epoch: 27 [12416/50000]	Loss: 1.0654	LR: 0.050000
Training Epoch: 27 [12544/50000]	Loss: 1.2119	LR: 0.050000
Training Epoch: 27 [12672/50000]	Loss: 0.7714	LR: 0.050000
Training Epoch: 27 [12800/50000]	Loss: 1.1211	LR: 0.050000
Training Epoch: 27 [12928/50000]	Loss: 0.8364	LR: 0.050000
Training Epoch: 27 [13056/50000]	Loss: 0.9830	LR: 0.050000
Training Epoch: 27 [13184/50000]	Loss: 0.9848	LR: 0.050000
Training Epoch: 27 [13312/50000]	Loss: 1.1104	LR: 0.050000
Training Epoch: 27 [13440/50000]	Loss: 0.8159	LR: 0.050000
Training Epoch: 27 [13568/50000]	Loss: 0.9148	LR: 0.050000
Training Epoch: 27 [13696/50000]	Loss: 0.8994	LR: 0.050000
Training Epoch: 27 [13824/50000]	Loss: 1.0205	LR: 0.050000
Training Epoch: 27 [13952/50000]	Loss: 0.9600	LR: 0.050000
Training Epoch: 27 [14080/50000]	Loss: 1.1751	LR: 0.050000
Training Epoch: 27 [14208/50000]	Loss: 0.9630	LR: 0.050000
Training Epoch: 27 [14336/50000]	Loss: 0.9253	LR: 0.050000
Training Epoch: 27 [14464/50000]	Loss: 1.1523	LR: 0.050000
Training Epoch: 27 [14592/50000]	Loss: 1.0215	LR: 0.050000
Training Epoch: 27 [14720/50000]	Loss: 0.9863	LR: 0.050000
Training Epoch: 27 [14848/50000]	Loss: 1.2985	LR: 0.050000
Training Epoch: 27 [14976/50000]	Loss: 0.8704	LR: 0.050000
Training Epoch: 27 [15104/50000]	Loss: 1.0988	LR: 0.050000
Training Epoch: 27 [15232/50000]	Loss: 1.1919	LR: 0.050000
Training Epoch: 27 [15360/50000]	Loss: 1.1164	LR: 0.050000
Training Epoch: 27 [15488/50000]	Loss: 1.0526	LR: 0.050000
Training Epoch: 27 [15616/50000]	Loss: 0.9221	LR: 0.050000
Training Epoch: 27 [15744/50000]	Loss: 0.9468	LR: 0.050000
Training Epoch: 27 [15872/50000]	Loss: 1.2116	LR: 0.050000
Training Epoch: 27 [16000/50000]	Loss: 1.0110	LR: 0.050000
Training Epoch: 27 [16128/50000]	Loss: 0.9494	LR: 0.050000
Training Epoch: 27 [16256/50000]	Loss: 0.9766	LR: 0.050000
Training Epoch: 27 [16384/50000]	Loss: 1.3637	LR: 0.050000
Training Epoch: 27 [16512/50000]	Loss: 1.1603	LR: 0.050000
Training Epoch: 27 [16640/50000]	Loss: 0.9999	LR: 0.050000
Training Epoch: 27 [16768/50000]	Loss: 0.9554	LR: 0.050000
Training Epoch: 27 [16896/50000]	Loss: 1.0599	LR: 0.050000
Training Epoch: 27 [17024/50000]	Loss: 1.0101	LR: 0.050000
Training Epoch: 27 [17152/50000]	Loss: 1.1621	LR: 0.050000
Training Epoch: 27 [17280/50000]	Loss: 1.1590	LR: 0.050000
Training Epoch: 27 [17408/50000]	Loss: 1.1029	LR: 0.050000
Training Epoch: 27 [17536/50000]	Loss: 0.9461	LR: 0.050000
Training Epoch: 27 [17664/50000]	Loss: 0.9651	LR: 0.050000
Training Epoch: 27 [17792/50000]	Loss: 1.1738	LR: 0.050000
Training Epoch: 27 [17920/50000]	Loss: 1.1259	LR: 0.050000
Training Epoch: 27 [18048/50000]	Loss: 1.0958	LR: 0.050000
Training Epoch: 27 [18176/50000]	Loss: 0.9981	LR: 0.050000
Training Epoch: 27 [18304/50000]	Loss: 1.0954	LR: 0.050000
Training Epoch: 27 [18432/50000]	Loss: 1.2480	LR: 0.050000
Training Epoch: 27 [18560/50000]	Loss: 1.0081	LR: 0.050000
Training Epoch: 27 [18688/50000]	Loss: 0.8589	LR: 0.050000
Training Epoch: 27 [18816/50000]	Loss: 1.1269	LR: 0.050000
Training Epoch: 27 [18944/50000]	Loss: 1.0734	LR: 0.050000
Training Epoch: 27 [19072/50000]	Loss: 1.0059	LR: 0.050000
Training Epoch: 27 [19200/50000]	Loss: 1.1857	LR: 0.050000
Training Epoch: 27 [19328/50000]	Loss: 1.2409	LR: 0.050000
Training Epoch: 27 [19456/50000]	Loss: 0.9715	LR: 0.050000
Training Epoch: 27 [19584/50000]	Loss: 0.9493	LR: 0.050000
Training Epoch: 27 [19712/50000]	Loss: 1.0377	LR: 0.050000
Training Epoch: 27 [19840/50000]	Loss: 1.4657	LR: 0.050000
Training Epoch: 27 [19968/50000]	Loss: 1.0471	LR: 0.050000
Training Epoch: 27 [20096/50000]	Loss: 1.0142	LR: 0.050000
Training Epoch: 27 [20224/50000]	Loss: 1.0648	LR: 0.050000
Training Epoch: 27 [20352/50000]	Loss: 0.8877	LR: 0.050000
Training Epoch: 27 [20480/50000]	Loss: 1.0597	LR: 0.050000
Training Epoch: 27 [20608/50000]	Loss: 0.9057	LR: 0.050000
Training Epoch: 27 [20736/50000]	Loss: 0.9986	LR: 0.050000
Training Epoch: 27 [20864/50000]	Loss: 0.9240	LR: 0.050000
Training Epoch: 27 [20992/50000]	Loss: 1.0726	LR: 0.050000
Training Epoch: 27 [21120/50000]	Loss: 1.0141	LR: 0.050000
Training Epoch: 27 [21248/50000]	Loss: 1.3961	LR: 0.050000
Training Epoch: 27 [21376/50000]	Loss: 0.9836	LR: 0.050000
Training Epoch: 27 [21504/50000]	Loss: 1.0429	LR: 0.050000
Training Epoch: 27 [21632/50000]	Loss: 0.8657	LR: 0.050000
Training Epoch: 27 [21760/50000]	Loss: 0.9294	LR: 0.050000
Training Epoch: 27 [21888/50000]	Loss: 1.0009	LR: 0.050000
Training Epoch: 27 [22016/50000]	Loss: 1.0687	LR: 0.050000
Training Epoch: 27 [22144/50000]	Loss: 1.0436	LR: 0.050000
Training Epoch: 27 [22272/50000]	Loss: 1.0314	LR: 0.050000
Training Epoch: 27 [22400/50000]	Loss: 1.0416	LR: 0.050000
Training Epoch: 27 [22528/50000]	Loss: 1.1252	LR: 0.050000
Training Epoch: 27 [22656/50000]	Loss: 1.1678	LR: 0.050000
Training Epoch: 27 [22784/50000]	Loss: 1.0475	LR: 0.050000
Training Epoch: 27 [22912/50000]	Loss: 0.9226	LR: 0.050000
Training Epoch: 27 [23040/50000]	Loss: 1.2044	LR: 0.050000
Training Epoch: 27 [23168/50000]	Loss: 1.0157	LR: 0.050000
Training Epoch: 27 [23296/50000]	Loss: 1.2159	LR: 0.050000
Training Epoch: 27 [23424/50000]	Loss: 1.3108	LR: 0.050000
Training Epoch: 27 [23552/50000]	Loss: 1.2663	LR: 0.050000
Training Epoch: 27 [23680/50000]	Loss: 1.2130	LR: 0.050000
Training Epoch: 27 [23808/50000]	Loss: 0.8898	LR: 0.050000
Training Epoch: 27 [23936/50000]	Loss: 0.7706	LR: 0.050000
Training Epoch: 27 [24064/50000]	Loss: 0.9353	LR: 0.050000
Training Epoch: 27 [24192/50000]	Loss: 0.9187	LR: 0.050000
Training Epoch: 27 [24320/50000]	Loss: 0.8277	LR: 0.050000
Training Epoch: 27 [24448/50000]	Loss: 0.7880	LR: 0.050000
Training Epoch: 27 [24576/50000]	Loss: 1.3331	LR: 0.050000
Training Epoch: 27 [24704/50000]	Loss: 1.0273	LR: 0.050000
Training Epoch: 27 [24832/50000]	Loss: 1.2331	LR: 0.050000
Training Epoch: 27 [24960/50000]	Loss: 1.0451	LR: 0.050000
Training Epoch: 27 [25088/50000]	Loss: 1.0480	LR: 0.050000
Training Epoch: 27 [25216/50000]	Loss: 1.0537	LR: 0.050000
Training Epoch: 27 [25344/50000]	Loss: 1.2390	LR: 0.050000
Training Epoch: 27 [25472/50000]	Loss: 1.0665	LR: 0.050000
Training Epoch: 27 [25600/50000]	Loss: 1.2028	LR: 0.050000
Training Epoch: 27 [25728/50000]	Loss: 1.0703	LR: 0.050000
Training Epoch: 27 [25856/50000]	Loss: 1.1481	LR: 0.050000
Training Epoch: 27 [25984/50000]	Loss: 1.0887	LR: 0.050000
Training Epoch: 27 [26112/50000]	Loss: 1.1438	LR: 0.050000
Training Epoch: 27 [26240/50000]	Loss: 1.0103	LR: 0.050000
Training Epoch: 27 [26368/50000]	Loss: 1.1279	LR: 0.050000
Training Epoch: 27 [26496/50000]	Loss: 1.1552	LR: 0.050000
Training Epoch: 27 [26624/50000]	Loss: 1.2482	LR: 0.050000
Training Epoch: 27 [26752/50000]	Loss: 1.0304	LR: 0.050000
Training Epoch: 27 [26880/50000]	Loss: 1.0881	LR: 0.050000
Training Epoch: 27 [27008/50000]	Loss: 1.2049	LR: 0.050000
Training Epoch: 27 [27136/50000]	Loss: 1.1290	LR: 0.050000
Training Epoch: 27 [27264/50000]	Loss: 0.9912	LR: 0.050000
Training Epoch: 27 [27392/50000]	Loss: 0.9791	LR: 0.050000
Training Epoch: 27 [27520/50000]	Loss: 1.1949	LR: 0.050000
Training Epoch: 27 [27648/50000]	Loss: 1.2065	LR: 0.050000
Training Epoch: 27 [27776/50000]	Loss: 0.9783	LR: 0.050000
Training Epoch: 27 [27904/50000]	Loss: 0.9721	LR: 0.050000
Training Epoch: 27 [28032/50000]	Loss: 1.0725	LR: 0.050000
Training Epoch: 27 [28160/50000]	Loss: 1.2062	LR: 0.050000
Training Epoch: 27 [28288/50000]	Loss: 1.2518	LR: 0.050000
Training Epoch: 27 [28416/50000]	Loss: 1.0351	LR: 0.050000
Training Epoch: 27 [28544/50000]	Loss: 0.9735	LR: 0.050000
Training Epoch: 27 [28672/50000]	Loss: 1.2281	LR: 0.050000
Training Epoch: 27 [28800/50000]	Loss: 1.1580	LR: 0.050000
Training Epoch: 27 [28928/50000]	Loss: 1.2215	LR: 0.050000
Training Epoch: 27 [29056/50000]	Loss: 1.1633	LR: 0.050000
Training Epoch: 27 [29184/50000]	Loss: 0.9687	LR: 0.050000
Training Epoch: 27 [29312/50000]	Loss: 1.1750	LR: 0.050000
Training Epoch: 27 [29440/50000]	Loss: 1.2347	LR: 0.050000
Training Epoch: 27 [29568/50000]	Loss: 1.2228	LR: 0.050000
Training Epoch: 27 [29696/50000]	Loss: 1.3448	LR: 0.050000
Training Epoch: 27 [29824/50000]	Loss: 1.2291	LR: 0.050000
Training Epoch: 27 [29952/50000]	Loss: 1.0392	LR: 0.050000
Training Epoch: 27 [30080/50000]	Loss: 0.9881	LR: 0.050000
Training Epoch: 27 [30208/50000]	Loss: 1.0194	LR: 0.050000
Training Epoch: 27 [30336/50000]	Loss: 1.1638	LR: 0.050000
Training Epoch: 27 [30464/50000]	Loss: 1.1221	LR: 0.050000
Training Epoch: 27 [30592/50000]	Loss: 1.2273	LR: 0.050000
Training Epoch: 27 [30720/50000]	Loss: 1.3214	LR: 0.050000
Training Epoch: 27 [30848/50000]	Loss: 1.3304	LR: 0.050000
Training Epoch: 27 [30976/50000]	Loss: 1.1511	LR: 0.050000
Training Epoch: 27 [31104/50000]	Loss: 1.1781	LR: 0.050000
Training Epoch: 27 [31232/50000]	Loss: 1.2073	LR: 0.050000
Training Epoch: 27 [31360/50000]	Loss: 1.0633	LR: 0.050000
Training Epoch: 27 [31488/50000]	Loss: 0.8907	LR: 0.050000
Training Epoch: 27 [31616/50000]	Loss: 0.9831	LR: 0.050000
Training Epoch: 27 [31744/50000]	Loss: 1.1166	LR: 0.050000
Training Epoch: 27 [31872/50000]	Loss: 1.0629	LR: 0.050000
Training Epoch: 27 [32000/50000]	Loss: 1.0140	LR: 0.050000
Training Epoch: 27 [32128/50000]	Loss: 1.2603	LR: 0.050000
Training Epoch: 27 [32256/50000]	Loss: 1.0717	LR: 0.050000
Training Epoch: 27 [32384/50000]	Loss: 1.0215	LR: 0.050000
Training Epoch: 27 [32512/50000]	Loss: 1.0439	LR: 0.050000
Training Epoch: 27 [32640/50000]	Loss: 1.1313	LR: 0.050000
Training Epoch: 27 [32768/50000]	Loss: 1.1673	LR: 0.050000
Training Epoch: 27 [32896/50000]	Loss: 1.3490	LR: 0.050000
Training Epoch: 27 [33024/50000]	Loss: 1.2595	LR: 0.050000
Training Epoch: 27 [33152/50000]	Loss: 0.9827	LR: 0.050000
Training Epoch: 27 [33280/50000]	Loss: 1.2057	LR: 0.050000
Training Epoch: 27 [33408/50000]	Loss: 1.0400	LR: 0.050000
Training Epoch: 27 [33536/50000]	Loss: 1.4302	LR: 0.050000
Training Epoch: 27 [33664/50000]	Loss: 1.2144	LR: 0.050000
Training Epoch: 27 [33792/50000]	Loss: 1.1223	LR: 0.050000
Training Epoch: 27 [33920/50000]	Loss: 0.9874	LR: 0.050000
Training Epoch: 27 [34048/50000]	Loss: 1.0990	LR: 0.050000
Training Epoch: 27 [34176/50000]	Loss: 1.0799	LR: 0.050000
Training Epoch: 27 [34304/50000]	Loss: 1.1591	LR: 0.050000
Training Epoch: 27 [34432/50000]	Loss: 1.0510	LR: 0.050000
Training Epoch: 27 [34560/50000]	Loss: 1.0022	LR: 0.050000
Training Epoch: 27 [34688/50000]	Loss: 0.9894	LR: 0.050000
Training Epoch: 27 [34816/50000]	Loss: 1.0721	LR: 0.050000
Training Epoch: 27 [34944/50000]	Loss: 1.3082	LR: 0.050000
Training Epoch: 27 [35072/50000]	Loss: 1.3664	LR: 0.050000
Training Epoch: 27 [35200/50000]	Loss: 1.2061	LR: 0.050000
Training Epoch: 27 [35328/50000]	Loss: 1.0715	LR: 0.050000
Training Epoch: 27 [35456/50000]	Loss: 1.1343	LR: 0.050000
Training Epoch: 27 [35584/50000]	Loss: 1.0298	LR: 0.050000
Training Epoch: 27 [35712/50000]	Loss: 1.1957	LR: 0.050000
Training Epoch: 27 [35840/50000]	Loss: 1.1572	LR: 0.050000
Training Epoch: 27 [35968/50000]	Loss: 1.3952	LR: 0.050000
Training Epoch: 27 [36096/50000]	Loss: 1.1371	LR: 0.050000
Training Epoch: 27 [36224/50000]	Loss: 0.9806	LR: 0.050000
Training Epoch: 27 [36352/50000]	Loss: 1.1156	LR: 0.050000
Training Epoch: 27 [36480/50000]	Loss: 1.0978	LR: 0.050000
Training Epoch: 27 [36608/50000]	Loss: 1.0385	LR: 0.050000
Training Epoch: 27 [36736/50000]	Loss: 1.4643	LR: 0.050000
Training Epoch: 27 [36864/50000]	Loss: 1.2153	LR: 0.050000
Training Epoch: 27 [36992/50000]	Loss: 1.1860	LR: 0.050000
Training Epoch: 27 [37120/50000]	Loss: 1.0147	LR: 0.050000
Training Epoch: 27 [37248/50000]	Loss: 1.2149	LR: 0.050000
Training Epoch: 27 [37376/50000]	Loss: 1.0037	LR: 0.050000
Training Epoch: 27 [37504/50000]	Loss: 1.2066	LR: 0.050000
Training Epoch: 27 [37632/50000]	Loss: 1.1959	LR: 0.050000
Training Epoch: 27 [37760/50000]	Loss: 1.0774	LR: 0.050000
Training Epoch: 27 [37888/50000]	Loss: 1.1056	LR: 0.050000
Training Epoch: 27 [38016/50000]	Loss: 1.1960	LR: 0.050000
Training Epoch: 27 [38144/50000]	Loss: 0.9126	LR: 0.050000
Training Epoch: 27 [38272/50000]	Loss: 1.0219	LR: 0.050000
Training Epoch: 27 [38400/50000]	Loss: 1.0496	LR: 0.050000
Training Epoch: 27 [38528/50000]	Loss: 1.1497	LR: 0.050000
Training Epoch: 27 [38656/50000]	Loss: 1.2128	LR: 0.050000
Training Epoch: 27 [38784/50000]	Loss: 1.0040	LR: 0.050000
Training Epoch: 27 [38912/50000]	Loss: 1.0045	LR: 0.050000
Training Epoch: 27 [39040/50000]	Loss: 1.2294	LR: 0.050000
Training Epoch: 27 [39168/50000]	Loss: 1.1329	LR: 0.050000
Training Epoch: 27 [39296/50000]	Loss: 0.9866	LR: 0.050000
Training Epoch: 27 [39424/50000]	Loss: 1.0585	LR: 0.050000
Training Epoch: 27 [39552/50000]	Loss: 1.2360	LR: 0.050000
Training Epoch: 27 [39680/50000]	Loss: 1.1747	LR: 0.050000
Training Epoch: 27 [39808/50000]	Loss: 1.0419	LR: 0.050000
Training Epoch: 27 [39936/50000]	Loss: 1.1329	LR: 0.050000
Training Epoch: 27 [40064/50000]	Loss: 1.1057	LR: 0.050000
Training Epoch: 27 [40192/50000]	Loss: 1.2371	LR: 0.050000
Training Epoch: 27 [40320/50000]	Loss: 1.3062	LR: 0.050000
Training Epoch: 27 [40448/50000]	Loss: 0.9784	LR: 0.050000
Training Epoch: 27 [40576/50000]	Loss: 1.1928	LR: 0.050000
Training Epoch: 27 [40704/50000]	Loss: 0.9596	LR: 0.050000
Training Epoch: 27 [40832/50000]	Loss: 1.1328	LR: 0.050000
Training Epoch: 27 [40960/50000]	Loss: 1.2662	LR: 0.050000
Training Epoch: 27 [41088/50000]	Loss: 1.0637	LR: 0.050000
Training Epoch: 27 [41216/50000]	Loss: 1.1591	LR: 0.050000
Training Epoch: 27 [41344/50000]	Loss: 1.1258	LR: 0.050000
Training Epoch: 27 [41472/50000]	Loss: 1.0416	LR: 0.050000
Training Epoch: 27 [41600/50000]	Loss: 1.1381	LR: 0.050000
Training Epoch: 27 [41728/50000]	Loss: 1.2079	LR: 0.050000
Training Epoch: 27 [41856/50000]	Loss: 1.0694	LR: 0.050000
Training Epoch: 27 [41984/50000]	Loss: 0.8953	LR: 0.050000
Training Epoch: 27 [42112/50000]	Loss: 1.2086	LR: 0.050000
Training Epoch: 27 [42240/50000]	Loss: 1.1045	LR: 0.050000
Training Epoch: 27 [42368/50000]	Loss: 1.0801	LR: 0.050000
Training Epoch: 27 [42496/50000]	Loss: 1.1884	LR: 0.050000
Training Epoch: 27 [42624/50000]	Loss: 1.0728	LR: 0.050000
Training Epoch: 27 [42752/50000]	Loss: 1.0586	LR: 0.050000
Training Epoch: 27 [42880/50000]	Loss: 1.1583	LR: 0.050000
Training Epoch: 27 [43008/50000]	Loss: 1.1225	LR: 0.050000
Training Epoch: 27 [43136/50000]	Loss: 1.1744	LR: 0.050000
Training Epoch: 27 [43264/50000]	Loss: 1.0677	LR: 0.050000
Training Epoch: 27 [43392/50000]	Loss: 1.1985	LR: 0.050000
Training Epoch: 27 [43520/50000]	Loss: 1.1627	LR: 0.050000
Training Epoch: 27 [43648/50000]	Loss: 0.9867	LR: 0.050000
Training Epoch: 27 [43776/50000]	Loss: 1.2299	LR: 0.050000
Training Epoch: 27 [43904/50000]	Loss: 1.0252	LR: 0.050000
Training Epoch: 27 [44032/50000]	Loss: 1.3286	LR: 0.050000
Training Epoch: 27 [44160/50000]	Loss: 1.0898	LR: 0.050000
Training Epoch: 27 [44288/50000]	Loss: 1.1134	LR: 0.050000
Training Epoch: 27 [44416/50000]	Loss: 1.0667	LR: 0.050000
Training Epoch: 27 [44544/50000]	Loss: 1.3051	LR: 0.050000
Training Epoch: 27 [44672/50000]	Loss: 1.2283	LR: 0.050000
Training Epoch: 27 [44800/50000]	Loss: 1.0920	LR: 0.050000
Training Epoch: 27 [44928/50000]	Loss: 1.1940	LR: 0.050000
Training Epoch: 27 [45056/50000]	Loss: 1.1111	LR: 0.050000
Training Epoch: 27 [45184/50000]	Loss: 0.9295	LR: 0.050000
Training Epoch: 27 [45312/50000]	Loss: 1.1781	LR: 0.050000
Training Epoch: 27 [45440/50000]	Loss: 1.4327	LR: 0.050000
Training Epoch: 27 [45568/50000]	Loss: 0.9763	LR: 0.050000
Training Epoch: 27 [45696/50000]	Loss: 1.0864	LR: 0.050000
Training Epoch: 27 [45824/50000]	Loss: 1.1596	LR: 0.050000
Training Epoch: 27 [45952/50000]	Loss: 0.9853	LR: 0.050000
Training Epoch: 27 [46080/50000]	Loss: 1.2193	LR: 0.050000
Training Epoch: 27 [46208/50000]	Loss: 1.2743	LR: 0.050000
Training Epoch: 27 [46336/50000]	Loss: 1.0470	LR: 0.050000
Training Epoch: 27 [46464/50000]	Loss: 1.3179	LR: 0.050000
Training Epoch: 27 [46592/50000]	Loss: 1.0404	LR: 0.050000
Training Epoch: 27 [46720/50000]	Loss: 1.3024	LR: 0.050000
Training Epoch: 27 [46848/50000]	Loss: 1.3430	LR: 0.050000
Training Epoch: 27 [46976/50000]	Loss: 1.1757	LR: 0.050000
Training Epoch: 27 [47104/50000]	Loss: 1.0902	LR: 0.050000
Training Epoch: 27 [47232/50000]	Loss: 1.3463	LR: 0.050000
Training Epoch: 27 [47360/50000]	Loss: 1.2888	LR: 0.050000
Training Epoch: 27 [47488/50000]	Loss: 1.2252	LR: 0.050000
Training Epoch: 27 [47616/50000]	Loss: 1.1373	LR: 0.050000
Training Epoch: 27 [47744/50000]	Loss: 1.2551	LR: 0.050000
Training Epoch: 27 [47872/50000]	Loss: 1.0695	LR: 0.050000
Training Epoch: 27 [48000/50000]	Loss: 1.1977	LR: 0.050000
Training Epoch: 27 [48128/50000]	Loss: 1.2320	LR: 0.050000
Training Epoch: 27 [48256/50000]	Loss: 1.2533	LR: 0.050000
Training Epoch: 27 [48384/50000]	Loss: 1.2723	LR: 0.050000
Training Epoch: 27 [48512/50000]	Loss: 1.0271	LR: 0.050000
Training Epoch: 27 [48640/50000]	Loss: 1.2764	LR: 0.050000
Training Epoch: 27 [48768/50000]	Loss: 1.1133	LR: 0.050000
Training Epoch: 27 [48896/50000]	Loss: 1.0985	LR: 0.050000
Training Epoch: 27 [49024/50000]	Loss: 1.3787	LR: 0.050000
Training Epoch: 27 [49152/50000]	Loss: 1.1386	LR: 0.050000
Training Epoch: 27 [49280/50000]	Loss: 1.3518	LR: 0.050000
Training Epoch: 27 [49408/50000]	Loss: 1.0201	LR: 0.050000
Training Epoch: 27 [49536/50000]	Loss: 1.2901	LR: 0.050000
Training Epoch: 27 [49664/50000]	Loss: 1.2750	LR: 0.050000
Training Epoch: 27 [49792/50000]	Loss: 1.3198	LR: 0.050000
Training Epoch: 27 [49920/50000]	Loss: 1.0839	LR: 0.050000
Training Epoch: 27 [50000/50000]	Loss: 1.3130	LR: 0.050000
epoch 27 training time consumed: 53.89s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |   99092 GB |   99091 GB |
|       from large pool |  123392 KB |    1034 MB |   98994 GB |   98994 GB |
|       from small pool |   10798 KB |      13 MB |      97 GB |      97 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |   99092 GB |   99091 GB |
|       from large pool |  123392 KB |    1034 MB |   98994 GB |   98994 GB |
|       from small pool |   10798 KB |      13 MB |      97 GB |      97 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   43608 GB |   43608 GB |
|       from large pool |  155136 KB |  433088 KB |   43500 GB |   43500 GB |
|       from small pool |    1490 KB |    3494 KB |     107 GB |     107 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    3824 K  |    3823 K  |
|       from large pool |      24    |      65    |    1995 K  |    1995 K  |
|       from small pool |     231    |     274    |    1828 K  |    1827 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    3824 K  |    3823 K  |
|       from large pool |      24    |      65    |    1995 K  |    1995 K  |
|       from small pool |     231    |     274    |    1828 K  |    1827 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1891 K  |    1891 K  |
|       from large pool |       9    |      14    |     966 K  |     966 K  |
|       from small pool |      12    |      16    |     925 K  |     925 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 27, Average loss: 0.0117, Accuracy: 0.6049, Time consumed:3.46s

Training Epoch: 28 [128/50000]	Loss: 1.0604	LR: 0.050000
Training Epoch: 28 [256/50000]	Loss: 0.9294	LR: 0.050000
Training Epoch: 28 [384/50000]	Loss: 0.9743	LR: 0.050000
Training Epoch: 28 [512/50000]	Loss: 1.0122	LR: 0.050000
Training Epoch: 28 [640/50000]	Loss: 1.0993	LR: 0.050000
Training Epoch: 28 [768/50000]	Loss: 1.0209	LR: 0.050000
Training Epoch: 28 [896/50000]	Loss: 1.0109	LR: 0.050000
Training Epoch: 28 [1024/50000]	Loss: 1.0970	LR: 0.050000
Training Epoch: 28 [1152/50000]	Loss: 0.9542	LR: 0.050000
Training Epoch: 28 [1280/50000]	Loss: 0.9421	LR: 0.050000
Training Epoch: 28 [1408/50000]	Loss: 1.0117	LR: 0.050000
Training Epoch: 28 [1536/50000]	Loss: 1.1093	LR: 0.050000
Training Epoch: 28 [1664/50000]	Loss: 1.1954	LR: 0.050000
Training Epoch: 28 [1792/50000]	Loss: 0.9444	LR: 0.050000
Training Epoch: 28 [1920/50000]	Loss: 1.0344	LR: 0.050000
Training Epoch: 28 [2048/50000]	Loss: 0.9592	LR: 0.050000
Training Epoch: 28 [2176/50000]	Loss: 0.9283	LR: 0.050000
Training Epoch: 28 [2304/50000]	Loss: 1.2177	LR: 0.050000
Training Epoch: 28 [2432/50000]	Loss: 0.9938	LR: 0.050000
Training Epoch: 28 [2560/50000]	Loss: 1.0745	LR: 0.050000
Training Epoch: 28 [2688/50000]	Loss: 0.9343	LR: 0.050000
Training Epoch: 28 [2816/50000]	Loss: 0.9603	LR: 0.050000
Training Epoch: 28 [2944/50000]	Loss: 0.8153	LR: 0.050000
Training Epoch: 28 [3072/50000]	Loss: 0.9845	LR: 0.050000
Training Epoch: 28 [3200/50000]	Loss: 0.9694	LR: 0.050000
Training Epoch: 28 [3328/50000]	Loss: 1.0589	LR: 0.050000
Training Epoch: 28 [3456/50000]	Loss: 0.8218	LR: 0.050000
Training Epoch: 28 [3584/50000]	Loss: 1.0356	LR: 0.050000
Training Epoch: 28 [3712/50000]	Loss: 0.8564	LR: 0.050000
Training Epoch: 28 [3840/50000]	Loss: 1.1161	LR: 0.050000
Training Epoch: 28 [3968/50000]	Loss: 0.8597	LR: 0.050000
Training Epoch: 28 [4096/50000]	Loss: 1.2532	LR: 0.050000
Training Epoch: 28 [4224/50000]	Loss: 0.9538	LR: 0.050000
Training Epoch: 28 [4352/50000]	Loss: 0.9597	LR: 0.050000
Training Epoch: 28 [4480/50000]	Loss: 0.7828	LR: 0.050000
Training Epoch: 28 [4608/50000]	Loss: 1.2344	LR: 0.050000
Training Epoch: 28 [4736/50000]	Loss: 1.0112	LR: 0.050000
Training Epoch: 28 [4864/50000]	Loss: 0.9021	LR: 0.050000
Training Epoch: 28 [4992/50000]	Loss: 1.1917	LR: 0.050000
Training Epoch: 28 [5120/50000]	Loss: 0.9675	LR: 0.050000
Training Epoch: 28 [5248/50000]	Loss: 1.0915	LR: 0.050000
Training Epoch: 28 [5376/50000]	Loss: 0.8913	LR: 0.050000
Training Epoch: 28 [5504/50000]	Loss: 1.0256	LR: 0.050000
Training Epoch: 28 [5632/50000]	Loss: 1.1318	LR: 0.050000
Training Epoch: 28 [5760/50000]	Loss: 0.8989	LR: 0.050000
Training Epoch: 28 [5888/50000]	Loss: 1.2252	LR: 0.050000
Training Epoch: 28 [6016/50000]	Loss: 1.0044	LR: 0.050000
Training Epoch: 28 [6144/50000]	Loss: 1.1461	LR: 0.050000
Training Epoch: 28 [6272/50000]	Loss: 0.9694	LR: 0.050000
Training Epoch: 28 [6400/50000]	Loss: 0.9826	LR: 0.050000
Training Epoch: 28 [6528/50000]	Loss: 1.0422	LR: 0.050000
Training Epoch: 28 [6656/50000]	Loss: 1.1759	LR: 0.050000
Training Epoch: 28 [6784/50000]	Loss: 0.9463	LR: 0.050000
Training Epoch: 28 [6912/50000]	Loss: 1.1449	LR: 0.050000
Training Epoch: 28 [7040/50000]	Loss: 1.0247	LR: 0.050000
Training Epoch: 28 [7168/50000]	Loss: 1.2761	LR: 0.050000
Training Epoch: 28 [7296/50000]	Loss: 1.2029	LR: 0.050000
Training Epoch: 28 [7424/50000]	Loss: 1.0673	LR: 0.050000
Training Epoch: 28 [7552/50000]	Loss: 0.9066	LR: 0.050000
Training Epoch: 28 [7680/50000]	Loss: 1.1362	LR: 0.050000
Training Epoch: 28 [7808/50000]	Loss: 0.9225	LR: 0.050000
Training Epoch: 28 [7936/50000]	Loss: 1.0630	LR: 0.050000
Training Epoch: 28 [8064/50000]	Loss: 0.8399	LR: 0.050000
Training Epoch: 28 [8192/50000]	Loss: 0.9058	LR: 0.050000
Training Epoch: 28 [8320/50000]	Loss: 1.1055	LR: 0.050000
Training Epoch: 28 [8448/50000]	Loss: 1.1479	LR: 0.050000
Training Epoch: 28 [8576/50000]	Loss: 0.8024	LR: 0.050000
Training Epoch: 28 [8704/50000]	Loss: 1.1093	LR: 0.050000
Training Epoch: 28 [8832/50000]	Loss: 1.0571	LR: 0.050000
Training Epoch: 28 [8960/50000]	Loss: 1.1756	LR: 0.050000
Training Epoch: 28 [9088/50000]	Loss: 1.0863	LR: 0.050000
Training Epoch: 28 [9216/50000]	Loss: 1.0770	LR: 0.050000
Training Epoch: 28 [9344/50000]	Loss: 0.9601	LR: 0.050000
Training Epoch: 28 [9472/50000]	Loss: 0.8967	LR: 0.050000
Training Epoch: 28 [9600/50000]	Loss: 0.9197	LR: 0.050000
Training Epoch: 28 [9728/50000]	Loss: 1.1111	LR: 0.050000
Training Epoch: 28 [9856/50000]	Loss: 1.1151	LR: 0.050000
Training Epoch: 28 [9984/50000]	Loss: 0.9862	LR: 0.050000
Training Epoch: 28 [10112/50000]	Loss: 1.1544	LR: 0.050000
Training Epoch: 28 [10240/50000]	Loss: 0.9470	LR: 0.050000
Training Epoch: 28 [10368/50000]	Loss: 1.1425	LR: 0.050000
Training Epoch: 28 [10496/50000]	Loss: 1.1256	LR: 0.050000
Training Epoch: 28 [10624/50000]	Loss: 1.2333	LR: 0.050000
Training Epoch: 28 [10752/50000]	Loss: 1.1226	LR: 0.050000
Training Epoch: 28 [10880/50000]	Loss: 1.6396	LR: 0.050000
Training Epoch: 28 [11008/50000]	Loss: 0.8712	LR: 0.050000
Training Epoch: 28 [11136/50000]	Loss: 1.0569	LR: 0.050000
Training Epoch: 28 [11264/50000]	Loss: 0.9138	LR: 0.050000
Training Epoch: 28 [11392/50000]	Loss: 1.1688	LR: 0.050000
Training Epoch: 28 [11520/50000]	Loss: 1.0709	LR: 0.050000
Training Epoch: 28 [11648/50000]	Loss: 0.9948	LR: 0.050000
Training Epoch: 28 [11776/50000]	Loss: 1.0008	LR: 0.050000
Training Epoch: 28 [11904/50000]	Loss: 0.9799	LR: 0.050000
Training Epoch: 28 [12032/50000]	Loss: 0.9843	LR: 0.050000
Training Epoch: 28 [12160/50000]	Loss: 1.1638	LR: 0.050000
Training Epoch: 28 [12288/50000]	Loss: 0.8486	LR: 0.050000
Training Epoch: 28 [12416/50000]	Loss: 0.9779	LR: 0.050000
Training Epoch: 28 [12544/50000]	Loss: 1.0334	LR: 0.050000
Training Epoch: 28 [12672/50000]	Loss: 0.9773	LR: 0.050000
Training Epoch: 28 [12800/50000]	Loss: 1.1044	LR: 0.050000
Training Epoch: 28 [12928/50000]	Loss: 1.0634	LR: 0.050000
Training Epoch: 28 [13056/50000]	Loss: 0.9166	LR: 0.050000
Training Epoch: 28 [13184/50000]	Loss: 1.0064	LR: 0.050000
Training Epoch: 28 [13312/50000]	Loss: 1.1702	LR: 0.050000
Training Epoch: 28 [13440/50000]	Loss: 1.0347	LR: 0.050000
Training Epoch: 28 [13568/50000]	Loss: 0.9613	LR: 0.050000
Training Epoch: 28 [13696/50000]	Loss: 1.1618	LR: 0.050000
Training Epoch: 28 [13824/50000]	Loss: 1.1008	LR: 0.050000
Training Epoch: 28 [13952/50000]	Loss: 0.8836	LR: 0.050000
Training Epoch: 28 [14080/50000]	Loss: 0.9361	LR: 0.050000
Training Epoch: 28 [14208/50000]	Loss: 1.0251	LR: 0.050000
Training Epoch: 28 [14336/50000]	Loss: 0.9375	LR: 0.050000
Training Epoch: 28 [14464/50000]	Loss: 0.9482	LR: 0.050000
Training Epoch: 28 [14592/50000]	Loss: 0.9575	LR: 0.050000
Training Epoch: 28 [14720/50000]	Loss: 0.9057	LR: 0.050000
Training Epoch: 28 [14848/50000]	Loss: 1.1538	LR: 0.050000
Training Epoch: 28 [14976/50000]	Loss: 1.0623	LR: 0.050000
Training Epoch: 28 [15104/50000]	Loss: 1.0571	LR: 0.050000
Training Epoch: 28 [15232/50000]	Loss: 1.0777	LR: 0.050000
Training Epoch: 28 [15360/50000]	Loss: 1.1183	LR: 0.050000
Training Epoch: 28 [15488/50000]	Loss: 1.0804	LR: 0.050000
Training Epoch: 28 [15616/50000]	Loss: 0.9419	LR: 0.050000
Training Epoch: 28 [15744/50000]	Loss: 0.8689	LR: 0.050000
Training Epoch: 28 [15872/50000]	Loss: 0.7842	LR: 0.050000
Training Epoch: 28 [16000/50000]	Loss: 0.9689	LR: 0.050000
Training Epoch: 28 [16128/50000]	Loss: 0.8773	LR: 0.050000
Training Epoch: 28 [16256/50000]	Loss: 0.9539	LR: 0.050000
Training Epoch: 28 [16384/50000]	Loss: 0.8984	LR: 0.050000
Training Epoch: 28 [16512/50000]	Loss: 1.1074	LR: 0.050000
Training Epoch: 28 [16640/50000]	Loss: 0.9943	LR: 0.050000
Training Epoch: 28 [16768/50000]	Loss: 0.9376	LR: 0.050000
Training Epoch: 28 [16896/50000]	Loss: 0.9014	LR: 0.050000
Training Epoch: 28 [17024/50000]	Loss: 1.1587	LR: 0.050000
Training Epoch: 28 [17152/50000]	Loss: 1.0721	LR: 0.050000
Training Epoch: 28 [17280/50000]	Loss: 0.9590	LR: 0.050000
Training Epoch: 28 [17408/50000]	Loss: 1.2008	LR: 0.050000
Training Epoch: 28 [17536/50000]	Loss: 0.9274	LR: 0.050000
Training Epoch: 28 [17664/50000]	Loss: 1.1697	LR: 0.050000
Training Epoch: 28 [17792/50000]	Loss: 0.7871	LR: 0.050000
Training Epoch: 28 [17920/50000]	Loss: 1.1705	LR: 0.050000
Training Epoch: 28 [18048/50000]	Loss: 0.9171	LR: 0.050000
Training Epoch: 28 [18176/50000]	Loss: 0.9820	LR: 0.050000
Training Epoch: 28 [18304/50000]	Loss: 0.7870	LR: 0.050000
Training Epoch: 28 [18432/50000]	Loss: 0.8975	LR: 0.050000
Training Epoch: 28 [18560/50000]	Loss: 1.0759	LR: 0.050000
Training Epoch: 28 [18688/50000]	Loss: 1.0445	LR: 0.050000
Training Epoch: 28 [18816/50000]	Loss: 1.1672	LR: 0.050000
Training Epoch: 28 [18944/50000]	Loss: 0.9795	LR: 0.050000
Training Epoch: 28 [19072/50000]	Loss: 1.0630	LR: 0.050000
Training Epoch: 28 [19200/50000]	Loss: 1.1092	LR: 0.050000
Training Epoch: 28 [19328/50000]	Loss: 0.9241	LR: 0.050000
Training Epoch: 28 [19456/50000]	Loss: 1.0968	LR: 0.050000
Training Epoch: 28 [19584/50000]	Loss: 1.0377	LR: 0.050000
Training Epoch: 28 [19712/50000]	Loss: 0.8479	LR: 0.050000
Training Epoch: 28 [19840/50000]	Loss: 1.1817	LR: 0.050000
Training Epoch: 28 [19968/50000]	Loss: 1.1802	LR: 0.050000
Training Epoch: 28 [20096/50000]	Loss: 1.1120	LR: 0.050000
Training Epoch: 28 [20224/50000]	Loss: 0.9556	LR: 0.050000
Training Epoch: 28 [20352/50000]	Loss: 0.9100	LR: 0.050000
Training Epoch: 28 [20480/50000]	Loss: 1.0181	LR: 0.050000
Training Epoch: 28 [20608/50000]	Loss: 1.1216	LR: 0.050000
Training Epoch: 28 [20736/50000]	Loss: 1.0595	LR: 0.050000
Training Epoch: 28 [20864/50000]	Loss: 0.9446	LR: 0.050000
Training Epoch: 28 [20992/50000]	Loss: 1.1536	LR: 0.050000
Training Epoch: 28 [21120/50000]	Loss: 1.2011	LR: 0.050000
Training Epoch: 28 [21248/50000]	Loss: 1.0539	LR: 0.050000
Training Epoch: 28 [21376/50000]	Loss: 1.1493	LR: 0.050000
Training Epoch: 28 [21504/50000]	Loss: 0.8528	LR: 0.050000
Training Epoch: 28 [21632/50000]	Loss: 0.9931	LR: 0.050000
Training Epoch: 28 [21760/50000]	Loss: 1.0044	LR: 0.050000
Training Epoch: 28 [21888/50000]	Loss: 1.1046	LR: 0.050000
Training Epoch: 28 [22016/50000]	Loss: 1.2675	LR: 0.050000
Training Epoch: 28 [22144/50000]	Loss: 1.1607	LR: 0.050000
Training Epoch: 28 [22272/50000]	Loss: 1.2314	LR: 0.050000
Training Epoch: 28 [22400/50000]	Loss: 0.8938	LR: 0.050000
Training Epoch: 28 [22528/50000]	Loss: 1.0467	LR: 0.050000
Training Epoch: 28 [22656/50000]	Loss: 1.0461	LR: 0.050000
Training Epoch: 28 [22784/50000]	Loss: 1.0575	LR: 0.050000
Training Epoch: 28 [22912/50000]	Loss: 0.8651	LR: 0.050000
Training Epoch: 28 [23040/50000]	Loss: 1.1866	LR: 0.050000
Training Epoch: 28 [23168/50000]	Loss: 0.8809	LR: 0.050000
Training Epoch: 28 [23296/50000]	Loss: 1.1347	LR: 0.050000
Training Epoch: 28 [23424/50000]	Loss: 0.9284	LR: 0.050000
Training Epoch: 28 [23552/50000]	Loss: 1.0687	LR: 0.050000
Training Epoch: 28 [23680/50000]	Loss: 1.0603	LR: 0.050000
Training Epoch: 28 [23808/50000]	Loss: 1.1339	LR: 0.050000
Training Epoch: 28 [23936/50000]	Loss: 0.9903	LR: 0.050000
Training Epoch: 28 [24064/50000]	Loss: 1.0882	LR: 0.050000
Training Epoch: 28 [24192/50000]	Loss: 0.9809	LR: 0.050000
Training Epoch: 28 [24320/50000]	Loss: 0.9663	LR: 0.050000
Training Epoch: 28 [24448/50000]	Loss: 0.9609	LR: 0.050000
Training Epoch: 28 [24576/50000]	Loss: 1.3264	LR: 0.050000
Training Epoch: 28 [24704/50000]	Loss: 1.1361	LR: 0.050000
Training Epoch: 28 [24832/50000]	Loss: 1.2197	LR: 0.050000
Training Epoch: 28 [24960/50000]	Loss: 0.9276	LR: 0.050000
Training Epoch: 28 [25088/50000]	Loss: 0.9762	LR: 0.050000
Training Epoch: 28 [25216/50000]	Loss: 1.0880	LR: 0.050000
Training Epoch: 28 [25344/50000]	Loss: 0.9254	LR: 0.050000
Training Epoch: 28 [25472/50000]	Loss: 1.0645	LR: 0.050000
Training Epoch: 28 [25600/50000]	Loss: 1.0537	LR: 0.050000
Training Epoch: 28 [25728/50000]	Loss: 1.2592	LR: 0.050000
Training Epoch: 28 [25856/50000]	Loss: 1.1739	LR: 0.050000
Training Epoch: 28 [25984/50000]	Loss: 1.0755	LR: 0.050000
Training Epoch: 28 [26112/50000]	Loss: 1.1506	LR: 0.050000
Training Epoch: 28 [26240/50000]	Loss: 1.2536	LR: 0.050000
Training Epoch: 28 [26368/50000]	Loss: 1.0908	LR: 0.050000
Training Epoch: 28 [26496/50000]	Loss: 1.2433	LR: 0.050000
Training Epoch: 28 [26624/50000]	Loss: 0.9197	LR: 0.050000
Training Epoch: 28 [26752/50000]	Loss: 0.9472	LR: 0.050000
Training Epoch: 28 [26880/50000]	Loss: 1.2785	LR: 0.050000
Training Epoch: 28 [27008/50000]	Loss: 1.1394	LR: 0.050000
Training Epoch: 28 [27136/50000]	Loss: 1.0856	LR: 0.050000
Training Epoch: 28 [27264/50000]	Loss: 1.1337	LR: 0.050000
Training Epoch: 28 [27392/50000]	Loss: 0.8304	LR: 0.050000
Training Epoch: 28 [27520/50000]	Loss: 1.2034	LR: 0.050000
Training Epoch: 28 [27648/50000]	Loss: 0.8183	LR: 0.050000
Training Epoch: 28 [27776/50000]	Loss: 1.0758	LR: 0.050000
Training Epoch: 28 [27904/50000]	Loss: 1.0240	LR: 0.050000
Training Epoch: 28 [28032/50000]	Loss: 1.3007	LR: 0.050000
Training Epoch: 28 [28160/50000]	Loss: 0.9185	LR: 0.050000
Training Epoch: 28 [28288/50000]	Loss: 1.3814	LR: 0.050000
Training Epoch: 28 [28416/50000]	Loss: 1.1938	LR: 0.050000
Training Epoch: 28 [28544/50000]	Loss: 1.0716	LR: 0.050000
Training Epoch: 28 [28672/50000]	Loss: 1.0502	LR: 0.050000
Training Epoch: 28 [28800/50000]	Loss: 1.1856	LR: 0.050000
Training Epoch: 28 [28928/50000]	Loss: 1.2663	LR: 0.050000
Training Epoch: 28 [29056/50000]	Loss: 1.0856	LR: 0.050000
Training Epoch: 28 [29184/50000]	Loss: 1.1372	LR: 0.050000
Training Epoch: 28 [29312/50000]	Loss: 1.0309	LR: 0.050000
Training Epoch: 28 [29440/50000]	Loss: 0.8777	LR: 0.050000
Training Epoch: 28 [29568/50000]	Loss: 1.0694	LR: 0.050000
Training Epoch: 28 [29696/50000]	Loss: 1.0930	LR: 0.050000
Training Epoch: 28 [29824/50000]	Loss: 1.1811	LR: 0.050000
Training Epoch: 28 [29952/50000]	Loss: 1.1995	LR: 0.050000
Training Epoch: 28 [30080/50000]	Loss: 0.8979	LR: 0.050000
Training Epoch: 28 [30208/50000]	Loss: 0.7982	LR: 0.050000
Training Epoch: 28 [30336/50000]	Loss: 0.9595	LR: 0.050000
Training Epoch: 28 [30464/50000]	Loss: 1.1062	LR: 0.050000
Training Epoch: 28 [30592/50000]	Loss: 1.2442	LR: 0.050000
Training Epoch: 28 [30720/50000]	Loss: 1.1702	LR: 0.050000
Training Epoch: 28 [30848/50000]	Loss: 1.1769	LR: 0.050000
Training Epoch: 28 [30976/50000]	Loss: 1.0196	LR: 0.050000
Training Epoch: 28 [31104/50000]	Loss: 0.9878	LR: 0.050000
Training Epoch: 28 [31232/50000]	Loss: 1.0842	LR: 0.050000
Training Epoch: 28 [31360/50000]	Loss: 1.0103	LR: 0.050000
Training Epoch: 28 [31488/50000]	Loss: 1.0256	LR: 0.050000
Training Epoch: 28 [31616/50000]	Loss: 1.0252	LR: 0.050000
Training Epoch: 28 [31744/50000]	Loss: 1.2062	LR: 0.050000
Training Epoch: 28 [31872/50000]	Loss: 1.0452	LR: 0.050000
Training Epoch: 28 [32000/50000]	Loss: 1.0328	LR: 0.050000
Training Epoch: 28 [32128/50000]	Loss: 1.0932	LR: 0.050000
Training Epoch: 28 [32256/50000]	Loss: 1.1660	LR: 0.050000
Training Epoch: 28 [32384/50000]	Loss: 1.1514	LR: 0.050000
Training Epoch: 28 [32512/50000]	Loss: 1.2197	LR: 0.050000
Training Epoch: 28 [32640/50000]	Loss: 1.0760	LR: 0.050000
Training Epoch: 28 [32768/50000]	Loss: 1.2348	LR: 0.050000
Training Epoch: 28 [32896/50000]	Loss: 1.1477	LR: 0.050000
Training Epoch: 28 [33024/50000]	Loss: 1.0358	LR: 0.050000
Training Epoch: 28 [33152/50000]	Loss: 1.0915	LR: 0.050000
Training Epoch: 28 [33280/50000]	Loss: 1.2527	LR: 0.050000
Training Epoch: 28 [33408/50000]	Loss: 0.9396	LR: 0.050000
Training Epoch: 28 [33536/50000]	Loss: 0.8232	LR: 0.050000
Training Epoch: 28 [33664/50000]	Loss: 1.1736	LR: 0.050000
Training Epoch: 28 [33792/50000]	Loss: 0.8308	LR: 0.050000
Training Epoch: 28 [33920/50000]	Loss: 1.1281	LR: 0.050000
Training Epoch: 28 [34048/50000]	Loss: 1.0473	LR: 0.050000
Training Epoch: 28 [34176/50000]	Loss: 1.1154	LR: 0.050000
Training Epoch: 28 [34304/50000]	Loss: 1.1359	LR: 0.050000
Training Epoch: 28 [34432/50000]	Loss: 1.0687	LR: 0.050000
Training Epoch: 28 [34560/50000]	Loss: 1.0690	LR: 0.050000
Training Epoch: 28 [34688/50000]	Loss: 1.1199	LR: 0.050000
Training Epoch: 28 [34816/50000]	Loss: 1.0397	LR: 0.050000
Training Epoch: 28 [34944/50000]	Loss: 1.1400	LR: 0.050000
Training Epoch: 28 [35072/50000]	Loss: 1.0281	LR: 0.050000
Training Epoch: 28 [35200/50000]	Loss: 0.8768	LR: 0.050000
Training Epoch: 28 [35328/50000]	Loss: 1.0364	LR: 0.050000
Training Epoch: 28 [35456/50000]	Loss: 1.1624	LR: 0.050000
Training Epoch: 28 [35584/50000]	Loss: 1.1905	LR: 0.050000
Training Epoch: 28 [35712/50000]	Loss: 0.9286	LR: 0.050000
Training Epoch: 28 [35840/50000]	Loss: 1.2149	LR: 0.050000
Training Epoch: 28 [35968/50000]	Loss: 0.9050	LR: 0.050000
Training Epoch: 28 [36096/50000]	Loss: 0.9246	LR: 0.050000
Training Epoch: 28 [36224/50000]	Loss: 0.9907	LR: 0.050000
Training Epoch: 28 [36352/50000]	Loss: 1.0045	LR: 0.050000
Training Epoch: 28 [36480/50000]	Loss: 1.2569	LR: 0.050000
Training Epoch: 28 [36608/50000]	Loss: 1.2805	LR: 0.050000
Training Epoch: 28 [36736/50000]	Loss: 1.3127	LR: 0.050000
Training Epoch: 28 [36864/50000]	Loss: 0.8832	LR: 0.050000
Training Epoch: 28 [36992/50000]	Loss: 1.0511	LR: 0.050000
Training Epoch: 28 [37120/50000]	Loss: 1.0518	LR: 0.050000
Training Epoch: 28 [37248/50000]	Loss: 1.0812	LR: 0.050000
Training Epoch: 28 [37376/50000]	Loss: 0.9836	LR: 0.050000
Training Epoch: 28 [37504/50000]	Loss: 1.2463	LR: 0.050000
Training Epoch: 28 [37632/50000]	Loss: 1.1424	LR: 0.050000
Training Epoch: 28 [37760/50000]	Loss: 1.0023	LR: 0.050000
Training Epoch: 28 [37888/50000]	Loss: 1.0899	LR: 0.050000
Training Epoch: 28 [38016/50000]	Loss: 1.1355	LR: 0.050000
Training Epoch: 28 [38144/50000]	Loss: 1.2046	LR: 0.050000
Training Epoch: 28 [38272/50000]	Loss: 0.9397	LR: 0.050000
Training Epoch: 28 [38400/50000]	Loss: 1.2189	LR: 0.050000
Training Epoch: 28 [38528/50000]	Loss: 1.2589	LR: 0.050000
Training Epoch: 28 [38656/50000]	Loss: 1.2035	LR: 0.050000
Training Epoch: 28 [38784/50000]	Loss: 1.0907	LR: 0.050000
Training Epoch: 28 [38912/50000]	Loss: 1.1014	LR: 0.050000
Training Epoch: 28 [39040/50000]	Loss: 1.0508	LR: 0.050000
Training Epoch: 28 [39168/50000]	Loss: 1.0261	LR: 0.050000
Training Epoch: 28 [39296/50000]	Loss: 1.2929	LR: 0.050000
Training Epoch: 28 [39424/50000]	Loss: 1.1143	LR: 0.050000
Training Epoch: 28 [39552/50000]	Loss: 1.1029	LR: 0.050000
Training Epoch: 28 [39680/50000]	Loss: 1.4444	LR: 0.050000
Training Epoch: 28 [39808/50000]	Loss: 1.0480	LR: 0.050000
Training Epoch: 28 [39936/50000]	Loss: 1.1421	LR: 0.050000
Training Epoch: 28 [40064/50000]	Loss: 1.2297	LR: 0.050000
Training Epoch: 28 [40192/50000]	Loss: 1.1495	LR: 0.050000
Training Epoch: 28 [40320/50000]	Loss: 1.0635	LR: 0.050000
Training Epoch: 28 [40448/50000]	Loss: 0.9924	LR: 0.050000
Training Epoch: 28 [40576/50000]	Loss: 1.3020	LR: 0.050000
Training Epoch: 28 [40704/50000]	Loss: 1.3689	LR: 0.050000
Training Epoch: 28 [40832/50000]	Loss: 1.3498	LR: 0.050000
Training Epoch: 28 [40960/50000]	Loss: 1.0583	LR: 0.050000
Training Epoch: 28 [41088/50000]	Loss: 0.9265	LR: 0.050000
Training Epoch: 28 [41216/50000]	Loss: 1.2040	LR: 0.050000
Training Epoch: 28 [41344/50000]	Loss: 1.1532	LR: 0.050000
Training Epoch: 28 [41472/50000]	Loss: 1.0284	LR: 0.050000
Training Epoch: 28 [41600/50000]	Loss: 0.9275	LR: 0.050000
Training Epoch: 28 [41728/50000]	Loss: 1.1481	LR: 0.050000
Training Epoch: 28 [41856/50000]	Loss: 1.1102	LR: 0.050000
Training Epoch: 28 [41984/50000]	Loss: 1.0538	LR: 0.050000
Training Epoch: 28 [42112/50000]	Loss: 1.2019	LR: 0.050000
Training Epoch: 28 [42240/50000]	Loss: 1.1636	LR: 0.050000
Training Epoch: 28 [42368/50000]	Loss: 1.2435	LR: 0.050000
Training Epoch: 28 [42496/50000]	Loss: 1.1911	LR: 0.050000
Training Epoch: 28 [42624/50000]	Loss: 0.9925	LR: 0.050000
Training Epoch: 28 [42752/50000]	Loss: 1.3927	LR: 0.050000
Training Epoch: 28 [42880/50000]	Loss: 1.0874	LR: 0.050000
Training Epoch: 28 [43008/50000]	Loss: 0.8345	LR: 0.050000
Training Epoch: 28 [43136/50000]	Loss: 1.2642	LR: 0.050000
Training Epoch: 28 [43264/50000]	Loss: 0.9638	LR: 0.050000
Training Epoch: 28 [43392/50000]	Loss: 1.1932	LR: 0.050000
Training Epoch: 28 [43520/50000]	Loss: 1.1020	LR: 0.050000
Training Epoch: 28 [43648/50000]	Loss: 1.0164	LR: 0.050000
Training Epoch: 28 [43776/50000]	Loss: 1.3462	LR: 0.050000
Training Epoch: 28 [43904/50000]	Loss: 1.1943	LR: 0.050000
Training Epoch: 28 [44032/50000]	Loss: 1.1104	LR: 0.050000
Training Epoch: 28 [44160/50000]	Loss: 1.2887	LR: 0.050000
Training Epoch: 28 [44288/50000]	Loss: 1.2003	LR: 0.050000
Training Epoch: 28 [44416/50000]	Loss: 1.1583	LR: 0.050000
Training Epoch: 28 [44544/50000]	Loss: 1.0833	LR: 0.050000
Training Epoch: 28 [44672/50000]	Loss: 1.0015	LR: 0.050000
Training Epoch: 28 [44800/50000]	Loss: 1.1512	LR: 0.050000
Training Epoch: 28 [44928/50000]	Loss: 1.1223	LR: 0.050000
Training Epoch: 28 [45056/50000]	Loss: 1.0154	LR: 0.050000
Training Epoch: 28 [45184/50000]	Loss: 1.1152	LR: 0.050000
Training Epoch: 28 [45312/50000]	Loss: 0.9394	LR: 0.050000
Training Epoch: 28 [45440/50000]	Loss: 1.0956	LR: 0.050000
Training Epoch: 28 [45568/50000]	Loss: 1.2294	LR: 0.050000
Training Epoch: 28 [45696/50000]	Loss: 1.0105	LR: 0.050000
Training Epoch: 28 [45824/50000]	Loss: 1.2798	LR: 0.050000
Training Epoch: 28 [45952/50000]	Loss: 0.9601	LR: 0.050000
Training Epoch: 28 [46080/50000]	Loss: 1.0395	LR: 0.050000
Training Epoch: 28 [46208/50000]	Loss: 1.1850	LR: 0.050000
Training Epoch: 28 [46336/50000]	Loss: 1.1785	LR: 0.050000
Training Epoch: 28 [46464/50000]	Loss: 1.1165	LR: 0.050000
Training Epoch: 28 [46592/50000]	Loss: 1.3788	LR: 0.050000
Training Epoch: 28 [46720/50000]	Loss: 1.0276	LR: 0.050000
Training Epoch: 28 [46848/50000]	Loss: 1.0005	LR: 0.050000
Training Epoch: 28 [46976/50000]	Loss: 0.9334	LR: 0.050000
Training Epoch: 28 [47104/50000]	Loss: 1.2457	LR: 0.050000
Training Epoch: 28 [47232/50000]	Loss: 1.4033	LR: 0.050000
Training Epoch: 28 [47360/50000]	Loss: 1.0971	LR: 0.050000
Training Epoch: 28 [47488/50000]	Loss: 1.1577	LR: 0.050000
Training Epoch: 28 [47616/50000]	Loss: 1.0532	LR: 0.050000
Training Epoch: 28 [47744/50000]	Loss: 1.0675	LR: 0.050000
Training Epoch: 28 [47872/50000]	Loss: 0.9807	LR: 0.050000
Training Epoch: 28 [48000/50000]	Loss: 1.1603	LR: 0.050000
Training Epoch: 28 [48128/50000]	Loss: 1.0512	LR: 0.050000
Training Epoch: 28 [48256/50000]	Loss: 1.0400	LR: 0.050000
Training Epoch: 28 [48384/50000]	Loss: 1.0956	LR: 0.050000
Training Epoch: 28 [48512/50000]	Loss: 0.9933	LR: 0.050000
Training Epoch: 28 [48640/50000]	Loss: 1.0318	LR: 0.050000
Training Epoch: 28 [48768/50000]	Loss: 1.0377	LR: 0.050000
Training Epoch: 28 [48896/50000]	Loss: 1.1757	LR: 0.050000
Training Epoch: 28 [49024/50000]	Loss: 1.1427	LR: 0.050000
Training Epoch: 28 [49152/50000]	Loss: 1.0825	LR: 0.050000
Training Epoch: 28 [49280/50000]	Loss: 1.1952	LR: 0.050000
Training Epoch: 28 [49408/50000]	Loss: 1.1400	LR: 0.050000
Training Epoch: 28 [49536/50000]	Loss: 1.0255	LR: 0.050000
Training Epoch: 28 [49664/50000]	Loss: 1.0033	LR: 0.050000
Training Epoch: 28 [49792/50000]	Loss: 1.1636	LR: 0.050000
Training Epoch: 28 [49920/50000]	Loss: 1.0236	LR: 0.050000
Training Epoch: 28 [50000/50000]	Loss: 1.0861	LR: 0.050000
epoch 28 training time consumed: 53.95s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  102762 GB |  102762 GB |
|       from large pool |  123392 KB |    1034 MB |  102660 GB |  102660 GB |
|       from small pool |   10798 KB |      13 MB |     101 GB |     101 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  102762 GB |  102762 GB |
|       from large pool |  123392 KB |    1034 MB |  102660 GB |  102660 GB |
|       from small pool |   10798 KB |      13 MB |     101 GB |     101 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   45223 GB |   45223 GB |
|       from large pool |  155136 KB |  433088 KB |   45111 GB |   45111 GB |
|       from small pool |    1490 KB |    3494 KB |     111 GB |     111 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    3965 K  |    3965 K  |
|       from large pool |      24    |      65    |    2069 K  |    2069 K  |
|       from small pool |     231    |     274    |    1895 K  |    1895 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    3965 K  |    3965 K  |
|       from large pool |      24    |      65    |    2069 K  |    2069 K  |
|       from small pool |     231    |     274    |    1895 K  |    1895 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    1961 K  |    1961 K  |
|       from large pool |       9    |      14    |    1001 K  |    1001 K  |
|       from small pool |      12    |      16    |     959 K  |     959 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 28, Average loss: 0.0113, Accuracy: 0.6179, Time consumed:3.46s

Training Epoch: 29 [128/50000]	Loss: 0.9785	LR: 0.050000
Training Epoch: 29 [256/50000]	Loss: 0.8304	LR: 0.050000
Training Epoch: 29 [384/50000]	Loss: 1.1538	LR: 0.050000
Training Epoch: 29 [512/50000]	Loss: 1.1499	LR: 0.050000
Training Epoch: 29 [640/50000]	Loss: 1.0538	LR: 0.050000
Training Epoch: 29 [768/50000]	Loss: 1.1044	LR: 0.050000
Training Epoch: 29 [896/50000]	Loss: 0.9840	LR: 0.050000
Training Epoch: 29 [1024/50000]	Loss: 0.8838	LR: 0.050000
Training Epoch: 29 [1152/50000]	Loss: 1.0397	LR: 0.050000
Training Epoch: 29 [1280/50000]	Loss: 0.9047	LR: 0.050000
Training Epoch: 29 [1408/50000]	Loss: 1.0240	LR: 0.050000
Training Epoch: 29 [1536/50000]	Loss: 1.0504	LR: 0.050000
Training Epoch: 29 [1664/50000]	Loss: 0.8697	LR: 0.050000
Training Epoch: 29 [1792/50000]	Loss: 1.2424	LR: 0.050000
Training Epoch: 29 [1920/50000]	Loss: 1.0769	LR: 0.050000
Training Epoch: 29 [2048/50000]	Loss: 1.0618	LR: 0.050000
Training Epoch: 29 [2176/50000]	Loss: 0.9684	LR: 0.050000
Training Epoch: 29 [2304/50000]	Loss: 0.9639	LR: 0.050000
Training Epoch: 29 [2432/50000]	Loss: 1.1165	LR: 0.050000
Training Epoch: 29 [2560/50000]	Loss: 0.9670	LR: 0.050000
Training Epoch: 29 [2688/50000]	Loss: 1.0061	LR: 0.050000
Training Epoch: 29 [2816/50000]	Loss: 0.9719	LR: 0.050000
Training Epoch: 29 [2944/50000]	Loss: 1.1182	LR: 0.050000
Training Epoch: 29 [3072/50000]	Loss: 0.9617	LR: 0.050000
Training Epoch: 29 [3200/50000]	Loss: 1.1530	LR: 0.050000
Training Epoch: 29 [3328/50000]	Loss: 0.9532	LR: 0.050000
Training Epoch: 29 [3456/50000]	Loss: 0.8562	LR: 0.050000
Training Epoch: 29 [3584/50000]	Loss: 1.0652	LR: 0.050000
Training Epoch: 29 [3712/50000]	Loss: 0.8693	LR: 0.050000
Training Epoch: 29 [3840/50000]	Loss: 1.0045	LR: 0.050000
Training Epoch: 29 [3968/50000]	Loss: 0.9472	LR: 0.050000
Training Epoch: 29 [4096/50000]	Loss: 0.9585	LR: 0.050000
Training Epoch: 29 [4224/50000]	Loss: 1.0849	LR: 0.050000
Training Epoch: 29 [4352/50000]	Loss: 1.1951	LR: 0.050000
Training Epoch: 29 [4480/50000]	Loss: 0.8142	LR: 0.050000
Training Epoch: 29 [4608/50000]	Loss: 0.9813	LR: 0.050000
Training Epoch: 29 [4736/50000]	Loss: 1.0293	LR: 0.050000
Training Epoch: 29 [4864/50000]	Loss: 1.0893	LR: 0.050000
Training Epoch: 29 [4992/50000]	Loss: 1.0653	LR: 0.050000
Training Epoch: 29 [5120/50000]	Loss: 0.8614	LR: 0.050000
Training Epoch: 29 [5248/50000]	Loss: 1.0121	LR: 0.050000
Training Epoch: 29 [5376/50000]	Loss: 1.3391	LR: 0.050000
Training Epoch: 29 [5504/50000]	Loss: 1.0020	LR: 0.050000
Training Epoch: 29 [5632/50000]	Loss: 1.0594	LR: 0.050000
Training Epoch: 29 [5760/50000]	Loss: 0.8795	LR: 0.050000
Training Epoch: 29 [5888/50000]	Loss: 0.6777	LR: 0.050000
Training Epoch: 29 [6016/50000]	Loss: 1.1009	LR: 0.050000
Training Epoch: 29 [6144/50000]	Loss: 1.1214	LR: 0.050000
Training Epoch: 29 [6272/50000]	Loss: 0.9159	LR: 0.050000
Training Epoch: 29 [6400/50000]	Loss: 1.0720	LR: 0.050000
Training Epoch: 29 [6528/50000]	Loss: 0.9943	LR: 0.050000
Training Epoch: 29 [6656/50000]	Loss: 0.9915	LR: 0.050000
Training Epoch: 29 [6784/50000]	Loss: 0.8056	LR: 0.050000
Training Epoch: 29 [6912/50000]	Loss: 0.8900	LR: 0.050000
Training Epoch: 29 [7040/50000]	Loss: 0.9970	LR: 0.050000
Training Epoch: 29 [7168/50000]	Loss: 1.2370	LR: 0.050000
Training Epoch: 29 [7296/50000]	Loss: 0.9313	LR: 0.050000
Training Epoch: 29 [7424/50000]	Loss: 1.0096	LR: 0.050000
Training Epoch: 29 [7552/50000]	Loss: 0.8891	LR: 0.050000
Training Epoch: 29 [7680/50000]	Loss: 1.0302	LR: 0.050000
Training Epoch: 29 [7808/50000]	Loss: 1.0326	LR: 0.050000
Training Epoch: 29 [7936/50000]	Loss: 0.8397	LR: 0.050000
Training Epoch: 29 [8064/50000]	Loss: 1.0711	LR: 0.050000
Training Epoch: 29 [8192/50000]	Loss: 1.0742	LR: 0.050000
Training Epoch: 29 [8320/50000]	Loss: 1.0710	LR: 0.050000
Training Epoch: 29 [8448/50000]	Loss: 1.2625	LR: 0.050000
Training Epoch: 29 [8576/50000]	Loss: 0.9974	LR: 0.050000
Training Epoch: 29 [8704/50000]	Loss: 0.9944	LR: 0.050000
Training Epoch: 29 [8832/50000]	Loss: 0.9170	LR: 0.050000
Training Epoch: 29 [8960/50000]	Loss: 1.0721	LR: 0.050000
Training Epoch: 29 [9088/50000]	Loss: 0.9252	LR: 0.050000
Training Epoch: 29 [9216/50000]	Loss: 1.2399	LR: 0.050000
Training Epoch: 29 [9344/50000]	Loss: 0.8584	LR: 0.050000
Training Epoch: 29 [9472/50000]	Loss: 0.9663	LR: 0.050000
Training Epoch: 29 [9600/50000]	Loss: 1.1281	LR: 0.050000
Training Epoch: 29 [9728/50000]	Loss: 0.9413	LR: 0.050000
Training Epoch: 29 [9856/50000]	Loss: 1.0759	LR: 0.050000
Training Epoch: 29 [9984/50000]	Loss: 1.0217	LR: 0.050000
Training Epoch: 29 [10112/50000]	Loss: 1.0716	LR: 0.050000
Training Epoch: 29 [10240/50000]	Loss: 0.9797	LR: 0.050000
Training Epoch: 29 [10368/50000]	Loss: 0.8174	LR: 0.050000
Training Epoch: 29 [10496/50000]	Loss: 0.7940	LR: 0.050000
Training Epoch: 29 [10624/50000]	Loss: 1.0574	LR: 0.050000
Training Epoch: 29 [10752/50000]	Loss: 0.8893	LR: 0.050000
Training Epoch: 29 [10880/50000]	Loss: 1.1474	LR: 0.050000
Training Epoch: 29 [11008/50000]	Loss: 1.0645	LR: 0.050000
Training Epoch: 29 [11136/50000]	Loss: 0.9139	LR: 0.050000
Training Epoch: 29 [11264/50000]	Loss: 0.6885	LR: 0.050000
Training Epoch: 29 [11392/50000]	Loss: 1.0647	LR: 0.050000
Training Epoch: 29 [11520/50000]	Loss: 1.1961	LR: 0.050000
Training Epoch: 29 [11648/50000]	Loss: 0.9333	LR: 0.050000
Training Epoch: 29 [11776/50000]	Loss: 0.9970	LR: 0.050000
Training Epoch: 29 [11904/50000]	Loss: 0.8531	LR: 0.050000
Training Epoch: 29 [12032/50000]	Loss: 1.2161	LR: 0.050000
Training Epoch: 29 [12160/50000]	Loss: 1.0435	LR: 0.050000
Training Epoch: 29 [12288/50000]	Loss: 0.8538	LR: 0.050000
Training Epoch: 29 [12416/50000]	Loss: 0.8962	LR: 0.050000
Training Epoch: 29 [12544/50000]	Loss: 0.9635	LR: 0.050000
Training Epoch: 29 [12672/50000]	Loss: 0.9775	LR: 0.050000
Training Epoch: 29 [12800/50000]	Loss: 0.9107	LR: 0.050000
Training Epoch: 29 [12928/50000]	Loss: 0.8190	LR: 0.050000
Training Epoch: 29 [13056/50000]	Loss: 1.0193	LR: 0.050000
Training Epoch: 29 [13184/50000]	Loss: 0.8523	LR: 0.050000
Training Epoch: 29 [13312/50000]	Loss: 0.9373	LR: 0.050000
Training Epoch: 29 [13440/50000]	Loss: 0.9086	LR: 0.050000
Training Epoch: 29 [13568/50000]	Loss: 1.0753	LR: 0.050000
Training Epoch: 29 [13696/50000]	Loss: 1.0574	LR: 0.050000
Training Epoch: 29 [13824/50000]	Loss: 1.0779	LR: 0.050000
Training Epoch: 29 [13952/50000]	Loss: 0.8710	LR: 0.050000
Training Epoch: 29 [14080/50000]	Loss: 0.9005	LR: 0.050000
Training Epoch: 29 [14208/50000]	Loss: 1.1766	LR: 0.050000
Training Epoch: 29 [14336/50000]	Loss: 1.0354	LR: 0.050000
Training Epoch: 29 [14464/50000]	Loss: 0.9652	LR: 0.050000
Training Epoch: 29 [14592/50000]	Loss: 1.0329	LR: 0.050000
Training Epoch: 29 [14720/50000]	Loss: 0.9527	LR: 0.050000
Training Epoch: 29 [14848/50000]	Loss: 0.9318	LR: 0.050000
Training Epoch: 29 [14976/50000]	Loss: 1.0668	LR: 0.050000
Training Epoch: 29 [15104/50000]	Loss: 0.8831	LR: 0.050000
Training Epoch: 29 [15232/50000]	Loss: 1.0310	LR: 0.050000
Training Epoch: 29 [15360/50000]	Loss: 0.6534	LR: 0.050000
Training Epoch: 29 [15488/50000]	Loss: 0.9643	LR: 0.050000
Training Epoch: 29 [15616/50000]	Loss: 1.0834	LR: 0.050000
Training Epoch: 29 [15744/50000]	Loss: 0.9696	LR: 0.050000
Training Epoch: 29 [15872/50000]	Loss: 1.3680	LR: 0.050000
Training Epoch: 29 [16000/50000]	Loss: 1.2266	LR: 0.050000
Training Epoch: 29 [16128/50000]	Loss: 1.0510	LR: 0.050000
Training Epoch: 29 [16256/50000]	Loss: 0.9056	LR: 0.050000
Training Epoch: 29 [16384/50000]	Loss: 0.8837	LR: 0.050000
Training Epoch: 29 [16512/50000]	Loss: 1.1727	LR: 0.050000
Training Epoch: 29 [16640/50000]	Loss: 0.8549	LR: 0.050000
Training Epoch: 29 [16768/50000]	Loss: 0.9149	LR: 0.050000
Training Epoch: 29 [16896/50000]	Loss: 1.0280	LR: 0.050000
Training Epoch: 29 [17024/50000]	Loss: 1.1089	LR: 0.050000
Training Epoch: 29 [17152/50000]	Loss: 1.1569	LR: 0.050000
Training Epoch: 29 [17280/50000]	Loss: 0.9655	LR: 0.050000
Training Epoch: 29 [17408/50000]	Loss: 0.9790	LR: 0.050000
Training Epoch: 29 [17536/50000]	Loss: 1.0409	LR: 0.050000
Training Epoch: 29 [17664/50000]	Loss: 1.1177	LR: 0.050000
Training Epoch: 29 [17792/50000]	Loss: 1.0153	LR: 0.050000
Training Epoch: 29 [17920/50000]	Loss: 1.1986	LR: 0.050000
Training Epoch: 29 [18048/50000]	Loss: 1.0007	LR: 0.050000
Training Epoch: 29 [18176/50000]	Loss: 1.0208	LR: 0.050000
Training Epoch: 29 [18304/50000]	Loss: 1.1846	LR: 0.050000
Training Epoch: 29 [18432/50000]	Loss: 1.0255	LR: 0.050000
Training Epoch: 29 [18560/50000]	Loss: 1.1207	LR: 0.050000
Training Epoch: 29 [18688/50000]	Loss: 0.9328	LR: 0.050000
Training Epoch: 29 [18816/50000]	Loss: 1.0290	LR: 0.050000
Training Epoch: 29 [18944/50000]	Loss: 1.1175	LR: 0.050000
Training Epoch: 29 [19072/50000]	Loss: 1.0027	LR: 0.050000
Training Epoch: 29 [19200/50000]	Loss: 1.2713	LR: 0.050000
Training Epoch: 29 [19328/50000]	Loss: 1.1462	LR: 0.050000
Training Epoch: 29 [19456/50000]	Loss: 1.1323	LR: 0.050000
Training Epoch: 29 [19584/50000]	Loss: 0.9032	LR: 0.050000
Training Epoch: 29 [19712/50000]	Loss: 0.9717	LR: 0.050000
Training Epoch: 29 [19840/50000]	Loss: 0.9676	LR: 0.050000
Training Epoch: 29 [19968/50000]	Loss: 1.1647	LR: 0.050000
Training Epoch: 29 [20096/50000]	Loss: 0.9518	LR: 0.050000
Training Epoch: 29 [20224/50000]	Loss: 0.8192	LR: 0.050000
Training Epoch: 29 [20352/50000]	Loss: 1.3238	LR: 0.050000
Training Epoch: 29 [20480/50000]	Loss: 1.0596	LR: 0.050000
Training Epoch: 29 [20608/50000]	Loss: 1.0008	LR: 0.050000
Training Epoch: 29 [20736/50000]	Loss: 1.0927	LR: 0.050000
Training Epoch: 29 [20864/50000]	Loss: 1.0846	LR: 0.050000
Training Epoch: 29 [20992/50000]	Loss: 1.1615	LR: 0.050000
Training Epoch: 29 [21120/50000]	Loss: 0.9563	LR: 0.050000
Training Epoch: 29 [21248/50000]	Loss: 1.0601	LR: 0.050000
Training Epoch: 29 [21376/50000]	Loss: 0.8870	LR: 0.050000
Training Epoch: 29 [21504/50000]	Loss: 1.1052	LR: 0.050000
Training Epoch: 29 [21632/50000]	Loss: 1.0161	LR: 0.050000
Training Epoch: 29 [21760/50000]	Loss: 0.8508	LR: 0.050000
Training Epoch: 29 [21888/50000]	Loss: 1.1259	LR: 0.050000
Training Epoch: 29 [22016/50000]	Loss: 1.2168	LR: 0.050000
Training Epoch: 29 [22144/50000]	Loss: 1.0322	LR: 0.050000
Training Epoch: 29 [22272/50000]	Loss: 1.0249	LR: 0.050000
Training Epoch: 29 [22400/50000]	Loss: 0.8807	LR: 0.050000
Training Epoch: 29 [22528/50000]	Loss: 1.1137	LR: 0.050000
Training Epoch: 29 [22656/50000]	Loss: 1.3471	LR: 0.050000
Training Epoch: 29 [22784/50000]	Loss: 1.0682	LR: 0.050000
Training Epoch: 29 [22912/50000]	Loss: 0.8297	LR: 0.050000
Training Epoch: 29 [23040/50000]	Loss: 1.1338	LR: 0.050000
Training Epoch: 29 [23168/50000]	Loss: 1.2470	LR: 0.050000
Training Epoch: 29 [23296/50000]	Loss: 0.9691	LR: 0.050000
Training Epoch: 29 [23424/50000]	Loss: 0.8619	LR: 0.050000
Training Epoch: 29 [23552/50000]	Loss: 1.2271	LR: 0.050000
Training Epoch: 29 [23680/50000]	Loss: 0.9492	LR: 0.050000
Training Epoch: 29 [23808/50000]	Loss: 1.3525	LR: 0.050000
Training Epoch: 29 [23936/50000]	Loss: 0.9641	LR: 0.050000
Training Epoch: 29 [24064/50000]	Loss: 1.0673	LR: 0.050000
Training Epoch: 29 [24192/50000]	Loss: 1.0131	LR: 0.050000
Training Epoch: 29 [24320/50000]	Loss: 1.2380	LR: 0.050000
Training Epoch: 29 [24448/50000]	Loss: 1.0687	LR: 0.050000
Training Epoch: 29 [24576/50000]	Loss: 1.0692	LR: 0.050000
Training Epoch: 29 [24704/50000]	Loss: 1.2429	LR: 0.050000
Training Epoch: 29 [24832/50000]	Loss: 0.9683	LR: 0.050000
Training Epoch: 29 [24960/50000]	Loss: 1.0411	LR: 0.050000
Training Epoch: 29 [25088/50000]	Loss: 1.2328	LR: 0.050000
Training Epoch: 29 [25216/50000]	Loss: 1.0148	LR: 0.050000
Training Epoch: 29 [25344/50000]	Loss: 0.9850	LR: 0.050000
Training Epoch: 29 [25472/50000]	Loss: 1.1087	LR: 0.050000
Training Epoch: 29 [25600/50000]	Loss: 0.9943	LR: 0.050000
Training Epoch: 29 [25728/50000]	Loss: 0.8323	LR: 0.050000
Training Epoch: 29 [25856/50000]	Loss: 1.1790	LR: 0.050000
Training Epoch: 29 [25984/50000]	Loss: 0.9063	LR: 0.050000
Training Epoch: 29 [26112/50000]	Loss: 1.1797	LR: 0.050000
Training Epoch: 29 [26240/50000]	Loss: 1.1712	LR: 0.050000
Training Epoch: 29 [26368/50000]	Loss: 0.8529	LR: 0.050000
Training Epoch: 29 [26496/50000]	Loss: 0.7929	LR: 0.050000
Training Epoch: 29 [26624/50000]	Loss: 0.9297	LR: 0.050000
Training Epoch: 29 [26752/50000]	Loss: 1.0518	LR: 0.050000
Training Epoch: 29 [26880/50000]	Loss: 0.9137	LR: 0.050000
Training Epoch: 29 [27008/50000]	Loss: 0.9662	LR: 0.050000
Training Epoch: 29 [27136/50000]	Loss: 0.9451	LR: 0.050000
Training Epoch: 29 [27264/50000]	Loss: 1.1102	LR: 0.050000
Training Epoch: 29 [27392/50000]	Loss: 1.0487	LR: 0.050000
Training Epoch: 29 [27520/50000]	Loss: 0.9639	LR: 0.050000
Training Epoch: 29 [27648/50000]	Loss: 1.2214	LR: 0.050000
Training Epoch: 29 [27776/50000]	Loss: 1.2603	LR: 0.050000
Training Epoch: 29 [27904/50000]	Loss: 1.3403	LR: 0.050000
Training Epoch: 29 [28032/50000]	Loss: 1.0898	LR: 0.050000
Training Epoch: 29 [28160/50000]	Loss: 1.2905	LR: 0.050000
Training Epoch: 29 [28288/50000]	Loss: 0.9665	LR: 0.050000
Training Epoch: 29 [28416/50000]	Loss: 1.3174	LR: 0.050000
Training Epoch: 29 [28544/50000]	Loss: 1.0491	LR: 0.050000
Training Epoch: 29 [28672/50000]	Loss: 1.2734	LR: 0.050000
Training Epoch: 29 [28800/50000]	Loss: 0.9682	LR: 0.050000
Training Epoch: 29 [28928/50000]	Loss: 1.1453	LR: 0.050000
Training Epoch: 29 [29056/50000]	Loss: 1.0856	LR: 0.050000
Training Epoch: 29 [29184/50000]	Loss: 1.0169	LR: 0.050000
Training Epoch: 29 [29312/50000]	Loss: 1.0553	LR: 0.050000
Training Epoch: 29 [29440/50000]	Loss: 0.8013	LR: 0.050000
Training Epoch: 29 [29568/50000]	Loss: 0.9510	LR: 0.050000
Training Epoch: 29 [29696/50000]	Loss: 1.2231	LR: 0.050000
Training Epoch: 29 [29824/50000]	Loss: 1.2359	LR: 0.050000
Training Epoch: 29 [29952/50000]	Loss: 1.3390	LR: 0.050000
Training Epoch: 29 [30080/50000]	Loss: 0.9428	LR: 0.050000
Training Epoch: 29 [30208/50000]	Loss: 0.9831	LR: 0.050000
Training Epoch: 29 [30336/50000]	Loss: 1.0806	LR: 0.050000
Training Epoch: 29 [30464/50000]	Loss: 1.2544	LR: 0.050000
Training Epoch: 29 [30592/50000]	Loss: 1.1550	LR: 0.050000
Training Epoch: 29 [30720/50000]	Loss: 1.2464	LR: 0.050000
Training Epoch: 29 [30848/50000]	Loss: 1.2847	LR: 0.050000
Training Epoch: 29 [30976/50000]	Loss: 0.9886	LR: 0.050000
Training Epoch: 29 [31104/50000]	Loss: 0.9144	LR: 0.050000
Training Epoch: 29 [31232/50000]	Loss: 1.1265	LR: 0.050000
Training Epoch: 29 [31360/50000]	Loss: 0.8792	LR: 0.050000
Training Epoch: 29 [31488/50000]	Loss: 1.0914	LR: 0.050000
Training Epoch: 29 [31616/50000]	Loss: 0.8417	LR: 0.050000
Training Epoch: 29 [31744/50000]	Loss: 1.1407	LR: 0.050000
Training Epoch: 29 [31872/50000]	Loss: 0.9949	LR: 0.050000
Training Epoch: 29 [32000/50000]	Loss: 1.0186	LR: 0.050000
Training Epoch: 29 [32128/50000]	Loss: 1.0952	LR: 0.050000
Training Epoch: 29 [32256/50000]	Loss: 0.9942	LR: 0.050000
Training Epoch: 29 [32384/50000]	Loss: 1.2174	LR: 0.050000
Training Epoch: 29 [32512/50000]	Loss: 1.2376	LR: 0.050000
Training Epoch: 29 [32640/50000]	Loss: 1.1593	LR: 0.050000
Training Epoch: 29 [32768/50000]	Loss: 1.1787	LR: 0.050000
Training Epoch: 29 [32896/50000]	Loss: 0.9439	LR: 0.050000
Training Epoch: 29 [33024/50000]	Loss: 1.0105	LR: 0.050000
Training Epoch: 29 [33152/50000]	Loss: 1.2123	LR: 0.050000
Training Epoch: 29 [33280/50000]	Loss: 0.8751	LR: 0.050000
Training Epoch: 29 [33408/50000]	Loss: 1.0079	LR: 0.050000
Training Epoch: 29 [33536/50000]	Loss: 1.1324	LR: 0.050000
Training Epoch: 29 [33664/50000]	Loss: 1.1317	LR: 0.050000
Training Epoch: 29 [33792/50000]	Loss: 1.0842	LR: 0.050000
Training Epoch: 29 [33920/50000]	Loss: 0.9957	LR: 0.050000
Training Epoch: 29 [34048/50000]	Loss: 0.8719	LR: 0.050000
Training Epoch: 29 [34176/50000]	Loss: 1.1076	LR: 0.050000
Training Epoch: 29 [34304/50000]	Loss: 1.0468	LR: 0.050000
Training Epoch: 29 [34432/50000]	Loss: 1.1000	LR: 0.050000
Training Epoch: 29 [34560/50000]	Loss: 0.9431	LR: 0.050000
Training Epoch: 29 [34688/50000]	Loss: 0.9392	LR: 0.050000
Training Epoch: 29 [34816/50000]	Loss: 0.9739	LR: 0.050000
Training Epoch: 29 [34944/50000]	Loss: 1.3249	LR: 0.050000
Training Epoch: 29 [35072/50000]	Loss: 0.9242	LR: 0.050000
Training Epoch: 29 [35200/50000]	Loss: 1.1628	LR: 0.050000
Training Epoch: 29 [35328/50000]	Loss: 1.0792	LR: 0.050000
Training Epoch: 29 [35456/50000]	Loss: 0.9501	LR: 0.050000
Training Epoch: 29 [35584/50000]	Loss: 1.2909	LR: 0.050000
Training Epoch: 29 [35712/50000]	Loss: 1.1187	LR: 0.050000
Training Epoch: 29 [35840/50000]	Loss: 0.9398	LR: 0.050000
Training Epoch: 29 [35968/50000]	Loss: 1.0244	LR: 0.050000
Training Epoch: 29 [36096/50000]	Loss: 1.1616	LR: 0.050000
Training Epoch: 29 [36224/50000]	Loss: 1.1008	LR: 0.050000
Training Epoch: 29 [36352/50000]	Loss: 1.0472	LR: 0.050000
Training Epoch: 29 [36480/50000]	Loss: 1.0894	LR: 0.050000
Training Epoch: 29 [36608/50000]	Loss: 0.9270	LR: 0.050000
Training Epoch: 29 [36736/50000]	Loss: 0.9952	LR: 0.050000
Training Epoch: 29 [36864/50000]	Loss: 0.9580	LR: 0.050000
Training Epoch: 29 [36992/50000]	Loss: 1.0817	LR: 0.050000
Training Epoch: 29 [37120/50000]	Loss: 1.2555	LR: 0.050000
Training Epoch: 29 [37248/50000]	Loss: 1.1679	LR: 0.050000
Training Epoch: 29 [37376/50000]	Loss: 0.9860	LR: 0.050000
Training Epoch: 29 [37504/50000]	Loss: 1.0445	LR: 0.050000
Training Epoch: 29 [37632/50000]	Loss: 1.0696	LR: 0.050000
Training Epoch: 29 [37760/50000]	Loss: 1.0575	LR: 0.050000
Training Epoch: 29 [37888/50000]	Loss: 1.0939	LR: 0.050000
Training Epoch: 29 [38016/50000]	Loss: 1.1601	LR: 0.050000
Training Epoch: 29 [38144/50000]	Loss: 1.1011	LR: 0.050000
Training Epoch: 29 [38272/50000]	Loss: 1.2649	LR: 0.050000
Training Epoch: 29 [38400/50000]	Loss: 1.2077	LR: 0.050000
Training Epoch: 29 [38528/50000]	Loss: 1.1214	LR: 0.050000
Training Epoch: 29 [38656/50000]	Loss: 1.0655	LR: 0.050000
Training Epoch: 29 [38784/50000]	Loss: 1.1604	LR: 0.050000
Training Epoch: 29 [38912/50000]	Loss: 1.0663	LR: 0.050000
Training Epoch: 29 [39040/50000]	Loss: 1.1503	LR: 0.050000
Training Epoch: 29 [39168/50000]	Loss: 1.0878	LR: 0.050000
Training Epoch: 29 [39296/50000]	Loss: 1.1615	LR: 0.050000
Training Epoch: 29 [39424/50000]	Loss: 1.1186	LR: 0.050000
Training Epoch: 29 [39552/50000]	Loss: 1.1504	LR: 0.050000
Training Epoch: 29 [39680/50000]	Loss: 1.1068	LR: 0.050000
Training Epoch: 29 [39808/50000]	Loss: 1.1711	LR: 0.050000
Training Epoch: 29 [39936/50000]	Loss: 1.0417	LR: 0.050000
Training Epoch: 29 [40064/50000]	Loss: 0.8936	LR: 0.050000
Training Epoch: 29 [40192/50000]	Loss: 1.2161	LR: 0.050000
Training Epoch: 29 [40320/50000]	Loss: 1.2656	LR: 0.050000
Training Epoch: 29 [40448/50000]	Loss: 0.9890	LR: 0.050000
Training Epoch: 29 [40576/50000]	Loss: 1.3441	LR: 0.050000
Training Epoch: 29 [40704/50000]	Loss: 1.0631	LR: 0.050000
Training Epoch: 29 [40832/50000]	Loss: 1.0276	LR: 0.050000
Training Epoch: 29 [40960/50000]	Loss: 1.1519	LR: 0.050000
Training Epoch: 29 [41088/50000]	Loss: 1.0149	LR: 0.050000
Training Epoch: 29 [41216/50000]	Loss: 0.9531	LR: 0.050000
Training Epoch: 29 [41344/50000]	Loss: 1.1573	LR: 0.050000
Training Epoch: 29 [41472/50000]	Loss: 1.2855	LR: 0.050000
Training Epoch: 29 [41600/50000]	Loss: 1.0907	LR: 0.050000
Training Epoch: 29 [41728/50000]	Loss: 1.2428	LR: 0.050000
Training Epoch: 29 [41856/50000]	Loss: 1.1533	LR: 0.050000
Training Epoch: 29 [41984/50000]	Loss: 0.9653	LR: 0.050000
Training Epoch: 29 [42112/50000]	Loss: 0.9857	LR: 0.050000
Training Epoch: 29 [42240/50000]	Loss: 0.9671	LR: 0.050000
Training Epoch: 29 [42368/50000]	Loss: 0.8886	LR: 0.050000
Training Epoch: 29 [42496/50000]	Loss: 1.2590	LR: 0.050000
Training Epoch: 29 [42624/50000]	Loss: 1.0124	LR: 0.050000
Training Epoch: 29 [42752/50000]	Loss: 1.0364	LR: 0.050000
Training Epoch: 29 [42880/50000]	Loss: 1.1385	LR: 0.050000
Training Epoch: 29 [43008/50000]	Loss: 1.4060	LR: 0.050000
Training Epoch: 29 [43136/50000]	Loss: 1.2183	LR: 0.050000
Training Epoch: 29 [43264/50000]	Loss: 1.1106	LR: 0.050000
Training Epoch: 29 [43392/50000]	Loss: 1.1323	LR: 0.050000
Training Epoch: 29 [43520/50000]	Loss: 1.0392	LR: 0.050000
Training Epoch: 29 [43648/50000]	Loss: 0.8936	LR: 0.050000
Training Epoch: 29 [43776/50000]	Loss: 1.0480	LR: 0.050000
Training Epoch: 29 [43904/50000]	Loss: 1.2358	LR: 0.050000
Training Epoch: 29 [44032/50000]	Loss: 1.3419	LR: 0.050000
Training Epoch: 29 [44160/50000]	Loss: 1.0744	LR: 0.050000
Training Epoch: 29 [44288/50000]	Loss: 1.1378	LR: 0.050000
Training Epoch: 29 [44416/50000]	Loss: 1.1095	LR: 0.050000
Training Epoch: 29 [44544/50000]	Loss: 1.2195	LR: 0.050000
Training Epoch: 29 [44672/50000]	Loss: 1.0324	LR: 0.050000
Training Epoch: 29 [44800/50000]	Loss: 1.1261	LR: 0.050000
Training Epoch: 29 [44928/50000]	Loss: 1.2068	LR: 0.050000
Training Epoch: 29 [45056/50000]	Loss: 1.0979	LR: 0.050000
Training Epoch: 29 [45184/50000]	Loss: 0.8217	LR: 0.050000
Training Epoch: 29 [45312/50000]	Loss: 1.0129	LR: 0.050000
Training Epoch: 29 [45440/50000]	Loss: 1.1487	LR: 0.050000
Training Epoch: 29 [45568/50000]	Loss: 0.8669	LR: 0.050000
Training Epoch: 29 [45696/50000]	Loss: 1.1677	LR: 0.050000
Training Epoch: 29 [45824/50000]	Loss: 1.0968	LR: 0.050000
Training Epoch: 29 [45952/50000]	Loss: 0.9687	LR: 0.050000
Training Epoch: 29 [46080/50000]	Loss: 1.2296	LR: 0.050000
Training Epoch: 29 [46208/50000]	Loss: 1.0034	LR: 0.050000
Training Epoch: 29 [46336/50000]	Loss: 1.0818	LR: 0.050000
Training Epoch: 29 [46464/50000]	Loss: 1.1243	LR: 0.050000
Training Epoch: 29 [46592/50000]	Loss: 1.1793	LR: 0.050000
Training Epoch: 29 [46720/50000]	Loss: 1.0783	LR: 0.050000
Training Epoch: 29 [46848/50000]	Loss: 0.8232	LR: 0.050000
Training Epoch: 29 [46976/50000]	Loss: 0.9813	LR: 0.050000
Training Epoch: 29 [47104/50000]	Loss: 1.1749	LR: 0.050000
Training Epoch: 29 [47232/50000]	Loss: 1.2224	LR: 0.050000
Training Epoch: 29 [47360/50000]	Loss: 1.2793	LR: 0.050000
Training Epoch: 29 [47488/50000]	Loss: 0.8478	LR: 0.050000
Training Epoch: 29 [47616/50000]	Loss: 1.1157	LR: 0.050000
Training Epoch: 29 [47744/50000]	Loss: 1.4976	LR: 0.050000
Training Epoch: 29 [47872/50000]	Loss: 0.8697	LR: 0.050000
Training Epoch: 29 [48000/50000]	Loss: 1.2187	LR: 0.050000
Training Epoch: 29 [48128/50000]	Loss: 1.0601	LR: 0.050000
Training Epoch: 29 [48256/50000]	Loss: 1.0677	LR: 0.050000
Training Epoch: 29 [48384/50000]	Loss: 1.0525	LR: 0.050000
Training Epoch: 29 [48512/50000]	Loss: 1.2508	LR: 0.050000
Training Epoch: 29 [48640/50000]	Loss: 0.9516	LR: 0.050000
Training Epoch: 29 [48768/50000]	Loss: 0.9849	LR: 0.050000
Training Epoch: 29 [48896/50000]	Loss: 0.9963	LR: 0.050000
Training Epoch: 29 [49024/50000]	Loss: 1.3849	LR: 0.050000
Training Epoch: 29 [49152/50000]	Loss: 0.9663	LR: 0.050000
Training Epoch: 29 [49280/50000]	Loss: 1.1721	LR: 0.050000
Training Epoch: 29 [49408/50000]	Loss: 1.0254	LR: 0.050000
Training Epoch: 29 [49536/50000]	Loss: 1.0214	LR: 0.050000
Training Epoch: 29 [49664/50000]	Loss: 0.9523	LR: 0.050000
Training Epoch: 29 [49792/50000]	Loss: 1.2446	LR: 0.050000
Training Epoch: 29 [49920/50000]	Loss: 0.8866	LR: 0.050000
Training Epoch: 29 [50000/50000]	Loss: 0.8059	LR: 0.050000
epoch 29 training time consumed: 53.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  106432 GB |  106432 GB |
|       from large pool |  123392 KB |    1034 MB |  106327 GB |  106327 GB |
|       from small pool |   10798 KB |      13 MB |     104 GB |     104 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  106432 GB |  106432 GB |
|       from large pool |  123392 KB |    1034 MB |  106327 GB |  106327 GB |
|       from small pool |   10798 KB |      13 MB |     104 GB |     104 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   46838 GB |   46838 GB |
|       from large pool |  155136 KB |  433088 KB |   46722 GB |   46722 GB |
|       from small pool |    1490 KB |    3494 KB |     115 GB |     115 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    4107 K  |    4106 K  |
|       from large pool |      24    |      65    |    2143 K  |    2143 K  |
|       from small pool |     231    |     274    |    1963 K  |    1963 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    4107 K  |    4106 K  |
|       from large pool |      24    |      65    |    2143 K  |    2143 K  |
|       from small pool |     231    |     274    |    1963 K  |    1963 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2031 K  |    2031 K  |
|       from large pool |       9    |      14    |    1037 K  |    1037 K  |
|       from small pool |      12    |      16    |     994 K  |     994 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 29, Average loss: 0.0130, Accuracy: 0.5738, Time consumed:3.45s

Training Epoch: 30 [128/50000]	Loss: 0.8191	LR: 0.050000
Training Epoch: 30 [256/50000]	Loss: 0.8963	LR: 0.050000
Training Epoch: 30 [384/50000]	Loss: 0.9697	LR: 0.050000
Training Epoch: 30 [512/50000]	Loss: 1.0292	LR: 0.050000
Training Epoch: 30 [640/50000]	Loss: 1.0732	LR: 0.050000
Training Epoch: 30 [768/50000]	Loss: 1.0133	LR: 0.050000
Training Epoch: 30 [896/50000]	Loss: 0.9102	LR: 0.050000
Training Epoch: 30 [1024/50000]	Loss: 0.9068	LR: 0.050000
Training Epoch: 30 [1152/50000]	Loss: 1.0251	LR: 0.050000
Training Epoch: 30 [1280/50000]	Loss: 1.1753	LR: 0.050000
Training Epoch: 30 [1408/50000]	Loss: 0.9339	LR: 0.050000
Training Epoch: 30 [1536/50000]	Loss: 0.9361	LR: 0.050000
Training Epoch: 30 [1664/50000]	Loss: 1.0789	LR: 0.050000
Training Epoch: 30 [1792/50000]	Loss: 1.0269	LR: 0.050000
Training Epoch: 30 [1920/50000]	Loss: 1.0072	LR: 0.050000
Training Epoch: 30 [2048/50000]	Loss: 1.0083	LR: 0.050000
Training Epoch: 30 [2176/50000]	Loss: 1.0020	LR: 0.050000
Training Epoch: 30 [2304/50000]	Loss: 0.8357	LR: 0.050000
Training Epoch: 30 [2432/50000]	Loss: 1.0598	LR: 0.050000
Training Epoch: 30 [2560/50000]	Loss: 1.0833	LR: 0.050000
Training Epoch: 30 [2688/50000]	Loss: 1.0220	LR: 0.050000
Training Epoch: 30 [2816/50000]	Loss: 0.9958	LR: 0.050000
Training Epoch: 30 [2944/50000]	Loss: 0.9030	LR: 0.050000
Training Epoch: 30 [3072/50000]	Loss: 1.1058	LR: 0.050000
Training Epoch: 30 [3200/50000]	Loss: 0.8912	LR: 0.050000
Training Epoch: 30 [3328/50000]	Loss: 0.9958	LR: 0.050000
Training Epoch: 30 [3456/50000]	Loss: 1.0303	LR: 0.050000
Training Epoch: 30 [3584/50000]	Loss: 0.8753	LR: 0.050000
Training Epoch: 30 [3712/50000]	Loss: 1.0220	LR: 0.050000
Training Epoch: 30 [3840/50000]	Loss: 1.1704	LR: 0.050000
Training Epoch: 30 [3968/50000]	Loss: 0.9129	LR: 0.050000
Training Epoch: 30 [4096/50000]	Loss: 0.9412	LR: 0.050000
Training Epoch: 30 [4224/50000]	Loss: 1.0614	LR: 0.050000
Training Epoch: 30 [4352/50000]	Loss: 0.9431	LR: 0.050000
Training Epoch: 30 [4480/50000]	Loss: 0.9546	LR: 0.050000
Training Epoch: 30 [4608/50000]	Loss: 1.1342	LR: 0.050000
Training Epoch: 30 [4736/50000]	Loss: 1.0865	LR: 0.050000
Training Epoch: 30 [4864/50000]	Loss: 0.9343	LR: 0.050000
Training Epoch: 30 [4992/50000]	Loss: 0.9167	LR: 0.050000
Training Epoch: 30 [5120/50000]	Loss: 1.0030	LR: 0.050000
Training Epoch: 30 [5248/50000]	Loss: 0.8597	LR: 0.050000
Training Epoch: 30 [5376/50000]	Loss: 0.9953	LR: 0.050000
Training Epoch: 30 [5504/50000]	Loss: 0.9121	LR: 0.050000
Training Epoch: 30 [5632/50000]	Loss: 1.0269	LR: 0.050000
Training Epoch: 30 [5760/50000]	Loss: 1.0924	LR: 0.050000
Training Epoch: 30 [5888/50000]	Loss: 0.9772	LR: 0.050000
Training Epoch: 30 [6016/50000]	Loss: 0.8771	LR: 0.050000
Training Epoch: 30 [6144/50000]	Loss: 0.9986	LR: 0.050000
Training Epoch: 30 [6272/50000]	Loss: 0.9124	LR: 0.050000
Training Epoch: 30 [6400/50000]	Loss: 0.8994	LR: 0.050000
Training Epoch: 30 [6528/50000]	Loss: 0.9975	LR: 0.050000
Training Epoch: 30 [6656/50000]	Loss: 1.1685	LR: 0.050000
Training Epoch: 30 [6784/50000]	Loss: 0.8836	LR: 0.050000
Training Epoch: 30 [6912/50000]	Loss: 1.1705	LR: 0.050000
Training Epoch: 30 [7040/50000]	Loss: 0.8421	LR: 0.050000
Training Epoch: 30 [7168/50000]	Loss: 0.9882	LR: 0.050000
Training Epoch: 30 [7296/50000]	Loss: 1.1313	LR: 0.050000
Training Epoch: 30 [7424/50000]	Loss: 1.0438	LR: 0.050000
Training Epoch: 30 [7552/50000]	Loss: 0.7342	LR: 0.050000
Training Epoch: 30 [7680/50000]	Loss: 1.0262	LR: 0.050000
Training Epoch: 30 [7808/50000]	Loss: 0.9603	LR: 0.050000
Training Epoch: 30 [7936/50000]	Loss: 1.0636	LR: 0.050000
Training Epoch: 30 [8064/50000]	Loss: 0.9671	LR: 0.050000
Training Epoch: 30 [8192/50000]	Loss: 0.9236	LR: 0.050000
Training Epoch: 30 [8320/50000]	Loss: 1.0971	LR: 0.050000
Training Epoch: 30 [8448/50000]	Loss: 1.0746	LR: 0.050000
Training Epoch: 30 [8576/50000]	Loss: 1.0595	LR: 0.050000
Training Epoch: 30 [8704/50000]	Loss: 1.2491	LR: 0.050000
Training Epoch: 30 [8832/50000]	Loss: 1.2392	LR: 0.050000
Training Epoch: 30 [8960/50000]	Loss: 0.9785	LR: 0.050000
Training Epoch: 30 [9088/50000]	Loss: 0.9833	LR: 0.050000
Training Epoch: 30 [9216/50000]	Loss: 0.9264	LR: 0.050000
Training Epoch: 30 [9344/50000]	Loss: 0.8876	LR: 0.050000
Training Epoch: 30 [9472/50000]	Loss: 0.8808	LR: 0.050000
Training Epoch: 30 [9600/50000]	Loss: 0.9280	LR: 0.050000
Training Epoch: 30 [9728/50000]	Loss: 0.9255	LR: 0.050000
Training Epoch: 30 [9856/50000]	Loss: 1.0478	LR: 0.050000
Training Epoch: 30 [9984/50000]	Loss: 0.9104	LR: 0.050000
Training Epoch: 30 [10112/50000]	Loss: 1.1245	LR: 0.050000
Training Epoch: 30 [10240/50000]	Loss: 0.9680	LR: 0.050000
Training Epoch: 30 [10368/50000]	Loss: 1.1629	LR: 0.050000
Training Epoch: 30 [10496/50000]	Loss: 0.9959	LR: 0.050000
Training Epoch: 30 [10624/50000]	Loss: 0.9292	LR: 0.050000
Training Epoch: 30 [10752/50000]	Loss: 1.1170	LR: 0.050000
Training Epoch: 30 [10880/50000]	Loss: 0.8767	LR: 0.050000
Training Epoch: 30 [11008/50000]	Loss: 1.1140	LR: 0.050000
Training Epoch: 30 [11136/50000]	Loss: 1.1271	LR: 0.050000
Training Epoch: 30 [11264/50000]	Loss: 0.7021	LR: 0.050000
Training Epoch: 30 [11392/50000]	Loss: 1.0226	LR: 0.050000
Training Epoch: 30 [11520/50000]	Loss: 0.8546	LR: 0.050000
Training Epoch: 30 [11648/50000]	Loss: 1.3127	LR: 0.050000
Training Epoch: 30 [11776/50000]	Loss: 0.7752	LR: 0.050000
Training Epoch: 30 [11904/50000]	Loss: 1.1024	LR: 0.050000
Training Epoch: 30 [12032/50000]	Loss: 0.9349	LR: 0.050000
Training Epoch: 30 [12160/50000]	Loss: 0.9585	LR: 0.050000
Training Epoch: 30 [12288/50000]	Loss: 1.0919	LR: 0.050000
Training Epoch: 30 [12416/50000]	Loss: 0.7911	LR: 0.050000
Training Epoch: 30 [12544/50000]	Loss: 0.9110	LR: 0.050000
Training Epoch: 30 [12672/50000]	Loss: 1.1133	LR: 0.050000
Training Epoch: 30 [12800/50000]	Loss: 0.7082	LR: 0.050000
Training Epoch: 30 [12928/50000]	Loss: 0.9623	LR: 0.050000
Training Epoch: 30 [13056/50000]	Loss: 0.9891	LR: 0.050000
Training Epoch: 30 [13184/50000]	Loss: 0.8128	LR: 0.050000
Training Epoch: 30 [13312/50000]	Loss: 1.0558	LR: 0.050000
Training Epoch: 30 [13440/50000]	Loss: 1.0139	LR: 0.050000
Training Epoch: 30 [13568/50000]	Loss: 1.0525	LR: 0.050000
Training Epoch: 30 [13696/50000]	Loss: 0.9053	LR: 0.050000
Training Epoch: 30 [13824/50000]	Loss: 0.9970	LR: 0.050000
Training Epoch: 30 [13952/50000]	Loss: 1.0141	LR: 0.050000
Training Epoch: 30 [14080/50000]	Loss: 0.8753	LR: 0.050000
Training Epoch: 30 [14208/50000]	Loss: 0.8900	LR: 0.050000
Training Epoch: 30 [14336/50000]	Loss: 0.8851	LR: 0.050000
Training Epoch: 30 [14464/50000]	Loss: 0.9842	LR: 0.050000
Training Epoch: 30 [14592/50000]	Loss: 0.9220	LR: 0.050000
Training Epoch: 30 [14720/50000]	Loss: 0.8545	LR: 0.050000
Training Epoch: 30 [14848/50000]	Loss: 0.9157	LR: 0.050000
Training Epoch: 30 [14976/50000]	Loss: 1.0504	LR: 0.050000
Training Epoch: 30 [15104/50000]	Loss: 0.9102	LR: 0.050000
Training Epoch: 30 [15232/50000]	Loss: 0.9789	LR: 0.050000
Training Epoch: 30 [15360/50000]	Loss: 0.9195	LR: 0.050000
Training Epoch: 30 [15488/50000]	Loss: 1.0864	LR: 0.050000
Training Epoch: 30 [15616/50000]	Loss: 1.0542	LR: 0.050000
Training Epoch: 30 [15744/50000]	Loss: 0.9855	LR: 0.050000
Training Epoch: 30 [15872/50000]	Loss: 1.0842	LR: 0.050000
Training Epoch: 30 [16000/50000]	Loss: 0.9361	LR: 0.050000
Training Epoch: 30 [16128/50000]	Loss: 1.0267	LR: 0.050000
Training Epoch: 30 [16256/50000]	Loss: 1.0618	LR: 0.050000
Training Epoch: 30 [16384/50000]	Loss: 0.6925	LR: 0.050000
Training Epoch: 30 [16512/50000]	Loss: 0.9265	LR: 0.050000
Training Epoch: 30 [16640/50000]	Loss: 0.9888	LR: 0.050000
Training Epoch: 30 [16768/50000]	Loss: 1.1654	LR: 0.050000
Training Epoch: 30 [16896/50000]	Loss: 0.8837	LR: 0.050000
Training Epoch: 30 [17024/50000]	Loss: 1.3885	LR: 0.050000
Training Epoch: 30 [17152/50000]	Loss: 1.0209	LR: 0.050000
Training Epoch: 30 [17280/50000]	Loss: 0.7015	LR: 0.050000
Training Epoch: 30 [17408/50000]	Loss: 0.9578	LR: 0.050000
Training Epoch: 30 [17536/50000]	Loss: 1.0118	LR: 0.050000
Training Epoch: 30 [17664/50000]	Loss: 0.9324	LR: 0.050000
Training Epoch: 30 [17792/50000]	Loss: 0.9252	LR: 0.050000
Training Epoch: 30 [17920/50000]	Loss: 1.3055	LR: 0.050000
Training Epoch: 30 [18048/50000]	Loss: 1.0893	LR: 0.050000
Training Epoch: 30 [18176/50000]	Loss: 0.8138	LR: 0.050000
Training Epoch: 30 [18304/50000]	Loss: 0.7900	LR: 0.050000
Training Epoch: 30 [18432/50000]	Loss: 1.1053	LR: 0.050000
Training Epoch: 30 [18560/50000]	Loss: 0.9496	LR: 0.050000
Training Epoch: 30 [18688/50000]	Loss: 0.9919	LR: 0.050000
Training Epoch: 30 [18816/50000]	Loss: 1.1198	LR: 0.050000
Training Epoch: 30 [18944/50000]	Loss: 0.9191	LR: 0.050000
Training Epoch: 30 [19072/50000]	Loss: 1.0538	LR: 0.050000
Training Epoch: 30 [19200/50000]	Loss: 1.1793	LR: 0.050000
Training Epoch: 30 [19328/50000]	Loss: 1.1066	LR: 0.050000
Training Epoch: 30 [19456/50000]	Loss: 1.3374	LR: 0.050000
Training Epoch: 30 [19584/50000]	Loss: 1.1982	LR: 0.050000
Training Epoch: 30 [19712/50000]	Loss: 0.8893	LR: 0.050000
Training Epoch: 30 [19840/50000]	Loss: 1.1357	LR: 0.050000
Training Epoch: 30 [19968/50000]	Loss: 0.8935	LR: 0.050000
Training Epoch: 30 [20096/50000]	Loss: 1.1475	LR: 0.050000
Training Epoch: 30 [20224/50000]	Loss: 1.1204	LR: 0.050000
Training Epoch: 30 [20352/50000]	Loss: 1.1264	LR: 0.050000
Training Epoch: 30 [20480/50000]	Loss: 1.1250	LR: 0.050000
Training Epoch: 30 [20608/50000]	Loss: 0.9569	LR: 0.050000
Training Epoch: 30 [20736/50000]	Loss: 1.1234	LR: 0.050000
Training Epoch: 30 [20864/50000]	Loss: 1.0236	LR: 0.050000
Training Epoch: 30 [20992/50000]	Loss: 1.0627	LR: 0.050000
Training Epoch: 30 [21120/50000]	Loss: 1.2127	LR: 0.050000
Training Epoch: 30 [21248/50000]	Loss: 1.0841	LR: 0.050000
Training Epoch: 30 [21376/50000]	Loss: 0.9187	LR: 0.050000
Training Epoch: 30 [21504/50000]	Loss: 1.1127	LR: 0.050000
Training Epoch: 30 [21632/50000]	Loss: 1.1290	LR: 0.050000
Training Epoch: 30 [21760/50000]	Loss: 0.9113	LR: 0.050000
Training Epoch: 30 [21888/50000]	Loss: 1.1105	LR: 0.050000
Training Epoch: 30 [22016/50000]	Loss: 0.7855	LR: 0.050000
Training Epoch: 30 [22144/50000]	Loss: 1.0316	LR: 0.050000
Training Epoch: 30 [22272/50000]	Loss: 1.0006	LR: 0.050000
Training Epoch: 30 [22400/50000]	Loss: 0.7186	LR: 0.050000
Training Epoch: 30 [22528/50000]	Loss: 1.2611	LR: 0.050000
Training Epoch: 30 [22656/50000]	Loss: 1.2071	LR: 0.050000
Training Epoch: 30 [22784/50000]	Loss: 1.0159	LR: 0.050000
Training Epoch: 30 [22912/50000]	Loss: 1.2008	LR: 0.050000
Training Epoch: 30 [23040/50000]	Loss: 1.0851	LR: 0.050000
Training Epoch: 30 [23168/50000]	Loss: 0.8640	LR: 0.050000
Training Epoch: 30 [23296/50000]	Loss: 1.0885	LR: 0.050000
Training Epoch: 30 [23424/50000]	Loss: 0.9713	LR: 0.050000
Training Epoch: 30 [23552/50000]	Loss: 1.0456	LR: 0.050000
Training Epoch: 30 [23680/50000]	Loss: 0.8887	LR: 0.050000
Training Epoch: 30 [23808/50000]	Loss: 0.9660	LR: 0.050000
Training Epoch: 30 [23936/50000]	Loss: 0.9772	LR: 0.050000
Training Epoch: 30 [24064/50000]	Loss: 1.1864	LR: 0.050000
Training Epoch: 30 [24192/50000]	Loss: 0.8452	LR: 0.050000
Training Epoch: 30 [24320/50000]	Loss: 1.1684	LR: 0.050000
Training Epoch: 30 [24448/50000]	Loss: 1.0969	LR: 0.050000
Training Epoch: 30 [24576/50000]	Loss: 1.1929	LR: 0.050000
Training Epoch: 30 [24704/50000]	Loss: 1.1021	LR: 0.050000
Training Epoch: 30 [24832/50000]	Loss: 0.9690	LR: 0.050000
Training Epoch: 30 [24960/50000]	Loss: 1.0310	LR: 0.050000
Training Epoch: 30 [25088/50000]	Loss: 1.0270	LR: 0.050000
Training Epoch: 30 [25216/50000]	Loss: 1.1650	LR: 0.050000
Training Epoch: 30 [25344/50000]	Loss: 1.0338	LR: 0.050000
Training Epoch: 30 [25472/50000]	Loss: 1.0145	LR: 0.050000
Training Epoch: 30 [25600/50000]	Loss: 0.9464	LR: 0.050000
Training Epoch: 30 [25728/50000]	Loss: 1.0507	LR: 0.050000
Training Epoch: 30 [25856/50000]	Loss: 0.9490	LR: 0.050000
Training Epoch: 30 [25984/50000]	Loss: 1.0876	LR: 0.050000
Training Epoch: 30 [26112/50000]	Loss: 1.0060	LR: 0.050000
Training Epoch: 30 [26240/50000]	Loss: 0.8992	LR: 0.050000
Training Epoch: 30 [26368/50000]	Loss: 1.0035	LR: 0.050000
Training Epoch: 30 [26496/50000]	Loss: 1.1720	LR: 0.050000
Training Epoch: 30 [26624/50000]	Loss: 0.9802	LR: 0.050000
Training Epoch: 30 [26752/50000]	Loss: 1.2245	LR: 0.050000
Training Epoch: 30 [26880/50000]	Loss: 0.7621	LR: 0.050000
Training Epoch: 30 [27008/50000]	Loss: 0.8685	LR: 0.050000
Training Epoch: 30 [27136/50000]	Loss: 0.8350	LR: 0.050000
Training Epoch: 30 [27264/50000]	Loss: 1.1004	LR: 0.050000
Training Epoch: 30 [27392/50000]	Loss: 1.2757	LR: 0.050000
Training Epoch: 30 [27520/50000]	Loss: 1.0759	LR: 0.050000
Training Epoch: 30 [27648/50000]	Loss: 1.0320	LR: 0.050000
Training Epoch: 30 [27776/50000]	Loss: 1.0155	LR: 0.050000
Training Epoch: 30 [27904/50000]	Loss: 0.9550	LR: 0.050000
Training Epoch: 30 [28032/50000]	Loss: 1.0073	LR: 0.050000
Training Epoch: 30 [28160/50000]	Loss: 1.1285	LR: 0.050000
Training Epoch: 30 [28288/50000]	Loss: 1.0517	LR: 0.050000
Training Epoch: 30 [28416/50000]	Loss: 1.0623	LR: 0.050000
Training Epoch: 30 [28544/50000]	Loss: 1.0382	LR: 0.050000
Training Epoch: 30 [28672/50000]	Loss: 1.0362	LR: 0.050000
Training Epoch: 30 [28800/50000]	Loss: 1.0055	LR: 0.050000
Training Epoch: 30 [28928/50000]	Loss: 1.1761	LR: 0.050000
Training Epoch: 30 [29056/50000]	Loss: 1.0438	LR: 0.050000
Training Epoch: 30 [29184/50000]	Loss: 1.1310	LR: 0.050000
Training Epoch: 30 [29312/50000]	Loss: 0.9251	LR: 0.050000
Training Epoch: 30 [29440/50000]	Loss: 1.0300	LR: 0.050000
Training Epoch: 30 [29568/50000]	Loss: 1.0451	LR: 0.050000
Training Epoch: 30 [29696/50000]	Loss: 1.1282	LR: 0.050000
Training Epoch: 30 [29824/50000]	Loss: 0.8017	LR: 0.050000
Training Epoch: 30 [29952/50000]	Loss: 1.0302	LR: 0.050000
Training Epoch: 30 [30080/50000]	Loss: 1.0262	LR: 0.050000
Training Epoch: 30 [30208/50000]	Loss: 1.0125	LR: 0.050000
Training Epoch: 30 [30336/50000]	Loss: 1.1858	LR: 0.050000
Training Epoch: 30 [30464/50000]	Loss: 1.2197	LR: 0.050000
Training Epoch: 30 [30592/50000]	Loss: 1.0799	LR: 0.050000
Training Epoch: 30 [30720/50000]	Loss: 1.0674	LR: 0.050000
Training Epoch: 30 [30848/50000]	Loss: 1.0140	LR: 0.050000
Training Epoch: 30 [30976/50000]	Loss: 0.9242	LR: 0.050000
Training Epoch: 30 [31104/50000]	Loss: 1.1298	LR: 0.050000
Training Epoch: 30 [31232/50000]	Loss: 0.9285	LR: 0.050000
Training Epoch: 30 [31360/50000]	Loss: 0.9619	LR: 0.050000
Training Epoch: 30 [31488/50000]	Loss: 1.2328	LR: 0.050000
Training Epoch: 30 [31616/50000]	Loss: 1.0724	LR: 0.050000
Training Epoch: 30 [31744/50000]	Loss: 1.1521	LR: 0.050000
Training Epoch: 30 [31872/50000]	Loss: 1.2306	LR: 0.050000
Training Epoch: 30 [32000/50000]	Loss: 1.1161	LR: 0.050000
Training Epoch: 30 [32128/50000]	Loss: 1.1744	LR: 0.050000
Training Epoch: 30 [32256/50000]	Loss: 1.1865	LR: 0.050000
Training Epoch: 30 [32384/50000]	Loss: 1.0596	LR: 0.050000
Training Epoch: 30 [32512/50000]	Loss: 1.3283	LR: 0.050000
Training Epoch: 30 [32640/50000]	Loss: 1.2542	LR: 0.050000
Training Epoch: 30 [32768/50000]	Loss: 1.0920	LR: 0.050000
Training Epoch: 30 [32896/50000]	Loss: 1.1924	LR: 0.050000
Training Epoch: 30 [33024/50000]	Loss: 1.0828	LR: 0.050000
Training Epoch: 30 [33152/50000]	Loss: 0.9866	LR: 0.050000
Training Epoch: 30 [33280/50000]	Loss: 0.9024	LR: 0.050000
Training Epoch: 30 [33408/50000]	Loss: 1.0728	LR: 0.050000
Training Epoch: 30 [33536/50000]	Loss: 1.1583	LR: 0.050000
Training Epoch: 30 [33664/50000]	Loss: 0.6797	LR: 0.050000
Training Epoch: 30 [33792/50000]	Loss: 1.0844	LR: 0.050000
Training Epoch: 30 [33920/50000]	Loss: 1.1903	LR: 0.050000
Training Epoch: 30 [34048/50000]	Loss: 1.0927	LR: 0.050000
Training Epoch: 30 [34176/50000]	Loss: 1.0175	LR: 0.050000
Training Epoch: 30 [34304/50000]	Loss: 0.9543	LR: 0.050000
Training Epoch: 30 [34432/50000]	Loss: 0.9732	LR: 0.050000
Training Epoch: 30 [34560/50000]	Loss: 0.9616	LR: 0.050000
Training Epoch: 30 [34688/50000]	Loss: 0.9878	LR: 0.050000
Training Epoch: 30 [34816/50000]	Loss: 1.2021	LR: 0.050000
Training Epoch: 30 [34944/50000]	Loss: 1.1071	LR: 0.050000
Training Epoch: 30 [35072/50000]	Loss: 0.8965	LR: 0.050000
Training Epoch: 30 [35200/50000]	Loss: 1.0497	LR: 0.050000
Training Epoch: 30 [35328/50000]	Loss: 0.9944	LR: 0.050000
Training Epoch: 30 [35456/50000]	Loss: 1.0900	LR: 0.050000
Training Epoch: 30 [35584/50000]	Loss: 1.0101	LR: 0.050000
Training Epoch: 30 [35712/50000]	Loss: 1.2005	LR: 0.050000
Training Epoch: 30 [35840/50000]	Loss: 1.1952	LR: 0.050000
Training Epoch: 30 [35968/50000]	Loss: 1.3004	LR: 0.050000
Training Epoch: 30 [36096/50000]	Loss: 1.2198	LR: 0.050000
Training Epoch: 30 [36224/50000]	Loss: 1.4642	LR: 0.050000
Training Epoch: 30 [36352/50000]	Loss: 1.1442	LR: 0.050000
Training Epoch: 30 [36480/50000]	Loss: 1.2500	LR: 0.050000
Training Epoch: 30 [36608/50000]	Loss: 1.0188	LR: 0.050000
Training Epoch: 30 [36736/50000]	Loss: 0.9302	LR: 0.050000
Training Epoch: 30 [36864/50000]	Loss: 1.2273	LR: 0.050000
Training Epoch: 30 [36992/50000]	Loss: 1.0293	LR: 0.050000
Training Epoch: 30 [37120/50000]	Loss: 1.1517	LR: 0.050000
Training Epoch: 30 [37248/50000]	Loss: 1.0510	LR: 0.050000
Training Epoch: 30 [37376/50000]	Loss: 0.8596	LR: 0.050000
Training Epoch: 30 [37504/50000]	Loss: 0.9466	LR: 0.050000
Training Epoch: 30 [37632/50000]	Loss: 1.2759	LR: 0.050000
Training Epoch: 30 [37760/50000]	Loss: 1.1309	LR: 0.050000
Training Epoch: 30 [37888/50000]	Loss: 1.2730	LR: 0.050000
Training Epoch: 30 [38016/50000]	Loss: 0.8771	LR: 0.050000
Training Epoch: 30 [38144/50000]	Loss: 1.3458	LR: 0.050000
Training Epoch: 30 [38272/50000]	Loss: 0.9182	LR: 0.050000
Training Epoch: 30 [38400/50000]	Loss: 0.8738	LR: 0.050000
Training Epoch: 30 [38528/50000]	Loss: 0.8965	LR: 0.050000
Training Epoch: 30 [38656/50000]	Loss: 0.9647	LR: 0.050000
Training Epoch: 30 [38784/50000]	Loss: 1.1622	LR: 0.050000
Training Epoch: 30 [38912/50000]	Loss: 1.1665	LR: 0.050000
Training Epoch: 30 [39040/50000]	Loss: 1.2864	LR: 0.050000
Training Epoch: 30 [39168/50000]	Loss: 1.1945	LR: 0.050000
Training Epoch: 30 [39296/50000]	Loss: 1.1430	LR: 0.050000
Training Epoch: 30 [39424/50000]	Loss: 0.9715	LR: 0.050000
Training Epoch: 30 [39552/50000]	Loss: 1.1073	LR: 0.050000
Training Epoch: 30 [39680/50000]	Loss: 1.0546	LR: 0.050000
Training Epoch: 30 [39808/50000]	Loss: 1.0068	LR: 0.050000
Training Epoch: 30 [39936/50000]	Loss: 1.0986	LR: 0.050000
Training Epoch: 30 [40064/50000]	Loss: 1.0947	LR: 0.050000
Training Epoch: 30 [40192/50000]	Loss: 0.9986	LR: 0.050000
Training Epoch: 30 [40320/50000]	Loss: 1.4031	LR: 0.050000
Training Epoch: 30 [40448/50000]	Loss: 0.9789	LR: 0.050000
Training Epoch: 30 [40576/50000]	Loss: 0.9529	LR: 0.050000
Training Epoch: 30 [40704/50000]	Loss: 1.1280	LR: 0.050000
Training Epoch: 30 [40832/50000]	Loss: 1.3794	LR: 0.050000
Training Epoch: 30 [40960/50000]	Loss: 1.1905	LR: 0.050000
Training Epoch: 30 [41088/50000]	Loss: 0.9433	LR: 0.050000
Training Epoch: 30 [41216/50000]	Loss: 1.2719	LR: 0.050000
Training Epoch: 30 [41344/50000]	Loss: 1.0116	LR: 0.050000
Training Epoch: 30 [41472/50000]	Loss: 0.9717	LR: 0.050000
Training Epoch: 30 [41600/50000]	Loss: 1.1589	LR: 0.050000
Training Epoch: 30 [41728/50000]	Loss: 0.9260	LR: 0.050000
Training Epoch: 30 [41856/50000]	Loss: 1.2679	LR: 0.050000
Training Epoch: 30 [41984/50000]	Loss: 1.1768	LR: 0.050000
Training Epoch: 30 [42112/50000]	Loss: 0.8761	LR: 0.050000
Training Epoch: 30 [42240/50000]	Loss: 1.0747	LR: 0.050000
Training Epoch: 30 [42368/50000]	Loss: 0.9271	LR: 0.050000
Training Epoch: 30 [42496/50000]	Loss: 1.1832	LR: 0.050000
Training Epoch: 30 [42624/50000]	Loss: 0.9493	LR: 0.050000
Training Epoch: 30 [42752/50000]	Loss: 1.0291	LR: 0.050000
Training Epoch: 30 [42880/50000]	Loss: 0.9343	LR: 0.050000
Training Epoch: 30 [43008/50000]	Loss: 1.0262	LR: 0.050000
Training Epoch: 30 [43136/50000]	Loss: 1.1108	LR: 0.050000
Training Epoch: 30 [43264/50000]	Loss: 1.3113	LR: 0.050000
Training Epoch: 30 [43392/50000]	Loss: 1.1164	LR: 0.050000
Training Epoch: 30 [43520/50000]	Loss: 1.0460	LR: 0.050000
Training Epoch: 30 [43648/50000]	Loss: 0.8989	LR: 0.050000
Training Epoch: 30 [43776/50000]	Loss: 0.9039	LR: 0.050000
Training Epoch: 30 [43904/50000]	Loss: 1.1846	LR: 0.050000
Training Epoch: 30 [44032/50000]	Loss: 1.0974	LR: 0.050000
Training Epoch: 30 [44160/50000]	Loss: 1.0598	LR: 0.050000
Training Epoch: 30 [44288/50000]	Loss: 1.2471	LR: 0.050000
Training Epoch: 30 [44416/50000]	Loss: 1.1081	LR: 0.050000
Training Epoch: 30 [44544/50000]	Loss: 1.0503	LR: 0.050000
Training Epoch: 30 [44672/50000]	Loss: 1.0576	LR: 0.050000
Training Epoch: 30 [44800/50000]	Loss: 1.1424	LR: 0.050000
Training Epoch: 30 [44928/50000]	Loss: 1.1155	LR: 0.050000
Training Epoch: 30 [45056/50000]	Loss: 1.0392	LR: 0.050000
Training Epoch: 30 [45184/50000]	Loss: 0.8860	LR: 0.050000
Training Epoch: 30 [45312/50000]	Loss: 1.0485	LR: 0.050000
Training Epoch: 30 [45440/50000]	Loss: 1.1772	LR: 0.050000
Training Epoch: 30 [45568/50000]	Loss: 0.9598	LR: 0.050000
Training Epoch: 30 [45696/50000]	Loss: 1.1888	LR: 0.050000
Training Epoch: 30 [45824/50000]	Loss: 1.0701	LR: 0.050000
Training Epoch: 30 [45952/50000]	Loss: 1.0994	LR: 0.050000
Training Epoch: 30 [46080/50000]	Loss: 1.0942	LR: 0.050000
Training Epoch: 30 [46208/50000]	Loss: 1.0570	LR: 0.050000
Training Epoch: 30 [46336/50000]	Loss: 1.0090	LR: 0.050000
Training Epoch: 30 [46464/50000]	Loss: 1.0670	LR: 0.050000
Training Epoch: 30 [46592/50000]	Loss: 1.1337	LR: 0.050000
Training Epoch: 30 [46720/50000]	Loss: 0.9906	LR: 0.050000
Training Epoch: 30 [46848/50000]	Loss: 1.0654	LR: 0.050000
Training Epoch: 30 [46976/50000]	Loss: 0.9132	LR: 0.050000
Training Epoch: 30 [47104/50000]	Loss: 1.1037	LR: 0.050000
Training Epoch: 30 [47232/50000]	Loss: 1.0934	LR: 0.050000
Training Epoch: 30 [47360/50000]	Loss: 1.3114	LR: 0.050000
Training Epoch: 30 [47488/50000]	Loss: 1.0498	LR: 0.050000
Training Epoch: 30 [47616/50000]	Loss: 1.1994	LR: 0.050000
Training Epoch: 30 [47744/50000]	Loss: 0.9369	LR: 0.050000
Training Epoch: 30 [47872/50000]	Loss: 0.9949	LR: 0.050000
Training Epoch: 30 [48000/50000]	Loss: 1.2853	LR: 0.050000
Training Epoch: 30 [48128/50000]	Loss: 1.2830	LR: 0.050000
Training Epoch: 30 [48256/50000]	Loss: 1.4052	LR: 0.050000
Training Epoch: 30 [48384/50000]	Loss: 1.2049	LR: 0.050000
Training Epoch: 30 [48512/50000]	Loss: 1.2102	LR: 0.050000
Training Epoch: 30 [48640/50000]	Loss: 0.9174	LR: 0.050000
Training Epoch: 30 [48768/50000]	Loss: 1.0240	LR: 0.050000
Training Epoch: 30 [48896/50000]	Loss: 0.8183	LR: 0.050000
Training Epoch: 30 [49024/50000]	Loss: 1.0366	LR: 0.050000
Training Epoch: 30 [49152/50000]	Loss: 1.0373	LR: 0.050000
Training Epoch: 30 [49280/50000]	Loss: 1.0664	LR: 0.050000
Training Epoch: 30 [49408/50000]	Loss: 1.0894	LR: 0.050000
Training Epoch: 30 [49536/50000]	Loss: 1.0892	LR: 0.050000
Training Epoch: 30 [49664/50000]	Loss: 1.2376	LR: 0.050000
Training Epoch: 30 [49792/50000]	Loss: 0.9420	LR: 0.050000
Training Epoch: 30 [49920/50000]	Loss: 1.1886	LR: 0.050000
Training Epoch: 30 [50000/50000]	Loss: 1.1527	LR: 0.050000
epoch 30 training time consumed: 53.89s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  110102 GB |  110102 GB |
|       from large pool |  123392 KB |    1034 MB |  109993 GB |  109993 GB |
|       from small pool |   10798 KB |      13 MB |     108 GB |     108 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  110102 GB |  110102 GB |
|       from large pool |  123392 KB |    1034 MB |  109993 GB |  109993 GB |
|       from small pool |   10798 KB |      13 MB |     108 GB |     108 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   48453 GB |   48453 GB |
|       from large pool |  155136 KB |  433088 KB |   48333 GB |   48333 GB |
|       from small pool |    1490 KB |    3494 KB |     119 GB |     119 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    4248 K  |    4248 K  |
|       from large pool |      24    |      65    |    2217 K  |    2217 K  |
|       from small pool |     231    |     274    |    2031 K  |    2030 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    4248 K  |    4248 K  |
|       from large pool |      24    |      65    |    2217 K  |    2217 K  |
|       from small pool |     231    |     274    |    2031 K  |    2030 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2101 K  |    2101 K  |
|       from large pool |       9    |      14    |    1073 K  |    1073 K  |
|       from small pool |      12    |      16    |    1028 K  |    1028 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 30, Average loss: 0.0122, Accuracy: 0.5938, Time consumed:3.47s

Training Epoch: 31 [128/50000]	Loss: 1.0708	LR: 0.050000
Training Epoch: 31 [256/50000]	Loss: 0.9884	LR: 0.050000
Training Epoch: 31 [384/50000]	Loss: 0.8478	LR: 0.050000
Training Epoch: 31 [512/50000]	Loss: 1.0543	LR: 0.050000
Training Epoch: 31 [640/50000]	Loss: 1.0514	LR: 0.050000
Training Epoch: 31 [768/50000]	Loss: 0.8933	LR: 0.050000
Training Epoch: 31 [896/50000]	Loss: 1.0417	LR: 0.050000
Training Epoch: 31 [1024/50000]	Loss: 0.9144	LR: 0.050000
Training Epoch: 31 [1152/50000]	Loss: 1.1176	LR: 0.050000
Training Epoch: 31 [1280/50000]	Loss: 0.9679	LR: 0.050000
Training Epoch: 31 [1408/50000]	Loss: 0.9405	LR: 0.050000
Training Epoch: 31 [1536/50000]	Loss: 1.0654	LR: 0.050000
Training Epoch: 31 [1664/50000]	Loss: 0.8603	LR: 0.050000
Training Epoch: 31 [1792/50000]	Loss: 0.9816	LR: 0.050000
Training Epoch: 31 [1920/50000]	Loss: 0.9865	LR: 0.050000
Training Epoch: 31 [2048/50000]	Loss: 0.9107	LR: 0.050000
Training Epoch: 31 [2176/50000]	Loss: 1.0822	LR: 0.050000
Training Epoch: 31 [2304/50000]	Loss: 1.0443	LR: 0.050000
Training Epoch: 31 [2432/50000]	Loss: 0.8952	LR: 0.050000
Training Epoch: 31 [2560/50000]	Loss: 1.0188	LR: 0.050000
Training Epoch: 31 [2688/50000]	Loss: 0.9786	LR: 0.050000
Training Epoch: 31 [2816/50000]	Loss: 1.0481	LR: 0.050000
Training Epoch: 31 [2944/50000]	Loss: 0.9962	LR: 0.050000
Training Epoch: 31 [3072/50000]	Loss: 1.1178	LR: 0.050000
Training Epoch: 31 [3200/50000]	Loss: 0.9049	LR: 0.050000
Training Epoch: 31 [3328/50000]	Loss: 0.9790	LR: 0.050000
Training Epoch: 31 [3456/50000]	Loss: 0.9792	LR: 0.050000
Training Epoch: 31 [3584/50000]	Loss: 0.9378	LR: 0.050000
Training Epoch: 31 [3712/50000]	Loss: 0.9255	LR: 0.050000
Training Epoch: 31 [3840/50000]	Loss: 1.0003	LR: 0.050000
Training Epoch: 31 [3968/50000]	Loss: 0.9614	LR: 0.050000
Training Epoch: 31 [4096/50000]	Loss: 1.0413	LR: 0.050000
Training Epoch: 31 [4224/50000]	Loss: 1.0695	LR: 0.050000
Training Epoch: 31 [4352/50000]	Loss: 1.0661	LR: 0.050000
Training Epoch: 31 [4480/50000]	Loss: 1.0362	LR: 0.050000
Training Epoch: 31 [4608/50000]	Loss: 0.8562	LR: 0.050000
Training Epoch: 31 [4736/50000]	Loss: 0.9218	LR: 0.050000
Training Epoch: 31 [4864/50000]	Loss: 0.7886	LR: 0.050000
Training Epoch: 31 [4992/50000]	Loss: 0.9758	LR: 0.050000
Training Epoch: 31 [5120/50000]	Loss: 1.0338	LR: 0.050000
Training Epoch: 31 [5248/50000]	Loss: 0.8800	LR: 0.050000
Training Epoch: 31 [5376/50000]	Loss: 0.9355	LR: 0.050000
Training Epoch: 31 [5504/50000]	Loss: 0.8298	LR: 0.050000
Training Epoch: 31 [5632/50000]	Loss: 1.0645	LR: 0.050000
Training Epoch: 31 [5760/50000]	Loss: 0.9065	LR: 0.050000
Training Epoch: 31 [5888/50000]	Loss: 1.0135	LR: 0.050000
Training Epoch: 31 [6016/50000]	Loss: 1.0321	LR: 0.050000
Training Epoch: 31 [6144/50000]	Loss: 0.7993	LR: 0.050000
Training Epoch: 31 [6272/50000]	Loss: 0.8623	LR: 0.050000
Training Epoch: 31 [6400/50000]	Loss: 0.9494	LR: 0.050000
Training Epoch: 31 [6528/50000]	Loss: 0.9548	LR: 0.050000
Training Epoch: 31 [6656/50000]	Loss: 0.7911	LR: 0.050000
Training Epoch: 31 [6784/50000]	Loss: 0.9593	LR: 0.050000
Training Epoch: 31 [6912/50000]	Loss: 1.0231	LR: 0.050000
Training Epoch: 31 [7040/50000]	Loss: 1.0840	LR: 0.050000
Training Epoch: 31 [7168/50000]	Loss: 0.9085	LR: 0.050000
Training Epoch: 31 [7296/50000]	Loss: 1.0961	LR: 0.050000
Training Epoch: 31 [7424/50000]	Loss: 0.9534	LR: 0.050000
Training Epoch: 31 [7552/50000]	Loss: 0.8384	LR: 0.050000
Training Epoch: 31 [7680/50000]	Loss: 0.8810	LR: 0.050000
Training Epoch: 31 [7808/50000]	Loss: 0.9140	LR: 0.050000
Training Epoch: 31 [7936/50000]	Loss: 1.1066	LR: 0.050000
Training Epoch: 31 [8064/50000]	Loss: 1.0263	LR: 0.050000
Training Epoch: 31 [8192/50000]	Loss: 0.9867	LR: 0.050000
Training Epoch: 31 [8320/50000]	Loss: 0.9831	LR: 0.050000
Training Epoch: 31 [8448/50000]	Loss: 0.8785	LR: 0.050000
Training Epoch: 31 [8576/50000]	Loss: 1.1793	LR: 0.050000
Training Epoch: 31 [8704/50000]	Loss: 0.8758	LR: 0.050000
Training Epoch: 31 [8832/50000]	Loss: 1.0061	LR: 0.050000
Training Epoch: 31 [8960/50000]	Loss: 1.0199	LR: 0.050000
Training Epoch: 31 [9088/50000]	Loss: 0.8019	LR: 0.050000
Training Epoch: 31 [9216/50000]	Loss: 1.0532	LR: 0.050000
Training Epoch: 31 [9344/50000]	Loss: 0.9428	LR: 0.050000
Training Epoch: 31 [9472/50000]	Loss: 0.8201	LR: 0.050000
Training Epoch: 31 [9600/50000]	Loss: 1.0841	LR: 0.050000
Training Epoch: 31 [9728/50000]	Loss: 0.7670	LR: 0.050000
Training Epoch: 31 [9856/50000]	Loss: 1.0770	LR: 0.050000
Training Epoch: 31 [9984/50000]	Loss: 1.0519	LR: 0.050000
Training Epoch: 31 [10112/50000]	Loss: 0.9914	LR: 0.050000
Training Epoch: 31 [10240/50000]	Loss: 1.0674	LR: 0.050000
Training Epoch: 31 [10368/50000]	Loss: 1.0834	LR: 0.050000
Training Epoch: 31 [10496/50000]	Loss: 1.0280	LR: 0.050000
Training Epoch: 31 [10624/50000]	Loss: 0.8530	LR: 0.050000
Training Epoch: 31 [10752/50000]	Loss: 1.0443	LR: 0.050000
Training Epoch: 31 [10880/50000]	Loss: 0.9177	LR: 0.050000
Training Epoch: 31 [11008/50000]	Loss: 0.8032	LR: 0.050000
Training Epoch: 31 [11136/50000]	Loss: 0.9385	LR: 0.050000
Training Epoch: 31 [11264/50000]	Loss: 0.9354	LR: 0.050000
Training Epoch: 31 [11392/50000]	Loss: 1.0570	LR: 0.050000
Training Epoch: 31 [11520/50000]	Loss: 0.8944	LR: 0.050000
Training Epoch: 31 [11648/50000]	Loss: 1.0995	LR: 0.050000
Training Epoch: 31 [11776/50000]	Loss: 0.7925	LR: 0.050000
Training Epoch: 31 [11904/50000]	Loss: 1.0451	LR: 0.050000
Training Epoch: 31 [12032/50000]	Loss: 0.9941	LR: 0.050000
Training Epoch: 31 [12160/50000]	Loss: 0.9810	LR: 0.050000
Training Epoch: 31 [12288/50000]	Loss: 0.8568	LR: 0.050000
Training Epoch: 31 [12416/50000]	Loss: 1.0552	LR: 0.050000
Training Epoch: 31 [12544/50000]	Loss: 0.8025	LR: 0.050000
Training Epoch: 31 [12672/50000]	Loss: 0.8907	LR: 0.050000
Training Epoch: 31 [12800/50000]	Loss: 1.2765	LR: 0.050000
Training Epoch: 31 [12928/50000]	Loss: 0.9570	LR: 0.050000
Training Epoch: 31 [13056/50000]	Loss: 1.1086	LR: 0.050000
Training Epoch: 31 [13184/50000]	Loss: 0.9385	LR: 0.050000
Training Epoch: 31 [13312/50000]	Loss: 1.0019	LR: 0.050000
Training Epoch: 31 [13440/50000]	Loss: 0.8583	LR: 0.050000
Training Epoch: 31 [13568/50000]	Loss: 1.0092	LR: 0.050000
Training Epoch: 31 [13696/50000]	Loss: 0.9781	LR: 0.050000
Training Epoch: 31 [13824/50000]	Loss: 0.6845	LR: 0.050000
Training Epoch: 31 [13952/50000]	Loss: 0.8372	LR: 0.050000
Training Epoch: 31 [14080/50000]	Loss: 1.0468	LR: 0.050000
Training Epoch: 31 [14208/50000]	Loss: 1.2350	LR: 0.050000
Training Epoch: 31 [14336/50000]	Loss: 0.8857	LR: 0.050000
Training Epoch: 31 [14464/50000]	Loss: 0.9628	LR: 0.050000
Training Epoch: 31 [14592/50000]	Loss: 1.2375	LR: 0.050000
Training Epoch: 31 [14720/50000]	Loss: 0.9029	LR: 0.050000
Training Epoch: 31 [14848/50000]	Loss: 0.9643	LR: 0.050000
Training Epoch: 31 [14976/50000]	Loss: 0.9877	LR: 0.050000
Training Epoch: 31 [15104/50000]	Loss: 1.0179	LR: 0.050000
Training Epoch: 31 [15232/50000]	Loss: 1.0362	LR: 0.050000
Training Epoch: 31 [15360/50000]	Loss: 0.7727	LR: 0.050000
Training Epoch: 31 [15488/50000]	Loss: 0.7965	LR: 0.050000
Training Epoch: 31 [15616/50000]	Loss: 0.9055	LR: 0.050000
Training Epoch: 31 [15744/50000]	Loss: 1.0030	LR: 0.050000
Training Epoch: 31 [15872/50000]	Loss: 1.1687	LR: 0.050000
Training Epoch: 31 [16000/50000]	Loss: 0.9218	LR: 0.050000
Training Epoch: 31 [16128/50000]	Loss: 0.9271	LR: 0.050000
Training Epoch: 31 [16256/50000]	Loss: 1.0601	LR: 0.050000
Training Epoch: 31 [16384/50000]	Loss: 0.8328	LR: 0.050000
Training Epoch: 31 [16512/50000]	Loss: 1.1188	LR: 0.050000
Training Epoch: 31 [16640/50000]	Loss: 1.0218	LR: 0.050000
Training Epoch: 31 [16768/50000]	Loss: 0.9920	LR: 0.050000
Training Epoch: 31 [16896/50000]	Loss: 1.0073	LR: 0.050000
Training Epoch: 31 [17024/50000]	Loss: 0.9543	LR: 0.050000
Training Epoch: 31 [17152/50000]	Loss: 0.9451	LR: 0.050000
Training Epoch: 31 [17280/50000]	Loss: 0.9156	LR: 0.050000
Training Epoch: 31 [17408/50000]	Loss: 0.8892	LR: 0.050000
Training Epoch: 31 [17536/50000]	Loss: 0.9122	LR: 0.050000
Training Epoch: 31 [17664/50000]	Loss: 1.1922	LR: 0.050000
Training Epoch: 31 [17792/50000]	Loss: 0.9231	LR: 0.050000
Training Epoch: 31 [17920/50000]	Loss: 0.9179	LR: 0.050000
Training Epoch: 31 [18048/50000]	Loss: 1.0606	LR: 0.050000
Training Epoch: 31 [18176/50000]	Loss: 1.0273	LR: 0.050000
Training Epoch: 31 [18304/50000]	Loss: 1.0288	LR: 0.050000
Training Epoch: 31 [18432/50000]	Loss: 0.9232	LR: 0.050000
Training Epoch: 31 [18560/50000]	Loss: 1.0795	LR: 0.050000
Training Epoch: 31 [18688/50000]	Loss: 1.0241	LR: 0.050000
Training Epoch: 31 [18816/50000]	Loss: 0.8365	LR: 0.050000
Training Epoch: 31 [18944/50000]	Loss: 1.0754	LR: 0.050000
Training Epoch: 31 [19072/50000]	Loss: 1.2904	LR: 0.050000
Training Epoch: 31 [19200/50000]	Loss: 0.8377	LR: 0.050000
Training Epoch: 31 [19328/50000]	Loss: 1.0894	LR: 0.050000
Training Epoch: 31 [19456/50000]	Loss: 1.2164	LR: 0.050000
Training Epoch: 31 [19584/50000]	Loss: 0.9585	LR: 0.050000
Training Epoch: 31 [19712/50000]	Loss: 0.8063	LR: 0.050000
Training Epoch: 31 [19840/50000]	Loss: 1.2584	LR: 0.050000
Training Epoch: 31 [19968/50000]	Loss: 0.9991	LR: 0.050000
Training Epoch: 31 [20096/50000]	Loss: 0.9207	LR: 0.050000
Training Epoch: 31 [20224/50000]	Loss: 1.0228	LR: 0.050000
Training Epoch: 31 [20352/50000]	Loss: 1.0153	LR: 0.050000
Training Epoch: 31 [20480/50000]	Loss: 0.8727	LR: 0.050000
Training Epoch: 31 [20608/50000]	Loss: 1.1398	LR: 0.050000
Training Epoch: 31 [20736/50000]	Loss: 1.1919	LR: 0.050000
Training Epoch: 31 [20864/50000]	Loss: 0.9242	LR: 0.050000
Training Epoch: 31 [20992/50000]	Loss: 1.1321	LR: 0.050000
Training Epoch: 31 [21120/50000]	Loss: 1.2539	LR: 0.050000
Training Epoch: 31 [21248/50000]	Loss: 1.0143	LR: 0.050000
Training Epoch: 31 [21376/50000]	Loss: 1.0189	LR: 0.050000
Training Epoch: 31 [21504/50000]	Loss: 1.0146	LR: 0.050000
Training Epoch: 31 [21632/50000]	Loss: 0.9896	LR: 0.050000
Training Epoch: 31 [21760/50000]	Loss: 0.8957	LR: 0.050000
Training Epoch: 31 [21888/50000]	Loss: 0.9129	LR: 0.050000
Training Epoch: 31 [22016/50000]	Loss: 0.8941	LR: 0.050000
Training Epoch: 31 [22144/50000]	Loss: 1.3334	LR: 0.050000
Training Epoch: 31 [22272/50000]	Loss: 1.0121	LR: 0.050000
Training Epoch: 31 [22400/50000]	Loss: 1.1238	LR: 0.050000
Training Epoch: 31 [22528/50000]	Loss: 0.8038	LR: 0.050000
Training Epoch: 31 [22656/50000]	Loss: 1.1445	LR: 0.050000
Training Epoch: 31 [22784/50000]	Loss: 0.8976	LR: 0.050000
Training Epoch: 31 [22912/50000]	Loss: 1.0085	LR: 0.050000
Training Epoch: 31 [23040/50000]	Loss: 0.8354	LR: 0.050000
Training Epoch: 31 [23168/50000]	Loss: 1.2148	LR: 0.050000
Training Epoch: 31 [23296/50000]	Loss: 0.8432	LR: 0.050000
Training Epoch: 31 [23424/50000]	Loss: 0.8791	LR: 0.050000
Training Epoch: 31 [23552/50000]	Loss: 1.1090	LR: 0.050000
Training Epoch: 31 [23680/50000]	Loss: 1.1710	LR: 0.050000
Training Epoch: 31 [23808/50000]	Loss: 0.8026	LR: 0.050000
Training Epoch: 31 [23936/50000]	Loss: 1.0613	LR: 0.050000
Training Epoch: 31 [24064/50000]	Loss: 1.2062	LR: 0.050000
Training Epoch: 31 [24192/50000]	Loss: 1.1136	LR: 0.050000
Training Epoch: 31 [24320/50000]	Loss: 0.9357	LR: 0.050000
Training Epoch: 31 [24448/50000]	Loss: 1.1332	LR: 0.050000
Training Epoch: 31 [24576/50000]	Loss: 0.9764	LR: 0.050000
Training Epoch: 31 [24704/50000]	Loss: 0.9033	LR: 0.050000
Training Epoch: 31 [24832/50000]	Loss: 1.1852	LR: 0.050000
Training Epoch: 31 [24960/50000]	Loss: 1.0487	LR: 0.050000
Training Epoch: 31 [25088/50000]	Loss: 1.1821	LR: 0.050000
Training Epoch: 31 [25216/50000]	Loss: 0.9804	LR: 0.050000
Training Epoch: 31 [25344/50000]	Loss: 0.9653	LR: 0.050000
Training Epoch: 31 [25472/50000]	Loss: 1.2528	LR: 0.050000
Training Epoch: 31 [25600/50000]	Loss: 1.0806	LR: 0.050000
Training Epoch: 31 [25728/50000]	Loss: 1.1098	LR: 0.050000
Training Epoch: 31 [25856/50000]	Loss: 1.2239	LR: 0.050000
Training Epoch: 31 [25984/50000]	Loss: 0.9871	LR: 0.050000
Training Epoch: 31 [26112/50000]	Loss: 1.1282	LR: 0.050000
Training Epoch: 31 [26240/50000]	Loss: 1.1063	LR: 0.050000
Training Epoch: 31 [26368/50000]	Loss: 0.9750	LR: 0.050000
Training Epoch: 31 [26496/50000]	Loss: 1.0107	LR: 0.050000
Training Epoch: 31 [26624/50000]	Loss: 1.0880	LR: 0.050000
Training Epoch: 31 [26752/50000]	Loss: 1.1793	LR: 0.050000
Training Epoch: 31 [26880/50000]	Loss: 0.9583	LR: 0.050000
Training Epoch: 31 [27008/50000]	Loss: 1.0432	LR: 0.050000
Training Epoch: 31 [27136/50000]	Loss: 1.0359	LR: 0.050000
Training Epoch: 31 [27264/50000]	Loss: 1.0864	LR: 0.050000
Training Epoch: 31 [27392/50000]	Loss: 0.9663	LR: 0.050000
Training Epoch: 31 [27520/50000]	Loss: 1.0791	LR: 0.050000
Training Epoch: 31 [27648/50000]	Loss: 1.1414	LR: 0.050000
Training Epoch: 31 [27776/50000]	Loss: 1.0519	LR: 0.050000
Training Epoch: 31 [27904/50000]	Loss: 1.0569	LR: 0.050000
Training Epoch: 31 [28032/50000]	Loss: 0.9653	LR: 0.050000
Training Epoch: 31 [28160/50000]	Loss: 1.1090	LR: 0.050000
Training Epoch: 31 [28288/50000]	Loss: 1.1161	LR: 0.050000
Training Epoch: 31 [28416/50000]	Loss: 1.1747	LR: 0.050000
Training Epoch: 31 [28544/50000]	Loss: 1.1017	LR: 0.050000
Training Epoch: 31 [28672/50000]	Loss: 1.0093	LR: 0.050000
Training Epoch: 31 [28800/50000]	Loss: 1.3474	LR: 0.050000
Training Epoch: 31 [28928/50000]	Loss: 1.1034	LR: 0.050000
Training Epoch: 31 [29056/50000]	Loss: 0.9160	LR: 0.050000
Training Epoch: 31 [29184/50000]	Loss: 1.1425	LR: 0.050000
Training Epoch: 31 [29312/50000]	Loss: 1.0073	LR: 0.050000
Training Epoch: 31 [29440/50000]	Loss: 0.9823	LR: 0.050000
Training Epoch: 31 [29568/50000]	Loss: 1.0961	LR: 0.050000
Training Epoch: 31 [29696/50000]	Loss: 1.1236	LR: 0.050000
Training Epoch: 31 [29824/50000]	Loss: 1.0472	LR: 0.050000
Training Epoch: 31 [29952/50000]	Loss: 1.1515	LR: 0.050000
Training Epoch: 31 [30080/50000]	Loss: 0.9721	LR: 0.050000
Training Epoch: 31 [30208/50000]	Loss: 0.9741	LR: 0.050000
Training Epoch: 31 [30336/50000]	Loss: 1.0706	LR: 0.050000
Training Epoch: 31 [30464/50000]	Loss: 0.8714	LR: 0.050000
Training Epoch: 31 [30592/50000]	Loss: 1.0384	LR: 0.050000
Training Epoch: 31 [30720/50000]	Loss: 1.0475	LR: 0.050000
Training Epoch: 31 [30848/50000]	Loss: 0.9760	LR: 0.050000
Training Epoch: 31 [30976/50000]	Loss: 1.0834	LR: 0.050000
Training Epoch: 31 [31104/50000]	Loss: 0.9528	LR: 0.050000
Training Epoch: 31 [31232/50000]	Loss: 0.9248	LR: 0.050000
Training Epoch: 31 [31360/50000]	Loss: 1.0628	LR: 0.050000
Training Epoch: 31 [31488/50000]	Loss: 0.8526	LR: 0.050000
Training Epoch: 31 [31616/50000]	Loss: 1.1000	LR: 0.050000
Training Epoch: 31 [31744/50000]	Loss: 1.1238	LR: 0.050000
Training Epoch: 31 [31872/50000]	Loss: 1.0863	LR: 0.050000
Training Epoch: 31 [32000/50000]	Loss: 1.0891	LR: 0.050000
Training Epoch: 31 [32128/50000]	Loss: 1.0543	LR: 0.050000
Training Epoch: 31 [32256/50000]	Loss: 1.0765	LR: 0.050000
Training Epoch: 31 [32384/50000]	Loss: 0.9310	LR: 0.050000
Training Epoch: 31 [32512/50000]	Loss: 1.0394	LR: 0.050000
Training Epoch: 31 [32640/50000]	Loss: 1.1185	LR: 0.050000
Training Epoch: 31 [32768/50000]	Loss: 1.0285	LR: 0.050000
Training Epoch: 31 [32896/50000]	Loss: 0.9199	LR: 0.050000
Training Epoch: 31 [33024/50000]	Loss: 1.1256	LR: 0.050000
Training Epoch: 31 [33152/50000]	Loss: 0.9659	LR: 0.050000
Training Epoch: 31 [33280/50000]	Loss: 1.0595	LR: 0.050000
Training Epoch: 31 [33408/50000]	Loss: 1.1655	LR: 0.050000
Training Epoch: 31 [33536/50000]	Loss: 1.1467	LR: 0.050000
Training Epoch: 31 [33664/50000]	Loss: 1.1108	LR: 0.050000
Training Epoch: 31 [33792/50000]	Loss: 1.0667	LR: 0.050000
Training Epoch: 31 [33920/50000]	Loss: 0.9970	LR: 0.050000
Training Epoch: 31 [34048/50000]	Loss: 1.0523	LR: 0.050000
Training Epoch: 31 [34176/50000]	Loss: 0.9625	LR: 0.050000
Training Epoch: 31 [34304/50000]	Loss: 0.9803	LR: 0.050000
Training Epoch: 31 [34432/50000]	Loss: 0.9020	LR: 0.050000
Training Epoch: 31 [34560/50000]	Loss: 1.1527	LR: 0.050000
Training Epoch: 31 [34688/50000]	Loss: 0.7740	LR: 0.050000
Training Epoch: 31 [34816/50000]	Loss: 0.8752	LR: 0.050000
Training Epoch: 31 [34944/50000]	Loss: 0.9814	LR: 0.050000
Training Epoch: 31 [35072/50000]	Loss: 1.2840	LR: 0.050000
Training Epoch: 31 [35200/50000]	Loss: 1.0797	LR: 0.050000
Training Epoch: 31 [35328/50000]	Loss: 1.0591	LR: 0.050000
Training Epoch: 31 [35456/50000]	Loss: 0.9977	LR: 0.050000
Training Epoch: 31 [35584/50000]	Loss: 1.0804	LR: 0.050000
Training Epoch: 31 [35712/50000]	Loss: 1.0352	LR: 0.050000
Training Epoch: 31 [35840/50000]	Loss: 0.9963	LR: 0.050000
Training Epoch: 31 [35968/50000]	Loss: 1.3785	LR: 0.050000
Training Epoch: 31 [36096/50000]	Loss: 0.9655	LR: 0.050000
Training Epoch: 31 [36224/50000]	Loss: 0.8875	LR: 0.050000
Training Epoch: 31 [36352/50000]	Loss: 0.9222	LR: 0.050000
Training Epoch: 31 [36480/50000]	Loss: 0.9533	LR: 0.050000
Training Epoch: 31 [36608/50000]	Loss: 1.1816	LR: 0.050000
Training Epoch: 31 [36736/50000]	Loss: 1.1751	LR: 0.050000
Training Epoch: 31 [36864/50000]	Loss: 0.9954	LR: 0.050000
Training Epoch: 31 [36992/50000]	Loss: 1.1436	LR: 0.050000
Training Epoch: 31 [37120/50000]	Loss: 1.0543	LR: 0.050000
Training Epoch: 31 [37248/50000]	Loss: 1.3246	LR: 0.050000
Training Epoch: 31 [37376/50000]	Loss: 1.0088	LR: 0.050000
Training Epoch: 31 [37504/50000]	Loss: 1.0852	LR: 0.050000
Training Epoch: 31 [37632/50000]	Loss: 1.1127	LR: 0.050000
Training Epoch: 31 [37760/50000]	Loss: 1.1135	LR: 0.050000
Training Epoch: 31 [37888/50000]	Loss: 1.2528	LR: 0.050000
Training Epoch: 31 [38016/50000]	Loss: 1.0481	LR: 0.050000
Training Epoch: 31 [38144/50000]	Loss: 0.8971	LR: 0.050000
Training Epoch: 31 [38272/50000]	Loss: 1.0924	LR: 0.050000
Training Epoch: 31 [38400/50000]	Loss: 1.0835	LR: 0.050000
Training Epoch: 31 [38528/50000]	Loss: 1.4134	LR: 0.050000
Training Epoch: 31 [38656/50000]	Loss: 1.0320	LR: 0.050000
Training Epoch: 31 [38784/50000]	Loss: 0.9627	LR: 0.050000
Training Epoch: 31 [38912/50000]	Loss: 1.1790	LR: 0.050000
Training Epoch: 31 [39040/50000]	Loss: 1.2280	LR: 0.050000
Training Epoch: 31 [39168/50000]	Loss: 1.3046	LR: 0.050000
Training Epoch: 31 [39296/50000]	Loss: 0.9150	LR: 0.050000
Training Epoch: 31 [39424/50000]	Loss: 1.3191	LR: 0.050000
Training Epoch: 31 [39552/50000]	Loss: 1.0019	LR: 0.050000
Training Epoch: 31 [39680/50000]	Loss: 1.0065	LR: 0.050000
Training Epoch: 31 [39808/50000]	Loss: 1.0936	LR: 0.050000
Training Epoch: 31 [39936/50000]	Loss: 1.0380	LR: 0.050000
Training Epoch: 31 [40064/50000]	Loss: 1.1530	LR: 0.050000
Training Epoch: 31 [40192/50000]	Loss: 0.9760	LR: 0.050000
Training Epoch: 31 [40320/50000]	Loss: 1.0142	LR: 0.050000
Training Epoch: 31 [40448/50000]	Loss: 1.0848	LR: 0.050000
Training Epoch: 31 [40576/50000]	Loss: 1.0655	LR: 0.050000
Training Epoch: 31 [40704/50000]	Loss: 1.0899	LR: 0.050000
Training Epoch: 31 [40832/50000]	Loss: 0.8560	LR: 0.050000
Training Epoch: 31 [40960/50000]	Loss: 1.2258	LR: 0.050000
Training Epoch: 31 [41088/50000]	Loss: 1.2147	LR: 0.050000
Training Epoch: 31 [41216/50000]	Loss: 1.1840	LR: 0.050000
Training Epoch: 31 [41344/50000]	Loss: 0.9326	LR: 0.050000
Training Epoch: 31 [41472/50000]	Loss: 1.1857	LR: 0.050000
Training Epoch: 31 [41600/50000]	Loss: 0.9546	LR: 0.050000
Training Epoch: 31 [41728/50000]	Loss: 1.3567	LR: 0.050000
Training Epoch: 31 [41856/50000]	Loss: 1.0264	LR: 0.050000
Training Epoch: 31 [41984/50000]	Loss: 1.0963	LR: 0.050000
Training Epoch: 31 [42112/50000]	Loss: 1.0510	LR: 0.050000
Training Epoch: 31 [42240/50000]	Loss: 1.1387	LR: 0.050000
Training Epoch: 31 [42368/50000]	Loss: 1.0312	LR: 0.050000
Training Epoch: 31 [42496/50000]	Loss: 1.1770	LR: 0.050000
Training Epoch: 31 [42624/50000]	Loss: 1.0460	LR: 0.050000
Training Epoch: 31 [42752/50000]	Loss: 0.9833	LR: 0.050000
Training Epoch: 31 [42880/50000]	Loss: 1.2721	LR: 0.050000
Training Epoch: 31 [43008/50000]	Loss: 1.1946	LR: 0.050000
Training Epoch: 31 [43136/50000]	Loss: 1.0443	LR: 0.050000
Training Epoch: 31 [43264/50000]	Loss: 0.9335	LR: 0.050000
Training Epoch: 31 [43392/50000]	Loss: 1.1910	LR: 0.050000
Training Epoch: 31 [43520/50000]	Loss: 1.1649	LR: 0.050000
Training Epoch: 31 [43648/50000]	Loss: 1.1569	LR: 0.050000
Training Epoch: 31 [43776/50000]	Loss: 0.9988	LR: 0.050000
Training Epoch: 31 [43904/50000]	Loss: 0.9726	LR: 0.050000
Training Epoch: 31 [44032/50000]	Loss: 0.9764	LR: 0.050000
Training Epoch: 31 [44160/50000]	Loss: 1.1848	LR: 0.050000
Training Epoch: 31 [44288/50000]	Loss: 0.9838	LR: 0.050000
Training Epoch: 31 [44416/50000]	Loss: 1.0939	LR: 0.050000
Training Epoch: 31 [44544/50000]	Loss: 0.9770	LR: 0.050000
Training Epoch: 31 [44672/50000]	Loss: 1.0453	LR: 0.050000
Training Epoch: 31 [44800/50000]	Loss: 1.1102	LR: 0.050000
Training Epoch: 31 [44928/50000]	Loss: 1.3178	LR: 0.050000
Training Epoch: 31 [45056/50000]	Loss: 1.0170	LR: 0.050000
Training Epoch: 31 [45184/50000]	Loss: 1.2953	LR: 0.050000
Training Epoch: 31 [45312/50000]	Loss: 1.0819	LR: 0.050000
Training Epoch: 31 [45440/50000]	Loss: 1.1446	LR: 0.050000
Training Epoch: 31 [45568/50000]	Loss: 0.9199	LR: 0.050000
Training Epoch: 31 [45696/50000]	Loss: 1.0409	LR: 0.050000
Training Epoch: 31 [45824/50000]	Loss: 1.2301	LR: 0.050000
Training Epoch: 31 [45952/50000]	Loss: 1.1810	LR: 0.050000
Training Epoch: 31 [46080/50000]	Loss: 1.2700	LR: 0.050000
Training Epoch: 31 [46208/50000]	Loss: 1.1255	LR: 0.050000
Training Epoch: 31 [46336/50000]	Loss: 1.1593	LR: 0.050000
Training Epoch: 31 [46464/50000]	Loss: 1.1097	LR: 0.050000
Training Epoch: 31 [46592/50000]	Loss: 1.0830	LR: 0.050000
Training Epoch: 31 [46720/50000]	Loss: 0.9817	LR: 0.050000
Training Epoch: 31 [46848/50000]	Loss: 0.9554	LR: 0.050000
Training Epoch: 31 [46976/50000]	Loss: 0.9400	LR: 0.050000
Training Epoch: 31 [47104/50000]	Loss: 1.1736	LR: 0.050000
Training Epoch: 31 [47232/50000]	Loss: 1.1643	LR: 0.050000
Training Epoch: 31 [47360/50000]	Loss: 1.0335	LR: 0.050000
Training Epoch: 31 [47488/50000]	Loss: 1.1547	LR: 0.050000
Training Epoch: 31 [47616/50000]	Loss: 1.1753	LR: 0.050000
Training Epoch: 31 [47744/50000]	Loss: 1.4050	LR: 0.050000
Training Epoch: 31 [47872/50000]	Loss: 1.0893	LR: 0.050000
Training Epoch: 31 [48000/50000]	Loss: 1.1444	LR: 0.050000
Training Epoch: 31 [48128/50000]	Loss: 1.1512	LR: 0.050000
Training Epoch: 31 [48256/50000]	Loss: 1.1599	LR: 0.050000
Training Epoch: 31 [48384/50000]	Loss: 1.1373	LR: 0.050000
Training Epoch: 31 [48512/50000]	Loss: 1.0637	LR: 0.050000
Training Epoch: 31 [48640/50000]	Loss: 1.1135	LR: 0.050000
Training Epoch: 31 [48768/50000]	Loss: 1.0255	LR: 0.050000
Training Epoch: 31 [48896/50000]	Loss: 1.1556	LR: 0.050000
Training Epoch: 31 [49024/50000]	Loss: 1.1055	LR: 0.050000
Training Epoch: 31 [49152/50000]	Loss: 1.1189	LR: 0.050000
Training Epoch: 31 [49280/50000]	Loss: 1.1134	LR: 0.050000
Training Epoch: 31 [49408/50000]	Loss: 1.1663	LR: 0.050000
Training Epoch: 31 [49536/50000]	Loss: 1.1342	LR: 0.050000
Training Epoch: 31 [49664/50000]	Loss: 0.9955	LR: 0.050000
Training Epoch: 31 [49792/50000]	Loss: 1.0834	LR: 0.050000
Training Epoch: 31 [49920/50000]	Loss: 0.9724	LR: 0.050000
Training Epoch: 31 [50000/50000]	Loss: 1.0408	LR: 0.050000
epoch 31 training time consumed: 54.02s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  113772 GB |  113772 GB |
|       from large pool |  123392 KB |    1034 MB |  113660 GB |  113660 GB |
|       from small pool |   10798 KB |      13 MB |     112 GB |     112 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  113772 GB |  113772 GB |
|       from large pool |  123392 KB |    1034 MB |  113660 GB |  113660 GB |
|       from small pool |   10798 KB |      13 MB |     112 GB |     112 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   50068 GB |   50068 GB |
|       from large pool |  155136 KB |  433088 KB |   49944 GB |   49944 GB |
|       from small pool |    1490 KB |    3494 KB |     123 GB |     123 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    4390 K  |    4390 K  |
|       from large pool |      24    |      65    |    2291 K  |    2291 K  |
|       from small pool |     231    |     274    |    2098 K  |    2098 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    4390 K  |    4390 K  |
|       from large pool |      24    |      65    |    2291 K  |    2291 K  |
|       from small pool |     231    |     274    |    2098 K  |    2098 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2171 K  |    2171 K  |
|       from large pool |       9    |      14    |    1109 K  |    1109 K  |
|       from small pool |      12    |      16    |    1062 K  |    1062 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 31, Average loss: 0.0121, Accuracy: 0.5914, Time consumed:3.47s

Training Epoch: 32 [128/50000]	Loss: 0.8845	LR: 0.050000
Training Epoch: 32 [256/50000]	Loss: 1.1345	LR: 0.050000
Training Epoch: 32 [384/50000]	Loss: 0.9475	LR: 0.050000
Training Epoch: 32 [512/50000]	Loss: 1.0423	LR: 0.050000
Training Epoch: 32 [640/50000]	Loss: 0.8685	LR: 0.050000
Training Epoch: 32 [768/50000]	Loss: 1.0244	LR: 0.050000
Training Epoch: 32 [896/50000]	Loss: 1.1983	LR: 0.050000
Training Epoch: 32 [1024/50000]	Loss: 1.0973	LR: 0.050000
Training Epoch: 32 [1152/50000]	Loss: 1.0139	LR: 0.050000
Training Epoch: 32 [1280/50000]	Loss: 1.0701	LR: 0.050000
Training Epoch: 32 [1408/50000]	Loss: 0.8845	LR: 0.050000
Training Epoch: 32 [1536/50000]	Loss: 0.8327	LR: 0.050000
Training Epoch: 32 [1664/50000]	Loss: 1.0769	LR: 0.050000
Training Epoch: 32 [1792/50000]	Loss: 0.9077	LR: 0.050000
Training Epoch: 32 [1920/50000]	Loss: 0.9653	LR: 0.050000
Training Epoch: 32 [2048/50000]	Loss: 0.9480	LR: 0.050000
Training Epoch: 32 [2176/50000]	Loss: 0.7319	LR: 0.050000
Training Epoch: 32 [2304/50000]	Loss: 0.7737	LR: 0.050000
Training Epoch: 32 [2432/50000]	Loss: 0.8301	LR: 0.050000
Training Epoch: 32 [2560/50000]	Loss: 0.8246	LR: 0.050000
Training Epoch: 32 [2688/50000]	Loss: 0.9846	LR: 0.050000
Training Epoch: 32 [2816/50000]	Loss: 0.8585	LR: 0.050000
Training Epoch: 32 [2944/50000]	Loss: 0.9334	LR: 0.050000
Training Epoch: 32 [3072/50000]	Loss: 1.0307	LR: 0.050000
Training Epoch: 32 [3200/50000]	Loss: 0.9960	LR: 0.050000
Training Epoch: 32 [3328/50000]	Loss: 1.0437	LR: 0.050000
Training Epoch: 32 [3456/50000]	Loss: 0.8718	LR: 0.050000
Training Epoch: 32 [3584/50000]	Loss: 1.0480	LR: 0.050000
Training Epoch: 32 [3712/50000]	Loss: 0.9114	LR: 0.050000
Training Epoch: 32 [3840/50000]	Loss: 0.9050	LR: 0.050000
Training Epoch: 32 [3968/50000]	Loss: 0.9648	LR: 0.050000
Training Epoch: 32 [4096/50000]	Loss: 0.8273	LR: 0.050000
Training Epoch: 32 [4224/50000]	Loss: 1.0022	LR: 0.050000
Training Epoch: 32 [4352/50000]	Loss: 1.0224	LR: 0.050000
Training Epoch: 32 [4480/50000]	Loss: 0.9026	LR: 0.050000
Training Epoch: 32 [4608/50000]	Loss: 0.9356	LR: 0.050000
Training Epoch: 32 [4736/50000]	Loss: 0.9630	LR: 0.050000
Training Epoch: 32 [4864/50000]	Loss: 1.2074	LR: 0.050000
Training Epoch: 32 [4992/50000]	Loss: 0.7260	LR: 0.050000
Training Epoch: 32 [5120/50000]	Loss: 1.0121	LR: 0.050000
Training Epoch: 32 [5248/50000]	Loss: 0.6818	LR: 0.050000
Training Epoch: 32 [5376/50000]	Loss: 0.8952	LR: 0.050000
Training Epoch: 32 [5504/50000]	Loss: 0.8222	LR: 0.050000
Training Epoch: 32 [5632/50000]	Loss: 0.9555	LR: 0.050000
Training Epoch: 32 [5760/50000]	Loss: 0.7907	LR: 0.050000
Training Epoch: 32 [5888/50000]	Loss: 0.8304	LR: 0.050000
Training Epoch: 32 [6016/50000]	Loss: 1.0086	LR: 0.050000
Training Epoch: 32 [6144/50000]	Loss: 0.9662	LR: 0.050000
Training Epoch: 32 [6272/50000]	Loss: 0.9221	LR: 0.050000
Training Epoch: 32 [6400/50000]	Loss: 0.9376	LR: 0.050000
Training Epoch: 32 [6528/50000]	Loss: 0.8926	LR: 0.050000
Training Epoch: 32 [6656/50000]	Loss: 0.9847	LR: 0.050000
Training Epoch: 32 [6784/50000]	Loss: 1.0086	LR: 0.050000
Training Epoch: 32 [6912/50000]	Loss: 0.9140	LR: 0.050000
Training Epoch: 32 [7040/50000]	Loss: 0.9658	LR: 0.050000
Training Epoch: 32 [7168/50000]	Loss: 0.8502	LR: 0.050000
Training Epoch: 32 [7296/50000]	Loss: 0.8107	LR: 0.050000
Training Epoch: 32 [7424/50000]	Loss: 0.9747	LR: 0.050000
Training Epoch: 32 [7552/50000]	Loss: 0.8560	LR: 0.050000
Training Epoch: 32 [7680/50000]	Loss: 0.9284	LR: 0.050000
Training Epoch: 32 [7808/50000]	Loss: 1.0326	LR: 0.050000
Training Epoch: 32 [7936/50000]	Loss: 0.9805	LR: 0.050000
Training Epoch: 32 [8064/50000]	Loss: 1.0199	LR: 0.050000
Training Epoch: 32 [8192/50000]	Loss: 0.9392	LR: 0.050000
Training Epoch: 32 [8320/50000]	Loss: 0.9382	LR: 0.050000
Training Epoch: 32 [8448/50000]	Loss: 1.0526	LR: 0.050000
Training Epoch: 32 [8576/50000]	Loss: 1.0069	LR: 0.050000
Training Epoch: 32 [8704/50000]	Loss: 0.9657	LR: 0.050000
Training Epoch: 32 [8832/50000]	Loss: 0.8468	LR: 0.050000
Training Epoch: 32 [8960/50000]	Loss: 0.9201	LR: 0.050000
Training Epoch: 32 [9088/50000]	Loss: 0.9215	LR: 0.050000
Training Epoch: 32 [9216/50000]	Loss: 1.0193	LR: 0.050000
Training Epoch: 32 [9344/50000]	Loss: 1.0040	LR: 0.050000
Training Epoch: 32 [9472/50000]	Loss: 1.0098	LR: 0.050000
Training Epoch: 32 [9600/50000]	Loss: 1.1694	LR: 0.050000
Training Epoch: 32 [9728/50000]	Loss: 0.9150	LR: 0.050000
Training Epoch: 32 [9856/50000]	Loss: 1.0148	LR: 0.050000
Training Epoch: 32 [9984/50000]	Loss: 0.9652	LR: 0.050000
Training Epoch: 32 [10112/50000]	Loss: 0.8235	LR: 0.050000
Training Epoch: 32 [10240/50000]	Loss: 1.3061	LR: 0.050000
Training Epoch: 32 [10368/50000]	Loss: 0.9566	LR: 0.050000
Training Epoch: 32 [10496/50000]	Loss: 0.8623	LR: 0.050000
Training Epoch: 32 [10624/50000]	Loss: 0.7647	LR: 0.050000
Training Epoch: 32 [10752/50000]	Loss: 1.0256	LR: 0.050000
Training Epoch: 32 [10880/50000]	Loss: 0.8198	LR: 0.050000
Training Epoch: 32 [11008/50000]	Loss: 0.9976	LR: 0.050000
Training Epoch: 32 [11136/50000]	Loss: 0.8529	LR: 0.050000
Training Epoch: 32 [11264/50000]	Loss: 1.0853	LR: 0.050000
Training Epoch: 32 [11392/50000]	Loss: 1.2978	LR: 0.050000
Training Epoch: 32 [11520/50000]	Loss: 0.9486	LR: 0.050000
Training Epoch: 32 [11648/50000]	Loss: 0.9889	LR: 0.050000
Training Epoch: 32 [11776/50000]	Loss: 1.1508	LR: 0.050000
Training Epoch: 32 [11904/50000]	Loss: 1.1128	LR: 0.050000
Training Epoch: 32 [12032/50000]	Loss: 1.0489	LR: 0.050000
Training Epoch: 32 [12160/50000]	Loss: 1.0655	LR: 0.050000
Training Epoch: 32 [12288/50000]	Loss: 1.2284	LR: 0.050000
Training Epoch: 32 [12416/50000]	Loss: 0.9241	LR: 0.050000
Training Epoch: 32 [12544/50000]	Loss: 0.8203	LR: 0.050000
Training Epoch: 32 [12672/50000]	Loss: 1.1042	LR: 0.050000
Training Epoch: 32 [12800/50000]	Loss: 0.9321	LR: 0.050000
Training Epoch: 32 [12928/50000]	Loss: 1.0783	LR: 0.050000
Training Epoch: 32 [13056/50000]	Loss: 1.0500	LR: 0.050000
Training Epoch: 32 [13184/50000]	Loss: 1.0499	LR: 0.050000
Training Epoch: 32 [13312/50000]	Loss: 1.0277	LR: 0.050000
Training Epoch: 32 [13440/50000]	Loss: 1.0491	LR: 0.050000
Training Epoch: 32 [13568/50000]	Loss: 0.8474	LR: 0.050000
Training Epoch: 32 [13696/50000]	Loss: 1.0633	LR: 0.050000
Training Epoch: 32 [13824/50000]	Loss: 0.7002	LR: 0.050000
Training Epoch: 32 [13952/50000]	Loss: 1.1946	LR: 0.050000
Training Epoch: 32 [14080/50000]	Loss: 1.1689	LR: 0.050000
Training Epoch: 32 [14208/50000]	Loss: 0.9249	LR: 0.050000
Training Epoch: 32 [14336/50000]	Loss: 0.8604	LR: 0.050000
Training Epoch: 32 [14464/50000]	Loss: 1.0599	LR: 0.050000
Training Epoch: 32 [14592/50000]	Loss: 0.9602	LR: 0.050000
Training Epoch: 32 [14720/50000]	Loss: 0.7959	LR: 0.050000
Training Epoch: 32 [14848/50000]	Loss: 0.8725	LR: 0.050000
Training Epoch: 32 [14976/50000]	Loss: 0.9344	LR: 0.050000
Training Epoch: 32 [15104/50000]	Loss: 0.8590	LR: 0.050000
Training Epoch: 32 [15232/50000]	Loss: 1.0619	LR: 0.050000
Training Epoch: 32 [15360/50000]	Loss: 1.0619	LR: 0.050000
Training Epoch: 32 [15488/50000]	Loss: 1.3005	LR: 0.050000
Training Epoch: 32 [15616/50000]	Loss: 0.9280	LR: 0.050000
Training Epoch: 32 [15744/50000]	Loss: 0.8749	LR: 0.050000
Training Epoch: 32 [15872/50000]	Loss: 0.9124	LR: 0.050000
Training Epoch: 32 [16000/50000]	Loss: 1.0725	LR: 0.050000
Training Epoch: 32 [16128/50000]	Loss: 1.0980	LR: 0.050000
Training Epoch: 32 [16256/50000]	Loss: 0.9455	LR: 0.050000
Training Epoch: 32 [16384/50000]	Loss: 1.0773	LR: 0.050000
Training Epoch: 32 [16512/50000]	Loss: 1.1135	LR: 0.050000
Training Epoch: 32 [16640/50000]	Loss: 0.9402	LR: 0.050000
Training Epoch: 32 [16768/50000]	Loss: 1.1308	LR: 0.050000
Training Epoch: 32 [16896/50000]	Loss: 1.0665	LR: 0.050000
Training Epoch: 32 [17024/50000]	Loss: 1.1338	LR: 0.050000
Training Epoch: 32 [17152/50000]	Loss: 1.0063	LR: 0.050000
Training Epoch: 32 [17280/50000]	Loss: 0.9429	LR: 0.050000
Training Epoch: 32 [17408/50000]	Loss: 0.9874	LR: 0.050000
Training Epoch: 32 [17536/50000]	Loss: 1.1155	LR: 0.050000
Training Epoch: 32 [17664/50000]	Loss: 0.9403	LR: 0.050000
Training Epoch: 32 [17792/50000]	Loss: 1.0219	LR: 0.050000
Training Epoch: 32 [17920/50000]	Loss: 0.9814	LR: 0.050000
Training Epoch: 32 [18048/50000]	Loss: 0.8141	LR: 0.050000
Training Epoch: 32 [18176/50000]	Loss: 0.8755	LR: 0.050000
Training Epoch: 32 [18304/50000]	Loss: 1.0426	LR: 0.050000
Training Epoch: 32 [18432/50000]	Loss: 1.3876	LR: 0.050000
Training Epoch: 32 [18560/50000]	Loss: 0.9897	LR: 0.050000
Training Epoch: 32 [18688/50000]	Loss: 0.9999	LR: 0.050000
Training Epoch: 32 [18816/50000]	Loss: 0.9219	LR: 0.050000
Training Epoch: 32 [18944/50000]	Loss: 0.9179	LR: 0.050000
Training Epoch: 32 [19072/50000]	Loss: 1.0313	LR: 0.050000
Training Epoch: 32 [19200/50000]	Loss: 1.0727	LR: 0.050000
Training Epoch: 32 [19328/50000]	Loss: 0.9527	LR: 0.050000
Training Epoch: 32 [19456/50000]	Loss: 0.9107	LR: 0.050000
Training Epoch: 32 [19584/50000]	Loss: 0.9994	LR: 0.050000
Training Epoch: 32 [19712/50000]	Loss: 0.9427	LR: 0.050000
Training Epoch: 32 [19840/50000]	Loss: 1.0143	LR: 0.050000
Training Epoch: 32 [19968/50000]	Loss: 0.9818	LR: 0.050000
Training Epoch: 32 [20096/50000]	Loss: 0.9351	LR: 0.050000
Training Epoch: 32 [20224/50000]	Loss: 1.0056	LR: 0.050000
Training Epoch: 32 [20352/50000]	Loss: 1.0009	LR: 0.050000
Training Epoch: 32 [20480/50000]	Loss: 0.9790	LR: 0.050000
Training Epoch: 32 [20608/50000]	Loss: 1.0622	LR: 0.050000
Training Epoch: 32 [20736/50000]	Loss: 0.9537	LR: 0.050000
Training Epoch: 32 [20864/50000]	Loss: 0.9160	LR: 0.050000
Training Epoch: 32 [20992/50000]	Loss: 0.9495	LR: 0.050000
Training Epoch: 32 [21120/50000]	Loss: 1.1040	LR: 0.050000
Training Epoch: 32 [21248/50000]	Loss: 1.0511	LR: 0.050000
Training Epoch: 32 [21376/50000]	Loss: 0.9035	LR: 0.050000
Training Epoch: 32 [21504/50000]	Loss: 1.2135	LR: 0.050000
Training Epoch: 32 [21632/50000]	Loss: 1.1158	LR: 0.050000
Training Epoch: 32 [21760/50000]	Loss: 0.9693	LR: 0.050000
Training Epoch: 32 [21888/50000]	Loss: 1.0681	LR: 0.050000
Training Epoch: 32 [22016/50000]	Loss: 0.9501	LR: 0.050000
Training Epoch: 32 [22144/50000]	Loss: 1.2870	LR: 0.050000
Training Epoch: 32 [22272/50000]	Loss: 0.9652	LR: 0.050000
Training Epoch: 32 [22400/50000]	Loss: 0.9853	LR: 0.050000
Training Epoch: 32 [22528/50000]	Loss: 1.0316	LR: 0.050000
Training Epoch: 32 [22656/50000]	Loss: 1.0808	LR: 0.050000
Training Epoch: 32 [22784/50000]	Loss: 1.1164	LR: 0.050000
Training Epoch: 32 [22912/50000]	Loss: 1.1770	LR: 0.050000
Training Epoch: 32 [23040/50000]	Loss: 0.9921	LR: 0.050000
Training Epoch: 32 [23168/50000]	Loss: 0.8545	LR: 0.050000
Training Epoch: 32 [23296/50000]	Loss: 0.9144	LR: 0.050000
Training Epoch: 32 [23424/50000]	Loss: 0.8666	LR: 0.050000
Training Epoch: 32 [23552/50000]	Loss: 1.1254	LR: 0.050000
Training Epoch: 32 [23680/50000]	Loss: 0.8738	LR: 0.050000
Training Epoch: 32 [23808/50000]	Loss: 0.9781	LR: 0.050000
Training Epoch: 32 [23936/50000]	Loss: 1.0306	LR: 0.050000
Training Epoch: 32 [24064/50000]	Loss: 1.0798	LR: 0.050000
Training Epoch: 32 [24192/50000]	Loss: 1.0245	LR: 0.050000
Training Epoch: 32 [24320/50000]	Loss: 0.9100	LR: 0.050000
Training Epoch: 32 [24448/50000]	Loss: 1.1533	LR: 0.050000
Training Epoch: 32 [24576/50000]	Loss: 0.9829	LR: 0.050000
Training Epoch: 32 [24704/50000]	Loss: 1.2163	LR: 0.050000
Training Epoch: 32 [24832/50000]	Loss: 1.1678	LR: 0.050000
Training Epoch: 32 [24960/50000]	Loss: 0.9550	LR: 0.050000
Training Epoch: 32 [25088/50000]	Loss: 0.8402	LR: 0.050000
Training Epoch: 32 [25216/50000]	Loss: 0.8425	LR: 0.050000
Training Epoch: 32 [25344/50000]	Loss: 1.0504	LR: 0.050000
Training Epoch: 32 [25472/50000]	Loss: 1.1454	LR: 0.050000
Training Epoch: 32 [25600/50000]	Loss: 1.2207	LR: 0.050000
Training Epoch: 32 [25728/50000]	Loss: 1.1929	LR: 0.050000
Training Epoch: 32 [25856/50000]	Loss: 0.9491	LR: 0.050000
Training Epoch: 32 [25984/50000]	Loss: 1.0802	LR: 0.050000
Training Epoch: 32 [26112/50000]	Loss: 1.0060	LR: 0.050000
Training Epoch: 32 [26240/50000]	Loss: 1.0458	LR: 0.050000
Training Epoch: 32 [26368/50000]	Loss: 0.9647	LR: 0.050000
Training Epoch: 32 [26496/50000]	Loss: 0.9554	LR: 0.050000
Training Epoch: 32 [26624/50000]	Loss: 1.1239	LR: 0.050000
Training Epoch: 32 [26752/50000]	Loss: 1.1438	LR: 0.050000
Training Epoch: 32 [26880/50000]	Loss: 1.0534	LR: 0.050000
Training Epoch: 32 [27008/50000]	Loss: 1.2500	LR: 0.050000
Training Epoch: 32 [27136/50000]	Loss: 0.7231	LR: 0.050000
Training Epoch: 32 [27264/50000]	Loss: 0.9838	LR: 0.050000
Training Epoch: 32 [27392/50000]	Loss: 0.9756	LR: 0.050000
Training Epoch: 32 [27520/50000]	Loss: 1.0439	LR: 0.050000
Training Epoch: 32 [27648/50000]	Loss: 0.9837	LR: 0.050000
Training Epoch: 32 [27776/50000]	Loss: 1.0834	LR: 0.050000
Training Epoch: 32 [27904/50000]	Loss: 0.9125	LR: 0.050000
Training Epoch: 32 [28032/50000]	Loss: 1.0867	LR: 0.050000
Training Epoch: 32 [28160/50000]	Loss: 0.8949	LR: 0.050000
Training Epoch: 32 [28288/50000]	Loss: 0.9765	LR: 0.050000
Training Epoch: 32 [28416/50000]	Loss: 0.9539	LR: 0.050000
Training Epoch: 32 [28544/50000]	Loss: 1.1443	LR: 0.050000
Training Epoch: 32 [28672/50000]	Loss: 0.9839	LR: 0.050000
Training Epoch: 32 [28800/50000]	Loss: 0.9145	LR: 0.050000
Training Epoch: 32 [28928/50000]	Loss: 0.9754	LR: 0.050000
Training Epoch: 32 [29056/50000]	Loss: 1.1919	LR: 0.050000
Training Epoch: 32 [29184/50000]	Loss: 1.0910	LR: 0.050000
Training Epoch: 32 [29312/50000]	Loss: 0.9172	LR: 0.050000
Training Epoch: 32 [29440/50000]	Loss: 0.9649	LR: 0.050000
Training Epoch: 32 [29568/50000]	Loss: 1.0294	LR: 0.050000
Training Epoch: 32 [29696/50000]	Loss: 1.0252	LR: 0.050000
Training Epoch: 32 [29824/50000]	Loss: 0.9917	LR: 0.050000
Training Epoch: 32 [29952/50000]	Loss: 1.0095	LR: 0.050000
Training Epoch: 32 [30080/50000]	Loss: 0.8491	LR: 0.050000
Training Epoch: 32 [30208/50000]	Loss: 1.1502	LR: 0.050000
Training Epoch: 32 [30336/50000]	Loss: 0.8693	LR: 0.050000
Training Epoch: 32 [30464/50000]	Loss: 0.8781	LR: 0.050000
Training Epoch: 32 [30592/50000]	Loss: 1.0237	LR: 0.050000
Training Epoch: 32 [30720/50000]	Loss: 0.9256	LR: 0.050000
Training Epoch: 32 [30848/50000]	Loss: 1.0523	LR: 0.050000
Training Epoch: 32 [30976/50000]	Loss: 0.8315	LR: 0.050000
Training Epoch: 32 [31104/50000]	Loss: 0.9676	LR: 0.050000
Training Epoch: 32 [31232/50000]	Loss: 0.8072	LR: 0.050000
Training Epoch: 32 [31360/50000]	Loss: 1.0475	LR: 0.050000
Training Epoch: 32 [31488/50000]	Loss: 0.8225	LR: 0.050000
Training Epoch: 32 [31616/50000]	Loss: 1.0432	LR: 0.050000
Training Epoch: 32 [31744/50000]	Loss: 1.0785	LR: 0.050000
Training Epoch: 32 [31872/50000]	Loss: 1.2491	LR: 0.050000
Training Epoch: 32 [32000/50000]	Loss: 0.9006	LR: 0.050000
Training Epoch: 32 [32128/50000]	Loss: 1.1402	LR: 0.050000
Training Epoch: 32 [32256/50000]	Loss: 1.2233	LR: 0.050000
Training Epoch: 32 [32384/50000]	Loss: 0.9560	LR: 0.050000
Training Epoch: 32 [32512/50000]	Loss: 1.1623	LR: 0.050000
Training Epoch: 32 [32640/50000]	Loss: 1.3065	LR: 0.050000
Training Epoch: 32 [32768/50000]	Loss: 1.0730	LR: 0.050000
Training Epoch: 32 [32896/50000]	Loss: 1.0292	LR: 0.050000
Training Epoch: 32 [33024/50000]	Loss: 1.0893	LR: 0.050000
Training Epoch: 32 [33152/50000]	Loss: 1.0273	LR: 0.050000
Training Epoch: 32 [33280/50000]	Loss: 1.0911	LR: 0.050000
Training Epoch: 32 [33408/50000]	Loss: 1.1338	LR: 0.050000
Training Epoch: 32 [33536/50000]	Loss: 1.1724	LR: 0.050000
Training Epoch: 32 [33664/50000]	Loss: 1.0240	LR: 0.050000
Training Epoch: 32 [33792/50000]	Loss: 1.0203	LR: 0.050000
Training Epoch: 32 [33920/50000]	Loss: 1.0489	LR: 0.050000
Training Epoch: 32 [34048/50000]	Loss: 0.8464	LR: 0.050000
Training Epoch: 32 [34176/50000]	Loss: 0.9549	LR: 0.050000
Training Epoch: 32 [34304/50000]	Loss: 1.2403	LR: 0.050000
Training Epoch: 32 [34432/50000]	Loss: 1.0291	LR: 0.050000
Training Epoch: 32 [34560/50000]	Loss: 1.1966	LR: 0.050000
Training Epoch: 32 [34688/50000]	Loss: 1.1574	LR: 0.050000
Training Epoch: 32 [34816/50000]	Loss: 1.0103	LR: 0.050000
Training Epoch: 32 [34944/50000]	Loss: 0.9425	LR: 0.050000
Training Epoch: 32 [35072/50000]	Loss: 1.1153	LR: 0.050000
Training Epoch: 32 [35200/50000]	Loss: 1.1967	LR: 0.050000
Training Epoch: 32 [35328/50000]	Loss: 1.0745	LR: 0.050000
Training Epoch: 32 [35456/50000]	Loss: 1.0191	LR: 0.050000
Training Epoch: 32 [35584/50000]	Loss: 1.0771	LR: 0.050000
Training Epoch: 32 [35712/50000]	Loss: 0.9629	LR: 0.050000
Training Epoch: 32 [35840/50000]	Loss: 1.0613	LR: 0.050000
Training Epoch: 32 [35968/50000]	Loss: 1.1884	LR: 0.050000
Training Epoch: 32 [36096/50000]	Loss: 0.9959	LR: 0.050000
Training Epoch: 32 [36224/50000]	Loss: 1.0374	LR: 0.050000
Training Epoch: 32 [36352/50000]	Loss: 1.2987	LR: 0.050000
Training Epoch: 32 [36480/50000]	Loss: 0.8697	LR: 0.050000
Training Epoch: 32 [36608/50000]	Loss: 1.1038	LR: 0.050000
Training Epoch: 32 [36736/50000]	Loss: 1.1202	LR: 0.050000
Training Epoch: 32 [36864/50000]	Loss: 1.3065	LR: 0.050000
Training Epoch: 32 [36992/50000]	Loss: 0.9005	LR: 0.050000
Training Epoch: 32 [37120/50000]	Loss: 0.9044	LR: 0.050000
Training Epoch: 32 [37248/50000]	Loss: 1.0932	LR: 0.050000
Training Epoch: 32 [37376/50000]	Loss: 1.1691	LR: 0.050000
Training Epoch: 32 [37504/50000]	Loss: 0.9376	LR: 0.050000
Training Epoch: 32 [37632/50000]	Loss: 1.0705	LR: 0.050000
Training Epoch: 32 [37760/50000]	Loss: 0.9954	LR: 0.050000
Training Epoch: 32 [37888/50000]	Loss: 0.9402	LR: 0.050000
Training Epoch: 32 [38016/50000]	Loss: 0.9697	LR: 0.050000
Training Epoch: 32 [38144/50000]	Loss: 1.1644	LR: 0.050000
Training Epoch: 32 [38272/50000]	Loss: 1.0582	LR: 0.050000
Training Epoch: 32 [38400/50000]	Loss: 0.9949	LR: 0.050000
Training Epoch: 32 [38528/50000]	Loss: 0.9824	LR: 0.050000
Training Epoch: 32 [38656/50000]	Loss: 0.9709	LR: 0.050000
Training Epoch: 32 [38784/50000]	Loss: 1.1073	LR: 0.050000
Training Epoch: 32 [38912/50000]	Loss: 1.0185	LR: 0.050000
Training Epoch: 32 [39040/50000]	Loss: 1.0286	LR: 0.050000
Training Epoch: 32 [39168/50000]	Loss: 0.9904	LR: 0.050000
Training Epoch: 32 [39296/50000]	Loss: 1.2071	LR: 0.050000
Training Epoch: 32 [39424/50000]	Loss: 1.4142	LR: 0.050000
Training Epoch: 32 [39552/50000]	Loss: 1.3069	LR: 0.050000
Training Epoch: 32 [39680/50000]	Loss: 1.0807	LR: 0.050000
Training Epoch: 32 [39808/50000]	Loss: 1.1197	LR: 0.050000
Training Epoch: 32 [39936/50000]	Loss: 0.8438	LR: 0.050000
Training Epoch: 32 [40064/50000]	Loss: 1.0532	LR: 0.050000
Training Epoch: 32 [40192/50000]	Loss: 0.9966	LR: 0.050000
Training Epoch: 32 [40320/50000]	Loss: 1.0297	LR: 0.050000
Training Epoch: 32 [40448/50000]	Loss: 1.3262	LR: 0.050000
Training Epoch: 32 [40576/50000]	Loss: 1.1179	LR: 0.050000
Training Epoch: 32 [40704/50000]	Loss: 1.1654	LR: 0.050000
Training Epoch: 32 [40832/50000]	Loss: 0.8924	LR: 0.050000
Training Epoch: 32 [40960/50000]	Loss: 1.1287	LR: 0.050000
Training Epoch: 32 [41088/50000]	Loss: 0.9746	LR: 0.050000
Training Epoch: 32 [41216/50000]	Loss: 1.0075	LR: 0.050000
Training Epoch: 32 [41344/50000]	Loss: 1.2146	LR: 0.050000
Training Epoch: 32 [41472/50000]	Loss: 0.9473	LR: 0.050000
Training Epoch: 32 [41600/50000]	Loss: 1.2043	LR: 0.050000
Training Epoch: 32 [41728/50000]	Loss: 1.1217	LR: 0.050000
Training Epoch: 32 [41856/50000]	Loss: 0.8916	LR: 0.050000
Training Epoch: 32 [41984/50000]	Loss: 0.9933	LR: 0.050000
Training Epoch: 32 [42112/50000]	Loss: 0.8917	LR: 0.050000
Training Epoch: 32 [42240/50000]	Loss: 0.7617	LR: 0.050000
Training Epoch: 32 [42368/50000]	Loss: 1.2558	LR: 0.050000
Training Epoch: 32 [42496/50000]	Loss: 1.0905	LR: 0.050000
Training Epoch: 32 [42624/50000]	Loss: 1.0832	LR: 0.050000
Training Epoch: 32 [42752/50000]	Loss: 0.9961	LR: 0.050000
Training Epoch: 32 [42880/50000]	Loss: 1.0014	LR: 0.050000
Training Epoch: 32 [43008/50000]	Loss: 1.2477	LR: 0.050000
Training Epoch: 32 [43136/50000]	Loss: 1.1407	LR: 0.050000
Training Epoch: 32 [43264/50000]	Loss: 1.0681	LR: 0.050000
Training Epoch: 32 [43392/50000]	Loss: 1.0570	LR: 0.050000
Training Epoch: 32 [43520/50000]	Loss: 1.0601	LR: 0.050000
Training Epoch: 32 [43648/50000]	Loss: 1.2821	LR: 0.050000
Training Epoch: 32 [43776/50000]	Loss: 0.8006	LR: 0.050000
Training Epoch: 32 [43904/50000]	Loss: 1.0949	LR: 0.050000
Training Epoch: 32 [44032/50000]	Loss: 1.0424	LR: 0.050000
Training Epoch: 32 [44160/50000]	Loss: 1.2561	LR: 0.050000
Training Epoch: 32 [44288/50000]	Loss: 1.1834	LR: 0.050000
Training Epoch: 32 [44416/50000]	Loss: 0.9021	LR: 0.050000
Training Epoch: 32 [44544/50000]	Loss: 1.1337	LR: 0.050000
Training Epoch: 32 [44672/50000]	Loss: 1.1424	LR: 0.050000
Training Epoch: 32 [44800/50000]	Loss: 1.3256	LR: 0.050000
Training Epoch: 32 [44928/50000]	Loss: 0.8910	LR: 0.050000
Training Epoch: 32 [45056/50000]	Loss: 0.9038	LR: 0.050000
Training Epoch: 32 [45184/50000]	Loss: 1.0763	LR: 0.050000
Training Epoch: 32 [45312/50000]	Loss: 1.1408	LR: 0.050000
Training Epoch: 32 [45440/50000]	Loss: 0.8396	LR: 0.050000
Training Epoch: 32 [45568/50000]	Loss: 1.2325	LR: 0.050000
Training Epoch: 32 [45696/50000]	Loss: 1.1342	LR: 0.050000
Training Epoch: 32 [45824/50000]	Loss: 1.2155	LR: 0.050000
Training Epoch: 32 [45952/50000]	Loss: 0.9212	LR: 0.050000
Training Epoch: 32 [46080/50000]	Loss: 1.1061	LR: 0.050000
Training Epoch: 32 [46208/50000]	Loss: 0.9026	LR: 0.050000
Training Epoch: 32 [46336/50000]	Loss: 1.1770	LR: 0.050000
Training Epoch: 32 [46464/50000]	Loss: 0.9972	LR: 0.050000
Training Epoch: 32 [46592/50000]	Loss: 1.1648	LR: 0.050000
Training Epoch: 32 [46720/50000]	Loss: 0.9791	LR: 0.050000
Training Epoch: 32 [46848/50000]	Loss: 0.8863	LR: 0.050000
Training Epoch: 32 [46976/50000]	Loss: 1.0815	LR: 0.050000
Training Epoch: 32 [47104/50000]	Loss: 1.1950	LR: 0.050000
Training Epoch: 32 [47232/50000]	Loss: 1.0858	LR: 0.050000
Training Epoch: 32 [47360/50000]	Loss: 1.0791	LR: 0.050000
Training Epoch: 32 [47488/50000]	Loss: 1.1246	LR: 0.050000
Training Epoch: 32 [47616/50000]	Loss: 1.1088	LR: 0.050000
Training Epoch: 32 [47744/50000]	Loss: 1.0455	LR: 0.050000
Training Epoch: 32 [47872/50000]	Loss: 1.0783	LR: 0.050000
Training Epoch: 32 [48000/50000]	Loss: 0.9723	LR: 0.050000
Training Epoch: 32 [48128/50000]	Loss: 1.0568	LR: 0.050000
Training Epoch: 32 [48256/50000]	Loss: 1.2828	LR: 0.050000
Training Epoch: 32 [48384/50000]	Loss: 0.8987	LR: 0.050000
Training Epoch: 32 [48512/50000]	Loss: 1.1594	LR: 0.050000
Training Epoch: 32 [48640/50000]	Loss: 1.1449	LR: 0.050000
Training Epoch: 32 [48768/50000]	Loss: 1.1020	LR: 0.050000
Training Epoch: 32 [48896/50000]	Loss: 0.9808	LR: 0.050000
Training Epoch: 32 [49024/50000]	Loss: 1.3222	LR: 0.050000
Training Epoch: 32 [49152/50000]	Loss: 1.3502	LR: 0.050000
Training Epoch: 32 [49280/50000]	Loss: 0.9361	LR: 0.050000
Training Epoch: 32 [49408/50000]	Loss: 1.0803	LR: 0.050000
Training Epoch: 32 [49536/50000]	Loss: 1.0641	LR: 0.050000
Training Epoch: 32 [49664/50000]	Loss: 1.0413	LR: 0.050000
Training Epoch: 32 [49792/50000]	Loss: 1.1869	LR: 0.050000
Training Epoch: 32 [49920/50000]	Loss: 1.1346	LR: 0.050000
Training Epoch: 32 [50000/50000]	Loss: 1.1183	LR: 0.050000
epoch 32 training time consumed: 53.97s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  117442 GB |  117442 GB |
|       from large pool |  123392 KB |    1034 MB |  117326 GB |  117326 GB |
|       from small pool |   10798 KB |      13 MB |     115 GB |     115 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  117442 GB |  117442 GB |
|       from large pool |  123392 KB |    1034 MB |  117326 GB |  117326 GB |
|       from small pool |   10798 KB |      13 MB |     115 GB |     115 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   51683 GB |   51683 GB |
|       from large pool |  155136 KB |  433088 KB |   51555 GB |   51555 GB |
|       from small pool |    1490 KB |    3494 KB |     127 GB |     127 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    4532 K  |    4531 K  |
|       from large pool |      24    |      65    |    2365 K  |    2365 K  |
|       from small pool |     231    |     274    |    2166 K  |    2166 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    4532 K  |    4531 K  |
|       from large pool |      24    |      65    |    2365 K  |    2365 K  |
|       from small pool |     231    |     274    |    2166 K  |    2166 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2241 K  |    2241 K  |
|       from large pool |       9    |      14    |    1144 K  |    1144 K  |
|       from small pool |      12    |      16    |    1096 K  |    1096 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 32, Average loss: 0.0127, Accuracy: 0.5925, Time consumed:3.45s

Training Epoch: 33 [128/50000]	Loss: 0.8073	LR: 0.050000
Training Epoch: 33 [256/50000]	Loss: 1.0665	LR: 0.050000
Training Epoch: 33 [384/50000]	Loss: 0.8598	LR: 0.050000
Training Epoch: 33 [512/50000]	Loss: 0.9943	LR: 0.050000
Training Epoch: 33 [640/50000]	Loss: 0.8798	LR: 0.050000
Training Epoch: 33 [768/50000]	Loss: 1.1156	LR: 0.050000
Training Epoch: 33 [896/50000]	Loss: 0.8980	LR: 0.050000
Training Epoch: 33 [1024/50000]	Loss: 0.9524	LR: 0.050000
Training Epoch: 33 [1152/50000]	Loss: 0.8774	LR: 0.050000
Training Epoch: 33 [1280/50000]	Loss: 0.7491	LR: 0.050000
Training Epoch: 33 [1408/50000]	Loss: 1.1135	LR: 0.050000
Training Epoch: 33 [1536/50000]	Loss: 1.1159	LR: 0.050000
Training Epoch: 33 [1664/50000]	Loss: 0.8557	LR: 0.050000
Training Epoch: 33 [1792/50000]	Loss: 1.0604	LR: 0.050000
Training Epoch: 33 [1920/50000]	Loss: 1.0116	LR: 0.050000
Training Epoch: 33 [2048/50000]	Loss: 0.8565	LR: 0.050000
Training Epoch: 33 [2176/50000]	Loss: 1.0626	LR: 0.050000
Training Epoch: 33 [2304/50000]	Loss: 0.7421	LR: 0.050000
Training Epoch: 33 [2432/50000]	Loss: 0.9071	LR: 0.050000
Training Epoch: 33 [2560/50000]	Loss: 1.1144	LR: 0.050000
Training Epoch: 33 [2688/50000]	Loss: 0.9400	LR: 0.050000
Training Epoch: 33 [2816/50000]	Loss: 0.8700	LR: 0.050000
Training Epoch: 33 [2944/50000]	Loss: 1.0311	LR: 0.050000
Training Epoch: 33 [3072/50000]	Loss: 1.0523	LR: 0.050000
Training Epoch: 33 [3200/50000]	Loss: 0.8473	LR: 0.050000
Training Epoch: 33 [3328/50000]	Loss: 0.8468	LR: 0.050000
Training Epoch: 33 [3456/50000]	Loss: 0.8906	LR: 0.050000
Training Epoch: 33 [3584/50000]	Loss: 0.7539	LR: 0.050000
Training Epoch: 33 [3712/50000]	Loss: 0.8421	LR: 0.050000
Training Epoch: 33 [3840/50000]	Loss: 0.9069	LR: 0.050000
Training Epoch: 33 [3968/50000]	Loss: 0.7805	LR: 0.050000
Training Epoch: 33 [4096/50000]	Loss: 0.9524	LR: 0.050000
Training Epoch: 33 [4224/50000]	Loss: 0.9557	LR: 0.050000
Training Epoch: 33 [4352/50000]	Loss: 1.0100	LR: 0.050000
Training Epoch: 33 [4480/50000]	Loss: 0.8944	LR: 0.050000
Training Epoch: 33 [4608/50000]	Loss: 0.7335	LR: 0.050000
Training Epoch: 33 [4736/50000]	Loss: 0.8167	LR: 0.050000
Training Epoch: 33 [4864/50000]	Loss: 1.0710	LR: 0.050000
Training Epoch: 33 [4992/50000]	Loss: 0.9187	LR: 0.050000
Training Epoch: 33 [5120/50000]	Loss: 0.9942	LR: 0.050000
Training Epoch: 33 [5248/50000]	Loss: 0.9543	LR: 0.050000
Training Epoch: 33 [5376/50000]	Loss: 0.8370	LR: 0.050000
Training Epoch: 33 [5504/50000]	Loss: 0.7454	LR: 0.050000
Training Epoch: 33 [5632/50000]	Loss: 0.9719	LR: 0.050000
Training Epoch: 33 [5760/50000]	Loss: 0.9546	LR: 0.050000
Training Epoch: 33 [5888/50000]	Loss: 1.1055	LR: 0.050000
Training Epoch: 33 [6016/50000]	Loss: 1.1536	LR: 0.050000
Training Epoch: 33 [6144/50000]	Loss: 0.9261	LR: 0.050000
Training Epoch: 33 [6272/50000]	Loss: 0.8953	LR: 0.050000
Training Epoch: 33 [6400/50000]	Loss: 1.0472	LR: 0.050000
Training Epoch: 33 [6528/50000]	Loss: 0.7365	LR: 0.050000
Training Epoch: 33 [6656/50000]	Loss: 0.9406	LR: 0.050000
Training Epoch: 33 [6784/50000]	Loss: 0.8766	LR: 0.050000
Training Epoch: 33 [6912/50000]	Loss: 0.9532	LR: 0.050000
Training Epoch: 33 [7040/50000]	Loss: 1.1008	LR: 0.050000
Training Epoch: 33 [7168/50000]	Loss: 0.7097	LR: 0.050000
Training Epoch: 33 [7296/50000]	Loss: 0.6952	LR: 0.050000
Training Epoch: 33 [7424/50000]	Loss: 1.0293	LR: 0.050000
Training Epoch: 33 [7552/50000]	Loss: 0.9605	LR: 0.050000
Training Epoch: 33 [7680/50000]	Loss: 0.9264	LR: 0.050000
Training Epoch: 33 [7808/50000]	Loss: 0.9628	LR: 0.050000
Training Epoch: 33 [7936/50000]	Loss: 1.0845	LR: 0.050000
Training Epoch: 33 [8064/50000]	Loss: 1.2119	LR: 0.050000
Training Epoch: 33 [8192/50000]	Loss: 0.7842	LR: 0.050000
Training Epoch: 33 [8320/50000]	Loss: 1.0922	LR: 0.050000
Training Epoch: 33 [8448/50000]	Loss: 0.8839	LR: 0.050000
Training Epoch: 33 [8576/50000]	Loss: 1.2712	LR: 0.050000
Training Epoch: 33 [8704/50000]	Loss: 0.9577	LR: 0.050000
Training Epoch: 33 [8832/50000]	Loss: 0.9777	LR: 0.050000
Training Epoch: 33 [8960/50000]	Loss: 1.0486	LR: 0.050000
Training Epoch: 33 [9088/50000]	Loss: 0.9635	LR: 0.050000
Training Epoch: 33 [9216/50000]	Loss: 0.8727	LR: 0.050000
Training Epoch: 33 [9344/50000]	Loss: 0.8855	LR: 0.050000
Training Epoch: 33 [9472/50000]	Loss: 1.0729	LR: 0.050000
Training Epoch: 33 [9600/50000]	Loss: 0.9055	LR: 0.050000
Training Epoch: 33 [9728/50000]	Loss: 1.0385	LR: 0.050000
Training Epoch: 33 [9856/50000]	Loss: 0.9675	LR: 0.050000
Training Epoch: 33 [9984/50000]	Loss: 0.8537	LR: 0.050000
Training Epoch: 33 [10112/50000]	Loss: 0.8087	LR: 0.050000
Training Epoch: 33 [10240/50000]	Loss: 1.0420	LR: 0.050000
Training Epoch: 33 [10368/50000]	Loss: 1.1158	LR: 0.050000
Training Epoch: 33 [10496/50000]	Loss: 0.9302	LR: 0.050000
Training Epoch: 33 [10624/50000]	Loss: 1.1758	LR: 0.050000
Training Epoch: 33 [10752/50000]	Loss: 1.0783	LR: 0.050000
Training Epoch: 33 [10880/50000]	Loss: 0.8219	LR: 0.050000
Training Epoch: 33 [11008/50000]	Loss: 0.9868	LR: 0.050000
Training Epoch: 33 [11136/50000]	Loss: 0.9897	LR: 0.050000
Training Epoch: 33 [11264/50000]	Loss: 0.8293	LR: 0.050000
Training Epoch: 33 [11392/50000]	Loss: 1.0909	LR: 0.050000
Training Epoch: 33 [11520/50000]	Loss: 1.0397	LR: 0.050000
Training Epoch: 33 [11648/50000]	Loss: 1.4253	LR: 0.050000
Training Epoch: 33 [11776/50000]	Loss: 1.1028	LR: 0.050000
Training Epoch: 33 [11904/50000]	Loss: 0.9308	LR: 0.050000
Training Epoch: 33 [12032/50000]	Loss: 1.0921	LR: 0.050000
Training Epoch: 33 [12160/50000]	Loss: 1.0084	LR: 0.050000
Training Epoch: 33 [12288/50000]	Loss: 0.8803	LR: 0.050000
Training Epoch: 33 [12416/50000]	Loss: 1.2043	LR: 0.050000
Training Epoch: 33 [12544/50000]	Loss: 0.9305	LR: 0.050000
Training Epoch: 33 [12672/50000]	Loss: 0.9288	LR: 0.050000
Training Epoch: 33 [12800/50000]	Loss: 1.0535	LR: 0.050000
Training Epoch: 33 [12928/50000]	Loss: 0.9780	LR: 0.050000
Training Epoch: 33 [13056/50000]	Loss: 0.9546	LR: 0.050000
Training Epoch: 33 [13184/50000]	Loss: 1.1802	LR: 0.050000
Training Epoch: 33 [13312/50000]	Loss: 1.0142	LR: 0.050000
Training Epoch: 33 [13440/50000]	Loss: 1.1563	LR: 0.050000
Training Epoch: 33 [13568/50000]	Loss: 1.0278	LR: 0.050000
Training Epoch: 33 [13696/50000]	Loss: 1.1777	LR: 0.050000
Training Epoch: 33 [13824/50000]	Loss: 0.7967	LR: 0.050000
Training Epoch: 33 [13952/50000]	Loss: 0.9839	LR: 0.050000
Training Epoch: 33 [14080/50000]	Loss: 1.1251	LR: 0.050000
Training Epoch: 33 [14208/50000]	Loss: 1.0087	LR: 0.050000
Training Epoch: 33 [14336/50000]	Loss: 1.1383	LR: 0.050000
Training Epoch: 33 [14464/50000]	Loss: 0.9639	LR: 0.050000
Training Epoch: 33 [14592/50000]	Loss: 0.9552	LR: 0.050000
Training Epoch: 33 [14720/50000]	Loss: 0.9176	LR: 0.050000
Training Epoch: 33 [14848/50000]	Loss: 0.6746	LR: 0.050000
Training Epoch: 33 [14976/50000]	Loss: 0.8787	LR: 0.050000
Training Epoch: 33 [15104/50000]	Loss: 0.8162	LR: 0.050000
Training Epoch: 33 [15232/50000]	Loss: 0.6794	LR: 0.050000
Training Epoch: 33 [15360/50000]	Loss: 0.8099	LR: 0.050000
Training Epoch: 33 [15488/50000]	Loss: 1.1114	LR: 0.050000
Training Epoch: 33 [15616/50000]	Loss: 1.1752	LR: 0.050000
Training Epoch: 33 [15744/50000]	Loss: 1.0326	LR: 0.050000
Training Epoch: 33 [15872/50000]	Loss: 0.8659	LR: 0.050000
Training Epoch: 33 [16000/50000]	Loss: 0.9080	LR: 0.050000
Training Epoch: 33 [16128/50000]	Loss: 1.0654	LR: 0.050000
Training Epoch: 33 [16256/50000]	Loss: 0.9560	LR: 0.050000
Training Epoch: 33 [16384/50000]	Loss: 1.0655	LR: 0.050000
Training Epoch: 33 [16512/50000]	Loss: 0.8684	LR: 0.050000
Training Epoch: 33 [16640/50000]	Loss: 0.8864	LR: 0.050000
Training Epoch: 33 [16768/50000]	Loss: 0.9463	LR: 0.050000
Training Epoch: 33 [16896/50000]	Loss: 0.9177	LR: 0.050000
Training Epoch: 33 [17024/50000]	Loss: 0.9951	LR: 0.050000
Training Epoch: 33 [17152/50000]	Loss: 0.9717	LR: 0.050000
Training Epoch: 33 [17280/50000]	Loss: 0.8854	LR: 0.050000
Training Epoch: 33 [17408/50000]	Loss: 0.9359	LR: 0.050000
Training Epoch: 33 [17536/50000]	Loss: 0.9616	LR: 0.050000
Training Epoch: 33 [17664/50000]	Loss: 0.9806	LR: 0.050000
Training Epoch: 33 [17792/50000]	Loss: 1.0684	LR: 0.050000
Training Epoch: 33 [17920/50000]	Loss: 1.2357	LR: 0.050000
Training Epoch: 33 [18048/50000]	Loss: 0.9061	LR: 0.050000
Training Epoch: 33 [18176/50000]	Loss: 0.9130	LR: 0.050000
Training Epoch: 33 [18304/50000]	Loss: 0.8920	LR: 0.050000
Training Epoch: 33 [18432/50000]	Loss: 0.8085	LR: 0.050000
Training Epoch: 33 [18560/50000]	Loss: 1.0625	LR: 0.050000
Training Epoch: 33 [18688/50000]	Loss: 0.9894	LR: 0.050000
Training Epoch: 33 [18816/50000]	Loss: 1.0414	LR: 0.050000
Training Epoch: 33 [18944/50000]	Loss: 0.8153	LR: 0.050000
Training Epoch: 33 [19072/50000]	Loss: 1.0362	LR: 0.050000
Training Epoch: 33 [19200/50000]	Loss: 1.0365	LR: 0.050000
Training Epoch: 33 [19328/50000]	Loss: 0.8308	LR: 0.050000
Training Epoch: 33 [19456/50000]	Loss: 0.7958	LR: 0.050000
Training Epoch: 33 [19584/50000]	Loss: 0.9916	LR: 0.050000
Training Epoch: 33 [19712/50000]	Loss: 1.0864	LR: 0.050000
Training Epoch: 33 [19840/50000]	Loss: 0.8162	LR: 0.050000
Training Epoch: 33 [19968/50000]	Loss: 0.9833	LR: 0.050000
Training Epoch: 33 [20096/50000]	Loss: 0.9009	LR: 0.050000
Training Epoch: 33 [20224/50000]	Loss: 0.9187	LR: 0.050000
Training Epoch: 33 [20352/50000]	Loss: 0.9677	LR: 0.050000
Training Epoch: 33 [20480/50000]	Loss: 0.9436	LR: 0.050000
Training Epoch: 33 [20608/50000]	Loss: 1.2880	LR: 0.050000
Training Epoch: 33 [20736/50000]	Loss: 0.9327	LR: 0.050000
Training Epoch: 33 [20864/50000]	Loss: 0.9966	LR: 0.050000
Training Epoch: 33 [20992/50000]	Loss: 0.6258	LR: 0.050000
Training Epoch: 33 [21120/50000]	Loss: 1.1307	LR: 0.050000
Training Epoch: 33 [21248/50000]	Loss: 0.9448	LR: 0.050000
Training Epoch: 33 [21376/50000]	Loss: 0.9074	LR: 0.050000
Training Epoch: 33 [21504/50000]	Loss: 1.0711	LR: 0.050000
Training Epoch: 33 [21632/50000]	Loss: 1.2087	LR: 0.050000
Training Epoch: 33 [21760/50000]	Loss: 0.8937	LR: 0.050000
Training Epoch: 33 [21888/50000]	Loss: 0.9570	LR: 0.050000
Training Epoch: 33 [22016/50000]	Loss: 0.8230	LR: 0.050000
Training Epoch: 33 [22144/50000]	Loss: 0.8628	LR: 0.050000
Training Epoch: 33 [22272/50000]	Loss: 0.9492	LR: 0.050000
Training Epoch: 33 [22400/50000]	Loss: 0.8244	LR: 0.050000
Training Epoch: 33 [22528/50000]	Loss: 1.0300	LR: 0.050000
Training Epoch: 33 [22656/50000]	Loss: 0.9229	LR: 0.050000
Training Epoch: 33 [22784/50000]	Loss: 1.0497	LR: 0.050000
Training Epoch: 33 [22912/50000]	Loss: 0.9911	LR: 0.050000
Training Epoch: 33 [23040/50000]	Loss: 0.9502	LR: 0.050000
Training Epoch: 33 [23168/50000]	Loss: 0.9430	LR: 0.050000
Training Epoch: 33 [23296/50000]	Loss: 1.1096	LR: 0.050000
Training Epoch: 33 [23424/50000]	Loss: 0.9969	LR: 0.050000
Training Epoch: 33 [23552/50000]	Loss: 1.0072	LR: 0.050000
Training Epoch: 33 [23680/50000]	Loss: 1.1311	LR: 0.050000
Training Epoch: 33 [23808/50000]	Loss: 0.9227	LR: 0.050000
Training Epoch: 33 [23936/50000]	Loss: 0.7610	LR: 0.050000
Training Epoch: 33 [24064/50000]	Loss: 0.9819	LR: 0.050000
Training Epoch: 33 [24192/50000]	Loss: 1.1781	LR: 0.050000
Training Epoch: 33 [24320/50000]	Loss: 0.9000	LR: 0.050000
Training Epoch: 33 [24448/50000]	Loss: 0.9921	LR: 0.050000
Training Epoch: 33 [24576/50000]	Loss: 0.9060	LR: 0.050000
Training Epoch: 33 [24704/50000]	Loss: 0.9414	LR: 0.050000
Training Epoch: 33 [24832/50000]	Loss: 0.8766	LR: 0.050000
Training Epoch: 33 [24960/50000]	Loss: 0.8794	LR: 0.050000
Training Epoch: 33 [25088/50000]	Loss: 0.8366	LR: 0.050000
Training Epoch: 33 [25216/50000]	Loss: 0.8122	LR: 0.050000
Training Epoch: 33 [25344/50000]	Loss: 0.7923	LR: 0.050000
Training Epoch: 33 [25472/50000]	Loss: 0.9786	LR: 0.050000
Training Epoch: 33 [25600/50000]	Loss: 0.9521	LR: 0.050000
Training Epoch: 33 [25728/50000]	Loss: 1.0975	LR: 0.050000
Training Epoch: 33 [25856/50000]	Loss: 1.0832	LR: 0.050000
Training Epoch: 33 [25984/50000]	Loss: 1.0787	LR: 0.050000
Training Epoch: 33 [26112/50000]	Loss: 1.0505	LR: 0.050000
Training Epoch: 33 [26240/50000]	Loss: 0.9728	LR: 0.050000
Training Epoch: 33 [26368/50000]	Loss: 0.7717	LR: 0.050000
Training Epoch: 33 [26496/50000]	Loss: 1.1395	LR: 0.050000
Training Epoch: 33 [26624/50000]	Loss: 1.0279	LR: 0.050000
Training Epoch: 33 [26752/50000]	Loss: 0.8972	LR: 0.050000
Training Epoch: 33 [26880/50000]	Loss: 0.8492	LR: 0.050000
Training Epoch: 33 [27008/50000]	Loss: 1.0011	LR: 0.050000
Training Epoch: 33 [27136/50000]	Loss: 0.9816	LR: 0.050000
Training Epoch: 33 [27264/50000]	Loss: 0.8656	LR: 0.050000
Training Epoch: 33 [27392/50000]	Loss: 1.3147	LR: 0.050000
Training Epoch: 33 [27520/50000]	Loss: 1.0547	LR: 0.050000
Training Epoch: 33 [27648/50000]	Loss: 1.0165	LR: 0.050000
Training Epoch: 33 [27776/50000]	Loss: 0.8280	LR: 0.050000
Training Epoch: 33 [27904/50000]	Loss: 1.0549	LR: 0.050000
Training Epoch: 33 [28032/50000]	Loss: 0.9183	LR: 0.050000
Training Epoch: 33 [28160/50000]	Loss: 1.0182	LR: 0.050000
Training Epoch: 33 [28288/50000]	Loss: 1.0674	LR: 0.050000
Training Epoch: 33 [28416/50000]	Loss: 0.9631	LR: 0.050000
Training Epoch: 33 [28544/50000]	Loss: 1.0542	LR: 0.050000
Training Epoch: 33 [28672/50000]	Loss: 1.1862	LR: 0.050000
Training Epoch: 33 [28800/50000]	Loss: 0.7704	LR: 0.050000
Training Epoch: 33 [28928/50000]	Loss: 1.1511	LR: 0.050000
Training Epoch: 33 [29056/50000]	Loss: 1.1251	LR: 0.050000
Training Epoch: 33 [29184/50000]	Loss: 1.1287	LR: 0.050000
Training Epoch: 33 [29312/50000]	Loss: 1.0128	LR: 0.050000
Training Epoch: 33 [29440/50000]	Loss: 1.1117	LR: 0.050000
Training Epoch: 33 [29568/50000]	Loss: 1.2255	LR: 0.050000
Training Epoch: 33 [29696/50000]	Loss: 1.1614	LR: 0.050000
Training Epoch: 33 [29824/50000]	Loss: 1.0263	LR: 0.050000
Training Epoch: 33 [29952/50000]	Loss: 1.2059	LR: 0.050000
Training Epoch: 33 [30080/50000]	Loss: 0.8700	LR: 0.050000
Training Epoch: 33 [30208/50000]	Loss: 1.0313	LR: 0.050000
Training Epoch: 33 [30336/50000]	Loss: 1.0711	LR: 0.050000
Training Epoch: 33 [30464/50000]	Loss: 0.9011	LR: 0.050000
Training Epoch: 33 [30592/50000]	Loss: 1.0444	LR: 0.050000
Training Epoch: 33 [30720/50000]	Loss: 1.1109	LR: 0.050000
Training Epoch: 33 [30848/50000]	Loss: 0.9874	LR: 0.050000
Training Epoch: 33 [30976/50000]	Loss: 1.1778	LR: 0.050000
Training Epoch: 33 [31104/50000]	Loss: 1.0473	LR: 0.050000
Training Epoch: 33 [31232/50000]	Loss: 1.2674	LR: 0.050000
Training Epoch: 33 [31360/50000]	Loss: 0.9918	LR: 0.050000
Training Epoch: 33 [31488/50000]	Loss: 0.9846	LR: 0.050000
Training Epoch: 33 [31616/50000]	Loss: 0.8808	LR: 0.050000
Training Epoch: 33 [31744/50000]	Loss: 0.9211	LR: 0.050000
Training Epoch: 33 [31872/50000]	Loss: 1.1336	LR: 0.050000
Training Epoch: 33 [32000/50000]	Loss: 1.1288	LR: 0.050000
Training Epoch: 33 [32128/50000]	Loss: 0.9434	LR: 0.050000
Training Epoch: 33 [32256/50000]	Loss: 1.0072	LR: 0.050000
Training Epoch: 33 [32384/50000]	Loss: 1.1995	LR: 0.050000
Training Epoch: 33 [32512/50000]	Loss: 0.9819	LR: 0.050000
Training Epoch: 33 [32640/50000]	Loss: 1.2331	LR: 0.050000
Training Epoch: 33 [32768/50000]	Loss: 0.8902	LR: 0.050000
Training Epoch: 33 [32896/50000]	Loss: 1.1789	LR: 0.050000
Training Epoch: 33 [33024/50000]	Loss: 1.0138	LR: 0.050000
Training Epoch: 33 [33152/50000]	Loss: 0.8340	LR: 0.050000
Training Epoch: 33 [33280/50000]	Loss: 1.0538	LR: 0.050000
Training Epoch: 33 [33408/50000]	Loss: 1.0500	LR: 0.050000
Training Epoch: 33 [33536/50000]	Loss: 0.9720	LR: 0.050000
Training Epoch: 33 [33664/50000]	Loss: 0.9278	LR: 0.050000
Training Epoch: 33 [33792/50000]	Loss: 1.1838	LR: 0.050000
Training Epoch: 33 [33920/50000]	Loss: 1.1207	LR: 0.050000
Training Epoch: 33 [34048/50000]	Loss: 1.1167	LR: 0.050000
Training Epoch: 33 [34176/50000]	Loss: 1.1809	LR: 0.050000
Training Epoch: 33 [34304/50000]	Loss: 1.0023	LR: 0.050000
Training Epoch: 33 [34432/50000]	Loss: 0.9857	LR: 0.050000
Training Epoch: 33 [34560/50000]	Loss: 1.1457	LR: 0.050000
Training Epoch: 33 [34688/50000]	Loss: 1.0846	LR: 0.050000
Training Epoch: 33 [34816/50000]	Loss: 1.0097	LR: 0.050000
Training Epoch: 33 [34944/50000]	Loss: 1.1493	LR: 0.050000
Training Epoch: 33 [35072/50000]	Loss: 0.9095	LR: 0.050000
Training Epoch: 33 [35200/50000]	Loss: 1.3012	LR: 0.050000
Training Epoch: 33 [35328/50000]	Loss: 0.9479	LR: 0.050000
Training Epoch: 33 [35456/50000]	Loss: 0.8205	LR: 0.050000
Training Epoch: 33 [35584/50000]	Loss: 0.9674	LR: 0.050000
Training Epoch: 33 [35712/50000]	Loss: 1.0472	LR: 0.050000
Training Epoch: 33 [35840/50000]	Loss: 0.9753	LR: 0.050000
Training Epoch: 33 [35968/50000]	Loss: 1.0505	LR: 0.050000
Training Epoch: 33 [36096/50000]	Loss: 1.0877	LR: 0.050000
Training Epoch: 33 [36224/50000]	Loss: 1.2714	LR: 0.050000
Training Epoch: 33 [36352/50000]	Loss: 0.9698	LR: 0.050000
Training Epoch: 33 [36480/50000]	Loss: 1.0789	LR: 0.050000
Training Epoch: 33 [36608/50000]	Loss: 1.0494	LR: 0.050000
Training Epoch: 33 [36736/50000]	Loss: 1.0527	LR: 0.050000
Training Epoch: 33 [36864/50000]	Loss: 0.9937	LR: 0.050000
Training Epoch: 33 [36992/50000]	Loss: 0.9268	LR: 0.050000
Training Epoch: 33 [37120/50000]	Loss: 1.2201	LR: 0.050000
Training Epoch: 33 [37248/50000]	Loss: 1.0616	LR: 0.050000
Training Epoch: 33 [37376/50000]	Loss: 1.0737	LR: 0.050000
Training Epoch: 33 [37504/50000]	Loss: 0.9589	LR: 0.050000
Training Epoch: 33 [37632/50000]	Loss: 0.8447	LR: 0.050000
Training Epoch: 33 [37760/50000]	Loss: 1.1323	LR: 0.050000
Training Epoch: 33 [37888/50000]	Loss: 1.0749	LR: 0.050000
Training Epoch: 33 [38016/50000]	Loss: 1.2611	LR: 0.050000
Training Epoch: 33 [38144/50000]	Loss: 0.9293	LR: 0.050000
Training Epoch: 33 [38272/50000]	Loss: 1.0481	LR: 0.050000
Training Epoch: 33 [38400/50000]	Loss: 0.9836	LR: 0.050000
Training Epoch: 33 [38528/50000]	Loss: 1.1844	LR: 0.050000
Training Epoch: 33 [38656/50000]	Loss: 1.0088	LR: 0.050000
Training Epoch: 33 [38784/50000]	Loss: 0.9216	LR: 0.050000
Training Epoch: 33 [38912/50000]	Loss: 0.9403	LR: 0.050000
Training Epoch: 33 [39040/50000]	Loss: 1.2047	LR: 0.050000
Training Epoch: 33 [39168/50000]	Loss: 1.2538	LR: 0.050000
Training Epoch: 33 [39296/50000]	Loss: 1.0317	LR: 0.050000
Training Epoch: 33 [39424/50000]	Loss: 1.2198	LR: 0.050000
Training Epoch: 33 [39552/50000]	Loss: 1.2820	LR: 0.050000
Training Epoch: 33 [39680/50000]	Loss: 1.1318	LR: 0.050000
Training Epoch: 33 [39808/50000]	Loss: 1.0761	LR: 0.050000
Training Epoch: 33 [39936/50000]	Loss: 0.7886	LR: 0.050000
Training Epoch: 33 [40064/50000]	Loss: 1.1849	LR: 0.050000
Training Epoch: 33 [40192/50000]	Loss: 1.1912	LR: 0.050000
Training Epoch: 33 [40320/50000]	Loss: 0.9522	LR: 0.050000
Training Epoch: 33 [40448/50000]	Loss: 1.1644	LR: 0.050000
Training Epoch: 33 [40576/50000]	Loss: 1.0325	LR: 0.050000
Training Epoch: 33 [40704/50000]	Loss: 1.2708	LR: 0.050000
Training Epoch: 33 [40832/50000]	Loss: 1.2560	LR: 0.050000
Training Epoch: 33 [40960/50000]	Loss: 1.0374	LR: 0.050000
Training Epoch: 33 [41088/50000]	Loss: 1.0801	LR: 0.050000
Training Epoch: 33 [41216/50000]	Loss: 1.0915	LR: 0.050000
Training Epoch: 33 [41344/50000]	Loss: 1.0044	LR: 0.050000
Training Epoch: 33 [41472/50000]	Loss: 1.3895	LR: 0.050000
Training Epoch: 33 [41600/50000]	Loss: 1.0185	LR: 0.050000
Training Epoch: 33 [41728/50000]	Loss: 1.0363	LR: 0.050000
Training Epoch: 33 [41856/50000]	Loss: 1.0123	LR: 0.050000
Training Epoch: 33 [41984/50000]	Loss: 1.1303	LR: 0.050000
Training Epoch: 33 [42112/50000]	Loss: 0.9317	LR: 0.050000
Training Epoch: 33 [42240/50000]	Loss: 1.0375	LR: 0.050000
Training Epoch: 33 [42368/50000]	Loss: 1.0046	LR: 0.050000
Training Epoch: 33 [42496/50000]	Loss: 0.9796	LR: 0.050000
Training Epoch: 33 [42624/50000]	Loss: 1.0531	LR: 0.050000
Training Epoch: 33 [42752/50000]	Loss: 1.1802	LR: 0.050000
Training Epoch: 33 [42880/50000]	Loss: 0.9071	LR: 0.050000
Training Epoch: 33 [43008/50000]	Loss: 1.0333	LR: 0.050000
Training Epoch: 33 [43136/50000]	Loss: 0.9652	LR: 0.050000
Training Epoch: 33 [43264/50000]	Loss: 1.1825	LR: 0.050000
Training Epoch: 33 [43392/50000]	Loss: 1.1945	LR: 0.050000
Training Epoch: 33 [43520/50000]	Loss: 1.0272	LR: 0.050000
Training Epoch: 33 [43648/50000]	Loss: 1.1360	LR: 0.050000
Training Epoch: 33 [43776/50000]	Loss: 1.0911	LR: 0.050000
Training Epoch: 33 [43904/50000]	Loss: 1.0796	LR: 0.050000
Training Epoch: 33 [44032/50000]	Loss: 1.0807	LR: 0.050000
Training Epoch: 33 [44160/50000]	Loss: 1.1013	LR: 0.050000
Training Epoch: 33 [44288/50000]	Loss: 0.9742	LR: 0.050000
Training Epoch: 33 [44416/50000]	Loss: 0.9444	LR: 0.050000
Training Epoch: 33 [44544/50000]	Loss: 0.9952	LR: 0.050000
Training Epoch: 33 [44672/50000]	Loss: 1.0427	LR: 0.050000
Training Epoch: 33 [44800/50000]	Loss: 0.9102	LR: 0.050000
Training Epoch: 33 [44928/50000]	Loss: 0.9413	LR: 0.050000
Training Epoch: 33 [45056/50000]	Loss: 1.0654	LR: 0.050000
Training Epoch: 33 [45184/50000]	Loss: 1.0494	LR: 0.050000
Training Epoch: 33 [45312/50000]	Loss: 0.9887	LR: 0.050000
Training Epoch: 33 [45440/50000]	Loss: 1.1274	LR: 0.050000
Training Epoch: 33 [45568/50000]	Loss: 1.1350	LR: 0.050000
Training Epoch: 33 [45696/50000]	Loss: 0.9972	LR: 0.050000
Training Epoch: 33 [45824/50000]	Loss: 1.1590	LR: 0.050000
Training Epoch: 33 [45952/50000]	Loss: 1.1732	LR: 0.050000
Training Epoch: 33 [46080/50000]	Loss: 0.9618	LR: 0.050000
Training Epoch: 33 [46208/50000]	Loss: 0.9564	LR: 0.050000
Training Epoch: 33 [46336/50000]	Loss: 1.4482	LR: 0.050000
Training Epoch: 33 [46464/50000]	Loss: 0.9821	LR: 0.050000
Training Epoch: 33 [46592/50000]	Loss: 1.0959	LR: 0.050000
Training Epoch: 33 [46720/50000]	Loss: 1.1172	LR: 0.050000
Training Epoch: 33 [46848/50000]	Loss: 1.1639	LR: 0.050000
Training Epoch: 33 [46976/50000]	Loss: 0.8961	LR: 0.050000
Training Epoch: 33 [47104/50000]	Loss: 0.9601	LR: 0.050000
Training Epoch: 33 [47232/50000]	Loss: 1.0678	LR: 0.050000
Training Epoch: 33 [47360/50000]	Loss: 1.2416	LR: 0.050000
Training Epoch: 33 [47488/50000]	Loss: 1.0655	LR: 0.050000
Training Epoch: 33 [47616/50000]	Loss: 1.1246	LR: 0.050000
Training Epoch: 33 [47744/50000]	Loss: 1.3266	LR: 0.050000
Training Epoch: 33 [47872/50000]	Loss: 1.1838	LR: 0.050000
Training Epoch: 33 [48000/50000]	Loss: 1.1136	LR: 0.050000
Training Epoch: 33 [48128/50000]	Loss: 0.9883	LR: 0.050000
Training Epoch: 33 [48256/50000]	Loss: 1.0518	LR: 0.050000
Training Epoch: 33 [48384/50000]	Loss: 1.0148	LR: 0.050000
Training Epoch: 33 [48512/50000]	Loss: 1.1008	LR: 0.050000
Training Epoch: 33 [48640/50000]	Loss: 0.8303	LR: 0.050000
Training Epoch: 33 [48768/50000]	Loss: 1.2223	LR: 0.050000
Training Epoch: 33 [48896/50000]	Loss: 1.0871	LR: 0.050000
Training Epoch: 33 [49024/50000]	Loss: 0.9537	LR: 0.050000
Training Epoch: 33 [49152/50000]	Loss: 1.0844	LR: 0.050000
Training Epoch: 33 [49280/50000]	Loss: 1.0170	LR: 0.050000
Training Epoch: 33 [49408/50000]	Loss: 1.2662	LR: 0.050000
Training Epoch: 33 [49536/50000]	Loss: 1.0966	LR: 0.050000
Training Epoch: 33 [49664/50000]	Loss: 1.0480	LR: 0.050000
Training Epoch: 33 [49792/50000]	Loss: 1.0146	LR: 0.050000
Training Epoch: 33 [49920/50000]	Loss: 1.0237	LR: 0.050000
Training Epoch: 33 [50000/50000]	Loss: 0.9693	LR: 0.050000
epoch 33 training time consumed: 53.98s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  121112 GB |  121112 GB |
|       from large pool |  123392 KB |    1034 MB |  120993 GB |  120992 GB |
|       from small pool |   10798 KB |      13 MB |     119 GB |     119 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  121112 GB |  121112 GB |
|       from large pool |  123392 KB |    1034 MB |  120993 GB |  120992 GB |
|       from small pool |   10798 KB |      13 MB |     119 GB |     119 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   53298 GB |   53298 GB |
|       from large pool |  155136 KB |  433088 KB |   53166 GB |   53166 GB |
|       from small pool |    1490 KB |    3494 KB |     131 GB |     131 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    4673 K  |    4673 K  |
|       from large pool |      24    |      65    |    2439 K  |    2439 K  |
|       from small pool |     231    |     274    |    2234 K  |    2234 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    4673 K  |    4673 K  |
|       from large pool |      24    |      65    |    2439 K  |    2439 K  |
|       from small pool |     231    |     274    |    2234 K  |    2234 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2311 K  |    2311 K  |
|       from large pool |       9    |      14    |    1180 K  |    1180 K  |
|       from small pool |      12    |      16    |    1130 K  |    1130 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 33, Average loss: 0.0120, Accuracy: 0.5995, Time consumed:3.47s

Training Epoch: 34 [128/50000]	Loss: 0.9311	LR: 0.050000
Training Epoch: 34 [256/50000]	Loss: 0.8496	LR: 0.050000
Training Epoch: 34 [384/50000]	Loss: 1.1864	LR: 0.050000
Training Epoch: 34 [512/50000]	Loss: 0.9372	LR: 0.050000
Training Epoch: 34 [640/50000]	Loss: 1.0883	LR: 0.050000
Training Epoch: 34 [768/50000]	Loss: 0.7560	LR: 0.050000
Training Epoch: 34 [896/50000]	Loss: 0.7338	LR: 0.050000
Training Epoch: 34 [1024/50000]	Loss: 0.8951	LR: 0.050000
Training Epoch: 34 [1152/50000]	Loss: 0.7637	LR: 0.050000
Training Epoch: 34 [1280/50000]	Loss: 0.7528	LR: 0.050000
Training Epoch: 34 [1408/50000]	Loss: 0.9116	LR: 0.050000
Training Epoch: 34 [1536/50000]	Loss: 0.8789	LR: 0.050000
Training Epoch: 34 [1664/50000]	Loss: 0.8749	LR: 0.050000
Training Epoch: 34 [1792/50000]	Loss: 0.9723	LR: 0.050000
Training Epoch: 34 [1920/50000]	Loss: 1.0812	LR: 0.050000
Training Epoch: 34 [2048/50000]	Loss: 0.8723	LR: 0.050000
Training Epoch: 34 [2176/50000]	Loss: 0.9589	LR: 0.050000
Training Epoch: 34 [2304/50000]	Loss: 1.1160	LR: 0.050000
Training Epoch: 34 [2432/50000]	Loss: 0.8280	LR: 0.050000
Training Epoch: 34 [2560/50000]	Loss: 0.9300	LR: 0.050000
Training Epoch: 34 [2688/50000]	Loss: 0.9472	LR: 0.050000
Training Epoch: 34 [2816/50000]	Loss: 0.7982	LR: 0.050000
Training Epoch: 34 [2944/50000]	Loss: 0.9303	LR: 0.050000
Training Epoch: 34 [3072/50000]	Loss: 0.8246	LR: 0.050000
Training Epoch: 34 [3200/50000]	Loss: 0.8209	LR: 0.050000
Training Epoch: 34 [3328/50000]	Loss: 0.8653	LR: 0.050000
Training Epoch: 34 [3456/50000]	Loss: 1.1110	LR: 0.050000
Training Epoch: 34 [3584/50000]	Loss: 0.9671	LR: 0.050000
Training Epoch: 34 [3712/50000]	Loss: 0.7772	LR: 0.050000
Training Epoch: 34 [3840/50000]	Loss: 0.9882	LR: 0.050000
Training Epoch: 34 [3968/50000]	Loss: 0.8557	LR: 0.050000
Training Epoch: 34 [4096/50000]	Loss: 1.0535	LR: 0.050000
Training Epoch: 34 [4224/50000]	Loss: 1.0107	LR: 0.050000
Training Epoch: 34 [4352/50000]	Loss: 0.7793	LR: 0.050000
Training Epoch: 34 [4480/50000]	Loss: 0.8883	LR: 0.050000
Training Epoch: 34 [4608/50000]	Loss: 0.8441	LR: 0.050000
Training Epoch: 34 [4736/50000]	Loss: 0.9517	LR: 0.050000
Training Epoch: 34 [4864/50000]	Loss: 0.7897	LR: 0.050000
Training Epoch: 34 [4992/50000]	Loss: 0.7446	LR: 0.050000
Training Epoch: 34 [5120/50000]	Loss: 0.9300	LR: 0.050000
Training Epoch: 34 [5248/50000]	Loss: 1.1388	LR: 0.050000
Training Epoch: 34 [5376/50000]	Loss: 0.9850	LR: 0.050000
Training Epoch: 34 [5504/50000]	Loss: 1.0541	LR: 0.050000
Training Epoch: 34 [5632/50000]	Loss: 0.8496	LR: 0.050000
Training Epoch: 34 [5760/50000]	Loss: 1.0362	LR: 0.050000
Training Epoch: 34 [5888/50000]	Loss: 0.7191	LR: 0.050000
Training Epoch: 34 [6016/50000]	Loss: 0.7582	LR: 0.050000
Training Epoch: 34 [6144/50000]	Loss: 0.8784	LR: 0.050000
Training Epoch: 34 [6272/50000]	Loss: 0.8712	LR: 0.050000
Training Epoch: 34 [6400/50000]	Loss: 0.9127	LR: 0.050000
Training Epoch: 34 [6528/50000]	Loss: 0.9991	LR: 0.050000
Training Epoch: 34 [6656/50000]	Loss: 0.9422	LR: 0.050000
Training Epoch: 34 [6784/50000]	Loss: 0.9747	LR: 0.050000
Training Epoch: 34 [6912/50000]	Loss: 0.8165	LR: 0.050000
Training Epoch: 34 [7040/50000]	Loss: 1.0552	LR: 0.050000
Training Epoch: 34 [7168/50000]	Loss: 0.7227	LR: 0.050000
Training Epoch: 34 [7296/50000]	Loss: 0.8880	LR: 0.050000
Training Epoch: 34 [7424/50000]	Loss: 0.9786	LR: 0.050000
Training Epoch: 34 [7552/50000]	Loss: 0.8267	LR: 0.050000
Training Epoch: 34 [7680/50000]	Loss: 0.8630	LR: 0.050000
Training Epoch: 34 [7808/50000]	Loss: 1.0719	LR: 0.050000
Training Epoch: 34 [7936/50000]	Loss: 0.9913	LR: 0.050000
Training Epoch: 34 [8064/50000]	Loss: 1.1703	LR: 0.050000
Training Epoch: 34 [8192/50000]	Loss: 0.9852	LR: 0.050000
Training Epoch: 34 [8320/50000]	Loss: 1.0535	LR: 0.050000
Training Epoch: 34 [8448/50000]	Loss: 0.9577	LR: 0.050000
Training Epoch: 34 [8576/50000]	Loss: 1.0135	LR: 0.050000
Training Epoch: 34 [8704/50000]	Loss: 0.9679	LR: 0.050000
Training Epoch: 34 [8832/50000]	Loss: 0.9060	LR: 0.050000
Training Epoch: 34 [8960/50000]	Loss: 1.0988	LR: 0.050000
Training Epoch: 34 [9088/50000]	Loss: 0.9988	LR: 0.050000
Training Epoch: 34 [9216/50000]	Loss: 1.2255	LR: 0.050000
Training Epoch: 34 [9344/50000]	Loss: 1.1640	LR: 0.050000
Training Epoch: 34 [9472/50000]	Loss: 0.8976	LR: 0.050000
Training Epoch: 34 [9600/50000]	Loss: 0.8171	LR: 0.050000
Training Epoch: 34 [9728/50000]	Loss: 0.7398	LR: 0.050000
Training Epoch: 34 [9856/50000]	Loss: 0.9011	LR: 0.050000
Training Epoch: 34 [9984/50000]	Loss: 0.8877	LR: 0.050000
Training Epoch: 34 [10112/50000]	Loss: 0.7756	LR: 0.050000
Training Epoch: 34 [10240/50000]	Loss: 1.1462	LR: 0.050000
Training Epoch: 34 [10368/50000]	Loss: 0.9978	LR: 0.050000
Training Epoch: 34 [10496/50000]	Loss: 0.9214	LR: 0.050000
Training Epoch: 34 [10624/50000]	Loss: 0.9442	LR: 0.050000
Training Epoch: 34 [10752/50000]	Loss: 0.8334	LR: 0.050000
Training Epoch: 34 [10880/50000]	Loss: 0.8485	LR: 0.050000
Training Epoch: 34 [11008/50000]	Loss: 1.0615	LR: 0.050000
Training Epoch: 34 [11136/50000]	Loss: 0.8887	LR: 0.050000
Training Epoch: 34 [11264/50000]	Loss: 0.8958	LR: 0.050000
Training Epoch: 34 [11392/50000]	Loss: 0.7567	LR: 0.050000
Training Epoch: 34 [11520/50000]	Loss: 0.9738	LR: 0.050000
Training Epoch: 34 [11648/50000]	Loss: 0.9502	LR: 0.050000
Training Epoch: 34 [11776/50000]	Loss: 1.0569	LR: 0.050000
Training Epoch: 34 [11904/50000]	Loss: 0.8720	LR: 0.050000
Training Epoch: 34 [12032/50000]	Loss: 1.0139	LR: 0.050000
Training Epoch: 34 [12160/50000]	Loss: 1.0632	LR: 0.050000
Training Epoch: 34 [12288/50000]	Loss: 1.0564	LR: 0.050000
Training Epoch: 34 [12416/50000]	Loss: 0.9542	LR: 0.050000
Training Epoch: 34 [12544/50000]	Loss: 0.8523	LR: 0.050000
Training Epoch: 34 [12672/50000]	Loss: 0.8624	LR: 0.050000
Training Epoch: 34 [12800/50000]	Loss: 1.0291	LR: 0.050000
Training Epoch: 34 [12928/50000]	Loss: 0.9753	LR: 0.050000
Training Epoch: 34 [13056/50000]	Loss: 0.9183	LR: 0.050000
Training Epoch: 34 [13184/50000]	Loss: 1.1499	LR: 0.050000
Training Epoch: 34 [13312/50000]	Loss: 0.9694	LR: 0.050000
Training Epoch: 34 [13440/50000]	Loss: 1.1275	LR: 0.050000
Training Epoch: 34 [13568/50000]	Loss: 0.8267	LR: 0.050000
Training Epoch: 34 [13696/50000]	Loss: 1.1525	LR: 0.050000
Training Epoch: 34 [13824/50000]	Loss: 0.8260	LR: 0.050000
Training Epoch: 34 [13952/50000]	Loss: 1.0370	LR: 0.050000
Training Epoch: 34 [14080/50000]	Loss: 1.1396	LR: 0.050000
Training Epoch: 34 [14208/50000]	Loss: 0.8690	LR: 0.050000
Training Epoch: 34 [14336/50000]	Loss: 1.0106	LR: 0.050000
Training Epoch: 34 [14464/50000]	Loss: 0.9691	LR: 0.050000
Training Epoch: 34 [14592/50000]	Loss: 1.1245	LR: 0.050000
Training Epoch: 34 [14720/50000]	Loss: 0.8886	LR: 0.050000
Training Epoch: 34 [14848/50000]	Loss: 1.3078	LR: 0.050000
Training Epoch: 34 [14976/50000]	Loss: 0.9443	LR: 0.050000
Training Epoch: 34 [15104/50000]	Loss: 0.9569	LR: 0.050000
Training Epoch: 34 [15232/50000]	Loss: 1.0943	LR: 0.050000
Training Epoch: 34 [15360/50000]	Loss: 1.1475	LR: 0.050000
Training Epoch: 34 [15488/50000]	Loss: 0.7489	LR: 0.050000
Training Epoch: 34 [15616/50000]	Loss: 0.7802	LR: 0.050000
Training Epoch: 34 [15744/50000]	Loss: 0.9387	LR: 0.050000
Training Epoch: 34 [15872/50000]	Loss: 1.0253	LR: 0.050000
Training Epoch: 34 [16000/50000]	Loss: 0.9489	LR: 0.050000
Training Epoch: 34 [16128/50000]	Loss: 0.9265	LR: 0.050000
Training Epoch: 34 [16256/50000]	Loss: 1.1326	LR: 0.050000
Training Epoch: 34 [16384/50000]	Loss: 1.0880	LR: 0.050000
Training Epoch: 34 [16512/50000]	Loss: 0.8593	LR: 0.050000
Training Epoch: 34 [16640/50000]	Loss: 0.8313	LR: 0.050000
Training Epoch: 34 [16768/50000]	Loss: 1.0894	LR: 0.050000
Training Epoch: 34 [16896/50000]	Loss: 1.2256	LR: 0.050000
Training Epoch: 34 [17024/50000]	Loss: 1.0600	LR: 0.050000
Training Epoch: 34 [17152/50000]	Loss: 1.0753	LR: 0.050000
Training Epoch: 34 [17280/50000]	Loss: 1.0179	LR: 0.050000
Training Epoch: 34 [17408/50000]	Loss: 0.8567	LR: 0.050000
Training Epoch: 34 [17536/50000]	Loss: 1.0168	LR: 0.050000
Training Epoch: 34 [17664/50000]	Loss: 0.9334	LR: 0.050000
Training Epoch: 34 [17792/50000]	Loss: 0.9782	LR: 0.050000
Training Epoch: 34 [17920/50000]	Loss: 0.9316	LR: 0.050000
Training Epoch: 34 [18048/50000]	Loss: 0.8577	LR: 0.050000
Training Epoch: 34 [18176/50000]	Loss: 0.9361	LR: 0.050000
Training Epoch: 34 [18304/50000]	Loss: 0.9363	LR: 0.050000
Training Epoch: 34 [18432/50000]	Loss: 0.9488	LR: 0.050000
Training Epoch: 34 [18560/50000]	Loss: 1.0176	LR: 0.050000
Training Epoch: 34 [18688/50000]	Loss: 1.0524	LR: 0.050000
Training Epoch: 34 [18816/50000]	Loss: 0.8995	LR: 0.050000
Training Epoch: 34 [18944/50000]	Loss: 0.9651	LR: 0.050000
Training Epoch: 34 [19072/50000]	Loss: 0.9961	LR: 0.050000
Training Epoch: 34 [19200/50000]	Loss: 0.8554	LR: 0.050000
Training Epoch: 34 [19328/50000]	Loss: 1.2091	LR: 0.050000
Training Epoch: 34 [19456/50000]	Loss: 0.8923	LR: 0.050000
Training Epoch: 34 [19584/50000]	Loss: 1.0423	LR: 0.050000
Training Epoch: 34 [19712/50000]	Loss: 0.9102	LR: 0.050000
Training Epoch: 34 [19840/50000]	Loss: 0.8774	LR: 0.050000
Training Epoch: 34 [19968/50000]	Loss: 0.9346	LR: 0.050000
Training Epoch: 34 [20096/50000]	Loss: 0.9583	LR: 0.050000
Training Epoch: 34 [20224/50000]	Loss: 0.8032	LR: 0.050000
Training Epoch: 34 [20352/50000]	Loss: 1.0288	LR: 0.050000
Training Epoch: 34 [20480/50000]	Loss: 1.0730	LR: 0.050000
Training Epoch: 34 [20608/50000]	Loss: 0.9183	LR: 0.050000
Training Epoch: 34 [20736/50000]	Loss: 0.9827	LR: 0.050000
Training Epoch: 34 [20864/50000]	Loss: 1.0039	LR: 0.050000
Training Epoch: 34 [20992/50000]	Loss: 1.1667	LR: 0.050000
Training Epoch: 34 [21120/50000]	Loss: 1.1340	LR: 0.050000
Training Epoch: 34 [21248/50000]	Loss: 0.9169	LR: 0.050000
Training Epoch: 34 [21376/50000]	Loss: 0.9939	LR: 0.050000
Training Epoch: 34 [21504/50000]	Loss: 1.0234	LR: 0.050000
Training Epoch: 34 [21632/50000]	Loss: 0.6639	LR: 0.050000
Training Epoch: 34 [21760/50000]	Loss: 1.0068	LR: 0.050000
Training Epoch: 34 [21888/50000]	Loss: 1.2145	LR: 0.050000
Training Epoch: 34 [22016/50000]	Loss: 1.0014	LR: 0.050000
Training Epoch: 34 [22144/50000]	Loss: 0.9500	LR: 0.050000
Training Epoch: 34 [22272/50000]	Loss: 0.7819	LR: 0.050000
Training Epoch: 34 [22400/50000]	Loss: 1.0675	LR: 0.050000
Training Epoch: 34 [22528/50000]	Loss: 1.0011	LR: 0.050000
Training Epoch: 34 [22656/50000]	Loss: 1.1584	LR: 0.050000
Training Epoch: 34 [22784/50000]	Loss: 1.0124	LR: 0.050000
Training Epoch: 34 [22912/50000]	Loss: 0.9468	LR: 0.050000
Training Epoch: 34 [23040/50000]	Loss: 1.1551	LR: 0.050000
Training Epoch: 34 [23168/50000]	Loss: 1.1613	LR: 0.050000
Training Epoch: 34 [23296/50000]	Loss: 0.8493	LR: 0.050000
Training Epoch: 34 [23424/50000]	Loss: 1.0512	LR: 0.050000
Training Epoch: 34 [23552/50000]	Loss: 1.1267	LR: 0.050000
Training Epoch: 34 [23680/50000]	Loss: 1.1647	LR: 0.050000
Training Epoch: 34 [23808/50000]	Loss: 0.9531	LR: 0.050000
Training Epoch: 34 [23936/50000]	Loss: 0.9626	LR: 0.050000
Training Epoch: 34 [24064/50000]	Loss: 1.0080	LR: 0.050000
Training Epoch: 34 [24192/50000]	Loss: 1.0477	LR: 0.050000
Training Epoch: 34 [24320/50000]	Loss: 1.1231	LR: 0.050000
Training Epoch: 34 [24448/50000]	Loss: 0.9660	LR: 0.050000
Training Epoch: 34 [24576/50000]	Loss: 0.8382	LR: 0.050000
Training Epoch: 34 [24704/50000]	Loss: 0.9116	LR: 0.050000
Training Epoch: 34 [24832/50000]	Loss: 0.9033	LR: 0.050000
Training Epoch: 34 [24960/50000]	Loss: 1.0283	LR: 0.050000
Training Epoch: 34 [25088/50000]	Loss: 1.1313	LR: 0.050000
Training Epoch: 34 [25216/50000]	Loss: 0.9925	LR: 0.050000
Training Epoch: 34 [25344/50000]	Loss: 1.1110	LR: 0.050000
Training Epoch: 34 [25472/50000]	Loss: 1.0401	LR: 0.050000
Training Epoch: 34 [25600/50000]	Loss: 1.0585	LR: 0.050000
Training Epoch: 34 [25728/50000]	Loss: 1.0186	LR: 0.050000
Training Epoch: 34 [25856/50000]	Loss: 0.9693	LR: 0.050000
Training Epoch: 34 [25984/50000]	Loss: 1.0440	LR: 0.050000
Training Epoch: 34 [26112/50000]	Loss: 0.6997	LR: 0.050000
Training Epoch: 34 [26240/50000]	Loss: 1.0798	LR: 0.050000
Training Epoch: 34 [26368/50000]	Loss: 1.0023	LR: 0.050000
Training Epoch: 34 [26496/50000]	Loss: 0.9686	LR: 0.050000
Training Epoch: 34 [26624/50000]	Loss: 1.1177	LR: 0.050000
Training Epoch: 34 [26752/50000]	Loss: 1.1425	LR: 0.050000
Training Epoch: 34 [26880/50000]	Loss: 1.0300	LR: 0.050000
Training Epoch: 34 [27008/50000]	Loss: 1.1682	LR: 0.050000
Training Epoch: 34 [27136/50000]	Loss: 0.8301	LR: 0.050000
Training Epoch: 34 [27264/50000]	Loss: 1.0263	LR: 0.050000
Training Epoch: 34 [27392/50000]	Loss: 1.0017	LR: 0.050000
Training Epoch: 34 [27520/50000]	Loss: 0.9650	LR: 0.050000
Training Epoch: 34 [27648/50000]	Loss: 0.9750	LR: 0.050000
Training Epoch: 34 [27776/50000]	Loss: 1.0537	LR: 0.050000
Training Epoch: 34 [27904/50000]	Loss: 1.1788	LR: 0.050000
Training Epoch: 34 [28032/50000]	Loss: 1.0609	LR: 0.050000
Training Epoch: 34 [28160/50000]	Loss: 0.9503	LR: 0.050000
Training Epoch: 34 [28288/50000]	Loss: 1.0339	LR: 0.050000
Training Epoch: 34 [28416/50000]	Loss: 0.8877	LR: 0.050000
Training Epoch: 34 [28544/50000]	Loss: 1.1406	LR: 0.050000
Training Epoch: 34 [28672/50000]	Loss: 0.9335	LR: 0.050000
Training Epoch: 34 [28800/50000]	Loss: 1.3595	LR: 0.050000
Training Epoch: 34 [28928/50000]	Loss: 0.9590	LR: 0.050000
Training Epoch: 34 [29056/50000]	Loss: 1.0038	LR: 0.050000
Training Epoch: 34 [29184/50000]	Loss: 1.1082	LR: 0.050000
Training Epoch: 34 [29312/50000]	Loss: 1.0052	LR: 0.050000
Training Epoch: 34 [29440/50000]	Loss: 1.0968	LR: 0.050000
Training Epoch: 34 [29568/50000]	Loss: 0.8280	LR: 0.050000
Training Epoch: 34 [29696/50000]	Loss: 0.8985	LR: 0.050000
Training Epoch: 34 [29824/50000]	Loss: 0.9003	LR: 0.050000
Training Epoch: 34 [29952/50000]	Loss: 1.1459	LR: 0.050000
Training Epoch: 34 [30080/50000]	Loss: 1.0121	LR: 0.050000
Training Epoch: 34 [30208/50000]	Loss: 0.9912	LR: 0.050000
Training Epoch: 34 [30336/50000]	Loss: 0.9212	LR: 0.050000
Training Epoch: 34 [30464/50000]	Loss: 1.4228	LR: 0.050000
Training Epoch: 34 [30592/50000]	Loss: 1.1547	LR: 0.050000
Training Epoch: 34 [30720/50000]	Loss: 1.0781	LR: 0.050000
Training Epoch: 34 [30848/50000]	Loss: 1.1401	LR: 0.050000
Training Epoch: 34 [30976/50000]	Loss: 1.0575	LR: 0.050000
Training Epoch: 34 [31104/50000]	Loss: 0.9296	LR: 0.050000
Training Epoch: 34 [31232/50000]	Loss: 1.2135	LR: 0.050000
Training Epoch: 34 [31360/50000]	Loss: 1.0546	LR: 0.050000
Training Epoch: 34 [31488/50000]	Loss: 0.9455	LR: 0.050000
Training Epoch: 34 [31616/50000]	Loss: 0.7942	LR: 0.050000
Training Epoch: 34 [31744/50000]	Loss: 0.8376	LR: 0.050000
Training Epoch: 34 [31872/50000]	Loss: 0.9983	LR: 0.050000
Training Epoch: 34 [32000/50000]	Loss: 1.0452	LR: 0.050000
Training Epoch: 34 [32128/50000]	Loss: 0.9758	LR: 0.050000
Training Epoch: 34 [32256/50000]	Loss: 1.1417	LR: 0.050000
Training Epoch: 34 [32384/50000]	Loss: 1.0855	LR: 0.050000
Training Epoch: 34 [32512/50000]	Loss: 1.0370	LR: 0.050000
Training Epoch: 34 [32640/50000]	Loss: 0.9772	LR: 0.050000
Training Epoch: 34 [32768/50000]	Loss: 1.1564	LR: 0.050000
Training Epoch: 34 [32896/50000]	Loss: 1.0919	LR: 0.050000
Training Epoch: 34 [33024/50000]	Loss: 1.1166	LR: 0.050000
Training Epoch: 34 [33152/50000]	Loss: 1.1722	LR: 0.050000
Training Epoch: 34 [33280/50000]	Loss: 0.9503	LR: 0.050000
Training Epoch: 34 [33408/50000]	Loss: 1.1281	LR: 0.050000
Training Epoch: 34 [33536/50000]	Loss: 0.9874	LR: 0.050000
Training Epoch: 34 [33664/50000]	Loss: 1.2024	LR: 0.050000
Training Epoch: 34 [33792/50000]	Loss: 1.0176	LR: 0.050000
Training Epoch: 34 [33920/50000]	Loss: 0.9137	LR: 0.050000
Training Epoch: 34 [34048/50000]	Loss: 1.2629	LR: 0.050000
Training Epoch: 34 [34176/50000]	Loss: 1.1314	LR: 0.050000
Training Epoch: 34 [34304/50000]	Loss: 1.0596	LR: 0.050000
Training Epoch: 34 [34432/50000]	Loss: 0.9445	LR: 0.050000
Training Epoch: 34 [34560/50000]	Loss: 0.9913	LR: 0.050000
Training Epoch: 34 [34688/50000]	Loss: 1.2131	LR: 0.050000
Training Epoch: 34 [34816/50000]	Loss: 0.9445	LR: 0.050000
Training Epoch: 34 [34944/50000]	Loss: 0.9785	LR: 0.050000
Training Epoch: 34 [35072/50000]	Loss: 1.1460	LR: 0.050000
Training Epoch: 34 [35200/50000]	Loss: 0.8948	LR: 0.050000
Training Epoch: 34 [35328/50000]	Loss: 1.0480	LR: 0.050000
Training Epoch: 34 [35456/50000]	Loss: 0.9739	LR: 0.050000
Training Epoch: 34 [35584/50000]	Loss: 0.7874	LR: 0.050000
Training Epoch: 34 [35712/50000]	Loss: 1.1752	LR: 0.050000
Training Epoch: 34 [35840/50000]	Loss: 1.1437	LR: 0.050000
Training Epoch: 34 [35968/50000]	Loss: 0.9029	LR: 0.050000
Training Epoch: 34 [36096/50000]	Loss: 0.9040	LR: 0.050000
Training Epoch: 34 [36224/50000]	Loss: 1.1660	LR: 0.050000
Training Epoch: 34 [36352/50000]	Loss: 0.9113	LR: 0.050000
Training Epoch: 34 [36480/50000]	Loss: 0.9783	LR: 0.050000
Training Epoch: 34 [36608/50000]	Loss: 1.1003	LR: 0.050000
Training Epoch: 34 [36736/50000]	Loss: 1.0936	LR: 0.050000
Training Epoch: 34 [36864/50000]	Loss: 1.0573	LR: 0.050000
Training Epoch: 34 [36992/50000]	Loss: 0.8681	LR: 0.050000
Training Epoch: 34 [37120/50000]	Loss: 1.0847	LR: 0.050000
Training Epoch: 34 [37248/50000]	Loss: 1.0930	LR: 0.050000
Training Epoch: 34 [37376/50000]	Loss: 0.8888	LR: 0.050000
Training Epoch: 34 [37504/50000]	Loss: 1.3547	LR: 0.050000
Training Epoch: 34 [37632/50000]	Loss: 0.9953	LR: 0.050000
Training Epoch: 34 [37760/50000]	Loss: 1.1056	LR: 0.050000
Training Epoch: 34 [37888/50000]	Loss: 1.0605	LR: 0.050000
Training Epoch: 34 [38016/50000]	Loss: 0.9451	LR: 0.050000
Training Epoch: 34 [38144/50000]	Loss: 1.1844	LR: 0.050000
Training Epoch: 34 [38272/50000]	Loss: 1.1920	LR: 0.050000
Training Epoch: 34 [38400/50000]	Loss: 1.1995	LR: 0.050000
Training Epoch: 34 [38528/50000]	Loss: 1.0663	LR: 0.050000
Training Epoch: 34 [38656/50000]	Loss: 0.8842	LR: 0.050000
Training Epoch: 34 [38784/50000]	Loss: 1.2893	LR: 0.050000
Training Epoch: 34 [38912/50000]	Loss: 1.1091	LR: 0.050000
Training Epoch: 34 [39040/50000]	Loss: 0.9670	LR: 0.050000
Training Epoch: 34 [39168/50000]	Loss: 1.1691	LR: 0.050000
Training Epoch: 34 [39296/50000]	Loss: 1.2163	LR: 0.050000
Training Epoch: 34 [39424/50000]	Loss: 1.1471	LR: 0.050000
Training Epoch: 34 [39552/50000]	Loss: 1.1400	LR: 0.050000
Training Epoch: 34 [39680/50000]	Loss: 1.1232	LR: 0.050000
Training Epoch: 34 [39808/50000]	Loss: 0.9659	LR: 0.050000
Training Epoch: 34 [39936/50000]	Loss: 0.9431	LR: 0.050000
Training Epoch: 34 [40064/50000]	Loss: 1.0946	LR: 0.050000
Training Epoch: 34 [40192/50000]	Loss: 1.2768	LR: 0.050000
Training Epoch: 34 [40320/50000]	Loss: 0.9916	LR: 0.050000
Training Epoch: 34 [40448/50000]	Loss: 1.0325	LR: 0.050000
Training Epoch: 34 [40576/50000]	Loss: 0.7906	LR: 0.050000
Training Epoch: 34 [40704/50000]	Loss: 0.9502	LR: 0.050000
Training Epoch: 34 [40832/50000]	Loss: 1.0211	LR: 0.050000
Training Epoch: 34 [40960/50000]	Loss: 1.0706	LR: 0.050000
Training Epoch: 34 [41088/50000]	Loss: 1.0568	LR: 0.050000
Training Epoch: 34 [41216/50000]	Loss: 1.1352	LR: 0.050000
Training Epoch: 34 [41344/50000]	Loss: 1.0653	LR: 0.050000
Training Epoch: 34 [41472/50000]	Loss: 0.9993	LR: 0.050000
Training Epoch: 34 [41600/50000]	Loss: 0.9928	LR: 0.050000
Training Epoch: 34 [41728/50000]	Loss: 0.9785	LR: 0.050000
Training Epoch: 34 [41856/50000]	Loss: 1.0188	LR: 0.050000
Training Epoch: 34 [41984/50000]	Loss: 0.9012	LR: 0.050000
Training Epoch: 34 [42112/50000]	Loss: 1.0343	LR: 0.050000
Training Epoch: 34 [42240/50000]	Loss: 1.1117	LR: 0.050000
Training Epoch: 34 [42368/50000]	Loss: 1.0893	LR: 0.050000
Training Epoch: 34 [42496/50000]	Loss: 1.1045	LR: 0.050000
Training Epoch: 34 [42624/50000]	Loss: 1.0620	LR: 0.050000
Training Epoch: 34 [42752/50000]	Loss: 0.9931	LR: 0.050000
Training Epoch: 34 [42880/50000]	Loss: 1.0736	LR: 0.050000
Training Epoch: 34 [43008/50000]	Loss: 0.9606	LR: 0.050000
Training Epoch: 34 [43136/50000]	Loss: 1.1335	LR: 0.050000
Training Epoch: 34 [43264/50000]	Loss: 1.0094	LR: 0.050000
Training Epoch: 34 [43392/50000]	Loss: 1.0556	LR: 0.050000
Training Epoch: 34 [43520/50000]	Loss: 1.0269	LR: 0.050000
Training Epoch: 34 [43648/50000]	Loss: 1.1945	LR: 0.050000
Training Epoch: 34 [43776/50000]	Loss: 1.0844	LR: 0.050000
Training Epoch: 34 [43904/50000]	Loss: 1.0915	LR: 0.050000
Training Epoch: 34 [44032/50000]	Loss: 1.1510	LR: 0.050000
Training Epoch: 34 [44160/50000]	Loss: 1.0299	LR: 0.050000
Training Epoch: 34 [44288/50000]	Loss: 0.8528	LR: 0.050000
Training Epoch: 34 [44416/50000]	Loss: 0.9545	LR: 0.050000
Training Epoch: 34 [44544/50000]	Loss: 0.8890	LR: 0.050000
Training Epoch: 34 [44672/50000]	Loss: 0.9738	LR: 0.050000
Training Epoch: 34 [44800/50000]	Loss: 0.9723	LR: 0.050000
Training Epoch: 34 [44928/50000]	Loss: 0.8990	LR: 0.050000
Training Epoch: 34 [45056/50000]	Loss: 1.1407	LR: 0.050000
Training Epoch: 34 [45184/50000]	Loss: 1.2719	LR: 0.050000
Training Epoch: 34 [45312/50000]	Loss: 0.8646	LR: 0.050000
Training Epoch: 34 [45440/50000]	Loss: 0.8169	LR: 0.050000
Training Epoch: 34 [45568/50000]	Loss: 1.0322	LR: 0.050000
Training Epoch: 34 [45696/50000]	Loss: 1.1087	LR: 0.050000
Training Epoch: 34 [45824/50000]	Loss: 0.8401	LR: 0.050000
Training Epoch: 34 [45952/50000]	Loss: 0.9270	LR: 0.050000
Training Epoch: 34 [46080/50000]	Loss: 1.0444	LR: 0.050000
Training Epoch: 34 [46208/50000]	Loss: 1.0385	LR: 0.050000
Training Epoch: 34 [46336/50000]	Loss: 0.8152	LR: 0.050000
Training Epoch: 34 [46464/50000]	Loss: 1.2336	LR: 0.050000
Training Epoch: 34 [46592/50000]	Loss: 1.0347	LR: 0.050000
Training Epoch: 34 [46720/50000]	Loss: 1.1365	LR: 0.050000
Training Epoch: 34 [46848/50000]	Loss: 0.9652	LR: 0.050000
Training Epoch: 34 [46976/50000]	Loss: 1.0292	LR: 0.050000
Training Epoch: 34 [47104/50000]	Loss: 0.9031	LR: 0.050000
Training Epoch: 34 [47232/50000]	Loss: 1.0477	LR: 0.050000
Training Epoch: 34 [47360/50000]	Loss: 1.0188	LR: 0.050000
Training Epoch: 34 [47488/50000]	Loss: 0.8918	LR: 0.050000
Training Epoch: 34 [47616/50000]	Loss: 0.9026	LR: 0.050000
Training Epoch: 34 [47744/50000]	Loss: 0.9930	LR: 0.050000
Training Epoch: 34 [47872/50000]	Loss: 1.0959	LR: 0.050000
Training Epoch: 34 [48000/50000]	Loss: 1.0734	LR: 0.050000
Training Epoch: 34 [48128/50000]	Loss: 1.0325	LR: 0.050000
Training Epoch: 34 [48256/50000]	Loss: 1.2913	LR: 0.050000
Training Epoch: 34 [48384/50000]	Loss: 1.3527	LR: 0.050000
Training Epoch: 34 [48512/50000]	Loss: 1.1613	LR: 0.050000
Training Epoch: 34 [48640/50000]	Loss: 1.3390	LR: 0.050000
Training Epoch: 34 [48768/50000]	Loss: 0.9598	LR: 0.050000
Training Epoch: 34 [48896/50000]	Loss: 0.8401	LR: 0.050000
Training Epoch: 34 [49024/50000]	Loss: 1.1840	LR: 0.050000
Training Epoch: 34 [49152/50000]	Loss: 0.9609	LR: 0.050000
Training Epoch: 34 [49280/50000]	Loss: 1.0206	LR: 0.050000
Training Epoch: 34 [49408/50000]	Loss: 0.9094	LR: 0.050000
Training Epoch: 34 [49536/50000]	Loss: 1.3966	LR: 0.050000
Training Epoch: 34 [49664/50000]	Loss: 0.9595	LR: 0.050000
Training Epoch: 34 [49792/50000]	Loss: 1.1269	LR: 0.050000
Training Epoch: 34 [49920/50000]	Loss: 1.0525	LR: 0.050000
Training Epoch: 34 [50000/50000]	Loss: 1.2276	LR: 0.050000
epoch 34 training time consumed: 53.93s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  124782 GB |  124782 GB |
|       from large pool |  123392 KB |    1034 MB |  124659 GB |  124659 GB |
|       from small pool |   10798 KB |      13 MB |     122 GB |     122 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  124782 GB |  124782 GB |
|       from large pool |  123392 KB |    1034 MB |  124659 GB |  124659 GB |
|       from small pool |   10798 KB |      13 MB |     122 GB |     122 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   54913 GB |   54913 GB |
|       from large pool |  155136 KB |  433088 KB |   54777 GB |   54777 GB |
|       from small pool |    1490 KB |    3494 KB |     135 GB |     135 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    4815 K  |    4815 K  |
|       from large pool |      24    |      65    |    2513 K  |    2513 K  |
|       from small pool |     231    |     274    |    2301 K  |    2301 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    4815 K  |    4815 K  |
|       from large pool |      24    |      65    |    2513 K  |    2513 K  |
|       from small pool |     231    |     274    |    2301 K  |    2301 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2381 K  |    2381 K  |
|       from large pool |       9    |      14    |    1216 K  |    1216 K  |
|       from small pool |      12    |      16    |    1164 K  |    1164 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 34, Average loss: 0.0113, Accuracy: 0.6133, Time consumed:3.46s

Training Epoch: 35 [128/50000]	Loss: 0.9283	LR: 0.050000
Training Epoch: 35 [256/50000]	Loss: 0.9142	LR: 0.050000
Training Epoch: 35 [384/50000]	Loss: 0.9579	LR: 0.050000
Training Epoch: 35 [512/50000]	Loss: 0.9873	LR: 0.050000
Training Epoch: 35 [640/50000]	Loss: 0.8069	LR: 0.050000
Training Epoch: 35 [768/50000]	Loss: 0.9141	LR: 0.050000
Training Epoch: 35 [896/50000]	Loss: 1.0807	LR: 0.050000
Training Epoch: 35 [1024/50000]	Loss: 0.8988	LR: 0.050000
Training Epoch: 35 [1152/50000]	Loss: 0.9585	LR: 0.050000
Training Epoch: 35 [1280/50000]	Loss: 1.0368	LR: 0.050000
Training Epoch: 35 [1408/50000]	Loss: 0.9052	LR: 0.050000
Training Epoch: 35 [1536/50000]	Loss: 0.9379	LR: 0.050000
Training Epoch: 35 [1664/50000]	Loss: 1.1526	LR: 0.050000
Training Epoch: 35 [1792/50000]	Loss: 0.9851	LR: 0.050000
Training Epoch: 35 [1920/50000]	Loss: 0.9765	LR: 0.050000
Training Epoch: 35 [2048/50000]	Loss: 1.0020	LR: 0.050000
Training Epoch: 35 [2176/50000]	Loss: 1.1039	LR: 0.050000
Training Epoch: 35 [2304/50000]	Loss: 0.9247	LR: 0.050000
Training Epoch: 35 [2432/50000]	Loss: 1.0358	LR: 0.050000
Training Epoch: 35 [2560/50000]	Loss: 0.8111	LR: 0.050000
Training Epoch: 35 [2688/50000]	Loss: 1.1698	LR: 0.050000
Training Epoch: 35 [2816/50000]	Loss: 0.8403	LR: 0.050000
Training Epoch: 35 [2944/50000]	Loss: 0.8398	LR: 0.050000
Training Epoch: 35 [3072/50000]	Loss: 0.8045	LR: 0.050000
Training Epoch: 35 [3200/50000]	Loss: 0.9683	LR: 0.050000
Training Epoch: 35 [3328/50000]	Loss: 0.9766	LR: 0.050000
Training Epoch: 35 [3456/50000]	Loss: 0.9822	LR: 0.050000
Training Epoch: 35 [3584/50000]	Loss: 0.8863	LR: 0.050000
Training Epoch: 35 [3712/50000]	Loss: 0.9046	LR: 0.050000
Training Epoch: 35 [3840/50000]	Loss: 0.9007	LR: 0.050000
Training Epoch: 35 [3968/50000]	Loss: 0.9769	LR: 0.050000
Training Epoch: 35 [4096/50000]	Loss: 0.7990	LR: 0.050000
Training Epoch: 35 [4224/50000]	Loss: 1.0734	LR: 0.050000
Training Epoch: 35 [4352/50000]	Loss: 0.8174	LR: 0.050000
Training Epoch: 35 [4480/50000]	Loss: 0.9374	LR: 0.050000
Training Epoch: 35 [4608/50000]	Loss: 0.9612	LR: 0.050000
Training Epoch: 35 [4736/50000]	Loss: 0.8596	LR: 0.050000
Training Epoch: 35 [4864/50000]	Loss: 0.8304	LR: 0.050000
Training Epoch: 35 [4992/50000]	Loss: 0.8421	LR: 0.050000
Training Epoch: 35 [5120/50000]	Loss: 1.0089	LR: 0.050000
Training Epoch: 35 [5248/50000]	Loss: 0.8116	LR: 0.050000
Training Epoch: 35 [5376/50000]	Loss: 0.8826	LR: 0.050000
Training Epoch: 35 [5504/50000]	Loss: 0.9434	LR: 0.050000
Training Epoch: 35 [5632/50000]	Loss: 0.8337	LR: 0.050000
Training Epoch: 35 [5760/50000]	Loss: 1.2041	LR: 0.050000
Training Epoch: 35 [5888/50000]	Loss: 1.0539	LR: 0.050000
Training Epoch: 35 [6016/50000]	Loss: 1.0458	LR: 0.050000
Training Epoch: 35 [6144/50000]	Loss: 0.9083	LR: 0.050000
Training Epoch: 35 [6272/50000]	Loss: 0.9309	LR: 0.050000
Training Epoch: 35 [6400/50000]	Loss: 0.8092	LR: 0.050000
Training Epoch: 35 [6528/50000]	Loss: 0.9590	LR: 0.050000
Training Epoch: 35 [6656/50000]	Loss: 0.8005	LR: 0.050000
Training Epoch: 35 [6784/50000]	Loss: 0.9987	LR: 0.050000
Training Epoch: 35 [6912/50000]	Loss: 0.8905	LR: 0.050000
Training Epoch: 35 [7040/50000]	Loss: 0.9114	LR: 0.050000
Training Epoch: 35 [7168/50000]	Loss: 0.9053	LR: 0.050000
Training Epoch: 35 [7296/50000]	Loss: 1.0703	LR: 0.050000
Training Epoch: 35 [7424/50000]	Loss: 0.8388	LR: 0.050000
Training Epoch: 35 [7552/50000]	Loss: 1.0385	LR: 0.050000
Training Epoch: 35 [7680/50000]	Loss: 0.9833	LR: 0.050000
Training Epoch: 35 [7808/50000]	Loss: 0.9431	LR: 0.050000
Training Epoch: 35 [7936/50000]	Loss: 0.9429	LR: 0.050000
Training Epoch: 35 [8064/50000]	Loss: 1.0816	LR: 0.050000
Training Epoch: 35 [8192/50000]	Loss: 0.8913	LR: 0.050000
Training Epoch: 35 [8320/50000]	Loss: 0.9762	LR: 0.050000
Training Epoch: 35 [8448/50000]	Loss: 0.8512	LR: 0.050000
Training Epoch: 35 [8576/50000]	Loss: 1.0293	LR: 0.050000
Training Epoch: 35 [8704/50000]	Loss: 1.0010	LR: 0.050000
Training Epoch: 35 [8832/50000]	Loss: 0.9500	LR: 0.050000
Training Epoch: 35 [8960/50000]	Loss: 0.7233	LR: 0.050000
Training Epoch: 35 [9088/50000]	Loss: 1.0390	LR: 0.050000
Training Epoch: 35 [9216/50000]	Loss: 1.0087	LR: 0.050000
Training Epoch: 35 [9344/50000]	Loss: 0.9286	LR: 0.050000
Training Epoch: 35 [9472/50000]	Loss: 0.8199	LR: 0.050000
Training Epoch: 35 [9600/50000]	Loss: 1.1533	LR: 0.050000
Training Epoch: 35 [9728/50000]	Loss: 0.9572	LR: 0.050000
Training Epoch: 35 [9856/50000]	Loss: 0.7509	LR: 0.050000
Training Epoch: 35 [9984/50000]	Loss: 0.8221	LR: 0.050000
Training Epoch: 35 [10112/50000]	Loss: 0.9830	LR: 0.050000
Training Epoch: 35 [10240/50000]	Loss: 0.6837	LR: 0.050000
Training Epoch: 35 [10368/50000]	Loss: 0.9146	LR: 0.050000
Training Epoch: 35 [10496/50000]	Loss: 0.8514	LR: 0.050000
Training Epoch: 35 [10624/50000]	Loss: 0.9391	LR: 0.050000
Training Epoch: 35 [10752/50000]	Loss: 1.1190	LR: 0.050000
Training Epoch: 35 [10880/50000]	Loss: 0.9748	LR: 0.050000
Training Epoch: 35 [11008/50000]	Loss: 0.8271	LR: 0.050000
Training Epoch: 35 [11136/50000]	Loss: 0.7854	LR: 0.050000
Training Epoch: 35 [11264/50000]	Loss: 0.9451	LR: 0.050000
Training Epoch: 35 [11392/50000]	Loss: 0.9926	LR: 0.050000
Training Epoch: 35 [11520/50000]	Loss: 0.9793	LR: 0.050000
Training Epoch: 35 [11648/50000]	Loss: 0.8226	LR: 0.050000
Training Epoch: 35 [11776/50000]	Loss: 0.9638	LR: 0.050000
Training Epoch: 35 [11904/50000]	Loss: 0.9697	LR: 0.050000
Training Epoch: 35 [12032/50000]	Loss: 0.8984	LR: 0.050000
Training Epoch: 35 [12160/50000]	Loss: 1.1522	LR: 0.050000
Training Epoch: 35 [12288/50000]	Loss: 0.9245	LR: 0.050000
Training Epoch: 35 [12416/50000]	Loss: 0.9008	LR: 0.050000
Training Epoch: 35 [12544/50000]	Loss: 0.8742	LR: 0.050000
Training Epoch: 35 [12672/50000]	Loss: 0.8522	LR: 0.050000
Training Epoch: 35 [12800/50000]	Loss: 0.9345	LR: 0.050000
Training Epoch: 35 [12928/50000]	Loss: 0.8229	LR: 0.050000
Training Epoch: 35 [13056/50000]	Loss: 0.7986	LR: 0.050000
Training Epoch: 35 [13184/50000]	Loss: 0.7396	LR: 0.050000
Training Epoch: 35 [13312/50000]	Loss: 0.8963	LR: 0.050000
Training Epoch: 35 [13440/50000]	Loss: 0.6942	LR: 0.050000
Training Epoch: 35 [13568/50000]	Loss: 0.8074	LR: 0.050000
Training Epoch: 35 [13696/50000]	Loss: 1.0383	LR: 0.050000
Training Epoch: 35 [13824/50000]	Loss: 0.7884	LR: 0.050000
Training Epoch: 35 [13952/50000]	Loss: 0.9564	LR: 0.050000
Training Epoch: 35 [14080/50000]	Loss: 0.7703	LR: 0.050000
Training Epoch: 35 [14208/50000]	Loss: 0.9158	LR: 0.050000
Training Epoch: 35 [14336/50000]	Loss: 0.7638	LR: 0.050000
Training Epoch: 35 [14464/50000]	Loss: 0.9814	LR: 0.050000
Training Epoch: 35 [14592/50000]	Loss: 0.8122	LR: 0.050000
Training Epoch: 35 [14720/50000]	Loss: 1.0663	LR: 0.050000
Training Epoch: 35 [14848/50000]	Loss: 0.7818	LR: 0.050000
Training Epoch: 35 [14976/50000]	Loss: 1.0853	LR: 0.050000
Training Epoch: 35 [15104/50000]	Loss: 0.9953	LR: 0.050000
Training Epoch: 35 [15232/50000]	Loss: 0.8624	LR: 0.050000
Training Epoch: 35 [15360/50000]	Loss: 1.0612	LR: 0.050000
Training Epoch: 35 [15488/50000]	Loss: 1.0725	LR: 0.050000
Training Epoch: 35 [15616/50000]	Loss: 0.9877	LR: 0.050000
Training Epoch: 35 [15744/50000]	Loss: 1.0442	LR: 0.050000
Training Epoch: 35 [15872/50000]	Loss: 0.7406	LR: 0.050000
Training Epoch: 35 [16000/50000]	Loss: 0.9745	LR: 0.050000
Training Epoch: 35 [16128/50000]	Loss: 1.0738	LR: 0.050000
Training Epoch: 35 [16256/50000]	Loss: 0.8502	LR: 0.050000
Training Epoch: 35 [16384/50000]	Loss: 0.6747	LR: 0.050000
Training Epoch: 35 [16512/50000]	Loss: 1.0172	LR: 0.050000
Training Epoch: 35 [16640/50000]	Loss: 0.8313	LR: 0.050000
Training Epoch: 35 [16768/50000]	Loss: 1.1989	LR: 0.050000
Training Epoch: 35 [16896/50000]	Loss: 0.9841	LR: 0.050000
Training Epoch: 35 [17024/50000]	Loss: 0.9028	LR: 0.050000
Training Epoch: 35 [17152/50000]	Loss: 0.9161	LR: 0.050000
Training Epoch: 35 [17280/50000]	Loss: 0.8752	LR: 0.050000
Training Epoch: 35 [17408/50000]	Loss: 0.7678	LR: 0.050000
Training Epoch: 35 [17536/50000]	Loss: 1.1034	LR: 0.050000
Training Epoch: 35 [17664/50000]	Loss: 0.9258	LR: 0.050000
Training Epoch: 35 [17792/50000]	Loss: 1.1144	LR: 0.050000
Training Epoch: 35 [17920/50000]	Loss: 0.6869	LR: 0.050000
Training Epoch: 35 [18048/50000]	Loss: 0.9731	LR: 0.050000
Training Epoch: 35 [18176/50000]	Loss: 1.0312	LR: 0.050000
Training Epoch: 35 [18304/50000]	Loss: 1.0183	LR: 0.050000
Training Epoch: 35 [18432/50000]	Loss: 0.9386	LR: 0.050000
Training Epoch: 35 [18560/50000]	Loss: 0.9681	LR: 0.050000
Training Epoch: 35 [18688/50000]	Loss: 0.7572	LR: 0.050000
Training Epoch: 35 [18816/50000]	Loss: 0.9590	LR: 0.050000
Training Epoch: 35 [18944/50000]	Loss: 1.1040	LR: 0.050000
Training Epoch: 35 [19072/50000]	Loss: 1.0189	LR: 0.050000
Training Epoch: 35 [19200/50000]	Loss: 1.3260	LR: 0.050000
Training Epoch: 35 [19328/50000]	Loss: 0.9930	LR: 0.050000
Training Epoch: 35 [19456/50000]	Loss: 0.9528	LR: 0.050000
Training Epoch: 35 [19584/50000]	Loss: 0.9521	LR: 0.050000
Training Epoch: 35 [19712/50000]	Loss: 1.1590	LR: 0.050000
Training Epoch: 35 [19840/50000]	Loss: 0.8687	LR: 0.050000
Training Epoch: 35 [19968/50000]	Loss: 0.7972	LR: 0.050000
Training Epoch: 35 [20096/50000]	Loss: 0.9478	LR: 0.050000
Training Epoch: 35 [20224/50000]	Loss: 0.8708	LR: 0.050000
Training Epoch: 35 [20352/50000]	Loss: 0.9520	LR: 0.050000
Training Epoch: 35 [20480/50000]	Loss: 0.8033	LR: 0.050000
Training Epoch: 35 [20608/50000]	Loss: 1.1285	LR: 0.050000
Training Epoch: 35 [20736/50000]	Loss: 1.1917	LR: 0.050000
Training Epoch: 35 [20864/50000]	Loss: 0.9535	LR: 0.050000
Training Epoch: 35 [20992/50000]	Loss: 0.9148	LR: 0.050000
Training Epoch: 35 [21120/50000]	Loss: 0.9392	LR: 0.050000
Training Epoch: 35 [21248/50000]	Loss: 1.0054	LR: 0.050000
Training Epoch: 35 [21376/50000]	Loss: 1.0020	LR: 0.050000
Training Epoch: 35 [21504/50000]	Loss: 0.9623	LR: 0.050000
Training Epoch: 35 [21632/50000]	Loss: 0.9110	LR: 0.050000
Training Epoch: 35 [21760/50000]	Loss: 0.8440	LR: 0.050000
Training Epoch: 35 [21888/50000]	Loss: 1.1274	LR: 0.050000
Training Epoch: 35 [22016/50000]	Loss: 1.0266	LR: 0.050000
Training Epoch: 35 [22144/50000]	Loss: 1.0016	LR: 0.050000
Training Epoch: 35 [22272/50000]	Loss: 0.8523	LR: 0.050000
Training Epoch: 35 [22400/50000]	Loss: 1.1035	LR: 0.050000
Training Epoch: 35 [22528/50000]	Loss: 0.7959	LR: 0.050000
Training Epoch: 35 [22656/50000]	Loss: 1.0502	LR: 0.050000
Training Epoch: 35 [22784/50000]	Loss: 1.1627	LR: 0.050000
Training Epoch: 35 [22912/50000]	Loss: 0.8358	LR: 0.050000
Training Epoch: 35 [23040/50000]	Loss: 1.0841	LR: 0.050000
Training Epoch: 35 [23168/50000]	Loss: 1.0217	LR: 0.050000
Training Epoch: 35 [23296/50000]	Loss: 0.9051	LR: 0.050000
Training Epoch: 35 [23424/50000]	Loss: 1.1990	LR: 0.050000
Training Epoch: 35 [23552/50000]	Loss: 1.1430	LR: 0.050000
Training Epoch: 35 [23680/50000]	Loss: 0.7878	LR: 0.050000
Training Epoch: 35 [23808/50000]	Loss: 0.8517	LR: 0.050000
Training Epoch: 35 [23936/50000]	Loss: 0.8633	LR: 0.050000
Training Epoch: 35 [24064/50000]	Loss: 0.9730	LR: 0.050000
Training Epoch: 35 [24192/50000]	Loss: 1.0337	LR: 0.050000
Training Epoch: 35 [24320/50000]	Loss: 0.9186	LR: 0.050000
Training Epoch: 35 [24448/50000]	Loss: 0.9261	LR: 0.050000
Training Epoch: 35 [24576/50000]	Loss: 0.9755	LR: 0.050000
Training Epoch: 35 [24704/50000]	Loss: 1.0328	LR: 0.050000
Training Epoch: 35 [24832/50000]	Loss: 1.0416	LR: 0.050000
Training Epoch: 35 [24960/50000]	Loss: 0.9417	LR: 0.050000
Training Epoch: 35 [25088/50000]	Loss: 1.0711	LR: 0.050000
Training Epoch: 35 [25216/50000]	Loss: 0.8984	LR: 0.050000
Training Epoch: 35 [25344/50000]	Loss: 0.9044	LR: 0.050000
Training Epoch: 35 [25472/50000]	Loss: 1.1459	LR: 0.050000
Training Epoch: 35 [25600/50000]	Loss: 0.9295	LR: 0.050000
Training Epoch: 35 [25728/50000]	Loss: 0.9787	LR: 0.050000
Training Epoch: 35 [25856/50000]	Loss: 0.9665	LR: 0.050000
Training Epoch: 35 [25984/50000]	Loss: 0.9540	LR: 0.050000
Training Epoch: 35 [26112/50000]	Loss: 0.8723	LR: 0.050000
Training Epoch: 35 [26240/50000]	Loss: 1.1112	LR: 0.050000
Training Epoch: 35 [26368/50000]	Loss: 0.8715	LR: 0.050000
Training Epoch: 35 [26496/50000]	Loss: 0.8594	LR: 0.050000
Training Epoch: 35 [26624/50000]	Loss: 1.0446	LR: 0.050000
Training Epoch: 35 [26752/50000]	Loss: 0.9959	LR: 0.050000
Training Epoch: 35 [26880/50000]	Loss: 1.0397	LR: 0.050000
Training Epoch: 35 [27008/50000]	Loss: 0.8273	LR: 0.050000
Training Epoch: 35 [27136/50000]	Loss: 0.8994	LR: 0.050000
Training Epoch: 35 [27264/50000]	Loss: 0.9527	LR: 0.050000
Training Epoch: 35 [27392/50000]	Loss: 0.9698	LR: 0.050000
Training Epoch: 35 [27520/50000]	Loss: 0.9927	LR: 0.050000
Training Epoch: 35 [27648/50000]	Loss: 0.7856	LR: 0.050000
Training Epoch: 35 [27776/50000]	Loss: 0.8505	LR: 0.050000
Training Epoch: 35 [27904/50000]	Loss: 0.9346	LR: 0.050000
Training Epoch: 35 [28032/50000]	Loss: 1.0970	LR: 0.050000
Training Epoch: 35 [28160/50000]	Loss: 0.9168	LR: 0.050000
Training Epoch: 35 [28288/50000]	Loss: 0.9509	LR: 0.050000
Training Epoch: 35 [28416/50000]	Loss: 0.8264	LR: 0.050000
Training Epoch: 35 [28544/50000]	Loss: 1.0612	LR: 0.050000
Training Epoch: 35 [28672/50000]	Loss: 1.1232	LR: 0.050000
Training Epoch: 35 [28800/50000]	Loss: 0.9931	LR: 0.050000
Training Epoch: 35 [28928/50000]	Loss: 1.0440	LR: 0.050000
Training Epoch: 35 [29056/50000]	Loss: 0.9746	LR: 0.050000
Training Epoch: 35 [29184/50000]	Loss: 0.7494	LR: 0.050000
Training Epoch: 35 [29312/50000]	Loss: 1.1787	LR: 0.050000
Training Epoch: 35 [29440/50000]	Loss: 0.9835	LR: 0.050000
Training Epoch: 35 [29568/50000]	Loss: 0.9580	LR: 0.050000
Training Epoch: 35 [29696/50000]	Loss: 1.0135	LR: 0.050000
Training Epoch: 35 [29824/50000]	Loss: 0.9314	LR: 0.050000
Training Epoch: 35 [29952/50000]	Loss: 1.0599	LR: 0.050000
Training Epoch: 35 [30080/50000]	Loss: 1.0511	LR: 0.050000
Training Epoch: 35 [30208/50000]	Loss: 1.1144	LR: 0.050000
Training Epoch: 35 [30336/50000]	Loss: 1.1614	LR: 0.050000
Training Epoch: 35 [30464/50000]	Loss: 1.0238	LR: 0.050000
Training Epoch: 35 [30592/50000]	Loss: 1.0516	LR: 0.050000
Training Epoch: 35 [30720/50000]	Loss: 0.9901	LR: 0.050000
Training Epoch: 35 [30848/50000]	Loss: 0.9735	LR: 0.050000
Training Epoch: 35 [30976/50000]	Loss: 0.8540	LR: 0.050000
Training Epoch: 35 [31104/50000]	Loss: 0.9983	LR: 0.050000
Training Epoch: 35 [31232/50000]	Loss: 0.9604	LR: 0.050000
Training Epoch: 35 [31360/50000]	Loss: 0.9057	LR: 0.050000
Training Epoch: 35 [31488/50000]	Loss: 1.1619	LR: 0.050000
Training Epoch: 35 [31616/50000]	Loss: 1.0541	LR: 0.050000
Training Epoch: 35 [31744/50000]	Loss: 1.0646	LR: 0.050000
Training Epoch: 35 [31872/50000]	Loss: 1.1431	LR: 0.050000
Training Epoch: 35 [32000/50000]	Loss: 0.9690	LR: 0.050000
Training Epoch: 35 [32128/50000]	Loss: 0.8365	LR: 0.050000
Training Epoch: 35 [32256/50000]	Loss: 0.9555	LR: 0.050000
Training Epoch: 35 [32384/50000]	Loss: 1.0511	LR: 0.050000
Training Epoch: 35 [32512/50000]	Loss: 0.9751	LR: 0.050000
Training Epoch: 35 [32640/50000]	Loss: 0.9556	LR: 0.050000
Training Epoch: 35 [32768/50000]	Loss: 1.0265	LR: 0.050000
Training Epoch: 35 [32896/50000]	Loss: 0.9257	LR: 0.050000
Training Epoch: 35 [33024/50000]	Loss: 1.3522	LR: 0.050000
Training Epoch: 35 [33152/50000]	Loss: 1.0149	LR: 0.050000
Training Epoch: 35 [33280/50000]	Loss: 0.7776	LR: 0.050000
Training Epoch: 35 [33408/50000]	Loss: 1.1106	LR: 0.050000
Training Epoch: 35 [33536/50000]	Loss: 0.9791	LR: 0.050000
Training Epoch: 35 [33664/50000]	Loss: 0.9545	LR: 0.050000
Training Epoch: 35 [33792/50000]	Loss: 1.0418	LR: 0.050000
Training Epoch: 35 [33920/50000]	Loss: 0.9336	LR: 0.050000
Training Epoch: 35 [34048/50000]	Loss: 0.9832	LR: 0.050000
Training Epoch: 35 [34176/50000]	Loss: 1.2501	LR: 0.050000
Training Epoch: 35 [34304/50000]	Loss: 1.0886	LR: 0.050000
Training Epoch: 35 [34432/50000]	Loss: 0.9784	LR: 0.050000
Training Epoch: 35 [34560/50000]	Loss: 1.0740	LR: 0.050000
Training Epoch: 35 [34688/50000]	Loss: 0.9757	LR: 0.050000
Training Epoch: 35 [34816/50000]	Loss: 1.2604	LR: 0.050000
Training Epoch: 35 [34944/50000]	Loss: 0.9852	LR: 0.050000
Training Epoch: 35 [35072/50000]	Loss: 1.1502	LR: 0.050000
Training Epoch: 35 [35200/50000]	Loss: 1.0623	LR: 0.050000
Training Epoch: 35 [35328/50000]	Loss: 0.9864	LR: 0.050000
Training Epoch: 35 [35456/50000]	Loss: 0.9537	LR: 0.050000
Training Epoch: 35 [35584/50000]	Loss: 0.9854	LR: 0.050000
Training Epoch: 35 [35712/50000]	Loss: 0.8737	LR: 0.050000
Training Epoch: 35 [35840/50000]	Loss: 1.1543	LR: 0.050000
Training Epoch: 35 [35968/50000]	Loss: 0.8907	LR: 0.050000
Training Epoch: 35 [36096/50000]	Loss: 0.9289	LR: 0.050000
Training Epoch: 35 [36224/50000]	Loss: 1.0859	LR: 0.050000
Training Epoch: 35 [36352/50000]	Loss: 1.0074	LR: 0.050000
Training Epoch: 35 [36480/50000]	Loss: 0.9935	LR: 0.050000
Training Epoch: 35 [36608/50000]	Loss: 1.1031	LR: 0.050000
Training Epoch: 35 [36736/50000]	Loss: 1.1662	LR: 0.050000
Training Epoch: 35 [36864/50000]	Loss: 0.8993	LR: 0.050000
Training Epoch: 35 [36992/50000]	Loss: 0.9662	LR: 0.050000
Training Epoch: 35 [37120/50000]	Loss: 0.7675	LR: 0.050000
Training Epoch: 35 [37248/50000]	Loss: 1.1111	LR: 0.050000
Training Epoch: 35 [37376/50000]	Loss: 0.8843	LR: 0.050000
Training Epoch: 35 [37504/50000]	Loss: 0.9318	LR: 0.050000
Training Epoch: 35 [37632/50000]	Loss: 1.0439	LR: 0.050000
Training Epoch: 35 [37760/50000]	Loss: 0.9900	LR: 0.050000
Training Epoch: 35 [37888/50000]	Loss: 1.1036	LR: 0.050000
Training Epoch: 35 [38016/50000]	Loss: 1.0529	LR: 0.050000
Training Epoch: 35 [38144/50000]	Loss: 0.9062	LR: 0.050000
Training Epoch: 35 [38272/50000]	Loss: 0.9211	LR: 0.050000
Training Epoch: 35 [38400/50000]	Loss: 0.8355	LR: 0.050000
Training Epoch: 35 [38528/50000]	Loss: 0.8665	LR: 0.050000
Training Epoch: 35 [38656/50000]	Loss: 1.1759	LR: 0.050000
Training Epoch: 35 [38784/50000]	Loss: 1.0951	LR: 0.050000
Training Epoch: 35 [38912/50000]	Loss: 1.0451	LR: 0.050000
Training Epoch: 35 [39040/50000]	Loss: 1.2279	LR: 0.050000
Training Epoch: 35 [39168/50000]	Loss: 1.1399	LR: 0.050000
Training Epoch: 35 [39296/50000]	Loss: 0.9159	LR: 0.050000
Training Epoch: 35 [39424/50000]	Loss: 0.9446	LR: 0.050000
Training Epoch: 35 [39552/50000]	Loss: 0.8797	LR: 0.050000
Training Epoch: 35 [39680/50000]	Loss: 1.0676	LR: 0.050000
Training Epoch: 35 [39808/50000]	Loss: 1.2356	LR: 0.050000
Training Epoch: 35 [39936/50000]	Loss: 1.1887	LR: 0.050000
Training Epoch: 35 [40064/50000]	Loss: 1.1517	LR: 0.050000
Training Epoch: 35 [40192/50000]	Loss: 0.7870	LR: 0.050000
Training Epoch: 35 [40320/50000]	Loss: 1.1156	LR: 0.050000
Training Epoch: 35 [40448/50000]	Loss: 1.0364	LR: 0.050000
Training Epoch: 35 [40576/50000]	Loss: 1.1492	LR: 0.050000
Training Epoch: 35 [40704/50000]	Loss: 0.9970	LR: 0.050000
Training Epoch: 35 [40832/50000]	Loss: 1.0712	LR: 0.050000
Training Epoch: 35 [40960/50000]	Loss: 0.8753	LR: 0.050000
Training Epoch: 35 [41088/50000]	Loss: 0.9202	LR: 0.050000
Training Epoch: 35 [41216/50000]	Loss: 1.0558	LR: 0.050000
Training Epoch: 35 [41344/50000]	Loss: 0.9822	LR: 0.050000
Training Epoch: 35 [41472/50000]	Loss: 0.8306	LR: 0.050000
Training Epoch: 35 [41600/50000]	Loss: 0.8417	LR: 0.050000
Training Epoch: 35 [41728/50000]	Loss: 0.9420	LR: 0.050000
Training Epoch: 35 [41856/50000]	Loss: 1.1615	LR: 0.050000
Training Epoch: 35 [41984/50000]	Loss: 0.8942	LR: 0.050000
Training Epoch: 35 [42112/50000]	Loss: 0.8365	LR: 0.050000
Training Epoch: 35 [42240/50000]	Loss: 1.1256	LR: 0.050000
Training Epoch: 35 [42368/50000]	Loss: 1.1682	LR: 0.050000
Training Epoch: 35 [42496/50000]	Loss: 0.9083	LR: 0.050000
Training Epoch: 35 [42624/50000]	Loss: 0.9740	LR: 0.050000
Training Epoch: 35 [42752/50000]	Loss: 0.7511	LR: 0.050000
Training Epoch: 35 [42880/50000]	Loss: 1.0639	LR: 0.050000
Training Epoch: 35 [43008/50000]	Loss: 0.8489	LR: 0.050000
Training Epoch: 35 [43136/50000]	Loss: 0.8617	LR: 0.050000
Training Epoch: 35 [43264/50000]	Loss: 0.9494	LR: 0.050000
Training Epoch: 35 [43392/50000]	Loss: 1.1168	LR: 0.050000
Training Epoch: 35 [43520/50000]	Loss: 1.1020	LR: 0.050000
Training Epoch: 35 [43648/50000]	Loss: 1.0852	LR: 0.050000
Training Epoch: 35 [43776/50000]	Loss: 1.0809	LR: 0.050000
Training Epoch: 35 [43904/50000]	Loss: 0.9378	LR: 0.050000
Training Epoch: 35 [44032/50000]	Loss: 1.1398	LR: 0.050000
Training Epoch: 35 [44160/50000]	Loss: 0.8411	LR: 0.050000
Training Epoch: 35 [44288/50000]	Loss: 1.1523	LR: 0.050000
Training Epoch: 35 [44416/50000]	Loss: 1.2180	LR: 0.050000
Training Epoch: 35 [44544/50000]	Loss: 1.0578	LR: 0.050000
Training Epoch: 35 [44672/50000]	Loss: 0.9971	LR: 0.050000
Training Epoch: 35 [44800/50000]	Loss: 0.9411	LR: 0.050000
Training Epoch: 35 [44928/50000]	Loss: 1.1597	LR: 0.050000
Training Epoch: 35 [45056/50000]	Loss: 1.1554	LR: 0.050000
Training Epoch: 35 [45184/50000]	Loss: 0.9227	LR: 0.050000
Training Epoch: 35 [45312/50000]	Loss: 1.0551	LR: 0.050000
Training Epoch: 35 [45440/50000]	Loss: 1.0863	LR: 0.050000
Training Epoch: 35 [45568/50000]	Loss: 1.0251	LR: 0.050000
Training Epoch: 35 [45696/50000]	Loss: 1.1958	LR: 0.050000
Training Epoch: 35 [45824/50000]	Loss: 1.0259	LR: 0.050000
Training Epoch: 35 [45952/50000]	Loss: 1.0687	LR: 0.050000
Training Epoch: 35 [46080/50000]	Loss: 1.2336	LR: 0.050000
Training Epoch: 35 [46208/50000]	Loss: 0.9799	LR: 0.050000
Training Epoch: 35 [46336/50000]	Loss: 0.9371	LR: 0.050000
Training Epoch: 35 [46464/50000]	Loss: 1.0637	LR: 0.050000
Training Epoch: 35 [46592/50000]	Loss: 0.9674	LR: 0.050000
Training Epoch: 35 [46720/50000]	Loss: 1.1355	LR: 0.050000
Training Epoch: 35 [46848/50000]	Loss: 0.9959	LR: 0.050000
Training Epoch: 35 [46976/50000]	Loss: 0.9571	LR: 0.050000
Training Epoch: 35 [47104/50000]	Loss: 1.0442	LR: 0.050000
Training Epoch: 35 [47232/50000]	Loss: 1.1766	LR: 0.050000
Training Epoch: 35 [47360/50000]	Loss: 1.1225	LR: 0.050000
Training Epoch: 35 [47488/50000]	Loss: 1.0846	LR: 0.050000
Training Epoch: 35 [47616/50000]	Loss: 1.0373	LR: 0.050000
Training Epoch: 35 [47744/50000]	Loss: 0.9087	LR: 0.050000
Training Epoch: 35 [47872/50000]	Loss: 0.9944	LR: 0.050000
Training Epoch: 35 [48000/50000]	Loss: 0.9211	LR: 0.050000
Training Epoch: 35 [48128/50000]	Loss: 1.0763	LR: 0.050000
Training Epoch: 35 [48256/50000]	Loss: 0.7968	LR: 0.050000
Training Epoch: 35 [48384/50000]	Loss: 0.9431	LR: 0.050000
Training Epoch: 35 [48512/50000]	Loss: 1.0126	LR: 0.050000
Training Epoch: 35 [48640/50000]	Loss: 1.1114	LR: 0.050000
Training Epoch: 35 [48768/50000]	Loss: 1.0593	LR: 0.050000
Training Epoch: 35 [48896/50000]	Loss: 1.0461	LR: 0.050000
Training Epoch: 35 [49024/50000]	Loss: 0.8762	LR: 0.050000
Training Epoch: 35 [49152/50000]	Loss: 1.2289	LR: 0.050000
Training Epoch: 35 [49280/50000]	Loss: 0.9215	LR: 0.050000
Training Epoch: 35 [49408/50000]	Loss: 1.0901	LR: 0.050000
Training Epoch: 35 [49536/50000]	Loss: 1.1009	LR: 0.050000
Training Epoch: 35 [49664/50000]	Loss: 1.0722	LR: 0.050000
Training Epoch: 35 [49792/50000]	Loss: 1.0795	LR: 0.050000
Training Epoch: 35 [49920/50000]	Loss: 0.9432	LR: 0.050000
Training Epoch: 35 [50000/50000]	Loss: 0.8570	LR: 0.050000
epoch 35 training time consumed: 53.87s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  128452 GB |  128452 GB |
|       from large pool |  123392 KB |    1034 MB |  128325 GB |  128325 GB |
|       from small pool |   10798 KB |      13 MB |     126 GB |     126 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  128452 GB |  128452 GB |
|       from large pool |  123392 KB |    1034 MB |  128325 GB |  128325 GB |
|       from small pool |   10798 KB |      13 MB |     126 GB |     126 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   56528 GB |   56528 GB |
|       from large pool |  155136 KB |  433088 KB |   56388 GB |   56388 GB |
|       from small pool |    1490 KB |    3494 KB |     139 GB |     139 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    4956 K  |    4956 K  |
|       from large pool |      24    |      65    |    2587 K  |    2587 K  |
|       from small pool |     231    |     274    |    2369 K  |    2369 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    4956 K  |    4956 K  |
|       from large pool |      24    |      65    |    2587 K  |    2587 K  |
|       from small pool |     231    |     274    |    2369 K  |    2369 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2451 K  |    2451 K  |
|       from large pool |       9    |      14    |    1252 K  |    1252 K  |
|       from small pool |      12    |      16    |    1198 K  |    1198 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 35, Average loss: 0.0113, Accuracy: 0.6155, Time consumed:3.46s

Training Epoch: 36 [128/50000]	Loss: 0.7273	LR: 0.050000
Training Epoch: 36 [256/50000]	Loss: 0.9736	LR: 0.050000
Training Epoch: 36 [384/50000]	Loss: 1.0564	LR: 0.050000
Training Epoch: 36 [512/50000]	Loss: 0.9739	LR: 0.050000
Training Epoch: 36 [640/50000]	Loss: 0.9516	LR: 0.050000
Training Epoch: 36 [768/50000]	Loss: 0.8410	LR: 0.050000
Training Epoch: 36 [896/50000]	Loss: 0.8088	LR: 0.050000
Training Epoch: 36 [1024/50000]	Loss: 0.7827	LR: 0.050000
Training Epoch: 36 [1152/50000]	Loss: 0.8740	LR: 0.050000
Training Epoch: 36 [1280/50000]	Loss: 0.8866	LR: 0.050000
Training Epoch: 36 [1408/50000]	Loss: 1.0727	LR: 0.050000
Training Epoch: 36 [1536/50000]	Loss: 0.8261	LR: 0.050000
Training Epoch: 36 [1664/50000]	Loss: 1.0301	LR: 0.050000
Training Epoch: 36 [1792/50000]	Loss: 0.8103	LR: 0.050000
Training Epoch: 36 [1920/50000]	Loss: 0.7519	LR: 0.050000
Training Epoch: 36 [2048/50000]	Loss: 0.9981	LR: 0.050000
Training Epoch: 36 [2176/50000]	Loss: 0.8213	LR: 0.050000
Training Epoch: 36 [2304/50000]	Loss: 0.9341	LR: 0.050000
Training Epoch: 36 [2432/50000]	Loss: 1.0650	LR: 0.050000
Training Epoch: 36 [2560/50000]	Loss: 1.0836	LR: 0.050000
Training Epoch: 36 [2688/50000]	Loss: 0.8727	LR: 0.050000
Training Epoch: 36 [2816/50000]	Loss: 1.1051	LR: 0.050000
Training Epoch: 36 [2944/50000]	Loss: 0.9230	LR: 0.050000
Training Epoch: 36 [3072/50000]	Loss: 0.9169	LR: 0.050000
Training Epoch: 36 [3200/50000]	Loss: 0.9823	LR: 0.050000
Training Epoch: 36 [3328/50000]	Loss: 0.9433	LR: 0.050000
Training Epoch: 36 [3456/50000]	Loss: 0.8017	LR: 0.050000
Training Epoch: 36 [3584/50000]	Loss: 1.0410	LR: 0.050000
Training Epoch: 36 [3712/50000]	Loss: 0.7934	LR: 0.050000
Training Epoch: 36 [3840/50000]	Loss: 0.9760	LR: 0.050000
Training Epoch: 36 [3968/50000]	Loss: 0.9993	LR: 0.050000
Training Epoch: 36 [4096/50000]	Loss: 1.1505	LR: 0.050000
Training Epoch: 36 [4224/50000]	Loss: 0.9080	LR: 0.050000
Training Epoch: 36 [4352/50000]	Loss: 1.2497	LR: 0.050000
Training Epoch: 36 [4480/50000]	Loss: 0.9517	LR: 0.050000
Training Epoch: 36 [4608/50000]	Loss: 0.9373	LR: 0.050000
Training Epoch: 36 [4736/50000]	Loss: 0.8876	LR: 0.050000
Training Epoch: 36 [4864/50000]	Loss: 0.8387	LR: 0.050000
Training Epoch: 36 [4992/50000]	Loss: 0.9834	LR: 0.050000
Training Epoch: 36 [5120/50000]	Loss: 0.7593	LR: 0.050000
Training Epoch: 36 [5248/50000]	Loss: 0.8399	LR: 0.050000
Training Epoch: 36 [5376/50000]	Loss: 0.9654	LR: 0.050000
Training Epoch: 36 [5504/50000]	Loss: 1.0465	LR: 0.050000
Training Epoch: 36 [5632/50000]	Loss: 0.9400	LR: 0.050000
Training Epoch: 36 [5760/50000]	Loss: 1.1438	LR: 0.050000
Training Epoch: 36 [5888/50000]	Loss: 1.0558	LR: 0.050000
Training Epoch: 36 [6016/50000]	Loss: 1.0205	LR: 0.050000
Training Epoch: 36 [6144/50000]	Loss: 0.8074	LR: 0.050000
Training Epoch: 36 [6272/50000]	Loss: 0.8911	LR: 0.050000
Training Epoch: 36 [6400/50000]	Loss: 1.0373	LR: 0.050000
Training Epoch: 36 [6528/50000]	Loss: 1.0842	LR: 0.050000
Training Epoch: 36 [6656/50000]	Loss: 0.8160	LR: 0.050000
Training Epoch: 36 [6784/50000]	Loss: 0.9711	LR: 0.050000
Training Epoch: 36 [6912/50000]	Loss: 1.0889	LR: 0.050000
Training Epoch: 36 [7040/50000]	Loss: 1.0237	LR: 0.050000
Training Epoch: 36 [7168/50000]	Loss: 0.9667	LR: 0.050000
Training Epoch: 36 [7296/50000]	Loss: 0.8314	LR: 0.050000
Training Epoch: 36 [7424/50000]	Loss: 1.0038	LR: 0.050000
Training Epoch: 36 [7552/50000]	Loss: 0.9942	LR: 0.050000
Training Epoch: 36 [7680/50000]	Loss: 0.9240	LR: 0.050000
Training Epoch: 36 [7808/50000]	Loss: 0.9553	LR: 0.050000
Training Epoch: 36 [7936/50000]	Loss: 0.9837	LR: 0.050000
Training Epoch: 36 [8064/50000]	Loss: 0.9765	LR: 0.050000
Training Epoch: 36 [8192/50000]	Loss: 0.8087	LR: 0.050000
Training Epoch: 36 [8320/50000]	Loss: 0.7998	LR: 0.050000
Training Epoch: 36 [8448/50000]	Loss: 0.9626	LR: 0.050000
Training Epoch: 36 [8576/50000]	Loss: 1.0111	LR: 0.050000
Training Epoch: 36 [8704/50000]	Loss: 0.7303	LR: 0.050000
Training Epoch: 36 [8832/50000]	Loss: 0.9960	LR: 0.050000
Training Epoch: 36 [8960/50000]	Loss: 0.8990	LR: 0.050000
Training Epoch: 36 [9088/50000]	Loss: 0.9704	LR: 0.050000
Training Epoch: 36 [9216/50000]	Loss: 0.9922	LR: 0.050000
Training Epoch: 36 [9344/50000]	Loss: 0.8584	LR: 0.050000
Training Epoch: 36 [9472/50000]	Loss: 1.0449	LR: 0.050000
Training Epoch: 36 [9600/50000]	Loss: 0.8780	LR: 0.050000
Training Epoch: 36 [9728/50000]	Loss: 1.0515	LR: 0.050000
Training Epoch: 36 [9856/50000]	Loss: 1.1668	LR: 0.050000
Training Epoch: 36 [9984/50000]	Loss: 0.9994	LR: 0.050000
Training Epoch: 36 [10112/50000]	Loss: 0.9584	LR: 0.050000
Training Epoch: 36 [10240/50000]	Loss: 0.9357	LR: 0.050000
Training Epoch: 36 [10368/50000]	Loss: 0.8156	LR: 0.050000
Training Epoch: 36 [10496/50000]	Loss: 0.9607	LR: 0.050000
Training Epoch: 36 [10624/50000]	Loss: 0.9916	LR: 0.050000
Training Epoch: 36 [10752/50000]	Loss: 0.8024	LR: 0.050000
Training Epoch: 36 [10880/50000]	Loss: 0.8961	LR: 0.050000
Training Epoch: 36 [11008/50000]	Loss: 0.8975	LR: 0.050000
Training Epoch: 36 [11136/50000]	Loss: 0.9976	LR: 0.050000
Training Epoch: 36 [11264/50000]	Loss: 0.7928	LR: 0.050000
Training Epoch: 36 [11392/50000]	Loss: 0.8122	LR: 0.050000
Training Epoch: 36 [11520/50000]	Loss: 0.8890	LR: 0.050000
Training Epoch: 36 [11648/50000]	Loss: 1.1510	LR: 0.050000
Training Epoch: 36 [11776/50000]	Loss: 1.2225	LR: 0.050000
Training Epoch: 36 [11904/50000]	Loss: 0.8740	LR: 0.050000
Training Epoch: 36 [12032/50000]	Loss: 0.9379	LR: 0.050000
Training Epoch: 36 [12160/50000]	Loss: 1.0040	LR: 0.050000
Training Epoch: 36 [12288/50000]	Loss: 0.9985	LR: 0.050000
Training Epoch: 36 [12416/50000]	Loss: 1.1264	LR: 0.050000
Training Epoch: 36 [12544/50000]	Loss: 1.0710	LR: 0.050000
Training Epoch: 36 [12672/50000]	Loss: 0.9916	LR: 0.050000
Training Epoch: 36 [12800/50000]	Loss: 1.0628	LR: 0.050000
Training Epoch: 36 [12928/50000]	Loss: 0.8118	LR: 0.050000
Training Epoch: 36 [13056/50000]	Loss: 1.0402	LR: 0.050000
Training Epoch: 36 [13184/50000]	Loss: 0.8299	LR: 0.050000
Training Epoch: 36 [13312/50000]	Loss: 1.2513	LR: 0.050000
Training Epoch: 36 [13440/50000]	Loss: 0.9198	LR: 0.050000
Training Epoch: 36 [13568/50000]	Loss: 0.8962	LR: 0.050000
Training Epoch: 36 [13696/50000]	Loss: 0.8732	LR: 0.050000
Training Epoch: 36 [13824/50000]	Loss: 1.0409	LR: 0.050000
Training Epoch: 36 [13952/50000]	Loss: 1.1842	LR: 0.050000
Training Epoch: 36 [14080/50000]	Loss: 1.0392	LR: 0.050000
Training Epoch: 36 [14208/50000]	Loss: 0.8481	LR: 0.050000
Training Epoch: 36 [14336/50000]	Loss: 1.0172	LR: 0.050000
Training Epoch: 36 [14464/50000]	Loss: 1.0938	LR: 0.050000
Training Epoch: 36 [14592/50000]	Loss: 0.8567	LR: 0.050000
Training Epoch: 36 [14720/50000]	Loss: 0.9412	LR: 0.050000
Training Epoch: 36 [14848/50000]	Loss: 0.9598	LR: 0.050000
Training Epoch: 36 [14976/50000]	Loss: 1.0418	LR: 0.050000
Training Epoch: 36 [15104/50000]	Loss: 0.8093	LR: 0.050000
Training Epoch: 36 [15232/50000]	Loss: 0.8808	LR: 0.050000
Training Epoch: 36 [15360/50000]	Loss: 1.0870	LR: 0.050000
Training Epoch: 36 [15488/50000]	Loss: 0.9857	LR: 0.050000
Training Epoch: 36 [15616/50000]	Loss: 1.0720	LR: 0.050000
Training Epoch: 36 [15744/50000]	Loss: 0.8988	LR: 0.050000
Training Epoch: 36 [15872/50000]	Loss: 0.9415	LR: 0.050000
Training Epoch: 36 [16000/50000]	Loss: 0.7547	LR: 0.050000
Training Epoch: 36 [16128/50000]	Loss: 1.0229	LR: 0.050000
Training Epoch: 36 [16256/50000]	Loss: 1.0457	LR: 0.050000
Training Epoch: 36 [16384/50000]	Loss: 0.9428	LR: 0.050000
Training Epoch: 36 [16512/50000]	Loss: 0.8536	LR: 0.050000
Training Epoch: 36 [16640/50000]	Loss: 0.8492	LR: 0.050000
Training Epoch: 36 [16768/50000]	Loss: 0.9978	LR: 0.050000
Training Epoch: 36 [16896/50000]	Loss: 0.8685	LR: 0.050000
Training Epoch: 36 [17024/50000]	Loss: 0.8699	LR: 0.050000
Training Epoch: 36 [17152/50000]	Loss: 1.0371	LR: 0.050000
Training Epoch: 36 [17280/50000]	Loss: 0.9711	LR: 0.050000
Training Epoch: 36 [17408/50000]	Loss: 0.9066	LR: 0.050000
Training Epoch: 36 [17536/50000]	Loss: 0.9200	LR: 0.050000
Training Epoch: 36 [17664/50000]	Loss: 1.1990	LR: 0.050000
Training Epoch: 36 [17792/50000]	Loss: 1.0116	LR: 0.050000
Training Epoch: 36 [17920/50000]	Loss: 0.9372	LR: 0.050000
Training Epoch: 36 [18048/50000]	Loss: 0.9564	LR: 0.050000
Training Epoch: 36 [18176/50000]	Loss: 1.0288	LR: 0.050000
Training Epoch: 36 [18304/50000]	Loss: 0.8449	LR: 0.050000
Training Epoch: 36 [18432/50000]	Loss: 1.0301	LR: 0.050000
Training Epoch: 36 [18560/50000]	Loss: 0.7731	LR: 0.050000
Training Epoch: 36 [18688/50000]	Loss: 0.9562	LR: 0.050000
Training Epoch: 36 [18816/50000]	Loss: 0.8384	LR: 0.050000
Training Epoch: 36 [18944/50000]	Loss: 0.8697	LR: 0.050000
Training Epoch: 36 [19072/50000]	Loss: 1.0090	LR: 0.050000
Training Epoch: 36 [19200/50000]	Loss: 1.0864	LR: 0.050000
Training Epoch: 36 [19328/50000]	Loss: 0.8318	LR: 0.050000
Training Epoch: 36 [19456/50000]	Loss: 0.7931	LR: 0.050000
Training Epoch: 36 [19584/50000]	Loss: 1.0222	LR: 0.050000
Training Epoch: 36 [19712/50000]	Loss: 0.9791	LR: 0.050000
Training Epoch: 36 [19840/50000]	Loss: 0.9707	LR: 0.050000
Training Epoch: 36 [19968/50000]	Loss: 1.0301	LR: 0.050000
Training Epoch: 36 [20096/50000]	Loss: 1.2316	LR: 0.050000
Training Epoch: 36 [20224/50000]	Loss: 0.8886	LR: 0.050000
Training Epoch: 36 [20352/50000]	Loss: 1.0111	LR: 0.050000
Training Epoch: 36 [20480/50000]	Loss: 0.8220	LR: 0.050000
Training Epoch: 36 [20608/50000]	Loss: 0.8379	LR: 0.050000
Training Epoch: 36 [20736/50000]	Loss: 1.0515	LR: 0.050000
Training Epoch: 36 [20864/50000]	Loss: 0.7637	LR: 0.050000
Training Epoch: 36 [20992/50000]	Loss: 0.8959	LR: 0.050000
Training Epoch: 36 [21120/50000]	Loss: 1.1272	LR: 0.050000
Training Epoch: 36 [21248/50000]	Loss: 0.8138	LR: 0.050000
Training Epoch: 36 [21376/50000]	Loss: 0.7788	LR: 0.050000
Training Epoch: 36 [21504/50000]	Loss: 0.7626	LR: 0.050000
Training Epoch: 36 [21632/50000]	Loss: 1.0807	LR: 0.050000
Training Epoch: 36 [21760/50000]	Loss: 1.0445	LR: 0.050000
Training Epoch: 36 [21888/50000]	Loss: 1.3210	LR: 0.050000
Training Epoch: 36 [22016/50000]	Loss: 0.9876	LR: 0.050000
Training Epoch: 36 [22144/50000]	Loss: 0.9156	LR: 0.050000
Training Epoch: 36 [22272/50000]	Loss: 0.7618	LR: 0.050000
Training Epoch: 36 [22400/50000]	Loss: 0.9223	LR: 0.050000
Training Epoch: 36 [22528/50000]	Loss: 0.9861	LR: 0.050000
Training Epoch: 36 [22656/50000]	Loss: 1.0030	LR: 0.050000
Training Epoch: 36 [22784/50000]	Loss: 0.9299	LR: 0.050000
Training Epoch: 36 [22912/50000]	Loss: 0.9875	LR: 0.050000
Training Epoch: 36 [23040/50000]	Loss: 0.7896	LR: 0.050000
Training Epoch: 36 [23168/50000]	Loss: 1.0589	LR: 0.050000
Training Epoch: 36 [23296/50000]	Loss: 0.9791	LR: 0.050000
Training Epoch: 36 [23424/50000]	Loss: 0.8750	LR: 0.050000
Training Epoch: 36 [23552/50000]	Loss: 0.8398	LR: 0.050000
Training Epoch: 36 [23680/50000]	Loss: 1.0767	LR: 0.050000
Training Epoch: 36 [23808/50000]	Loss: 1.0123	LR: 0.050000
Training Epoch: 36 [23936/50000]	Loss: 0.8227	LR: 0.050000
Training Epoch: 36 [24064/50000]	Loss: 0.9207	LR: 0.050000
Training Epoch: 36 [24192/50000]	Loss: 1.1069	LR: 0.050000
Training Epoch: 36 [24320/50000]	Loss: 1.0567	LR: 0.050000
Training Epoch: 36 [24448/50000]	Loss: 0.8385	LR: 0.050000
Training Epoch: 36 [24576/50000]	Loss: 0.8474	LR: 0.050000
Training Epoch: 36 [24704/50000]	Loss: 1.0996	LR: 0.050000
Training Epoch: 36 [24832/50000]	Loss: 0.8683	LR: 0.050000
Training Epoch: 36 [24960/50000]	Loss: 1.1856	LR: 0.050000
Training Epoch: 36 [25088/50000]	Loss: 1.0064	LR: 0.050000
Training Epoch: 36 [25216/50000]	Loss: 1.0651	LR: 0.050000
Training Epoch: 36 [25344/50000]	Loss: 0.9578	LR: 0.050000
Training Epoch: 36 [25472/50000]	Loss: 0.9611	LR: 0.050000
Training Epoch: 36 [25600/50000]	Loss: 0.8307	LR: 0.050000
Training Epoch: 36 [25728/50000]	Loss: 1.1309	LR: 0.050000
Training Epoch: 36 [25856/50000]	Loss: 1.0889	LR: 0.050000
Training Epoch: 36 [25984/50000]	Loss: 0.9140	LR: 0.050000
Training Epoch: 36 [26112/50000]	Loss: 0.9160	LR: 0.050000
Training Epoch: 36 [26240/50000]	Loss: 1.0651	LR: 0.050000
Training Epoch: 36 [26368/50000]	Loss: 1.0526	LR: 0.050000
Training Epoch: 36 [26496/50000]	Loss: 0.9277	LR: 0.050000
Training Epoch: 36 [26624/50000]	Loss: 0.7871	LR: 0.050000
Training Epoch: 36 [26752/50000]	Loss: 1.1466	LR: 0.050000
Training Epoch: 36 [26880/50000]	Loss: 0.9904	LR: 0.050000
Training Epoch: 36 [27008/50000]	Loss: 1.0213	LR: 0.050000
Training Epoch: 36 [27136/50000]	Loss: 1.0662	LR: 0.050000
Training Epoch: 36 [27264/50000]	Loss: 1.0503	LR: 0.050000
Training Epoch: 36 [27392/50000]	Loss: 1.0077	LR: 0.050000
Training Epoch: 36 [27520/50000]	Loss: 0.9238	LR: 0.050000
Training Epoch: 36 [27648/50000]	Loss: 1.1999	LR: 0.050000
Training Epoch: 36 [27776/50000]	Loss: 1.1309	LR: 0.050000
Training Epoch: 36 [27904/50000]	Loss: 0.7905	LR: 0.050000
Training Epoch: 36 [28032/50000]	Loss: 0.9590	LR: 0.050000
Training Epoch: 36 [28160/50000]	Loss: 1.1162	LR: 0.050000
Training Epoch: 36 [28288/50000]	Loss: 1.0840	LR: 0.050000
Training Epoch: 36 [28416/50000]	Loss: 1.0753	LR: 0.050000
Training Epoch: 36 [28544/50000]	Loss: 1.0242	LR: 0.050000
Training Epoch: 36 [28672/50000]	Loss: 1.1330	LR: 0.050000
Training Epoch: 36 [28800/50000]	Loss: 1.1194	LR: 0.050000
Training Epoch: 36 [28928/50000]	Loss: 1.0552	LR: 0.050000
Training Epoch: 36 [29056/50000]	Loss: 0.9396	LR: 0.050000
Training Epoch: 36 [29184/50000]	Loss: 1.1024	LR: 0.050000
Training Epoch: 36 [29312/50000]	Loss: 1.0380	LR: 0.050000
Training Epoch: 36 [29440/50000]	Loss: 0.8892	LR: 0.050000
Training Epoch: 36 [29568/50000]	Loss: 1.1220	LR: 0.050000
Training Epoch: 36 [29696/50000]	Loss: 0.8537	LR: 0.050000
Training Epoch: 36 [29824/50000]	Loss: 1.1077	LR: 0.050000
Training Epoch: 36 [29952/50000]	Loss: 0.8013	LR: 0.050000
Training Epoch: 36 [30080/50000]	Loss: 1.1738	LR: 0.050000
Training Epoch: 36 [30208/50000]	Loss: 1.0361	LR: 0.050000
Training Epoch: 36 [30336/50000]	Loss: 1.0540	LR: 0.050000
Training Epoch: 36 [30464/50000]	Loss: 0.9645	LR: 0.050000
Training Epoch: 36 [30592/50000]	Loss: 1.1840	LR: 0.050000
Training Epoch: 36 [30720/50000]	Loss: 1.2254	LR: 0.050000
Training Epoch: 36 [30848/50000]	Loss: 0.9457	LR: 0.050000
Training Epoch: 36 [30976/50000]	Loss: 1.1529	LR: 0.050000
Training Epoch: 36 [31104/50000]	Loss: 0.9725	LR: 0.050000
Training Epoch: 36 [31232/50000]	Loss: 1.0933	LR: 0.050000
Training Epoch: 36 [31360/50000]	Loss: 1.1400	LR: 0.050000
Training Epoch: 36 [31488/50000]	Loss: 1.0595	LR: 0.050000
Training Epoch: 36 [31616/50000]	Loss: 0.9413	LR: 0.050000
Training Epoch: 36 [31744/50000]	Loss: 1.0201	LR: 0.050000
Training Epoch: 36 [31872/50000]	Loss: 0.8677	LR: 0.050000
Training Epoch: 36 [32000/50000]	Loss: 0.9446	LR: 0.050000
Training Epoch: 36 [32128/50000]	Loss: 0.9982	LR: 0.050000
Training Epoch: 36 [32256/50000]	Loss: 1.0363	LR: 0.050000
Training Epoch: 36 [32384/50000]	Loss: 1.0880	LR: 0.050000
Training Epoch: 36 [32512/50000]	Loss: 1.1670	LR: 0.050000
Training Epoch: 36 [32640/50000]	Loss: 0.9139	LR: 0.050000
Training Epoch: 36 [32768/50000]	Loss: 0.9630	LR: 0.050000
Training Epoch: 36 [32896/50000]	Loss: 1.2202	LR: 0.050000
Training Epoch: 36 [33024/50000]	Loss: 1.0311	LR: 0.050000
Training Epoch: 36 [33152/50000]	Loss: 1.0440	LR: 0.050000
Training Epoch: 36 [33280/50000]	Loss: 1.0981	LR: 0.050000
Training Epoch: 36 [33408/50000]	Loss: 0.9461	LR: 0.050000
Training Epoch: 36 [33536/50000]	Loss: 1.0503	LR: 0.050000
Training Epoch: 36 [33664/50000]	Loss: 1.0979	LR: 0.050000
Training Epoch: 36 [33792/50000]	Loss: 1.0671	LR: 0.050000
Training Epoch: 36 [33920/50000]	Loss: 0.8024	LR: 0.050000
Training Epoch: 36 [34048/50000]	Loss: 0.9667	LR: 0.050000
Training Epoch: 36 [34176/50000]	Loss: 0.9777	LR: 0.050000
Training Epoch: 36 [34304/50000]	Loss: 1.3528	LR: 0.050000
Training Epoch: 36 [34432/50000]	Loss: 0.9697	LR: 0.050000
Training Epoch: 36 [34560/50000]	Loss: 1.0597	LR: 0.050000
Training Epoch: 36 [34688/50000]	Loss: 0.9187	LR: 0.050000
Training Epoch: 36 [34816/50000]	Loss: 0.9251	LR: 0.050000
Training Epoch: 36 [34944/50000]	Loss: 1.1606	LR: 0.050000
Training Epoch: 36 [35072/50000]	Loss: 1.1741	LR: 0.050000
Training Epoch: 36 [35200/50000]	Loss: 1.0416	LR: 0.050000
Training Epoch: 36 [35328/50000]	Loss: 0.8979	LR: 0.050000
Training Epoch: 36 [35456/50000]	Loss: 1.0038	LR: 0.050000
Training Epoch: 36 [35584/50000]	Loss: 0.9635	LR: 0.050000
Training Epoch: 36 [35712/50000]	Loss: 1.1739	LR: 0.050000
Training Epoch: 36 [35840/50000]	Loss: 1.0611	LR: 0.050000
Training Epoch: 36 [35968/50000]	Loss: 0.7866	LR: 0.050000
Training Epoch: 36 [36096/50000]	Loss: 1.0958	LR: 0.050000
Training Epoch: 36 [36224/50000]	Loss: 1.0413	LR: 0.050000
Training Epoch: 36 [36352/50000]	Loss: 0.8277	LR: 0.050000
Training Epoch: 36 [36480/50000]	Loss: 0.9833	LR: 0.050000
Training Epoch: 36 [36608/50000]	Loss: 1.1572	LR: 0.050000
Training Epoch: 36 [36736/50000]	Loss: 0.8384	LR: 0.050000
Training Epoch: 36 [36864/50000]	Loss: 0.8193	LR: 0.050000
Training Epoch: 36 [36992/50000]	Loss: 1.0846	LR: 0.050000
Training Epoch: 36 [37120/50000]	Loss: 1.0660	LR: 0.050000
Training Epoch: 36 [37248/50000]	Loss: 1.0747	LR: 0.050000
Training Epoch: 36 [37376/50000]	Loss: 1.0423	LR: 0.050000
Training Epoch: 36 [37504/50000]	Loss: 1.1144	LR: 0.050000
Training Epoch: 36 [37632/50000]	Loss: 0.8755	LR: 0.050000
Training Epoch: 36 [37760/50000]	Loss: 0.8158	LR: 0.050000
Training Epoch: 36 [37888/50000]	Loss: 1.1141	LR: 0.050000
Training Epoch: 36 [38016/50000]	Loss: 0.8986	LR: 0.050000
Training Epoch: 36 [38144/50000]	Loss: 1.3600	LR: 0.050000
Training Epoch: 36 [38272/50000]	Loss: 1.0353	LR: 0.050000
Training Epoch: 36 [38400/50000]	Loss: 1.0379	LR: 0.050000
Training Epoch: 36 [38528/50000]	Loss: 0.9289	LR: 0.050000
Training Epoch: 36 [38656/50000]	Loss: 0.7171	LR: 0.050000
Training Epoch: 36 [38784/50000]	Loss: 1.0995	LR: 0.050000
Training Epoch: 36 [38912/50000]	Loss: 1.0894	LR: 0.050000
Training Epoch: 36 [39040/50000]	Loss: 1.0185	LR: 0.050000
Training Epoch: 36 [39168/50000]	Loss: 1.1267	LR: 0.050000
Training Epoch: 36 [39296/50000]	Loss: 0.9403	LR: 0.050000
Training Epoch: 36 [39424/50000]	Loss: 1.0109	LR: 0.050000
Training Epoch: 36 [39552/50000]	Loss: 0.9460	LR: 0.050000
Training Epoch: 36 [39680/50000]	Loss: 0.8744	LR: 0.050000
Training Epoch: 36 [39808/50000]	Loss: 0.9651	LR: 0.050000
Training Epoch: 36 [39936/50000]	Loss: 1.0818	LR: 0.050000
Training Epoch: 36 [40064/50000]	Loss: 0.9915	LR: 0.050000
Training Epoch: 36 [40192/50000]	Loss: 1.0587	LR: 0.050000
Training Epoch: 36 [40320/50000]	Loss: 1.1208	LR: 0.050000
Training Epoch: 36 [40448/50000]	Loss: 1.0252	LR: 0.050000
Training Epoch: 36 [40576/50000]	Loss: 1.0708	LR: 0.050000
Training Epoch: 36 [40704/50000]	Loss: 1.1660	LR: 0.050000
Training Epoch: 36 [40832/50000]	Loss: 1.1367	LR: 0.050000
Training Epoch: 36 [40960/50000]	Loss: 0.9113	LR: 0.050000
Training Epoch: 36 [41088/50000]	Loss: 0.7538	LR: 0.050000
Training Epoch: 36 [41216/50000]	Loss: 1.2362	LR: 0.050000
Training Epoch: 36 [41344/50000]	Loss: 1.1248	LR: 0.050000
Training Epoch: 36 [41472/50000]	Loss: 1.2293	LR: 0.050000
Training Epoch: 36 [41600/50000]	Loss: 1.0440	LR: 0.050000
Training Epoch: 36 [41728/50000]	Loss: 0.9335	LR: 0.050000
Training Epoch: 36 [41856/50000]	Loss: 1.0112	LR: 0.050000
Training Epoch: 36 [41984/50000]	Loss: 1.2597	LR: 0.050000
Training Epoch: 36 [42112/50000]	Loss: 0.9405	LR: 0.050000
Training Epoch: 36 [42240/50000]	Loss: 1.0002	LR: 0.050000
Training Epoch: 36 [42368/50000]	Loss: 0.9031	LR: 0.050000
Training Epoch: 36 [42496/50000]	Loss: 1.2230	LR: 0.050000
Training Epoch: 36 [42624/50000]	Loss: 0.9487	LR: 0.050000
Training Epoch: 36 [42752/50000]	Loss: 0.9746	LR: 0.050000
Training Epoch: 36 [42880/50000]	Loss: 0.9124	LR: 0.050000
Training Epoch: 36 [43008/50000]	Loss: 1.2113	LR: 0.050000
Training Epoch: 36 [43136/50000]	Loss: 1.0224	LR: 0.050000
Training Epoch: 36 [43264/50000]	Loss: 1.0424	LR: 0.050000
Training Epoch: 36 [43392/50000]	Loss: 1.0798	LR: 0.050000
Training Epoch: 36 [43520/50000]	Loss: 0.9172	LR: 0.050000
Training Epoch: 36 [43648/50000]	Loss: 1.0831	LR: 0.050000
Training Epoch: 36 [43776/50000]	Loss: 1.0793	LR: 0.050000
Training Epoch: 36 [43904/50000]	Loss: 0.9902	LR: 0.050000
Training Epoch: 36 [44032/50000]	Loss: 1.0076	LR: 0.050000
Training Epoch: 36 [44160/50000]	Loss: 0.8874	LR: 0.050000
Training Epoch: 36 [44288/50000]	Loss: 0.8360	LR: 0.050000
Training Epoch: 36 [44416/50000]	Loss: 1.2221	LR: 0.050000
Training Epoch: 36 [44544/50000]	Loss: 1.1690	LR: 0.050000
Training Epoch: 36 [44672/50000]	Loss: 0.9049	LR: 0.050000
Training Epoch: 36 [44800/50000]	Loss: 1.0361	LR: 0.050000
Training Epoch: 36 [44928/50000]	Loss: 0.9917	LR: 0.050000
Training Epoch: 36 [45056/50000]	Loss: 0.9239	LR: 0.050000
Training Epoch: 36 [45184/50000]	Loss: 1.1132	LR: 0.050000
Training Epoch: 36 [45312/50000]	Loss: 0.9454	LR: 0.050000
Training Epoch: 36 [45440/50000]	Loss: 1.0241	LR: 0.050000
Training Epoch: 36 [45568/50000]	Loss: 1.0862	LR: 0.050000
Training Epoch: 36 [45696/50000]	Loss: 1.0395	LR: 0.050000
Training Epoch: 36 [45824/50000]	Loss: 1.1798	LR: 0.050000
Training Epoch: 36 [45952/50000]	Loss: 0.9704	LR: 0.050000
Training Epoch: 36 [46080/50000]	Loss: 1.0220	LR: 0.050000
Training Epoch: 36 [46208/50000]	Loss: 1.1332	LR: 0.050000
Training Epoch: 36 [46336/50000]	Loss: 1.1886	LR: 0.050000
Training Epoch: 36 [46464/50000]	Loss: 1.0095	LR: 0.050000
Training Epoch: 36 [46592/50000]	Loss: 1.0507	LR: 0.050000
Training Epoch: 36 [46720/50000]	Loss: 0.8826	LR: 0.050000
Training Epoch: 36 [46848/50000]	Loss: 0.8867	LR: 0.050000
Training Epoch: 36 [46976/50000]	Loss: 0.8657	LR: 0.050000
Training Epoch: 36 [47104/50000]	Loss: 1.1216	LR: 0.050000
Training Epoch: 36 [47232/50000]	Loss: 1.0955	LR: 0.050000
Training Epoch: 36 [47360/50000]	Loss: 1.0317	LR: 0.050000
Training Epoch: 36 [47488/50000]	Loss: 0.9429	LR: 0.050000
Training Epoch: 36 [47616/50000]	Loss: 0.9844	LR: 0.050000
Training Epoch: 36 [47744/50000]	Loss: 0.9529	LR: 0.050000
Training Epoch: 36 [47872/50000]	Loss: 1.0972	LR: 0.050000
Training Epoch: 36 [48000/50000]	Loss: 0.9103	LR: 0.050000
Training Epoch: 36 [48128/50000]	Loss: 1.0297	LR: 0.050000
Training Epoch: 36 [48256/50000]	Loss: 1.2169	LR: 0.050000
Training Epoch: 36 [48384/50000]	Loss: 0.8877	LR: 0.050000
Training Epoch: 36 [48512/50000]	Loss: 1.1273	LR: 0.050000
Training Epoch: 36 [48640/50000]	Loss: 0.9718	LR: 0.050000
Training Epoch: 36 [48768/50000]	Loss: 1.1195	LR: 0.050000
Training Epoch: 36 [48896/50000]	Loss: 1.2430	LR: 0.050000
Training Epoch: 36 [49024/50000]	Loss: 0.7068	LR: 0.050000
Training Epoch: 36 [49152/50000]	Loss: 0.9728	LR: 0.050000
Training Epoch: 36 [49280/50000]	Loss: 1.0286	LR: 0.050000
Training Epoch: 36 [49408/50000]	Loss: 0.9873	LR: 0.050000
Training Epoch: 36 [49536/50000]	Loss: 1.1230	LR: 0.050000
Training Epoch: 36 [49664/50000]	Loss: 0.9941	LR: 0.050000
Training Epoch: 36 [49792/50000]	Loss: 0.7029	LR: 0.050000
Training Epoch: 36 [49920/50000]	Loss: 0.9048	LR: 0.050000
Training Epoch: 36 [50000/50000]	Loss: 1.0782	LR: 0.050000
epoch 36 training time consumed: 53.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  132122 GB |  132122 GB |
|       from large pool |  123392 KB |    1034 MB |  131992 GB |  131992 GB |
|       from small pool |   10798 KB |      13 MB |     130 GB |     130 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  132122 GB |  132122 GB |
|       from large pool |  123392 KB |    1034 MB |  131992 GB |  131992 GB |
|       from small pool |   10798 KB |      13 MB |     130 GB |     130 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   58143 GB |   58143 GB |
|       from large pool |  155136 KB |  433088 KB |   57999 GB |   57999 GB |
|       from small pool |    1490 KB |    3494 KB |     143 GB |     143 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    5098 K  |    5098 K  |
|       from large pool |      24    |      65    |    2661 K  |    2661 K  |
|       from small pool |     231    |     274    |    2437 K  |    2437 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    5098 K  |    5098 K  |
|       from large pool |      24    |      65    |    2661 K  |    2661 K  |
|       from small pool |     231    |     274    |    2437 K  |    2437 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2521 K  |    2521 K  |
|       from large pool |       9    |      14    |    1288 K  |    1288 K  |
|       from small pool |      12    |      16    |    1233 K  |    1233 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 36, Average loss: 0.0124, Accuracy: 0.5957, Time consumed:3.46s

Training Epoch: 37 [128/50000]	Loss: 1.0179	LR: 0.050000
Training Epoch: 37 [256/50000]	Loss: 0.7847	LR: 0.050000
Training Epoch: 37 [384/50000]	Loss: 0.8526	LR: 0.050000
Training Epoch: 37 [512/50000]	Loss: 0.8404	LR: 0.050000
Training Epoch: 37 [640/50000]	Loss: 0.8275	LR: 0.050000
Training Epoch: 37 [768/50000]	Loss: 1.1155	LR: 0.050000
Training Epoch: 37 [896/50000]	Loss: 1.0113	LR: 0.050000
Training Epoch: 37 [1024/50000]	Loss: 0.7457	LR: 0.050000
Training Epoch: 37 [1152/50000]	Loss: 0.9627	LR: 0.050000
Training Epoch: 37 [1280/50000]	Loss: 0.9982	LR: 0.050000
Training Epoch: 37 [1408/50000]	Loss: 0.7736	LR: 0.050000
Training Epoch: 37 [1536/50000]	Loss: 0.9388	LR: 0.050000
Training Epoch: 37 [1664/50000]	Loss: 0.8456	LR: 0.050000
Training Epoch: 37 [1792/50000]	Loss: 0.8206	LR: 0.050000
Training Epoch: 37 [1920/50000]	Loss: 1.0363	LR: 0.050000
Training Epoch: 37 [2048/50000]	Loss: 0.9472	LR: 0.050000
Training Epoch: 37 [2176/50000]	Loss: 0.8098	LR: 0.050000
Training Epoch: 37 [2304/50000]	Loss: 0.8459	LR: 0.050000
Training Epoch: 37 [2432/50000]	Loss: 0.8766	LR: 0.050000
Training Epoch: 37 [2560/50000]	Loss: 1.1656	LR: 0.050000
Training Epoch: 37 [2688/50000]	Loss: 0.8570	LR: 0.050000
Training Epoch: 37 [2816/50000]	Loss: 1.0362	LR: 0.050000
Training Epoch: 37 [2944/50000]	Loss: 0.7052	LR: 0.050000
Training Epoch: 37 [3072/50000]	Loss: 0.8709	LR: 0.050000
Training Epoch: 37 [3200/50000]	Loss: 0.9971	LR: 0.050000
Training Epoch: 37 [3328/50000]	Loss: 0.8427	LR: 0.050000
Training Epoch: 37 [3456/50000]	Loss: 0.7349	LR: 0.050000
Training Epoch: 37 [3584/50000]	Loss: 0.8798	LR: 0.050000
Training Epoch: 37 [3712/50000]	Loss: 0.9018	LR: 0.050000
Training Epoch: 37 [3840/50000]	Loss: 0.8992	LR: 0.050000
Training Epoch: 37 [3968/50000]	Loss: 0.9645	LR: 0.050000
Training Epoch: 37 [4096/50000]	Loss: 0.8465	LR: 0.050000
Training Epoch: 37 [4224/50000]	Loss: 1.0042	LR: 0.050000
Training Epoch: 37 [4352/50000]	Loss: 0.7890	LR: 0.050000
Training Epoch: 37 [4480/50000]	Loss: 0.7303	LR: 0.050000
Training Epoch: 37 [4608/50000]	Loss: 0.8970	LR: 0.050000
Training Epoch: 37 [4736/50000]	Loss: 0.8297	LR: 0.050000
Training Epoch: 37 [4864/50000]	Loss: 0.8155	LR: 0.050000
Training Epoch: 37 [4992/50000]	Loss: 0.8616	LR: 0.050000
Training Epoch: 37 [5120/50000]	Loss: 0.9467	LR: 0.050000
Training Epoch: 37 [5248/50000]	Loss: 0.8562	LR: 0.050000
Training Epoch: 37 [5376/50000]	Loss: 1.0277	LR: 0.050000
Training Epoch: 37 [5504/50000]	Loss: 0.9013	LR: 0.050000
Training Epoch: 37 [5632/50000]	Loss: 0.7473	LR: 0.050000
Training Epoch: 37 [5760/50000]	Loss: 1.1566	LR: 0.050000
Training Epoch: 37 [5888/50000]	Loss: 0.7739	LR: 0.050000
Training Epoch: 37 [6016/50000]	Loss: 0.9402	LR: 0.050000
Training Epoch: 37 [6144/50000]	Loss: 0.8370	LR: 0.050000
Training Epoch: 37 [6272/50000]	Loss: 0.8168	LR: 0.050000
Training Epoch: 37 [6400/50000]	Loss: 0.8760	LR: 0.050000
Training Epoch: 37 [6528/50000]	Loss: 0.8827	LR: 0.050000
Training Epoch: 37 [6656/50000]	Loss: 0.9339	LR: 0.050000
Training Epoch: 37 [6784/50000]	Loss: 0.7522	LR: 0.050000
Training Epoch: 37 [6912/50000]	Loss: 0.8914	LR: 0.050000
Training Epoch: 37 [7040/50000]	Loss: 0.8709	LR: 0.050000
Training Epoch: 37 [7168/50000]	Loss: 0.8439	LR: 0.050000
Training Epoch: 37 [7296/50000]	Loss: 0.8452	LR: 0.050000
Training Epoch: 37 [7424/50000]	Loss: 0.9798	LR: 0.050000
Training Epoch: 37 [7552/50000]	Loss: 0.8496	LR: 0.050000
Training Epoch: 37 [7680/50000]	Loss: 0.8094	LR: 0.050000
Training Epoch: 37 [7808/50000]	Loss: 0.8457	LR: 0.050000
Training Epoch: 37 [7936/50000]	Loss: 0.9328	LR: 0.050000
Training Epoch: 37 [8064/50000]	Loss: 0.9039	LR: 0.050000
Training Epoch: 37 [8192/50000]	Loss: 0.8945	LR: 0.050000
Training Epoch: 37 [8320/50000]	Loss: 0.8795	LR: 0.050000
Training Epoch: 37 [8448/50000]	Loss: 1.0232	LR: 0.050000
Training Epoch: 37 [8576/50000]	Loss: 0.9048	LR: 0.050000
Training Epoch: 37 [8704/50000]	Loss: 0.9285	LR: 0.050000
Training Epoch: 37 [8832/50000]	Loss: 0.9605	LR: 0.050000
Training Epoch: 37 [8960/50000]	Loss: 0.7796	LR: 0.050000
Training Epoch: 37 [9088/50000]	Loss: 0.7139	LR: 0.050000
Training Epoch: 37 [9216/50000]	Loss: 0.9121	LR: 0.050000
Training Epoch: 37 [9344/50000]	Loss: 1.0448	LR: 0.050000
Training Epoch: 37 [9472/50000]	Loss: 1.1734	LR: 0.050000
Training Epoch: 37 [9600/50000]	Loss: 0.7616	LR: 0.050000
Training Epoch: 37 [9728/50000]	Loss: 0.9997	LR: 0.050000
Training Epoch: 37 [9856/50000]	Loss: 0.9582	LR: 0.050000
Training Epoch: 37 [9984/50000]	Loss: 0.8743	LR: 0.050000
Training Epoch: 37 [10112/50000]	Loss: 0.8614	LR: 0.050000
Training Epoch: 37 [10240/50000]	Loss: 0.7807	LR: 0.050000
Training Epoch: 37 [10368/50000]	Loss: 0.9444	LR: 0.050000
Training Epoch: 37 [10496/50000]	Loss: 0.9084	LR: 0.050000
Training Epoch: 37 [10624/50000]	Loss: 1.1177	LR: 0.050000
Training Epoch: 37 [10752/50000]	Loss: 0.9388	LR: 0.050000
Training Epoch: 37 [10880/50000]	Loss: 0.9824	LR: 0.050000
Training Epoch: 37 [11008/50000]	Loss: 1.1300	LR: 0.050000
Training Epoch: 37 [11136/50000]	Loss: 0.8653	LR: 0.050000
Training Epoch: 37 [11264/50000]	Loss: 0.8094	LR: 0.050000
Training Epoch: 37 [11392/50000]	Loss: 1.0469	LR: 0.050000
Training Epoch: 37 [11520/50000]	Loss: 0.9585	LR: 0.050000
Training Epoch: 37 [11648/50000]	Loss: 0.9530	LR: 0.050000
Training Epoch: 37 [11776/50000]	Loss: 0.7847	LR: 0.050000
Training Epoch: 37 [11904/50000]	Loss: 0.8779	LR: 0.050000
Training Epoch: 37 [12032/50000]	Loss: 0.9513	LR: 0.050000
Training Epoch: 37 [12160/50000]	Loss: 0.9552	LR: 0.050000
Training Epoch: 37 [12288/50000]	Loss: 1.2672	LR: 0.050000
Training Epoch: 37 [12416/50000]	Loss: 1.0233	LR: 0.050000
Training Epoch: 37 [12544/50000]	Loss: 0.9232	LR: 0.050000
Training Epoch: 37 [12672/50000]	Loss: 1.0984	LR: 0.050000
Training Epoch: 37 [12800/50000]	Loss: 1.0185	LR: 0.050000
Training Epoch: 37 [12928/50000]	Loss: 1.1531	LR: 0.050000
Training Epoch: 37 [13056/50000]	Loss: 0.8857	LR: 0.050000
Training Epoch: 37 [13184/50000]	Loss: 1.0118	LR: 0.050000
Training Epoch: 37 [13312/50000]	Loss: 1.0551	LR: 0.050000
Training Epoch: 37 [13440/50000]	Loss: 0.8214	LR: 0.050000
Training Epoch: 37 [13568/50000]	Loss: 1.0225	LR: 0.050000
Training Epoch: 37 [13696/50000]	Loss: 1.0427	LR: 0.050000
Training Epoch: 37 [13824/50000]	Loss: 1.0095	LR: 0.050000
Training Epoch: 37 [13952/50000]	Loss: 1.1390	LR: 0.050000
Training Epoch: 37 [14080/50000]	Loss: 0.9228	LR: 0.050000
Training Epoch: 37 [14208/50000]	Loss: 0.9531	LR: 0.050000
Training Epoch: 37 [14336/50000]	Loss: 0.8496	LR: 0.050000
Training Epoch: 37 [14464/50000]	Loss: 1.0653	LR: 0.050000
Training Epoch: 37 [14592/50000]	Loss: 0.9335	LR: 0.050000
Training Epoch: 37 [14720/50000]	Loss: 1.0332	LR: 0.050000
Training Epoch: 37 [14848/50000]	Loss: 0.6595	LR: 0.050000
Training Epoch: 37 [14976/50000]	Loss: 1.0345	LR: 0.050000
Training Epoch: 37 [15104/50000]	Loss: 0.8441	LR: 0.050000
Training Epoch: 37 [15232/50000]	Loss: 1.0913	LR: 0.050000
Training Epoch: 37 [15360/50000]	Loss: 1.0271	LR: 0.050000
Training Epoch: 37 [15488/50000]	Loss: 1.0598	LR: 0.050000
Training Epoch: 37 [15616/50000]	Loss: 0.8713	LR: 0.050000
Training Epoch: 37 [15744/50000]	Loss: 0.9611	LR: 0.050000
Training Epoch: 37 [15872/50000]	Loss: 0.8557	LR: 0.050000
Training Epoch: 37 [16000/50000]	Loss: 0.8415	LR: 0.050000
Training Epoch: 37 [16128/50000]	Loss: 0.8118	LR: 0.050000
Training Epoch: 37 [16256/50000]	Loss: 1.0218	LR: 0.050000
Training Epoch: 37 [16384/50000]	Loss: 1.0689	LR: 0.050000
Training Epoch: 37 [16512/50000]	Loss: 1.0884	LR: 0.050000
Training Epoch: 37 [16640/50000]	Loss: 1.1442	LR: 0.050000
Training Epoch: 37 [16768/50000]	Loss: 0.8984	LR: 0.050000
Training Epoch: 37 [16896/50000]	Loss: 0.9376	LR: 0.050000
Training Epoch: 37 [17024/50000]	Loss: 1.2313	LR: 0.050000
Training Epoch: 37 [17152/50000]	Loss: 1.1127	LR: 0.050000
Training Epoch: 37 [17280/50000]	Loss: 0.8563	LR: 0.050000
Training Epoch: 37 [17408/50000]	Loss: 0.8318	LR: 0.050000
Training Epoch: 37 [17536/50000]	Loss: 1.0968	LR: 0.050000
Training Epoch: 37 [17664/50000]	Loss: 0.8237	LR: 0.050000
Training Epoch: 37 [17792/50000]	Loss: 1.1907	LR: 0.050000
Training Epoch: 37 [17920/50000]	Loss: 0.7447	LR: 0.050000
Training Epoch: 37 [18048/50000]	Loss: 0.9037	LR: 0.050000
Training Epoch: 37 [18176/50000]	Loss: 0.9847	LR: 0.050000
Training Epoch: 37 [18304/50000]	Loss: 0.7403	LR: 0.050000
Training Epoch: 37 [18432/50000]	Loss: 0.9557	LR: 0.050000
Training Epoch: 37 [18560/50000]	Loss: 1.1694	LR: 0.050000
Training Epoch: 37 [18688/50000]	Loss: 1.0079	LR: 0.050000
Training Epoch: 37 [18816/50000]	Loss: 0.8360	LR: 0.050000
Training Epoch: 37 [18944/50000]	Loss: 0.9245	LR: 0.050000
Training Epoch: 37 [19072/50000]	Loss: 0.9318	LR: 0.050000
Training Epoch: 37 [19200/50000]	Loss: 1.2292	LR: 0.050000
Training Epoch: 37 [19328/50000]	Loss: 0.9711	LR: 0.050000
Training Epoch: 37 [19456/50000]	Loss: 1.1415	LR: 0.050000
Training Epoch: 37 [19584/50000]	Loss: 1.0404	LR: 0.050000
Training Epoch: 37 [19712/50000]	Loss: 1.0982	LR: 0.050000
Training Epoch: 37 [19840/50000]	Loss: 1.0504	LR: 0.050000
Training Epoch: 37 [19968/50000]	Loss: 0.9063	LR: 0.050000
Training Epoch: 37 [20096/50000]	Loss: 1.0158	LR: 0.050000
Training Epoch: 37 [20224/50000]	Loss: 0.8917	LR: 0.050000
Training Epoch: 37 [20352/50000]	Loss: 0.9774	LR: 0.050000
Training Epoch: 37 [20480/50000]	Loss: 1.0850	LR: 0.050000
Training Epoch: 37 [20608/50000]	Loss: 0.9229	LR: 0.050000
Training Epoch: 37 [20736/50000]	Loss: 0.8587	LR: 0.050000
Training Epoch: 37 [20864/50000]	Loss: 0.9823	LR: 0.050000
Training Epoch: 37 [20992/50000]	Loss: 1.0119	LR: 0.050000
Training Epoch: 37 [21120/50000]	Loss: 0.7997	LR: 0.050000
Training Epoch: 37 [21248/50000]	Loss: 1.0276	LR: 0.050000
Training Epoch: 37 [21376/50000]	Loss: 0.9831	LR: 0.050000
Training Epoch: 37 [21504/50000]	Loss: 1.0250	LR: 0.050000
Training Epoch: 37 [21632/50000]	Loss: 1.1116	LR: 0.050000
Training Epoch: 37 [21760/50000]	Loss: 1.1805	LR: 0.050000
Training Epoch: 37 [21888/50000]	Loss: 0.9298	LR: 0.050000
Training Epoch: 37 [22016/50000]	Loss: 0.9796	LR: 0.050000
Training Epoch: 37 [22144/50000]	Loss: 1.1170	LR: 0.050000
Training Epoch: 37 [22272/50000]	Loss: 1.0048	LR: 0.050000
Training Epoch: 37 [22400/50000]	Loss: 1.1777	LR: 0.050000
Training Epoch: 37 [22528/50000]	Loss: 0.9891	LR: 0.050000
Training Epoch: 37 [22656/50000]	Loss: 0.8080	LR: 0.050000
Training Epoch: 37 [22784/50000]	Loss: 0.9242	LR: 0.050000
Training Epoch: 37 [22912/50000]	Loss: 1.1443	LR: 0.050000
Training Epoch: 37 [23040/50000]	Loss: 1.0586	LR: 0.050000
Training Epoch: 37 [23168/50000]	Loss: 0.9959	LR: 0.050000
Training Epoch: 37 [23296/50000]	Loss: 0.8608	LR: 0.050000
Training Epoch: 37 [23424/50000]	Loss: 0.9134	LR: 0.050000
Training Epoch: 37 [23552/50000]	Loss: 1.0104	LR: 0.050000
Training Epoch: 37 [23680/50000]	Loss: 1.2830	LR: 0.050000
Training Epoch: 37 [23808/50000]	Loss: 1.2003	LR: 0.050000
Training Epoch: 37 [23936/50000]	Loss: 0.9860	LR: 0.050000
Training Epoch: 37 [24064/50000]	Loss: 1.0067	LR: 0.050000
Training Epoch: 37 [24192/50000]	Loss: 0.8745	LR: 0.050000
Training Epoch: 37 [24320/50000]	Loss: 0.8660	LR: 0.050000
Training Epoch: 37 [24448/50000]	Loss: 0.8980	LR: 0.050000
Training Epoch: 37 [24576/50000]	Loss: 1.0394	LR: 0.050000
Training Epoch: 37 [24704/50000]	Loss: 0.9863	LR: 0.050000
Training Epoch: 37 [24832/50000]	Loss: 0.9182	LR: 0.050000
Training Epoch: 37 [24960/50000]	Loss: 1.0302	LR: 0.050000
Training Epoch: 37 [25088/50000]	Loss: 0.7134	LR: 0.050000
Training Epoch: 37 [25216/50000]	Loss: 0.9999	LR: 0.050000
Training Epoch: 37 [25344/50000]	Loss: 0.8754	LR: 0.050000
Training Epoch: 37 [25472/50000]	Loss: 1.0012	LR: 0.050000
Training Epoch: 37 [25600/50000]	Loss: 0.6285	LR: 0.050000
Training Epoch: 37 [25728/50000]	Loss: 0.9035	LR: 0.050000
Training Epoch: 37 [25856/50000]	Loss: 1.0948	LR: 0.050000
Training Epoch: 37 [25984/50000]	Loss: 1.1120	LR: 0.050000
Training Epoch: 37 [26112/50000]	Loss: 0.8985	LR: 0.050000
Training Epoch: 37 [26240/50000]	Loss: 0.9531	LR: 0.050000
Training Epoch: 37 [26368/50000]	Loss: 0.9813	LR: 0.050000
Training Epoch: 37 [26496/50000]	Loss: 0.8891	LR: 0.050000
Training Epoch: 37 [26624/50000]	Loss: 0.8808	LR: 0.050000
Training Epoch: 37 [26752/50000]	Loss: 0.9414	LR: 0.050000
Training Epoch: 37 [26880/50000]	Loss: 0.9520	LR: 0.050000
Training Epoch: 37 [27008/50000]	Loss: 0.9969	LR: 0.050000
Training Epoch: 37 [27136/50000]	Loss: 0.9546	LR: 0.050000
Training Epoch: 37 [27264/50000]	Loss: 1.1097	LR: 0.050000
Training Epoch: 37 [27392/50000]	Loss: 0.7673	LR: 0.050000
Training Epoch: 37 [27520/50000]	Loss: 1.0216	LR: 0.050000
Training Epoch: 37 [27648/50000]	Loss: 0.8482	LR: 0.050000
Training Epoch: 37 [27776/50000]	Loss: 0.9423	LR: 0.050000
Training Epoch: 37 [27904/50000]	Loss: 0.9764	LR: 0.050000
Training Epoch: 37 [28032/50000]	Loss: 0.7322	LR: 0.050000
Training Epoch: 37 [28160/50000]	Loss: 1.0633	LR: 0.050000
Training Epoch: 37 [28288/50000]	Loss: 0.9695	LR: 0.050000
Training Epoch: 37 [28416/50000]	Loss: 0.8727	LR: 0.050000
Training Epoch: 37 [28544/50000]	Loss: 0.9254	LR: 0.050000
Training Epoch: 37 [28672/50000]	Loss: 1.0063	LR: 0.050000
Training Epoch: 37 [28800/50000]	Loss: 0.8722	LR: 0.050000
Training Epoch: 37 [28928/50000]	Loss: 1.0115	LR: 0.050000
Training Epoch: 37 [29056/50000]	Loss: 0.7710	LR: 0.050000
Training Epoch: 37 [29184/50000]	Loss: 0.9402	LR: 0.050000
Training Epoch: 37 [29312/50000]	Loss: 0.9130	LR: 0.050000
Training Epoch: 37 [29440/50000]	Loss: 0.8289	LR: 0.050000
Training Epoch: 37 [29568/50000]	Loss: 1.0196	LR: 0.050000
Training Epoch: 37 [29696/50000]	Loss: 0.9818	LR: 0.050000
Training Epoch: 37 [29824/50000]	Loss: 0.9682	LR: 0.050000
Training Epoch: 37 [29952/50000]	Loss: 1.0980	LR: 0.050000
Training Epoch: 37 [30080/50000]	Loss: 0.9203	LR: 0.050000
Training Epoch: 37 [30208/50000]	Loss: 1.1658	LR: 0.050000
Training Epoch: 37 [30336/50000]	Loss: 0.8946	LR: 0.050000
Training Epoch: 37 [30464/50000]	Loss: 1.0853	LR: 0.050000
Training Epoch: 37 [30592/50000]	Loss: 1.0020	LR: 0.050000
Training Epoch: 37 [30720/50000]	Loss: 1.2030	LR: 0.050000
Training Epoch: 37 [30848/50000]	Loss: 0.8230	LR: 0.050000
Training Epoch: 37 [30976/50000]	Loss: 1.1165	LR: 0.050000
Training Epoch: 37 [31104/50000]	Loss: 0.7570	LR: 0.050000
Training Epoch: 37 [31232/50000]	Loss: 1.0779	LR: 0.050000
Training Epoch: 37 [31360/50000]	Loss: 1.0007	LR: 0.050000
Training Epoch: 37 [31488/50000]	Loss: 1.0229	LR: 0.050000
Training Epoch: 37 [31616/50000]	Loss: 1.0263	LR: 0.050000
Training Epoch: 37 [31744/50000]	Loss: 0.9538	LR: 0.050000
Training Epoch: 37 [31872/50000]	Loss: 0.9842	LR: 0.050000
Training Epoch: 37 [32000/50000]	Loss: 1.0072	LR: 0.050000
Training Epoch: 37 [32128/50000]	Loss: 1.0057	LR: 0.050000
Training Epoch: 37 [32256/50000]	Loss: 1.0645	LR: 0.050000
Training Epoch: 37 [32384/50000]	Loss: 1.0385	LR: 0.050000
Training Epoch: 37 [32512/50000]	Loss: 1.0054	LR: 0.050000
Training Epoch: 37 [32640/50000]	Loss: 1.1768	LR: 0.050000
Training Epoch: 37 [32768/50000]	Loss: 0.9403	LR: 0.050000
Training Epoch: 37 [32896/50000]	Loss: 0.8727	LR: 0.050000
Training Epoch: 37 [33024/50000]	Loss: 1.0641	LR: 0.050000
Training Epoch: 37 [33152/50000]	Loss: 0.8599	LR: 0.050000
Training Epoch: 37 [33280/50000]	Loss: 0.9897	LR: 0.050000
Training Epoch: 37 [33408/50000]	Loss: 1.0091	LR: 0.050000
Training Epoch: 37 [33536/50000]	Loss: 1.2004	LR: 0.050000
Training Epoch: 37 [33664/50000]	Loss: 0.8489	LR: 0.050000
Training Epoch: 37 [33792/50000]	Loss: 0.9562	LR: 0.050000
Training Epoch: 37 [33920/50000]	Loss: 0.9520	LR: 0.050000
Training Epoch: 37 [34048/50000]	Loss: 0.9561	LR: 0.050000
Training Epoch: 37 [34176/50000]	Loss: 1.0466	LR: 0.050000
Training Epoch: 37 [34304/50000]	Loss: 1.0702	LR: 0.050000
Training Epoch: 37 [34432/50000]	Loss: 0.8948	LR: 0.050000
Training Epoch: 37 [34560/50000]	Loss: 0.9757	LR: 0.050000
Training Epoch: 37 [34688/50000]	Loss: 1.0392	LR: 0.050000
Training Epoch: 37 [34816/50000]	Loss: 1.0245	LR: 0.050000
Training Epoch: 37 [34944/50000]	Loss: 1.0698	LR: 0.050000
Training Epoch: 37 [35072/50000]	Loss: 1.3412	LR: 0.050000
Training Epoch: 37 [35200/50000]	Loss: 0.7433	LR: 0.050000
Training Epoch: 37 [35328/50000]	Loss: 1.1324	LR: 0.050000
Training Epoch: 37 [35456/50000]	Loss: 0.9354	LR: 0.050000
Training Epoch: 37 [35584/50000]	Loss: 0.9549	LR: 0.050000
Training Epoch: 37 [35712/50000]	Loss: 1.0521	LR: 0.050000
Training Epoch: 37 [35840/50000]	Loss: 0.9254	LR: 0.050000
Training Epoch: 37 [35968/50000]	Loss: 1.0646	LR: 0.050000
Training Epoch: 37 [36096/50000]	Loss: 0.9479	LR: 0.050000
Training Epoch: 37 [36224/50000]	Loss: 0.8866	LR: 0.050000
Training Epoch: 37 [36352/50000]	Loss: 1.1712	LR: 0.050000
Training Epoch: 37 [36480/50000]	Loss: 0.8459	LR: 0.050000
Training Epoch: 37 [36608/50000]	Loss: 0.9599	LR: 0.050000
Training Epoch: 37 [36736/50000]	Loss: 1.1609	LR: 0.050000
Training Epoch: 37 [36864/50000]	Loss: 1.1297	LR: 0.050000
Training Epoch: 37 [36992/50000]	Loss: 0.8155	LR: 0.050000
Training Epoch: 37 [37120/50000]	Loss: 0.7405	LR: 0.050000
Training Epoch: 37 [37248/50000]	Loss: 0.9044	LR: 0.050000
Training Epoch: 37 [37376/50000]	Loss: 0.8435	LR: 0.050000
Training Epoch: 37 [37504/50000]	Loss: 0.9414	LR: 0.050000
Training Epoch: 37 [37632/50000]	Loss: 0.9520	LR: 0.050000
Training Epoch: 37 [37760/50000]	Loss: 1.1746	LR: 0.050000
Training Epoch: 37 [37888/50000]	Loss: 1.1638	LR: 0.050000
Training Epoch: 37 [38016/50000]	Loss: 0.8500	LR: 0.050000
Training Epoch: 37 [38144/50000]	Loss: 1.1081	LR: 0.050000
Training Epoch: 37 [38272/50000]	Loss: 1.0760	LR: 0.050000
Training Epoch: 37 [38400/50000]	Loss: 0.8105	LR: 0.050000
Training Epoch: 37 [38528/50000]	Loss: 1.0075	LR: 0.050000
Training Epoch: 37 [38656/50000]	Loss: 1.0050	LR: 0.050000
Training Epoch: 37 [38784/50000]	Loss: 0.8460	LR: 0.050000
Training Epoch: 37 [38912/50000]	Loss: 0.9290	LR: 0.050000
Training Epoch: 37 [39040/50000]	Loss: 1.0505	LR: 0.050000
Training Epoch: 37 [39168/50000]	Loss: 0.9809	LR: 0.050000
Training Epoch: 37 [39296/50000]	Loss: 1.0816	LR: 0.050000
Training Epoch: 37 [39424/50000]	Loss: 0.9982	LR: 0.050000
Training Epoch: 37 [39552/50000]	Loss: 0.9737	LR: 0.050000
Training Epoch: 37 [39680/50000]	Loss: 0.8737	LR: 0.050000
Training Epoch: 37 [39808/50000]	Loss: 1.2076	LR: 0.050000
Training Epoch: 37 [39936/50000]	Loss: 1.0973	LR: 0.050000
Training Epoch: 37 [40064/50000]	Loss: 0.8858	LR: 0.050000
Training Epoch: 37 [40192/50000]	Loss: 0.9300	LR: 0.050000
Training Epoch: 37 [40320/50000]	Loss: 0.8432	LR: 0.050000
Training Epoch: 37 [40448/50000]	Loss: 1.1991	LR: 0.050000
Training Epoch: 37 [40576/50000]	Loss: 1.1429	LR: 0.050000
Training Epoch: 37 [40704/50000]	Loss: 1.1284	LR: 0.050000
Training Epoch: 37 [40832/50000]	Loss: 0.9968	LR: 0.050000
Training Epoch: 37 [40960/50000]	Loss: 0.8859	LR: 0.050000
Training Epoch: 37 [41088/50000]	Loss: 1.2385	LR: 0.050000
Training Epoch: 37 [41216/50000]	Loss: 0.8977	LR: 0.050000
Training Epoch: 37 [41344/50000]	Loss: 0.8797	LR: 0.050000
Training Epoch: 37 [41472/50000]	Loss: 1.0273	LR: 0.050000
Training Epoch: 37 [41600/50000]	Loss: 1.0723	LR: 0.050000
Training Epoch: 37 [41728/50000]	Loss: 0.8812	LR: 0.050000
Training Epoch: 37 [41856/50000]	Loss: 1.0609	LR: 0.050000
Training Epoch: 37 [41984/50000]	Loss: 1.1198	LR: 0.050000
Training Epoch: 37 [42112/50000]	Loss: 0.8937	LR: 0.050000
Training Epoch: 37 [42240/50000]	Loss: 1.0421	LR: 0.050000
Training Epoch: 37 [42368/50000]	Loss: 0.9917	LR: 0.050000
Training Epoch: 37 [42496/50000]	Loss: 0.9694	LR: 0.050000
Training Epoch: 37 [42624/50000]	Loss: 1.1245	LR: 0.050000
Training Epoch: 37 [42752/50000]	Loss: 1.0706	LR: 0.050000
Training Epoch: 37 [42880/50000]	Loss: 0.8840	LR: 0.050000
Training Epoch: 37 [43008/50000]	Loss: 1.1906	LR: 0.050000
Training Epoch: 37 [43136/50000]	Loss: 0.9558	LR: 0.050000
Training Epoch: 37 [43264/50000]	Loss: 1.1372	LR: 0.050000
Training Epoch: 37 [43392/50000]	Loss: 1.1164	LR: 0.050000
Training Epoch: 37 [43520/50000]	Loss: 0.9638	LR: 0.050000
Training Epoch: 37 [43648/50000]	Loss: 1.0494	LR: 0.050000
Training Epoch: 37 [43776/50000]	Loss: 0.7786	LR: 0.050000
Training Epoch: 37 [43904/50000]	Loss: 1.2002	LR: 0.050000
Training Epoch: 37 [44032/50000]	Loss: 1.1360	LR: 0.050000
Training Epoch: 37 [44160/50000]	Loss: 1.2649	LR: 0.050000
Training Epoch: 37 [44288/50000]	Loss: 1.0470	LR: 0.050000
Training Epoch: 37 [44416/50000]	Loss: 0.8399	LR: 0.050000
Training Epoch: 37 [44544/50000]	Loss: 0.9371	LR: 0.050000
Training Epoch: 37 [44672/50000]	Loss: 0.9028	LR: 0.050000
Training Epoch: 37 [44800/50000]	Loss: 1.0682	LR: 0.050000
Training Epoch: 37 [44928/50000]	Loss: 1.0582	LR: 0.050000
Training Epoch: 37 [45056/50000]	Loss: 0.9416	LR: 0.050000
Training Epoch: 37 [45184/50000]	Loss: 0.8064	LR: 0.050000
Training Epoch: 37 [45312/50000]	Loss: 0.8670	LR: 0.050000
Training Epoch: 37 [45440/50000]	Loss: 0.8757	LR: 0.050000
Training Epoch: 37 [45568/50000]	Loss: 1.1870	LR: 0.050000
Training Epoch: 37 [45696/50000]	Loss: 1.1957	LR: 0.050000
Training Epoch: 37 [45824/50000]	Loss: 0.9516	LR: 0.050000
Training Epoch: 37 [45952/50000]	Loss: 1.0524	LR: 0.050000
Training Epoch: 37 [46080/50000]	Loss: 0.9170	LR: 0.050000
Training Epoch: 37 [46208/50000]	Loss: 1.1380	LR: 0.050000
Training Epoch: 37 [46336/50000]	Loss: 1.2047	LR: 0.050000
Training Epoch: 37 [46464/50000]	Loss: 0.7798	LR: 0.050000
Training Epoch: 37 [46592/50000]	Loss: 1.0644	LR: 0.050000
Training Epoch: 37 [46720/50000]	Loss: 1.1842	LR: 0.050000
Training Epoch: 37 [46848/50000]	Loss: 0.9707	LR: 0.050000
Training Epoch: 37 [46976/50000]	Loss: 1.1082	LR: 0.050000
Training Epoch: 37 [47104/50000]	Loss: 1.0122	LR: 0.050000
Training Epoch: 37 [47232/50000]	Loss: 1.0091	LR: 0.050000
Training Epoch: 37 [47360/50000]	Loss: 0.9713	LR: 0.050000
Training Epoch: 37 [47488/50000]	Loss: 0.9313	LR: 0.050000
Training Epoch: 37 [47616/50000]	Loss: 0.9617	LR: 0.050000
Training Epoch: 37 [47744/50000]	Loss: 0.9803	LR: 0.050000
Training Epoch: 37 [47872/50000]	Loss: 1.0220	LR: 0.050000
Training Epoch: 37 [48000/50000]	Loss: 1.1351	LR: 0.050000
Training Epoch: 37 [48128/50000]	Loss: 1.0376	LR: 0.050000
Training Epoch: 37 [48256/50000]	Loss: 1.0184	LR: 0.050000
Training Epoch: 37 [48384/50000]	Loss: 1.1016	LR: 0.050000
Training Epoch: 37 [48512/50000]	Loss: 1.0523	LR: 0.050000
Training Epoch: 37 [48640/50000]	Loss: 0.9886	LR: 0.050000
Training Epoch: 37 [48768/50000]	Loss: 1.1379	LR: 0.050000
Training Epoch: 37 [48896/50000]	Loss: 1.0309	LR: 0.050000
Training Epoch: 37 [49024/50000]	Loss: 1.1718	LR: 0.050000
Training Epoch: 37 [49152/50000]	Loss: 1.0709	LR: 0.050000
Training Epoch: 37 [49280/50000]	Loss: 0.8212	LR: 0.050000
Training Epoch: 37 [49408/50000]	Loss: 0.9608	LR: 0.050000
Training Epoch: 37 [49536/50000]	Loss: 0.9336	LR: 0.050000
Training Epoch: 37 [49664/50000]	Loss: 0.9727	LR: 0.050000
Training Epoch: 37 [49792/50000]	Loss: 1.2814	LR: 0.050000
Training Epoch: 37 [49920/50000]	Loss: 1.1063	LR: 0.050000
Training Epoch: 37 [50000/50000]	Loss: 0.8912	LR: 0.050000
epoch 37 training time consumed: 53.87s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  135792 GB |  135792 GB |
|       from large pool |  123392 KB |    1034 MB |  135658 GB |  135658 GB |
|       from small pool |   10798 KB |      13 MB |     133 GB |     133 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  135792 GB |  135792 GB |
|       from large pool |  123392 KB |    1034 MB |  135658 GB |  135658 GB |
|       from small pool |   10798 KB |      13 MB |     133 GB |     133 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   59758 GB |   59758 GB |
|       from large pool |  155136 KB |  433088 KB |   59610 GB |   59610 GB |
|       from small pool |    1490 KB |    3494 KB |     147 GB |     147 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    5240 K  |    5239 K  |
|       from large pool |      24    |      65    |    2735 K  |    2735 K  |
|       from small pool |     231    |     274    |    2504 K  |    2504 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    5240 K  |    5239 K  |
|       from large pool |      24    |      65    |    2735 K  |    2735 K  |
|       from small pool |     231    |     274    |    2504 K  |    2504 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2591 K  |    2591 K  |
|       from large pool |       9    |      14    |    1323 K  |    1323 K  |
|       from small pool |      12    |      16    |    1267 K  |    1267 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 37, Average loss: 0.0116, Accuracy: 0.6085, Time consumed:3.46s

Training Epoch: 38 [128/50000]	Loss: 1.0992	LR: 0.050000
Training Epoch: 38 [256/50000]	Loss: 0.8825	LR: 0.050000
Training Epoch: 38 [384/50000]	Loss: 0.8224	LR: 0.050000
Training Epoch: 38 [512/50000]	Loss: 0.8173	LR: 0.050000
Training Epoch: 38 [640/50000]	Loss: 0.9430	LR: 0.050000
Training Epoch: 38 [768/50000]	Loss: 0.7474	LR: 0.050000
Training Epoch: 38 [896/50000]	Loss: 0.9163	LR: 0.050000
Training Epoch: 38 [1024/50000]	Loss: 0.8123	LR: 0.050000
Training Epoch: 38 [1152/50000]	Loss: 0.6977	LR: 0.050000
Training Epoch: 38 [1280/50000]	Loss: 0.7795	LR: 0.050000
Training Epoch: 38 [1408/50000]	Loss: 0.9228	LR: 0.050000
Training Epoch: 38 [1536/50000]	Loss: 0.9041	LR: 0.050000
Training Epoch: 38 [1664/50000]	Loss: 0.7404	LR: 0.050000
Training Epoch: 38 [1792/50000]	Loss: 0.6864	LR: 0.050000
Training Epoch: 38 [1920/50000]	Loss: 0.8752	LR: 0.050000
Training Epoch: 38 [2048/50000]	Loss: 0.7839	LR: 0.050000
Training Epoch: 38 [2176/50000]	Loss: 1.1735	LR: 0.050000
Training Epoch: 38 [2304/50000]	Loss: 0.8030	LR: 0.050000
Training Epoch: 38 [2432/50000]	Loss: 0.8859	LR: 0.050000
Training Epoch: 38 [2560/50000]	Loss: 0.9199	LR: 0.050000
Training Epoch: 38 [2688/50000]	Loss: 0.9026	LR: 0.050000
Training Epoch: 38 [2816/50000]	Loss: 0.8974	LR: 0.050000
Training Epoch: 38 [2944/50000]	Loss: 0.6787	LR: 0.050000
Training Epoch: 38 [3072/50000]	Loss: 0.8533	LR: 0.050000
Training Epoch: 38 [3200/50000]	Loss: 0.9839	LR: 0.050000
Training Epoch: 38 [3328/50000]	Loss: 1.0467	LR: 0.050000
Training Epoch: 38 [3456/50000]	Loss: 0.9444	LR: 0.050000
Training Epoch: 38 [3584/50000]	Loss: 0.9938	LR: 0.050000
Training Epoch: 38 [3712/50000]	Loss: 0.7862	LR: 0.050000
Training Epoch: 38 [3840/50000]	Loss: 0.7924	LR: 0.050000
Training Epoch: 38 [3968/50000]	Loss: 0.7972	LR: 0.050000
Training Epoch: 38 [4096/50000]	Loss: 0.7315	LR: 0.050000
Training Epoch: 38 [4224/50000]	Loss: 0.7090	LR: 0.050000
Training Epoch: 38 [4352/50000]	Loss: 0.9063	LR: 0.050000
Training Epoch: 38 [4480/50000]	Loss: 0.8333	LR: 0.050000
Training Epoch: 38 [4608/50000]	Loss: 0.8691	LR: 0.050000
Training Epoch: 38 [4736/50000]	Loss: 0.9513	LR: 0.050000
Training Epoch: 38 [4864/50000]	Loss: 0.9592	LR: 0.050000
Training Epoch: 38 [4992/50000]	Loss: 0.9640	LR: 0.050000
Training Epoch: 38 [5120/50000]	Loss: 0.7072	LR: 0.050000
Training Epoch: 38 [5248/50000]	Loss: 0.7760	LR: 0.050000
Training Epoch: 38 [5376/50000]	Loss: 0.8173	LR: 0.050000
Training Epoch: 38 [5504/50000]	Loss: 0.9656	LR: 0.050000
Training Epoch: 38 [5632/50000]	Loss: 0.8552	LR: 0.050000
Training Epoch: 38 [5760/50000]	Loss: 1.0049	LR: 0.050000
Training Epoch: 38 [5888/50000]	Loss: 0.8364	LR: 0.050000
Training Epoch: 38 [6016/50000]	Loss: 0.8228	LR: 0.050000
Training Epoch: 38 [6144/50000]	Loss: 0.8425	LR: 0.050000
Training Epoch: 38 [6272/50000]	Loss: 0.9032	LR: 0.050000
Training Epoch: 38 [6400/50000]	Loss: 0.8997	LR: 0.050000
Training Epoch: 38 [6528/50000]	Loss: 0.9002	LR: 0.050000
Training Epoch: 38 [6656/50000]	Loss: 0.8365	LR: 0.050000
Training Epoch: 38 [6784/50000]	Loss: 0.8738	LR: 0.050000
Training Epoch: 38 [6912/50000]	Loss: 0.7922	LR: 0.050000
Training Epoch: 38 [7040/50000]	Loss: 0.8203	LR: 0.050000
Training Epoch: 38 [7168/50000]	Loss: 0.9567	LR: 0.050000
Training Epoch: 38 [7296/50000]	Loss: 0.9167	LR: 0.050000
Training Epoch: 38 [7424/50000]	Loss: 0.8622	LR: 0.050000
Training Epoch: 38 [7552/50000]	Loss: 0.9450	LR: 0.050000
Training Epoch: 38 [7680/50000]	Loss: 0.9625	LR: 0.050000
Training Epoch: 38 [7808/50000]	Loss: 0.8396	LR: 0.050000
Training Epoch: 38 [7936/50000]	Loss: 0.9646	LR: 0.050000
Training Epoch: 38 [8064/50000]	Loss: 0.9068	LR: 0.050000
Training Epoch: 38 [8192/50000]	Loss: 0.9085	LR: 0.050000
Training Epoch: 38 [8320/50000]	Loss: 0.9089	LR: 0.050000
Training Epoch: 38 [8448/50000]	Loss: 0.9165	LR: 0.050000
Training Epoch: 38 [8576/50000]	Loss: 0.8151	LR: 0.050000
Training Epoch: 38 [8704/50000]	Loss: 0.8632	LR: 0.050000
Training Epoch: 38 [8832/50000]	Loss: 0.9048	LR: 0.050000
Training Epoch: 38 [8960/50000]	Loss: 0.8529	LR: 0.050000
Training Epoch: 38 [9088/50000]	Loss: 0.8124	LR: 0.050000
Training Epoch: 38 [9216/50000]	Loss: 0.6432	LR: 0.050000
Training Epoch: 38 [9344/50000]	Loss: 0.9933	LR: 0.050000
Training Epoch: 38 [9472/50000]	Loss: 0.8847	LR: 0.050000
Training Epoch: 38 [9600/50000]	Loss: 0.9347	LR: 0.050000
Training Epoch: 38 [9728/50000]	Loss: 0.9106	LR: 0.050000
Training Epoch: 38 [9856/50000]	Loss: 0.7808	LR: 0.050000
Training Epoch: 38 [9984/50000]	Loss: 0.6480	LR: 0.050000
Training Epoch: 38 [10112/50000]	Loss: 0.8619	LR: 0.050000
Training Epoch: 38 [10240/50000]	Loss: 0.8014	LR: 0.050000
Training Epoch: 38 [10368/50000]	Loss: 0.7849	LR: 0.050000
Training Epoch: 38 [10496/50000]	Loss: 0.9161	LR: 0.050000
Training Epoch: 38 [10624/50000]	Loss: 0.7795	LR: 0.050000
Training Epoch: 38 [10752/50000]	Loss: 0.7969	LR: 0.050000
Training Epoch: 38 [10880/50000]	Loss: 0.8715	LR: 0.050000
Training Epoch: 38 [11008/50000]	Loss: 0.8344	LR: 0.050000
Training Epoch: 38 [11136/50000]	Loss: 0.7969	LR: 0.050000
Training Epoch: 38 [11264/50000]	Loss: 0.8794	LR: 0.050000
Training Epoch: 38 [11392/50000]	Loss: 0.8647	LR: 0.050000
Training Epoch: 38 [11520/50000]	Loss: 0.9609	LR: 0.050000
Training Epoch: 38 [11648/50000]	Loss: 0.7714	LR: 0.050000
Training Epoch: 38 [11776/50000]	Loss: 0.8854	LR: 0.050000
Training Epoch: 38 [11904/50000]	Loss: 1.0061	LR: 0.050000
Training Epoch: 38 [12032/50000]	Loss: 0.8439	LR: 0.050000
Training Epoch: 38 [12160/50000]	Loss: 1.0168	LR: 0.050000
Training Epoch: 38 [12288/50000]	Loss: 0.8336	LR: 0.050000
Training Epoch: 38 [12416/50000]	Loss: 0.9717	LR: 0.050000
Training Epoch: 38 [12544/50000]	Loss: 0.8311	LR: 0.050000
Training Epoch: 38 [12672/50000]	Loss: 0.8482	LR: 0.050000
Training Epoch: 38 [12800/50000]	Loss: 0.8102	LR: 0.050000
Training Epoch: 38 [12928/50000]	Loss: 1.0546	LR: 0.050000
Training Epoch: 38 [13056/50000]	Loss: 0.9186	LR: 0.050000
Training Epoch: 38 [13184/50000]	Loss: 0.9310	LR: 0.050000
Training Epoch: 38 [13312/50000]	Loss: 1.0097	LR: 0.050000
Training Epoch: 38 [13440/50000]	Loss: 1.1707	LR: 0.050000
Training Epoch: 38 [13568/50000]	Loss: 1.1850	LR: 0.050000
Training Epoch: 38 [13696/50000]	Loss: 0.8866	LR: 0.050000
Training Epoch: 38 [13824/50000]	Loss: 0.9457	LR: 0.050000
Training Epoch: 38 [13952/50000]	Loss: 0.9278	LR: 0.050000
Training Epoch: 38 [14080/50000]	Loss: 0.9623	LR: 0.050000
Training Epoch: 38 [14208/50000]	Loss: 0.9360	LR: 0.050000
Training Epoch: 38 [14336/50000]	Loss: 0.8743	LR: 0.050000
Training Epoch: 38 [14464/50000]	Loss: 0.8888	LR: 0.050000
Training Epoch: 38 [14592/50000]	Loss: 0.8316	LR: 0.050000
Training Epoch: 38 [14720/50000]	Loss: 0.8710	LR: 0.050000
Training Epoch: 38 [14848/50000]	Loss: 0.9727	LR: 0.050000
Training Epoch: 38 [14976/50000]	Loss: 0.9693	LR: 0.050000
Training Epoch: 38 [15104/50000]	Loss: 0.7845	LR: 0.050000
Training Epoch: 38 [15232/50000]	Loss: 0.9042	LR: 0.050000
Training Epoch: 38 [15360/50000]	Loss: 0.9159	LR: 0.050000
Training Epoch: 38 [15488/50000]	Loss: 0.8835	LR: 0.050000
Training Epoch: 38 [15616/50000]	Loss: 0.9282	LR: 0.050000
Training Epoch: 38 [15744/50000]	Loss: 0.9557	LR: 0.050000
Training Epoch: 38 [15872/50000]	Loss: 0.9033	LR: 0.050000
Training Epoch: 38 [16000/50000]	Loss: 0.8792	LR: 0.050000
Training Epoch: 38 [16128/50000]	Loss: 0.8814	LR: 0.050000
Training Epoch: 38 [16256/50000]	Loss: 0.9396	LR: 0.050000
Training Epoch: 38 [16384/50000]	Loss: 0.8617	LR: 0.050000
Training Epoch: 38 [16512/50000]	Loss: 1.0503	LR: 0.050000
Training Epoch: 38 [16640/50000]	Loss: 0.8333	LR: 0.050000
Training Epoch: 38 [16768/50000]	Loss: 1.0036	LR: 0.050000
Training Epoch: 38 [16896/50000]	Loss: 0.8658	LR: 0.050000
Training Epoch: 38 [17024/50000]	Loss: 0.7805	LR: 0.050000
Training Epoch: 38 [17152/50000]	Loss: 0.7552	LR: 0.050000
Training Epoch: 38 [17280/50000]	Loss: 1.1784	LR: 0.050000
Training Epoch: 38 [17408/50000]	Loss: 0.8282	LR: 0.050000
Training Epoch: 38 [17536/50000]	Loss: 0.9991	LR: 0.050000
Training Epoch: 38 [17664/50000]	Loss: 1.1483	LR: 0.050000
Training Epoch: 38 [17792/50000]	Loss: 0.9574	LR: 0.050000
Training Epoch: 38 [17920/50000]	Loss: 0.7980	LR: 0.050000
Training Epoch: 38 [18048/50000]	Loss: 0.8024	LR: 0.050000
Training Epoch: 38 [18176/50000]	Loss: 1.0753	LR: 0.050000
Training Epoch: 38 [18304/50000]	Loss: 1.0744	LR: 0.050000
Training Epoch: 38 [18432/50000]	Loss: 0.9247	LR: 0.050000
Training Epoch: 38 [18560/50000]	Loss: 0.7976	LR: 0.050000
Training Epoch: 38 [18688/50000]	Loss: 0.9184	LR: 0.050000
Training Epoch: 38 [18816/50000]	Loss: 0.9738	LR: 0.050000
Training Epoch: 38 [18944/50000]	Loss: 0.7736	LR: 0.050000
Training Epoch: 38 [19072/50000]	Loss: 0.8558	LR: 0.050000
Training Epoch: 38 [19200/50000]	Loss: 1.1252	LR: 0.050000
Training Epoch: 38 [19328/50000]	Loss: 0.7867	LR: 0.050000
Training Epoch: 38 [19456/50000]	Loss: 0.8557	LR: 0.050000
Training Epoch: 38 [19584/50000]	Loss: 0.8040	LR: 0.050000
Training Epoch: 38 [19712/50000]	Loss: 1.0248	LR: 0.050000
Training Epoch: 38 [19840/50000]	Loss: 0.9563	LR: 0.050000
Training Epoch: 38 [19968/50000]	Loss: 0.8440	LR: 0.050000
Training Epoch: 38 [20096/50000]	Loss: 0.8899	LR: 0.050000
Training Epoch: 38 [20224/50000]	Loss: 0.9299	LR: 0.050000
Training Epoch: 38 [20352/50000]	Loss: 0.9570	LR: 0.050000
Training Epoch: 38 [20480/50000]	Loss: 1.0550	LR: 0.050000
Training Epoch: 38 [20608/50000]	Loss: 0.8539	LR: 0.050000
Training Epoch: 38 [20736/50000]	Loss: 1.0703	LR: 0.050000
Training Epoch: 38 [20864/50000]	Loss: 0.9726	LR: 0.050000
Training Epoch: 38 [20992/50000]	Loss: 1.0849	LR: 0.050000
Training Epoch: 38 [21120/50000]	Loss: 0.9303	LR: 0.050000
Training Epoch: 38 [21248/50000]	Loss: 0.9146	LR: 0.050000
Training Epoch: 38 [21376/50000]	Loss: 0.8274	LR: 0.050000
Training Epoch: 38 [21504/50000]	Loss: 0.8319	LR: 0.050000
Training Epoch: 38 [21632/50000]	Loss: 1.1536	LR: 0.050000
Training Epoch: 38 [21760/50000]	Loss: 0.8813	LR: 0.050000
Training Epoch: 38 [21888/50000]	Loss: 1.1746	LR: 0.050000
Training Epoch: 38 [22016/50000]	Loss: 0.9017	LR: 0.050000
Training Epoch: 38 [22144/50000]	Loss: 0.9902	LR: 0.050000
Training Epoch: 38 [22272/50000]	Loss: 0.9627	LR: 0.050000
Training Epoch: 38 [22400/50000]	Loss: 0.9039	LR: 0.050000
Training Epoch: 38 [22528/50000]	Loss: 0.8794	LR: 0.050000
Training Epoch: 38 [22656/50000]	Loss: 0.8418	LR: 0.050000
Training Epoch: 38 [22784/50000]	Loss: 1.0226	LR: 0.050000
Training Epoch: 38 [22912/50000]	Loss: 0.9345	LR: 0.050000
Training Epoch: 38 [23040/50000]	Loss: 0.8153	LR: 0.050000
Training Epoch: 38 [23168/50000]	Loss: 0.9507	LR: 0.050000
Training Epoch: 38 [23296/50000]	Loss: 0.8945	LR: 0.050000
Training Epoch: 38 [23424/50000]	Loss: 0.9280	LR: 0.050000
Training Epoch: 38 [23552/50000]	Loss: 0.7738	LR: 0.050000
Training Epoch: 38 [23680/50000]	Loss: 1.1002	LR: 0.050000
Training Epoch: 38 [23808/50000]	Loss: 1.0441	LR: 0.050000
Training Epoch: 38 [23936/50000]	Loss: 1.2181	LR: 0.050000
Training Epoch: 38 [24064/50000]	Loss: 0.9517	LR: 0.050000
Training Epoch: 38 [24192/50000]	Loss: 0.8867	LR: 0.050000
Training Epoch: 38 [24320/50000]	Loss: 0.8064	LR: 0.050000
Training Epoch: 38 [24448/50000]	Loss: 0.7276	LR: 0.050000
Training Epoch: 38 [24576/50000]	Loss: 0.6521	LR: 0.050000
Training Epoch: 38 [24704/50000]	Loss: 0.9487	LR: 0.050000
Training Epoch: 38 [24832/50000]	Loss: 0.8162	LR: 0.050000
Training Epoch: 38 [24960/50000]	Loss: 0.9688	LR: 0.050000
Training Epoch: 38 [25088/50000]	Loss: 1.1208	LR: 0.050000
Training Epoch: 38 [25216/50000]	Loss: 1.0325	LR: 0.050000
Training Epoch: 38 [25344/50000]	Loss: 0.8450	LR: 0.050000
Training Epoch: 38 [25472/50000]	Loss: 1.1913	LR: 0.050000
Training Epoch: 38 [25600/50000]	Loss: 0.9374	LR: 0.050000
Training Epoch: 38 [25728/50000]	Loss: 1.0774	LR: 0.050000
Training Epoch: 38 [25856/50000]	Loss: 1.2065	LR: 0.050000
Training Epoch: 38 [25984/50000]	Loss: 0.9956	LR: 0.050000
Training Epoch: 38 [26112/50000]	Loss: 1.1247	LR: 0.050000
Training Epoch: 38 [26240/50000]	Loss: 1.0376	LR: 0.050000
Training Epoch: 38 [26368/50000]	Loss: 1.0047	LR: 0.050000
Training Epoch: 38 [26496/50000]	Loss: 0.9190	LR: 0.050000
Training Epoch: 38 [26624/50000]	Loss: 1.0333	LR: 0.050000
Training Epoch: 38 [26752/50000]	Loss: 0.8441	LR: 0.050000
Training Epoch: 38 [26880/50000]	Loss: 0.8437	LR: 0.050000
Training Epoch: 38 [27008/50000]	Loss: 1.0035	LR: 0.050000
Training Epoch: 38 [27136/50000]	Loss: 0.9012	LR: 0.050000
Training Epoch: 38 [27264/50000]	Loss: 1.0258	LR: 0.050000
Training Epoch: 38 [27392/50000]	Loss: 1.0108	LR: 0.050000
Training Epoch: 38 [27520/50000]	Loss: 0.8888	LR: 0.050000
Training Epoch: 38 [27648/50000]	Loss: 1.0674	LR: 0.050000
Training Epoch: 38 [27776/50000]	Loss: 0.8350	LR: 0.050000
Training Epoch: 38 [27904/50000]	Loss: 0.9489	LR: 0.050000
Training Epoch: 38 [28032/50000]	Loss: 0.9089	LR: 0.050000
Training Epoch: 38 [28160/50000]	Loss: 1.0677	LR: 0.050000
Training Epoch: 38 [28288/50000]	Loss: 1.1612	LR: 0.050000
Training Epoch: 38 [28416/50000]	Loss: 0.9140	LR: 0.050000
Training Epoch: 38 [28544/50000]	Loss: 0.7443	LR: 0.050000
Training Epoch: 38 [28672/50000]	Loss: 1.1135	LR: 0.050000
Training Epoch: 38 [28800/50000]	Loss: 0.9609	LR: 0.050000
Training Epoch: 38 [28928/50000]	Loss: 0.8074	LR: 0.050000
Training Epoch: 38 [29056/50000]	Loss: 0.9478	LR: 0.050000
Training Epoch: 38 [29184/50000]	Loss: 0.9238	LR: 0.050000
Training Epoch: 38 [29312/50000]	Loss: 1.0528	LR: 0.050000
Training Epoch: 38 [29440/50000]	Loss: 0.7742	LR: 0.050000
Training Epoch: 38 [29568/50000]	Loss: 1.0451	LR: 0.050000
Training Epoch: 38 [29696/50000]	Loss: 0.8783	LR: 0.050000
Training Epoch: 38 [29824/50000]	Loss: 0.9538	LR: 0.050000
Training Epoch: 38 [29952/50000]	Loss: 1.0306	LR: 0.050000
Training Epoch: 38 [30080/50000]	Loss: 0.8989	LR: 0.050000
Training Epoch: 38 [30208/50000]	Loss: 0.9934	LR: 0.050000
Training Epoch: 38 [30336/50000]	Loss: 1.1743	LR: 0.050000
Training Epoch: 38 [30464/50000]	Loss: 0.9577	LR: 0.050000
Training Epoch: 38 [30592/50000]	Loss: 0.8221	LR: 0.050000
Training Epoch: 38 [30720/50000]	Loss: 1.0617	LR: 0.050000
Training Epoch: 38 [30848/50000]	Loss: 0.9620	LR: 0.050000
Training Epoch: 38 [30976/50000]	Loss: 1.0129	LR: 0.050000
Training Epoch: 38 [31104/50000]	Loss: 1.1071	LR: 0.050000
Training Epoch: 38 [31232/50000]	Loss: 0.9939	LR: 0.050000
Training Epoch: 38 [31360/50000]	Loss: 0.8925	LR: 0.050000
Training Epoch: 38 [31488/50000]	Loss: 1.0762	LR: 0.050000
Training Epoch: 38 [31616/50000]	Loss: 1.0075	LR: 0.050000
Training Epoch: 38 [31744/50000]	Loss: 1.1013	LR: 0.050000
Training Epoch: 38 [31872/50000]	Loss: 1.0654	LR: 0.050000
Training Epoch: 38 [32000/50000]	Loss: 0.9541	LR: 0.050000
Training Epoch: 38 [32128/50000]	Loss: 1.1137	LR: 0.050000
Training Epoch: 38 [32256/50000]	Loss: 1.2233	LR: 0.050000
Training Epoch: 38 [32384/50000]	Loss: 1.1578	LR: 0.050000
Training Epoch: 38 [32512/50000]	Loss: 1.0093	LR: 0.050000
Training Epoch: 38 [32640/50000]	Loss: 0.9429	LR: 0.050000
Training Epoch: 38 [32768/50000]	Loss: 1.2457	LR: 0.050000
Training Epoch: 38 [32896/50000]	Loss: 0.9905	LR: 0.050000
Training Epoch: 38 [33024/50000]	Loss: 1.1536	LR: 0.050000
Training Epoch: 38 [33152/50000]	Loss: 1.0145	LR: 0.050000
Training Epoch: 38 [33280/50000]	Loss: 1.0514	LR: 0.050000
Training Epoch: 38 [33408/50000]	Loss: 0.9693	LR: 0.050000
Training Epoch: 38 [33536/50000]	Loss: 0.9629	LR: 0.050000
Training Epoch: 38 [33664/50000]	Loss: 1.0526	LR: 0.050000
Training Epoch: 38 [33792/50000]	Loss: 0.9905	LR: 0.050000
Training Epoch: 38 [33920/50000]	Loss: 1.0233	LR: 0.050000
Training Epoch: 38 [34048/50000]	Loss: 1.0979	LR: 0.050000
Training Epoch: 38 [34176/50000]	Loss: 1.1172	LR: 0.050000
Training Epoch: 38 [34304/50000]	Loss: 1.2392	LR: 0.050000
Training Epoch: 38 [34432/50000]	Loss: 1.2507	LR: 0.050000
Training Epoch: 38 [34560/50000]	Loss: 1.3761	LR: 0.050000
Training Epoch: 38 [34688/50000]	Loss: 0.9083	LR: 0.050000
Training Epoch: 38 [34816/50000]	Loss: 1.0117	LR: 0.050000
Training Epoch: 38 [34944/50000]	Loss: 0.8991	LR: 0.050000
Training Epoch: 38 [35072/50000]	Loss: 0.8403	LR: 0.050000
Training Epoch: 38 [35200/50000]	Loss: 1.0041	LR: 0.050000
Training Epoch: 38 [35328/50000]	Loss: 1.1691	LR: 0.050000
Training Epoch: 38 [35456/50000]	Loss: 1.1435	LR: 0.050000
Training Epoch: 38 [35584/50000]	Loss: 1.3763	LR: 0.050000
Training Epoch: 38 [35712/50000]	Loss: 0.9460	LR: 0.050000
Training Epoch: 38 [35840/50000]	Loss: 1.2198	LR: 0.050000
Training Epoch: 38 [35968/50000]	Loss: 0.9288	LR: 0.050000
Training Epoch: 38 [36096/50000]	Loss: 1.0848	LR: 0.050000
Training Epoch: 38 [36224/50000]	Loss: 0.9387	LR: 0.050000
Training Epoch: 38 [36352/50000]	Loss: 0.9723	LR: 0.050000
Training Epoch: 38 [36480/50000]	Loss: 0.9728	LR: 0.050000
Training Epoch: 38 [36608/50000]	Loss: 1.1994	LR: 0.050000
Training Epoch: 38 [36736/50000]	Loss: 0.9067	LR: 0.050000
Training Epoch: 38 [36864/50000]	Loss: 0.9585	LR: 0.050000
Training Epoch: 38 [36992/50000]	Loss: 1.1511	LR: 0.050000
Training Epoch: 38 [37120/50000]	Loss: 0.9020	LR: 0.050000
Training Epoch: 38 [37248/50000]	Loss: 0.9953	LR: 0.050000
Training Epoch: 38 [37376/50000]	Loss: 1.1579	LR: 0.050000
Training Epoch: 38 [37504/50000]	Loss: 1.1004	LR: 0.050000
Training Epoch: 38 [37632/50000]	Loss: 0.9683	LR: 0.050000
Training Epoch: 38 [37760/50000]	Loss: 0.9175	LR: 0.050000
Training Epoch: 38 [37888/50000]	Loss: 1.0095	LR: 0.050000
Training Epoch: 38 [38016/50000]	Loss: 1.0445	LR: 0.050000
Training Epoch: 38 [38144/50000]	Loss: 0.8117	LR: 0.050000
Training Epoch: 38 [38272/50000]	Loss: 0.9691	LR: 0.050000
Training Epoch: 38 [38400/50000]	Loss: 0.9228	LR: 0.050000
Training Epoch: 38 [38528/50000]	Loss: 0.8054	LR: 0.050000
Training Epoch: 38 [38656/50000]	Loss: 0.9429	LR: 0.050000
Training Epoch: 38 [38784/50000]	Loss: 1.0196	LR: 0.050000
Training Epoch: 38 [38912/50000]	Loss: 0.9179	LR: 0.050000
Training Epoch: 38 [39040/50000]	Loss: 0.9973	LR: 0.050000
Training Epoch: 38 [39168/50000]	Loss: 0.8720	LR: 0.050000
Training Epoch: 38 [39296/50000]	Loss: 0.8453	LR: 0.050000
Training Epoch: 38 [39424/50000]	Loss: 0.9432	LR: 0.050000
Training Epoch: 38 [39552/50000]	Loss: 1.1221	LR: 0.050000
Training Epoch: 38 [39680/50000]	Loss: 0.9311	LR: 0.050000
Training Epoch: 38 [39808/50000]	Loss: 1.0200	LR: 0.050000
Training Epoch: 38 [39936/50000]	Loss: 1.1148	LR: 0.050000
Training Epoch: 38 [40064/50000]	Loss: 1.1118	LR: 0.050000
Training Epoch: 38 [40192/50000]	Loss: 0.8725	LR: 0.050000
Training Epoch: 38 [40320/50000]	Loss: 1.0873	LR: 0.050000
Training Epoch: 38 [40448/50000]	Loss: 1.1401	LR: 0.050000
Training Epoch: 38 [40576/50000]	Loss: 1.0472	LR: 0.050000
Training Epoch: 38 [40704/50000]	Loss: 0.9893	LR: 0.050000
Training Epoch: 38 [40832/50000]	Loss: 0.9102	LR: 0.050000
Training Epoch: 38 [40960/50000]	Loss: 1.1021	LR: 0.050000
Training Epoch: 38 [41088/50000]	Loss: 0.9907	LR: 0.050000
Training Epoch: 38 [41216/50000]	Loss: 1.1486	LR: 0.050000
Training Epoch: 38 [41344/50000]	Loss: 1.0006	LR: 0.050000
Training Epoch: 38 [41472/50000]	Loss: 1.1729	LR: 0.050000
Training Epoch: 38 [41600/50000]	Loss: 1.0059	LR: 0.050000
Training Epoch: 38 [41728/50000]	Loss: 1.0494	LR: 0.050000
Training Epoch: 38 [41856/50000]	Loss: 1.3940	LR: 0.050000
Training Epoch: 38 [41984/50000]	Loss: 1.0242	LR: 0.050000
Training Epoch: 38 [42112/50000]	Loss: 0.9833	LR: 0.050000
Training Epoch: 38 [42240/50000]	Loss: 1.2131	LR: 0.050000
Training Epoch: 38 [42368/50000]	Loss: 1.1845	LR: 0.050000
Training Epoch: 38 [42496/50000]	Loss: 0.9775	LR: 0.050000
Training Epoch: 38 [42624/50000]	Loss: 1.1195	LR: 0.050000
Training Epoch: 38 [42752/50000]	Loss: 0.8193	LR: 0.050000
Training Epoch: 38 [42880/50000]	Loss: 1.0414	LR: 0.050000
Training Epoch: 38 [43008/50000]	Loss: 0.9942	LR: 0.050000
Training Epoch: 38 [43136/50000]	Loss: 1.0198	LR: 0.050000
Training Epoch: 38 [43264/50000]	Loss: 0.8724	LR: 0.050000
Training Epoch: 38 [43392/50000]	Loss: 1.2058	LR: 0.050000
Training Epoch: 38 [43520/50000]	Loss: 0.9163	LR: 0.050000
Training Epoch: 38 [43648/50000]	Loss: 0.9426	LR: 0.050000
Training Epoch: 38 [43776/50000]	Loss: 1.0130	LR: 0.050000
Training Epoch: 38 [43904/50000]	Loss: 0.8046	LR: 0.050000
Training Epoch: 38 [44032/50000]	Loss: 0.8219	LR: 0.050000
Training Epoch: 38 [44160/50000]	Loss: 0.8951	LR: 0.050000
Training Epoch: 38 [44288/50000]	Loss: 1.3022	LR: 0.050000
Training Epoch: 38 [44416/50000]	Loss: 1.3325	LR: 0.050000
Training Epoch: 38 [44544/50000]	Loss: 0.9977	LR: 0.050000
Training Epoch: 38 [44672/50000]	Loss: 0.9166	LR: 0.050000
Training Epoch: 38 [44800/50000]	Loss: 1.0448	LR: 0.050000
Training Epoch: 38 [44928/50000]	Loss: 0.9970	LR: 0.050000
Training Epoch: 38 [45056/50000]	Loss: 0.9367	LR: 0.050000
Training Epoch: 38 [45184/50000]	Loss: 1.1662	LR: 0.050000
Training Epoch: 38 [45312/50000]	Loss: 1.1823	LR: 0.050000
Training Epoch: 38 [45440/50000]	Loss: 1.2685	LR: 0.050000
Training Epoch: 38 [45568/50000]	Loss: 1.4210	LR: 0.050000
Training Epoch: 38 [45696/50000]	Loss: 0.7809	LR: 0.050000
Training Epoch: 38 [45824/50000]	Loss: 1.0433	LR: 0.050000
Training Epoch: 38 [45952/50000]	Loss: 1.1691	LR: 0.050000
Training Epoch: 38 [46080/50000]	Loss: 1.2270	LR: 0.050000
Training Epoch: 38 [46208/50000]	Loss: 0.8595	LR: 0.050000
Training Epoch: 38 [46336/50000]	Loss: 0.9975	LR: 0.050000
Training Epoch: 38 [46464/50000]	Loss: 1.0759	LR: 0.050000
Training Epoch: 38 [46592/50000]	Loss: 1.0843	LR: 0.050000
Training Epoch: 38 [46720/50000]	Loss: 1.0595	LR: 0.050000
Training Epoch: 38 [46848/50000]	Loss: 1.0187	LR: 0.050000
Training Epoch: 38 [46976/50000]	Loss: 1.2049	LR: 0.050000
Training Epoch: 38 [47104/50000]	Loss: 0.9507	LR: 0.050000
Training Epoch: 38 [47232/50000]	Loss: 0.9156	LR: 0.050000
Training Epoch: 38 [47360/50000]	Loss: 1.0393	LR: 0.050000
Training Epoch: 38 [47488/50000]	Loss: 0.9939	LR: 0.050000
Training Epoch: 38 [47616/50000]	Loss: 0.9802	LR: 0.050000
Training Epoch: 38 [47744/50000]	Loss: 0.8569	LR: 0.050000
Training Epoch: 38 [47872/50000]	Loss: 1.0071	LR: 0.050000
Training Epoch: 38 [48000/50000]	Loss: 1.2599	LR: 0.050000
Training Epoch: 38 [48128/50000]	Loss: 0.9989	LR: 0.050000
Training Epoch: 38 [48256/50000]	Loss: 1.3210	LR: 0.050000
Training Epoch: 38 [48384/50000]	Loss: 1.0886	LR: 0.050000
Training Epoch: 38 [48512/50000]	Loss: 1.1311	LR: 0.050000
Training Epoch: 38 [48640/50000]	Loss: 1.1651	LR: 0.050000
Training Epoch: 38 [48768/50000]	Loss: 0.9728	LR: 0.050000
Training Epoch: 38 [48896/50000]	Loss: 0.7986	LR: 0.050000
Training Epoch: 38 [49024/50000]	Loss: 0.9828	LR: 0.050000
Training Epoch: 38 [49152/50000]	Loss: 1.1419	LR: 0.050000
Training Epoch: 38 [49280/50000]	Loss: 1.0641	LR: 0.050000
Training Epoch: 38 [49408/50000]	Loss: 0.8358	LR: 0.050000
Training Epoch: 38 [49536/50000]	Loss: 1.0131	LR: 0.050000
Training Epoch: 38 [49664/50000]	Loss: 1.0210	LR: 0.050000
Training Epoch: 38 [49792/50000]	Loss: 0.9576	LR: 0.050000
Training Epoch: 38 [49920/50000]	Loss: 1.1629	LR: 0.050000
Training Epoch: 38 [50000/50000]	Loss: 0.9841	LR: 0.050000
epoch 38 training time consumed: 53.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  139462 GB |  139462 GB |
|       from large pool |  123392 KB |    1034 MB |  139325 GB |  139325 GB |
|       from small pool |   10798 KB |      13 MB |     137 GB |     137 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  139462 GB |  139462 GB |
|       from large pool |  123392 KB |    1034 MB |  139325 GB |  139325 GB |
|       from small pool |   10798 KB |      13 MB |     137 GB |     137 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   61373 GB |   61373 GB |
|       from large pool |  155136 KB |  433088 KB |   61221 GB |   61221 GB |
|       from small pool |    1490 KB |    3494 KB |     151 GB |     151 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    5381 K  |    5381 K  |
|       from large pool |      24    |      65    |    2809 K  |    2809 K  |
|       from small pool |     231    |     274    |    2572 K  |    2572 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    5381 K  |    5381 K  |
|       from large pool |      24    |      65    |    2809 K  |    2809 K  |
|       from small pool |     231    |     274    |    2572 K  |    2572 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2660 K  |    2660 K  |
|       from large pool |       9    |      14    |    1359 K  |    1359 K  |
|       from small pool |      12    |      16    |    1301 K  |    1301 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 38, Average loss: 0.0120, Accuracy: 0.6035, Time consumed:3.45s

Training Epoch: 39 [128/50000]	Loss: 0.8543	LR: 0.050000
Training Epoch: 39 [256/50000]	Loss: 0.8911	LR: 0.050000
Training Epoch: 39 [384/50000]	Loss: 0.9426	LR: 0.050000
Training Epoch: 39 [512/50000]	Loss: 0.9972	LR: 0.050000
Training Epoch: 39 [640/50000]	Loss: 0.7587	LR: 0.050000
Training Epoch: 39 [768/50000]	Loss: 0.7780	LR: 0.050000
Training Epoch: 39 [896/50000]	Loss: 0.8644	LR: 0.050000
Training Epoch: 39 [1024/50000]	Loss: 0.5791	LR: 0.050000
Training Epoch: 39 [1152/50000]	Loss: 0.7795	LR: 0.050000
Training Epoch: 39 [1280/50000]	Loss: 0.9149	LR: 0.050000
Training Epoch: 39 [1408/50000]	Loss: 1.0365	LR: 0.050000
Training Epoch: 39 [1536/50000]	Loss: 0.8658	LR: 0.050000
Training Epoch: 39 [1664/50000]	Loss: 0.8367	LR: 0.050000
Training Epoch: 39 [1792/50000]	Loss: 0.7750	LR: 0.050000
Training Epoch: 39 [1920/50000]	Loss: 0.8259	LR: 0.050000
Training Epoch: 39 [2048/50000]	Loss: 0.8875	LR: 0.050000
Training Epoch: 39 [2176/50000]	Loss: 0.9732	LR: 0.050000
Training Epoch: 39 [2304/50000]	Loss: 0.9800	LR: 0.050000
Training Epoch: 39 [2432/50000]	Loss: 0.8220	LR: 0.050000
Training Epoch: 39 [2560/50000]	Loss: 0.8133	LR: 0.050000
Training Epoch: 39 [2688/50000]	Loss: 0.8534	LR: 0.050000
Training Epoch: 39 [2816/50000]	Loss: 0.9599	LR: 0.050000
Training Epoch: 39 [2944/50000]	Loss: 0.8487	LR: 0.050000
Training Epoch: 39 [3072/50000]	Loss: 0.7884	LR: 0.050000
Training Epoch: 39 [3200/50000]	Loss: 0.8092	LR: 0.050000
Training Epoch: 39 [3328/50000]	Loss: 0.9115	LR: 0.050000
Training Epoch: 39 [3456/50000]	Loss: 0.9835	LR: 0.050000
Training Epoch: 39 [3584/50000]	Loss: 0.8345	LR: 0.050000
Training Epoch: 39 [3712/50000]	Loss: 0.9010	LR: 0.050000
Training Epoch: 39 [3840/50000]	Loss: 1.1132	LR: 0.050000
Training Epoch: 39 [3968/50000]	Loss: 0.8408	LR: 0.050000
Training Epoch: 39 [4096/50000]	Loss: 0.8287	LR: 0.050000
Training Epoch: 39 [4224/50000]	Loss: 0.8505	LR: 0.050000
Training Epoch: 39 [4352/50000]	Loss: 0.8087	LR: 0.050000
Training Epoch: 39 [4480/50000]	Loss: 0.9285	LR: 0.050000
Training Epoch: 39 [4608/50000]	Loss: 0.6151	LR: 0.050000
Training Epoch: 39 [4736/50000]	Loss: 0.7981	LR: 0.050000
Training Epoch: 39 [4864/50000]	Loss: 1.0396	LR: 0.050000
Training Epoch: 39 [4992/50000]	Loss: 0.8127	LR: 0.050000
Training Epoch: 39 [5120/50000]	Loss: 0.8851	LR: 0.050000
Training Epoch: 39 [5248/50000]	Loss: 1.0065	LR: 0.050000
Training Epoch: 39 [5376/50000]	Loss: 0.6991	LR: 0.050000
Training Epoch: 39 [5504/50000]	Loss: 0.8402	LR: 0.050000
Training Epoch: 39 [5632/50000]	Loss: 0.8605	LR: 0.050000
Training Epoch: 39 [5760/50000]	Loss: 1.0074	LR: 0.050000
Training Epoch: 39 [5888/50000]	Loss: 1.2372	LR: 0.050000
Training Epoch: 39 [6016/50000]	Loss: 0.8838	LR: 0.050000
Training Epoch: 39 [6144/50000]	Loss: 0.7354	LR: 0.050000
Training Epoch: 39 [6272/50000]	Loss: 0.7379	LR: 0.050000
Training Epoch: 39 [6400/50000]	Loss: 0.8693	LR: 0.050000
Training Epoch: 39 [6528/50000]	Loss: 0.8186	LR: 0.050000
Training Epoch: 39 [6656/50000]	Loss: 0.8784	LR: 0.050000
Training Epoch: 39 [6784/50000]	Loss: 0.8120	LR: 0.050000
Training Epoch: 39 [6912/50000]	Loss: 0.8270	LR: 0.050000
Training Epoch: 39 [7040/50000]	Loss: 0.9226	LR: 0.050000
Training Epoch: 39 [7168/50000]	Loss: 0.7774	LR: 0.050000
Training Epoch: 39 [7296/50000]	Loss: 0.8989	LR: 0.050000
Training Epoch: 39 [7424/50000]	Loss: 0.9721	LR: 0.050000
Training Epoch: 39 [7552/50000]	Loss: 0.8506	LR: 0.050000
Training Epoch: 39 [7680/50000]	Loss: 1.0648	LR: 0.050000
Training Epoch: 39 [7808/50000]	Loss: 1.0553	LR: 0.050000
Training Epoch: 39 [7936/50000]	Loss: 0.7764	LR: 0.050000
Training Epoch: 39 [8064/50000]	Loss: 0.9187	LR: 0.050000
Training Epoch: 39 [8192/50000]	Loss: 0.9752	LR: 0.050000
Training Epoch: 39 [8320/50000]	Loss: 0.7794	LR: 0.050000
Training Epoch: 39 [8448/50000]	Loss: 0.8428	LR: 0.050000
Training Epoch: 39 [8576/50000]	Loss: 0.8002	LR: 0.050000
Training Epoch: 39 [8704/50000]	Loss: 0.9291	LR: 0.050000
Training Epoch: 39 [8832/50000]	Loss: 0.7962	LR: 0.050000
Training Epoch: 39 [8960/50000]	Loss: 0.9565	LR: 0.050000
Training Epoch: 39 [9088/50000]	Loss: 0.9238	LR: 0.050000
Training Epoch: 39 [9216/50000]	Loss: 0.6795	LR: 0.050000
Training Epoch: 39 [9344/50000]	Loss: 1.0089	LR: 0.050000
Training Epoch: 39 [9472/50000]	Loss: 0.9313	LR: 0.050000
Training Epoch: 39 [9600/50000]	Loss: 0.8540	LR: 0.050000
Training Epoch: 39 [9728/50000]	Loss: 0.8799	LR: 0.050000
Training Epoch: 39 [9856/50000]	Loss: 0.6796	LR: 0.050000
Training Epoch: 39 [9984/50000]	Loss: 0.8244	LR: 0.050000
Training Epoch: 39 [10112/50000]	Loss: 0.8619	LR: 0.050000
Training Epoch: 39 [10240/50000]	Loss: 0.8035	LR: 0.050000
Training Epoch: 39 [10368/50000]	Loss: 0.9678	LR: 0.050000
Training Epoch: 39 [10496/50000]	Loss: 0.7262	LR: 0.050000
Training Epoch: 39 [10624/50000]	Loss: 0.9233	LR: 0.050000
Training Epoch: 39 [10752/50000]	Loss: 0.8267	LR: 0.050000
Training Epoch: 39 [10880/50000]	Loss: 0.8573	LR: 0.050000
Training Epoch: 39 [11008/50000]	Loss: 0.7643	LR: 0.050000
Training Epoch: 39 [11136/50000]	Loss: 0.9900	LR: 0.050000
Training Epoch: 39 [11264/50000]	Loss: 0.9521	LR: 0.050000
Training Epoch: 39 [11392/50000]	Loss: 0.9368	LR: 0.050000
Training Epoch: 39 [11520/50000]	Loss: 0.8828	LR: 0.050000
Training Epoch: 39 [11648/50000]	Loss: 1.0951	LR: 0.050000
Training Epoch: 39 [11776/50000]	Loss: 0.8202	LR: 0.050000
Training Epoch: 39 [11904/50000]	Loss: 1.0685	LR: 0.050000
Training Epoch: 39 [12032/50000]	Loss: 1.2151	LR: 0.050000
Training Epoch: 39 [12160/50000]	Loss: 0.9099	LR: 0.050000
Training Epoch: 39 [12288/50000]	Loss: 1.0291	LR: 0.050000
Training Epoch: 39 [12416/50000]	Loss: 0.7215	LR: 0.050000
Training Epoch: 39 [12544/50000]	Loss: 0.9527	LR: 0.050000
Training Epoch: 39 [12672/50000]	Loss: 0.8364	LR: 0.050000
Training Epoch: 39 [12800/50000]	Loss: 0.7839	LR: 0.050000
Training Epoch: 39 [12928/50000]	Loss: 0.9450	LR: 0.050000
Training Epoch: 39 [13056/50000]	Loss: 0.9560	LR: 0.050000
Training Epoch: 39 [13184/50000]	Loss: 0.7522	LR: 0.050000
Training Epoch: 39 [13312/50000]	Loss: 0.8990	LR: 0.050000
Training Epoch: 39 [13440/50000]	Loss: 0.9015	LR: 0.050000
Training Epoch: 39 [13568/50000]	Loss: 0.8762	LR: 0.050000
Training Epoch: 39 [13696/50000]	Loss: 0.8851	LR: 0.050000
Training Epoch: 39 [13824/50000]	Loss: 0.8361	LR: 0.050000
Training Epoch: 39 [13952/50000]	Loss: 0.9260	LR: 0.050000
Training Epoch: 39 [14080/50000]	Loss: 0.8702	LR: 0.050000
Training Epoch: 39 [14208/50000]	Loss: 0.9328	LR: 0.050000
Training Epoch: 39 [14336/50000]	Loss: 0.8723	LR: 0.050000
Training Epoch: 39 [14464/50000]	Loss: 0.9047	LR: 0.050000
Training Epoch: 39 [14592/50000]	Loss: 1.0379	LR: 0.050000
Training Epoch: 39 [14720/50000]	Loss: 0.8158	LR: 0.050000
Training Epoch: 39 [14848/50000]	Loss: 1.2164	LR: 0.050000
Training Epoch: 39 [14976/50000]	Loss: 0.8548	LR: 0.050000
Training Epoch: 39 [15104/50000]	Loss: 0.9058	LR: 0.050000
Training Epoch: 39 [15232/50000]	Loss: 0.8127	LR: 0.050000
Training Epoch: 39 [15360/50000]	Loss: 0.7705	LR: 0.050000
Training Epoch: 39 [15488/50000]	Loss: 0.9996	LR: 0.050000
Training Epoch: 39 [15616/50000]	Loss: 0.8949	LR: 0.050000
Training Epoch: 39 [15744/50000]	Loss: 0.7847	LR: 0.050000
Training Epoch: 39 [15872/50000]	Loss: 1.0063	LR: 0.050000
Training Epoch: 39 [16000/50000]	Loss: 0.8519	LR: 0.050000
Training Epoch: 39 [16128/50000]	Loss: 0.8566	LR: 0.050000
Training Epoch: 39 [16256/50000]	Loss: 0.7359	LR: 0.050000
Training Epoch: 39 [16384/50000]	Loss: 0.8899	LR: 0.050000
Training Epoch: 39 [16512/50000]	Loss: 0.9232	LR: 0.050000
Training Epoch: 39 [16640/50000]	Loss: 1.0055	LR: 0.050000
Training Epoch: 39 [16768/50000]	Loss: 1.2249	LR: 0.050000
Training Epoch: 39 [16896/50000]	Loss: 1.0435	LR: 0.050000
Training Epoch: 39 [17024/50000]	Loss: 0.8658	LR: 0.050000
Training Epoch: 39 [17152/50000]	Loss: 0.8199	LR: 0.050000
Training Epoch: 39 [17280/50000]	Loss: 1.0643	LR: 0.050000
Training Epoch: 39 [17408/50000]	Loss: 0.9943	LR: 0.050000
Training Epoch: 39 [17536/50000]	Loss: 1.0727	LR: 0.050000
Training Epoch: 39 [17664/50000]	Loss: 1.0576	LR: 0.050000
Training Epoch: 39 [17792/50000]	Loss: 0.6876	LR: 0.050000
Training Epoch: 39 [17920/50000]	Loss: 1.1823	LR: 0.050000
Training Epoch: 39 [18048/50000]	Loss: 1.0090	LR: 0.050000
Training Epoch: 39 [18176/50000]	Loss: 0.6932	LR: 0.050000
Training Epoch: 39 [18304/50000]	Loss: 0.7729	LR: 0.050000
Training Epoch: 39 [18432/50000]	Loss: 1.0762	LR: 0.050000
Training Epoch: 39 [18560/50000]	Loss: 0.9925	LR: 0.050000
Training Epoch: 39 [18688/50000]	Loss: 0.9392	LR: 0.050000
Training Epoch: 39 [18816/50000]	Loss: 0.7384	LR: 0.050000
Training Epoch: 39 [18944/50000]	Loss: 0.9149	LR: 0.050000
Training Epoch: 39 [19072/50000]	Loss: 1.1919	LR: 0.050000
Training Epoch: 39 [19200/50000]	Loss: 1.0098	LR: 0.050000
Training Epoch: 39 [19328/50000]	Loss: 0.8601	LR: 0.050000
Training Epoch: 39 [19456/50000]	Loss: 1.0609	LR: 0.050000
Training Epoch: 39 [19584/50000]	Loss: 1.1319	LR: 0.050000
Training Epoch: 39 [19712/50000]	Loss: 0.8600	LR: 0.050000
Training Epoch: 39 [19840/50000]	Loss: 0.8401	LR: 0.050000
Training Epoch: 39 [19968/50000]	Loss: 0.8601	LR: 0.050000
Training Epoch: 39 [20096/50000]	Loss: 1.0558	LR: 0.050000
Training Epoch: 39 [20224/50000]	Loss: 0.8946	LR: 0.050000
Training Epoch: 39 [20352/50000]	Loss: 0.8820	LR: 0.050000
Training Epoch: 39 [20480/50000]	Loss: 1.0242	LR: 0.050000
Training Epoch: 39 [20608/50000]	Loss: 0.9886	LR: 0.050000
Training Epoch: 39 [20736/50000]	Loss: 1.1571	LR: 0.050000
Training Epoch: 39 [20864/50000]	Loss: 1.1124	LR: 0.050000
Training Epoch: 39 [20992/50000]	Loss: 0.9491	LR: 0.050000
Training Epoch: 39 [21120/50000]	Loss: 1.0680	LR: 0.050000
Training Epoch: 39 [21248/50000]	Loss: 0.7528	LR: 0.050000
Training Epoch: 39 [21376/50000]	Loss: 0.9989	LR: 0.050000
Training Epoch: 39 [21504/50000]	Loss: 0.9732	LR: 0.050000
Training Epoch: 39 [21632/50000]	Loss: 0.8706	LR: 0.050000
Training Epoch: 39 [21760/50000]	Loss: 0.9201	LR: 0.050000
Training Epoch: 39 [21888/50000]	Loss: 0.9679	LR: 0.050000
Training Epoch: 39 [22016/50000]	Loss: 0.9190	LR: 0.050000
Training Epoch: 39 [22144/50000]	Loss: 1.1806	LR: 0.050000
Training Epoch: 39 [22272/50000]	Loss: 0.7616	LR: 0.050000
Training Epoch: 39 [22400/50000]	Loss: 0.9007	LR: 0.050000
Training Epoch: 39 [22528/50000]	Loss: 0.9705	LR: 0.050000
Training Epoch: 39 [22656/50000]	Loss: 0.9498	LR: 0.050000
Training Epoch: 39 [22784/50000]	Loss: 0.9692	LR: 0.050000
Training Epoch: 39 [22912/50000]	Loss: 1.0553	LR: 0.050000
Training Epoch: 39 [23040/50000]	Loss: 0.7098	LR: 0.050000
Training Epoch: 39 [23168/50000]	Loss: 1.0699	LR: 0.050000
Training Epoch: 39 [23296/50000]	Loss: 0.9579	LR: 0.050000
Training Epoch: 39 [23424/50000]	Loss: 1.1420	LR: 0.050000
Training Epoch: 39 [23552/50000]	Loss: 1.0912	LR: 0.050000
Training Epoch: 39 [23680/50000]	Loss: 0.8277	LR: 0.050000
Training Epoch: 39 [23808/50000]	Loss: 1.1905	LR: 0.050000
Training Epoch: 39 [23936/50000]	Loss: 0.9138	LR: 0.050000
Training Epoch: 39 [24064/50000]	Loss: 0.9665	LR: 0.050000
Training Epoch: 39 [24192/50000]	Loss: 1.0344	LR: 0.050000
Training Epoch: 39 [24320/50000]	Loss: 0.7515	LR: 0.050000
Training Epoch: 39 [24448/50000]	Loss: 0.9578	LR: 0.050000
Training Epoch: 39 [24576/50000]	Loss: 0.9154	LR: 0.050000
Training Epoch: 39 [24704/50000]	Loss: 0.8310	LR: 0.050000
Training Epoch: 39 [24832/50000]	Loss: 1.0818	LR: 0.050000
Training Epoch: 39 [24960/50000]	Loss: 1.0100	LR: 0.050000
Training Epoch: 39 [25088/50000]	Loss: 1.0290	LR: 0.050000
Training Epoch: 39 [25216/50000]	Loss: 1.1980	LR: 0.050000
Training Epoch: 39 [25344/50000]	Loss: 1.0657	LR: 0.050000
Training Epoch: 39 [25472/50000]	Loss: 0.9499	LR: 0.050000
Training Epoch: 39 [25600/50000]	Loss: 1.1786	LR: 0.050000
Training Epoch: 39 [25728/50000]	Loss: 0.8516	LR: 0.050000
Training Epoch: 39 [25856/50000]	Loss: 0.8980	LR: 0.050000
Training Epoch: 39 [25984/50000]	Loss: 0.9171	LR: 0.050000
Training Epoch: 39 [26112/50000]	Loss: 1.0788	LR: 0.050000
Training Epoch: 39 [26240/50000]	Loss: 1.0034	LR: 0.050000
Training Epoch: 39 [26368/50000]	Loss: 0.9982	LR: 0.050000
Training Epoch: 39 [26496/50000]	Loss: 0.9188	LR: 0.050000
Training Epoch: 39 [26624/50000]	Loss: 0.9825	LR: 0.050000
Training Epoch: 39 [26752/50000]	Loss: 0.9468	LR: 0.050000
Training Epoch: 39 [26880/50000]	Loss: 1.0181	LR: 0.050000
Training Epoch: 39 [27008/50000]	Loss: 0.9594	LR: 0.050000
Training Epoch: 39 [27136/50000]	Loss: 0.8417	LR: 0.050000
Training Epoch: 39 [27264/50000]	Loss: 0.8900	LR: 0.050000
Training Epoch: 39 [27392/50000]	Loss: 0.7634	LR: 0.050000
Training Epoch: 39 [27520/50000]	Loss: 0.9261	LR: 0.050000
Training Epoch: 39 [27648/50000]	Loss: 1.0950	LR: 0.050000
Training Epoch: 39 [27776/50000]	Loss: 1.0446	LR: 0.050000
Training Epoch: 39 [27904/50000]	Loss: 0.7818	LR: 0.050000
Training Epoch: 39 [28032/50000]	Loss: 0.8931	LR: 0.050000
Training Epoch: 39 [28160/50000]	Loss: 0.9538	LR: 0.050000
Training Epoch: 39 [28288/50000]	Loss: 0.8072	LR: 0.050000
Training Epoch: 39 [28416/50000]	Loss: 1.0792	LR: 0.050000
Training Epoch: 39 [28544/50000]	Loss: 1.0048	LR: 0.050000
Training Epoch: 39 [28672/50000]	Loss: 1.0548	LR: 0.050000
Training Epoch: 39 [28800/50000]	Loss: 1.0425	LR: 0.050000
Training Epoch: 39 [28928/50000]	Loss: 0.7609	LR: 0.050000
Training Epoch: 39 [29056/50000]	Loss: 0.9868	LR: 0.050000
Training Epoch: 39 [29184/50000]	Loss: 0.9594	LR: 0.050000
Training Epoch: 39 [29312/50000]	Loss: 0.6558	LR: 0.050000
Training Epoch: 39 [29440/50000]	Loss: 1.1294	LR: 0.050000
Training Epoch: 39 [29568/50000]	Loss: 1.2438	LR: 0.050000
Training Epoch: 39 [29696/50000]	Loss: 0.9685	LR: 0.050000
Training Epoch: 39 [29824/50000]	Loss: 0.9025	LR: 0.050000
Training Epoch: 39 [29952/50000]	Loss: 0.8666	LR: 0.050000
Training Epoch: 39 [30080/50000]	Loss: 0.9046	LR: 0.050000
Training Epoch: 39 [30208/50000]	Loss: 1.0077	LR: 0.050000
Training Epoch: 39 [30336/50000]	Loss: 1.0126	LR: 0.050000
Training Epoch: 39 [30464/50000]	Loss: 0.9652	LR: 0.050000
Training Epoch: 39 [30592/50000]	Loss: 0.8051	LR: 0.050000
Training Epoch: 39 [30720/50000]	Loss: 0.9294	LR: 0.050000
Training Epoch: 39 [30848/50000]	Loss: 1.2083	LR: 0.050000
Training Epoch: 39 [30976/50000]	Loss: 0.9093	LR: 0.050000
Training Epoch: 39 [31104/50000]	Loss: 1.0455	LR: 0.050000
Training Epoch: 39 [31232/50000]	Loss: 1.1666	LR: 0.050000
Training Epoch: 39 [31360/50000]	Loss: 0.9709	LR: 0.050000
Training Epoch: 39 [31488/50000]	Loss: 0.8515	LR: 0.050000
Training Epoch: 39 [31616/50000]	Loss: 0.9743	LR: 0.050000
Training Epoch: 39 [31744/50000]	Loss: 1.1688	LR: 0.050000
Training Epoch: 39 [31872/50000]	Loss: 0.9699	LR: 0.050000
Training Epoch: 39 [32000/50000]	Loss: 0.8736	LR: 0.050000
Training Epoch: 39 [32128/50000]	Loss: 1.0797	LR: 0.050000
Training Epoch: 39 [32256/50000]	Loss: 0.7622	LR: 0.050000
Training Epoch: 39 [32384/50000]	Loss: 0.9125	LR: 0.050000
Training Epoch: 39 [32512/50000]	Loss: 1.0056	LR: 0.050000
Training Epoch: 39 [32640/50000]	Loss: 0.9331	LR: 0.050000
Training Epoch: 39 [32768/50000]	Loss: 0.9626	LR: 0.050000
Training Epoch: 39 [32896/50000]	Loss: 1.1087	LR: 0.050000
Training Epoch: 39 [33024/50000]	Loss: 0.9791	LR: 0.050000
Training Epoch: 39 [33152/50000]	Loss: 0.9932	LR: 0.050000
Training Epoch: 39 [33280/50000]	Loss: 1.0787	LR: 0.050000
Training Epoch: 39 [33408/50000]	Loss: 1.1775	LR: 0.050000
Training Epoch: 39 [33536/50000]	Loss: 1.0128	LR: 0.050000
Training Epoch: 39 [33664/50000]	Loss: 0.7639	LR: 0.050000
Training Epoch: 39 [33792/50000]	Loss: 0.8469	LR: 0.050000
Training Epoch: 39 [33920/50000]	Loss: 1.0848	LR: 0.050000
Training Epoch: 39 [34048/50000]	Loss: 1.0530	LR: 0.050000
Training Epoch: 39 [34176/50000]	Loss: 0.9903	LR: 0.050000
Training Epoch: 39 [34304/50000]	Loss: 0.9403	LR: 0.050000
Training Epoch: 39 [34432/50000]	Loss: 0.9867	LR: 0.050000
Training Epoch: 39 [34560/50000]	Loss: 1.0320	LR: 0.050000
Training Epoch: 39 [34688/50000]	Loss: 1.1221	LR: 0.050000
Training Epoch: 39 [34816/50000]	Loss: 1.0240	LR: 0.050000
Training Epoch: 39 [34944/50000]	Loss: 1.1566	LR: 0.050000
Training Epoch: 39 [35072/50000]	Loss: 1.1327	LR: 0.050000
Training Epoch: 39 [35200/50000]	Loss: 0.9818	LR: 0.050000
Training Epoch: 39 [35328/50000]	Loss: 0.9129	LR: 0.050000
Training Epoch: 39 [35456/50000]	Loss: 0.9831	LR: 0.050000
Training Epoch: 39 [35584/50000]	Loss: 0.8942	LR: 0.050000
Training Epoch: 39 [35712/50000]	Loss: 1.2702	LR: 0.050000
Training Epoch: 39 [35840/50000]	Loss: 1.0744	LR: 0.050000
Training Epoch: 39 [35968/50000]	Loss: 0.9425	LR: 0.050000
Training Epoch: 39 [36096/50000]	Loss: 1.0458	LR: 0.050000
Training Epoch: 39 [36224/50000]	Loss: 1.0288	LR: 0.050000
Training Epoch: 39 [36352/50000]	Loss: 1.0245	LR: 0.050000
Training Epoch: 39 [36480/50000]	Loss: 1.0921	LR: 0.050000
Training Epoch: 39 [36608/50000]	Loss: 1.1096	LR: 0.050000
Training Epoch: 39 [36736/50000]	Loss: 1.1428	LR: 0.050000
Training Epoch: 39 [36864/50000]	Loss: 0.9853	LR: 0.050000
Training Epoch: 39 [36992/50000]	Loss: 1.3098	LR: 0.050000
Training Epoch: 39 [37120/50000]	Loss: 1.1745	LR: 0.050000
Training Epoch: 39 [37248/50000]	Loss: 0.9465	LR: 0.050000
Training Epoch: 39 [37376/50000]	Loss: 0.8215	LR: 0.050000
Training Epoch: 39 [37504/50000]	Loss: 0.9861	LR: 0.050000
Training Epoch: 39 [37632/50000]	Loss: 0.9005	LR: 0.050000
Training Epoch: 39 [37760/50000]	Loss: 0.8837	LR: 0.050000
Training Epoch: 39 [37888/50000]	Loss: 0.8745	LR: 0.050000
Training Epoch: 39 [38016/50000]	Loss: 1.0775	LR: 0.050000
Training Epoch: 39 [38144/50000]	Loss: 1.0774	LR: 0.050000
Training Epoch: 39 [38272/50000]	Loss: 0.8691	LR: 0.050000
Training Epoch: 39 [38400/50000]	Loss: 0.8952	LR: 0.050000
Training Epoch: 39 [38528/50000]	Loss: 1.0280	LR: 0.050000
Training Epoch: 39 [38656/50000]	Loss: 1.0426	LR: 0.050000
Training Epoch: 39 [38784/50000]	Loss: 1.0710	LR: 0.050000
Training Epoch: 39 [38912/50000]	Loss: 1.2238	LR: 0.050000
Training Epoch: 39 [39040/50000]	Loss: 0.8847	LR: 0.050000
Training Epoch: 39 [39168/50000]	Loss: 1.2560	LR: 0.050000
Training Epoch: 39 [39296/50000]	Loss: 1.0325	LR: 0.050000
Training Epoch: 39 [39424/50000]	Loss: 0.9865	LR: 0.050000
Training Epoch: 39 [39552/50000]	Loss: 0.9932	LR: 0.050000
Training Epoch: 39 [39680/50000]	Loss: 0.8606	LR: 0.050000
Training Epoch: 39 [39808/50000]	Loss: 1.0242	LR: 0.050000
Training Epoch: 39 [39936/50000]	Loss: 1.1128	LR: 0.050000
Training Epoch: 39 [40064/50000]	Loss: 0.9924	LR: 0.050000
Training Epoch: 39 [40192/50000]	Loss: 0.9086	LR: 0.050000
Training Epoch: 39 [40320/50000]	Loss: 0.9903	LR: 0.050000
Training Epoch: 39 [40448/50000]	Loss: 0.8649	LR: 0.050000
Training Epoch: 39 [40576/50000]	Loss: 1.0153	LR: 0.050000
Training Epoch: 39 [40704/50000]	Loss: 1.1466	LR: 0.050000
Training Epoch: 39 [40832/50000]	Loss: 1.0762	LR: 0.050000
Training Epoch: 39 [40960/50000]	Loss: 0.8672	LR: 0.050000
Training Epoch: 39 [41088/50000]	Loss: 0.9990	LR: 0.050000
Training Epoch: 39 [41216/50000]	Loss: 0.9310	LR: 0.050000
Training Epoch: 39 [41344/50000]	Loss: 1.1369	LR: 0.050000
Training Epoch: 39 [41472/50000]	Loss: 1.0152	LR: 0.050000
Training Epoch: 39 [41600/50000]	Loss: 0.8931	LR: 0.050000
Training Epoch: 39 [41728/50000]	Loss: 1.1337	LR: 0.050000
Training Epoch: 39 [41856/50000]	Loss: 0.8267	LR: 0.050000
Training Epoch: 39 [41984/50000]	Loss: 0.8690	LR: 0.050000
Training Epoch: 39 [42112/50000]	Loss: 1.1477	LR: 0.050000
Training Epoch: 39 [42240/50000]	Loss: 0.8579	LR: 0.050000
Training Epoch: 39 [42368/50000]	Loss: 0.9090	LR: 0.050000
Training Epoch: 39 [42496/50000]	Loss: 0.9094	LR: 0.050000
Training Epoch: 39 [42624/50000]	Loss: 1.0440	LR: 0.050000
Training Epoch: 39 [42752/50000]	Loss: 1.2035	LR: 0.050000
Training Epoch: 39 [42880/50000]	Loss: 0.9192	LR: 0.050000
Training Epoch: 39 [43008/50000]	Loss: 1.0355	LR: 0.050000
Training Epoch: 39 [43136/50000]	Loss: 0.9940	LR: 0.050000
Training Epoch: 39 [43264/50000]	Loss: 0.8383	LR: 0.050000
Training Epoch: 39 [43392/50000]	Loss: 0.9463	LR: 0.050000
Training Epoch: 39 [43520/50000]	Loss: 1.0409	LR: 0.050000
Training Epoch: 39 [43648/50000]	Loss: 0.7320	LR: 0.050000
Training Epoch: 39 [43776/50000]	Loss: 1.0783	LR: 0.050000
Training Epoch: 39 [43904/50000]	Loss: 0.7967	LR: 0.050000
Training Epoch: 39 [44032/50000]	Loss: 1.1408	LR: 0.050000
Training Epoch: 39 [44160/50000]	Loss: 0.8098	LR: 0.050000
Training Epoch: 39 [44288/50000]	Loss: 1.1544	LR: 0.050000
Training Epoch: 39 [44416/50000]	Loss: 0.9119	LR: 0.050000
Training Epoch: 39 [44544/50000]	Loss: 0.8648	LR: 0.050000
Training Epoch: 39 [44672/50000]	Loss: 0.7819	LR: 0.050000
Training Epoch: 39 [44800/50000]	Loss: 0.9197	LR: 0.050000
Training Epoch: 39 [44928/50000]	Loss: 1.2297	LR: 0.050000
Training Epoch: 39 [45056/50000]	Loss: 0.9920	LR: 0.050000
Training Epoch: 39 [45184/50000]	Loss: 0.9522	LR: 0.050000
Training Epoch: 39 [45312/50000]	Loss: 1.1397	LR: 0.050000
Training Epoch: 39 [45440/50000]	Loss: 1.1804	LR: 0.050000
Training Epoch: 39 [45568/50000]	Loss: 1.1538	LR: 0.050000
Training Epoch: 39 [45696/50000]	Loss: 1.1221	LR: 0.050000
Training Epoch: 39 [45824/50000]	Loss: 0.8575	LR: 0.050000
Training Epoch: 39 [45952/50000]	Loss: 0.8810	LR: 0.050000
Training Epoch: 39 [46080/50000]	Loss: 1.1535	LR: 0.050000
Training Epoch: 39 [46208/50000]	Loss: 0.9725	LR: 0.050000
Training Epoch: 39 [46336/50000]	Loss: 0.9199	LR: 0.050000
Training Epoch: 39 [46464/50000]	Loss: 0.9858	LR: 0.050000
Training Epoch: 39 [46592/50000]	Loss: 0.9875	LR: 0.050000
Training Epoch: 39 [46720/50000]	Loss: 1.1552	LR: 0.050000
Training Epoch: 39 [46848/50000]	Loss: 1.0670	LR: 0.050000
Training Epoch: 39 [46976/50000]	Loss: 1.0976	LR: 0.050000
Training Epoch: 39 [47104/50000]	Loss: 1.1055	LR: 0.050000
Training Epoch: 39 [47232/50000]	Loss: 1.0819	LR: 0.050000
Training Epoch: 39 [47360/50000]	Loss: 1.0077	LR: 0.050000
Training Epoch: 39 [47488/50000]	Loss: 1.0155	LR: 0.050000
Training Epoch: 39 [47616/50000]	Loss: 0.7334	LR: 0.050000
Training Epoch: 39 [47744/50000]	Loss: 0.9442	LR: 0.050000
Training Epoch: 39 [47872/50000]	Loss: 1.0267	LR: 0.050000
Training Epoch: 39 [48000/50000]	Loss: 1.0268	LR: 0.050000
Training Epoch: 39 [48128/50000]	Loss: 1.0954	LR: 0.050000
Training Epoch: 39 [48256/50000]	Loss: 1.0723	LR: 0.050000
Training Epoch: 39 [48384/50000]	Loss: 0.8444	LR: 0.050000
Training Epoch: 39 [48512/50000]	Loss: 0.8672	LR: 0.050000
Training Epoch: 39 [48640/50000]	Loss: 1.0290	LR: 0.050000
Training Epoch: 39 [48768/50000]	Loss: 1.0367	LR: 0.050000
Training Epoch: 39 [48896/50000]	Loss: 1.0015	LR: 0.050000
Training Epoch: 39 [49024/50000]	Loss: 0.8712	LR: 0.050000
Training Epoch: 39 [49152/50000]	Loss: 1.0315	LR: 0.050000
Training Epoch: 39 [49280/50000]	Loss: 1.0108	LR: 0.050000
Training Epoch: 39 [49408/50000]	Loss: 0.9567	LR: 0.050000
Training Epoch: 39 [49536/50000]	Loss: 0.9427	LR: 0.050000
Training Epoch: 39 [49664/50000]	Loss: 0.9219	LR: 0.050000
Training Epoch: 39 [49792/50000]	Loss: 1.0527	LR: 0.050000
Training Epoch: 39 [49920/50000]	Loss: 0.9962	LR: 0.050000
Training Epoch: 39 [50000/50000]	Loss: 0.8858	LR: 0.050000
epoch 39 training time consumed: 53.97s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  143132 GB |  143132 GB |
|       from large pool |  123392 KB |    1034 MB |  142991 GB |  142991 GB |
|       from small pool |   10798 KB |      13 MB |     141 GB |     141 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  143132 GB |  143132 GB |
|       from large pool |  123392 KB |    1034 MB |  142991 GB |  142991 GB |
|       from small pool |   10798 KB |      13 MB |     141 GB |     141 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   62988 GB |   62988 GB |
|       from large pool |  155136 KB |  433088 KB |   62832 GB |   62832 GB |
|       from small pool |    1490 KB |    3494 KB |     155 GB |     155 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    5523 K  |    5523 K  |
|       from large pool |      24    |      65    |    2882 K  |    2882 K  |
|       from small pool |     231    |     274    |    2640 K  |    2640 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    5523 K  |    5523 K  |
|       from large pool |      24    |      65    |    2882 K  |    2882 K  |
|       from small pool |     231    |     274    |    2640 K  |    2640 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2730 K  |    2730 K  |
|       from large pool |       9    |      14    |    1395 K  |    1395 K  |
|       from small pool |      12    |      16    |    1335 K  |    1335 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 39, Average loss: 0.0112, Accuracy: 0.6209, Time consumed:3.47s

Training Epoch: 40 [128/50000]	Loss: 0.9335	LR: 0.025000
Training Epoch: 40 [256/50000]	Loss: 0.9203	LR: 0.025000
Training Epoch: 40 [384/50000]	Loss: 0.8240	LR: 0.025000
Training Epoch: 40 [512/50000]	Loss: 0.8150	LR: 0.025000
Training Epoch: 40 [640/50000]	Loss: 0.7219	LR: 0.025000
Training Epoch: 40 [768/50000]	Loss: 0.8681	LR: 0.025000
Training Epoch: 40 [896/50000]	Loss: 0.8862	LR: 0.025000
Training Epoch: 40 [1024/50000]	Loss: 0.7590	LR: 0.025000
Training Epoch: 40 [1152/50000]	Loss: 0.9276	LR: 0.025000
Training Epoch: 40 [1280/50000]	Loss: 0.8087	LR: 0.025000
Training Epoch: 40 [1408/50000]	Loss: 0.7963	LR: 0.025000
Training Epoch: 40 [1536/50000]	Loss: 0.8213	LR: 0.025000
Training Epoch: 40 [1664/50000]	Loss: 0.7341	LR: 0.025000
Training Epoch: 40 [1792/50000]	Loss: 0.7922	LR: 0.025000
Training Epoch: 40 [1920/50000]	Loss: 0.7625	LR: 0.025000
Training Epoch: 40 [2048/50000]	Loss: 0.8379	LR: 0.025000
Training Epoch: 40 [2176/50000]	Loss: 0.8434	LR: 0.025000
Training Epoch: 40 [2304/50000]	Loss: 0.8534	LR: 0.025000
Training Epoch: 40 [2432/50000]	Loss: 0.8197	LR: 0.025000
Training Epoch: 40 [2560/50000]	Loss: 0.8550	LR: 0.025000
Training Epoch: 40 [2688/50000]	Loss: 0.7468	LR: 0.025000
Training Epoch: 40 [2816/50000]	Loss: 0.7353	LR: 0.025000
Training Epoch: 40 [2944/50000]	Loss: 0.5990	LR: 0.025000
Training Epoch: 40 [3072/50000]	Loss: 0.6979	LR: 0.025000
Training Epoch: 40 [3200/50000]	Loss: 0.6763	LR: 0.025000
Training Epoch: 40 [3328/50000]	Loss: 0.9279	LR: 0.025000
Training Epoch: 40 [3456/50000]	Loss: 0.5217	LR: 0.025000
Training Epoch: 40 [3584/50000]	Loss: 0.7321	LR: 0.025000
Training Epoch: 40 [3712/50000]	Loss: 0.6136	LR: 0.025000
Training Epoch: 40 [3840/50000]	Loss: 0.6185	LR: 0.025000
Training Epoch: 40 [3968/50000]	Loss: 0.9055	LR: 0.025000
Training Epoch: 40 [4096/50000]	Loss: 0.8002	LR: 0.025000
Training Epoch: 40 [4224/50000]	Loss: 0.7498	LR: 0.025000
Training Epoch: 40 [4352/50000]	Loss: 0.7358	LR: 0.025000
Training Epoch: 40 [4480/50000]	Loss: 0.6013	LR: 0.025000
Training Epoch: 40 [4608/50000]	Loss: 0.7034	LR: 0.025000
Training Epoch: 40 [4736/50000]	Loss: 0.6740	LR: 0.025000
Training Epoch: 40 [4864/50000]	Loss: 0.6480	LR: 0.025000
Training Epoch: 40 [4992/50000]	Loss: 0.6561	LR: 0.025000
Training Epoch: 40 [5120/50000]	Loss: 0.6927	LR: 0.025000
Training Epoch: 40 [5248/50000]	Loss: 0.7129	LR: 0.025000
Training Epoch: 40 [5376/50000]	Loss: 0.6671	LR: 0.025000
Training Epoch: 40 [5504/50000]	Loss: 0.9080	LR: 0.025000
Training Epoch: 40 [5632/50000]	Loss: 0.6136	LR: 0.025000
Training Epoch: 40 [5760/50000]	Loss: 0.6985	LR: 0.025000
Training Epoch: 40 [5888/50000]	Loss: 0.6941	LR: 0.025000
Training Epoch: 40 [6016/50000]	Loss: 0.7829	LR: 0.025000
Training Epoch: 40 [6144/50000]	Loss: 0.6385	LR: 0.025000
Training Epoch: 40 [6272/50000]	Loss: 0.7452	LR: 0.025000
Training Epoch: 40 [6400/50000]	Loss: 0.5947	LR: 0.025000
Training Epoch: 40 [6528/50000]	Loss: 0.5533	LR: 0.025000
Training Epoch: 40 [6656/50000]	Loss: 0.5994	LR: 0.025000
Training Epoch: 40 [6784/50000]	Loss: 0.6154	LR: 0.025000
Training Epoch: 40 [6912/50000]	Loss: 0.6027	LR: 0.025000
Training Epoch: 40 [7040/50000]	Loss: 0.6324	LR: 0.025000
Training Epoch: 40 [7168/50000]	Loss: 0.6702	LR: 0.025000
Training Epoch: 40 [7296/50000]	Loss: 0.7272	LR: 0.025000
Training Epoch: 40 [7424/50000]	Loss: 0.5451	LR: 0.025000
Training Epoch: 40 [7552/50000]	Loss: 0.5684	LR: 0.025000
Training Epoch: 40 [7680/50000]	Loss: 0.7597	LR: 0.025000
Training Epoch: 40 [7808/50000]	Loss: 0.8350	LR: 0.025000
Training Epoch: 40 [7936/50000]	Loss: 0.7199	LR: 0.025000
Training Epoch: 40 [8064/50000]	Loss: 0.7511	LR: 0.025000
Training Epoch: 40 [8192/50000]	Loss: 0.6410	LR: 0.025000
Training Epoch: 40 [8320/50000]	Loss: 0.6931	LR: 0.025000
Training Epoch: 40 [8448/50000]	Loss: 0.6153	LR: 0.025000
Training Epoch: 40 [8576/50000]	Loss: 0.6871	LR: 0.025000
Training Epoch: 40 [8704/50000]	Loss: 0.7097	LR: 0.025000
Training Epoch: 40 [8832/50000]	Loss: 0.6386	LR: 0.025000
Training Epoch: 40 [8960/50000]	Loss: 0.4369	LR: 0.025000
Training Epoch: 40 [9088/50000]	Loss: 0.7187	LR: 0.025000
Training Epoch: 40 [9216/50000]	Loss: 0.5169	LR: 0.025000
Training Epoch: 40 [9344/50000]	Loss: 0.6557	LR: 0.025000
Training Epoch: 40 [9472/50000]	Loss: 0.5983	LR: 0.025000
Training Epoch: 40 [9600/50000]	Loss: 0.7764	LR: 0.025000
Training Epoch: 40 [9728/50000]	Loss: 0.6839	LR: 0.025000
Training Epoch: 40 [9856/50000]	Loss: 0.6430	LR: 0.025000
Training Epoch: 40 [9984/50000]	Loss: 0.5778	LR: 0.025000
Training Epoch: 40 [10112/50000]	Loss: 0.7221	LR: 0.025000
Training Epoch: 40 [10240/50000]	Loss: 0.6624	LR: 0.025000
Training Epoch: 40 [10368/50000]	Loss: 0.5565	LR: 0.025000
Training Epoch: 40 [10496/50000]	Loss: 0.6525	LR: 0.025000
Training Epoch: 40 [10624/50000]	Loss: 0.4921	LR: 0.025000
Training Epoch: 40 [10752/50000]	Loss: 0.5471	LR: 0.025000
Training Epoch: 40 [10880/50000]	Loss: 0.6178	LR: 0.025000
Training Epoch: 40 [11008/50000]	Loss: 0.6117	LR: 0.025000
Training Epoch: 40 [11136/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 40 [11264/50000]	Loss: 0.5419	LR: 0.025000
Training Epoch: 40 [11392/50000]	Loss: 0.6967	LR: 0.025000
Training Epoch: 40 [11520/50000]	Loss: 0.5465	LR: 0.025000
Training Epoch: 40 [11648/50000]	Loss: 0.6453	LR: 0.025000
Training Epoch: 40 [11776/50000]	Loss: 0.6217	LR: 0.025000
Training Epoch: 40 [11904/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 40 [12032/50000]	Loss: 0.4472	LR: 0.025000
Training Epoch: 40 [12160/50000]	Loss: 0.6442	LR: 0.025000
Training Epoch: 40 [12288/50000]	Loss: 0.6548	LR: 0.025000
Training Epoch: 40 [12416/50000]	Loss: 0.6880	LR: 0.025000
Training Epoch: 40 [12544/50000]	Loss: 0.9128	LR: 0.025000
Training Epoch: 40 [12672/50000]	Loss: 0.6588	LR: 0.025000
Training Epoch: 40 [12800/50000]	Loss: 0.7171	LR: 0.025000
Training Epoch: 40 [12928/50000]	Loss: 0.6899	LR: 0.025000
Training Epoch: 40 [13056/50000]	Loss: 0.7132	LR: 0.025000
Training Epoch: 40 [13184/50000]	Loss: 0.5907	LR: 0.025000
Training Epoch: 40 [13312/50000]	Loss: 0.5903	LR: 0.025000
Training Epoch: 40 [13440/50000]	Loss: 0.6896	LR: 0.025000
Training Epoch: 40 [13568/50000]	Loss: 0.5741	LR: 0.025000
Training Epoch: 40 [13696/50000]	Loss: 0.5055	LR: 0.025000
Training Epoch: 40 [13824/50000]	Loss: 0.5857	LR: 0.025000
Training Epoch: 40 [13952/50000]	Loss: 0.7215	LR: 0.025000
Training Epoch: 40 [14080/50000]	Loss: 0.4343	LR: 0.025000
Training Epoch: 40 [14208/50000]	Loss: 0.7189	LR: 0.025000
Training Epoch: 40 [14336/50000]	Loss: 0.7856	LR: 0.025000
Training Epoch: 40 [14464/50000]	Loss: 0.5795	LR: 0.025000
Training Epoch: 40 [14592/50000]	Loss: 0.5958	LR: 0.025000
Training Epoch: 40 [14720/50000]	Loss: 0.6226	LR: 0.025000
Training Epoch: 40 [14848/50000]	Loss: 0.6655	LR: 0.025000
Training Epoch: 40 [14976/50000]	Loss: 0.7844	LR: 0.025000
Training Epoch: 40 [15104/50000]	Loss: 0.6474	LR: 0.025000
Training Epoch: 40 [15232/50000]	Loss: 0.5842	LR: 0.025000
Training Epoch: 40 [15360/50000]	Loss: 0.5900	LR: 0.025000
Training Epoch: 40 [15488/50000]	Loss: 0.6876	LR: 0.025000
Training Epoch: 40 [15616/50000]	Loss: 0.8099	LR: 0.025000
Training Epoch: 40 [15744/50000]	Loss: 0.8662	LR: 0.025000
Training Epoch: 40 [15872/50000]	Loss: 0.6052	LR: 0.025000
Training Epoch: 40 [16000/50000]	Loss: 0.6988	LR: 0.025000
Training Epoch: 40 [16128/50000]	Loss: 0.4789	LR: 0.025000
Training Epoch: 40 [16256/50000]	Loss: 0.5583	LR: 0.025000
Training Epoch: 40 [16384/50000]	Loss: 0.7602	LR: 0.025000
Training Epoch: 40 [16512/50000]	Loss: 0.7614	LR: 0.025000
Training Epoch: 40 [16640/50000]	Loss: 0.6846	LR: 0.025000
Training Epoch: 40 [16768/50000]	Loss: 0.7583	LR: 0.025000
Training Epoch: 40 [16896/50000]	Loss: 0.5899	LR: 0.025000
Training Epoch: 40 [17024/50000]	Loss: 0.6188	LR: 0.025000
Training Epoch: 40 [17152/50000]	Loss: 0.5919	LR: 0.025000
Training Epoch: 40 [17280/50000]	Loss: 0.5813	LR: 0.025000
Training Epoch: 40 [17408/50000]	Loss: 0.7735	LR: 0.025000
Training Epoch: 40 [17536/50000]	Loss: 0.5987	LR: 0.025000
Training Epoch: 40 [17664/50000]	Loss: 0.6953	LR: 0.025000
Training Epoch: 40 [17792/50000]	Loss: 0.5903	LR: 0.025000
Training Epoch: 40 [17920/50000]	Loss: 0.5665	LR: 0.025000
Training Epoch: 40 [18048/50000]	Loss: 0.5960	LR: 0.025000
Training Epoch: 40 [18176/50000]	Loss: 0.6057	LR: 0.025000
Training Epoch: 40 [18304/50000]	Loss: 0.6499	LR: 0.025000
Training Epoch: 40 [18432/50000]	Loss: 0.5727	LR: 0.025000
Training Epoch: 40 [18560/50000]	Loss: 0.6098	LR: 0.025000
Training Epoch: 40 [18688/50000]	Loss: 0.4893	LR: 0.025000
Training Epoch: 40 [18816/50000]	Loss: 0.7147	LR: 0.025000
Training Epoch: 40 [18944/50000]	Loss: 0.7114	LR: 0.025000
Training Epoch: 40 [19072/50000]	Loss: 0.5933	LR: 0.025000
Training Epoch: 40 [19200/50000]	Loss: 0.6457	LR: 0.025000
Training Epoch: 40 [19328/50000]	Loss: 0.6884	LR: 0.025000
Training Epoch: 40 [19456/50000]	Loss: 0.6327	LR: 0.025000
Training Epoch: 40 [19584/50000]	Loss: 0.5243	LR: 0.025000
Training Epoch: 40 [19712/50000]	Loss: 0.6827	LR: 0.025000
Training Epoch: 40 [19840/50000]	Loss: 0.5012	LR: 0.025000
Training Epoch: 40 [19968/50000]	Loss: 0.7270	LR: 0.025000
Training Epoch: 40 [20096/50000]	Loss: 0.5059	LR: 0.025000
Training Epoch: 40 [20224/50000]	Loss: 0.4882	LR: 0.025000
Training Epoch: 40 [20352/50000]	Loss: 0.6714	LR: 0.025000
Training Epoch: 40 [20480/50000]	Loss: 0.5544	LR: 0.025000
Training Epoch: 40 [20608/50000]	Loss: 0.7609	LR: 0.025000
Training Epoch: 40 [20736/50000]	Loss: 0.7065	LR: 0.025000
Training Epoch: 40 [20864/50000]	Loss: 0.7850	LR: 0.025000
Training Epoch: 40 [20992/50000]	Loss: 0.3773	LR: 0.025000
Training Epoch: 40 [21120/50000]	Loss: 0.6990	LR: 0.025000
Training Epoch: 40 [21248/50000]	Loss: 0.7950	LR: 0.025000
Training Epoch: 40 [21376/50000]	Loss: 0.6435	LR: 0.025000
Training Epoch: 40 [21504/50000]	Loss: 0.6444	LR: 0.025000
Training Epoch: 40 [21632/50000]	Loss: 0.7991	LR: 0.025000
Training Epoch: 40 [21760/50000]	Loss: 0.4830	LR: 0.025000
Training Epoch: 40 [21888/50000]	Loss: 0.7208	LR: 0.025000
Training Epoch: 40 [22016/50000]	Loss: 0.5548	LR: 0.025000
Training Epoch: 40 [22144/50000]	Loss: 0.6720	LR: 0.025000
Training Epoch: 40 [22272/50000]	Loss: 0.5699	LR: 0.025000
Training Epoch: 40 [22400/50000]	Loss: 0.7149	LR: 0.025000
Training Epoch: 40 [22528/50000]	Loss: 0.6605	LR: 0.025000
Training Epoch: 40 [22656/50000]	Loss: 0.7367	LR: 0.025000
Training Epoch: 40 [22784/50000]	Loss: 0.5986	LR: 0.025000
Training Epoch: 40 [22912/50000]	Loss: 0.6330	LR: 0.025000
Training Epoch: 40 [23040/50000]	Loss: 0.6889	LR: 0.025000
Training Epoch: 40 [23168/50000]	Loss: 0.5381	LR: 0.025000
Training Epoch: 40 [23296/50000]	Loss: 0.5165	LR: 0.025000
Training Epoch: 40 [23424/50000]	Loss: 0.6102	LR: 0.025000
Training Epoch: 40 [23552/50000]	Loss: 0.6075	LR: 0.025000
Training Epoch: 40 [23680/50000]	Loss: 0.5102	LR: 0.025000
Training Epoch: 40 [23808/50000]	Loss: 0.4637	LR: 0.025000
Training Epoch: 40 [23936/50000]	Loss: 0.6364	LR: 0.025000
Training Epoch: 40 [24064/50000]	Loss: 0.6352	LR: 0.025000
Training Epoch: 40 [24192/50000]	Loss: 0.5068	LR: 0.025000
Training Epoch: 40 [24320/50000]	Loss: 0.6155	LR: 0.025000
Training Epoch: 40 [24448/50000]	Loss: 0.6465	LR: 0.025000
Training Epoch: 40 [24576/50000]	Loss: 0.6369	LR: 0.025000
Training Epoch: 40 [24704/50000]	Loss: 0.5422	LR: 0.025000
Training Epoch: 40 [24832/50000]	Loss: 0.5084	LR: 0.025000
Training Epoch: 40 [24960/50000]	Loss: 0.8251	LR: 0.025000
Training Epoch: 40 [25088/50000]	Loss: 0.6008	LR: 0.025000
Training Epoch: 40 [25216/50000]	Loss: 0.5423	LR: 0.025000
Training Epoch: 40 [25344/50000]	Loss: 0.5608	LR: 0.025000
Training Epoch: 40 [25472/50000]	Loss: 0.5609	LR: 0.025000
Training Epoch: 40 [25600/50000]	Loss: 0.4984	LR: 0.025000
Training Epoch: 40 [25728/50000]	Loss: 0.5307	LR: 0.025000
Training Epoch: 40 [25856/50000]	Loss: 0.6483	LR: 0.025000
Training Epoch: 40 [25984/50000]	Loss: 0.5985	LR: 0.025000
Training Epoch: 40 [26112/50000]	Loss: 0.4896	LR: 0.025000
Training Epoch: 40 [26240/50000]	Loss: 0.5112	LR: 0.025000
Training Epoch: 40 [26368/50000]	Loss: 0.5641	LR: 0.025000
Training Epoch: 40 [26496/50000]	Loss: 0.4164	LR: 0.025000
Training Epoch: 40 [26624/50000]	Loss: 0.7187	LR: 0.025000
Training Epoch: 40 [26752/50000]	Loss: 0.7456	LR: 0.025000
Training Epoch: 40 [26880/50000]	Loss: 0.5215	LR: 0.025000
Training Epoch: 40 [27008/50000]	Loss: 0.5729	LR: 0.025000
Training Epoch: 40 [27136/50000]	Loss: 0.7751	LR: 0.025000
Training Epoch: 40 [27264/50000]	Loss: 0.6864	LR: 0.025000
Training Epoch: 40 [27392/50000]	Loss: 0.5114	LR: 0.025000
Training Epoch: 40 [27520/50000]	Loss: 0.5843	LR: 0.025000
Training Epoch: 40 [27648/50000]	Loss: 0.4962	LR: 0.025000
Training Epoch: 40 [27776/50000]	Loss: 0.7076	LR: 0.025000
Training Epoch: 40 [27904/50000]	Loss: 0.6861	LR: 0.025000
Training Epoch: 40 [28032/50000]	Loss: 0.7002	LR: 0.025000
Training Epoch: 40 [28160/50000]	Loss: 0.5796	LR: 0.025000
Training Epoch: 40 [28288/50000]	Loss: 0.5598	LR: 0.025000
Training Epoch: 40 [28416/50000]	Loss: 0.6022	LR: 0.025000
Training Epoch: 40 [28544/50000]	Loss: 0.5925	LR: 0.025000
Training Epoch: 40 [28672/50000]	Loss: 0.6469	LR: 0.025000
Training Epoch: 40 [28800/50000]	Loss: 0.6408	LR: 0.025000
Training Epoch: 40 [28928/50000]	Loss: 0.6388	LR: 0.025000
Training Epoch: 40 [29056/50000]	Loss: 0.7711	LR: 0.025000
Training Epoch: 40 [29184/50000]	Loss: 0.5703	LR: 0.025000
Training Epoch: 40 [29312/50000]	Loss: 0.4361	LR: 0.025000
Training Epoch: 40 [29440/50000]	Loss: 0.7890	LR: 0.025000
Training Epoch: 40 [29568/50000]	Loss: 0.5212	LR: 0.025000
Training Epoch: 40 [29696/50000]	Loss: 0.6881	LR: 0.025000
Training Epoch: 40 [29824/50000]	Loss: 0.6256	LR: 0.025000
Training Epoch: 40 [29952/50000]	Loss: 0.6312	LR: 0.025000
Training Epoch: 40 [30080/50000]	Loss: 0.5588	LR: 0.025000
Training Epoch: 40 [30208/50000]	Loss: 0.5251	LR: 0.025000
Training Epoch: 40 [30336/50000]	Loss: 0.6692	LR: 0.025000
Training Epoch: 40 [30464/50000]	Loss: 0.6164	LR: 0.025000
Training Epoch: 40 [30592/50000]	Loss: 0.7227	LR: 0.025000
Training Epoch: 40 [30720/50000]	Loss: 0.6311	LR: 0.025000
Training Epoch: 40 [30848/50000]	Loss: 0.7736	LR: 0.025000
Training Epoch: 40 [30976/50000]	Loss: 0.8327	LR: 0.025000
Training Epoch: 40 [31104/50000]	Loss: 0.6613	LR: 0.025000
Training Epoch: 40 [31232/50000]	Loss: 0.6364	LR: 0.025000
Training Epoch: 40 [31360/50000]	Loss: 0.7647	LR: 0.025000
Training Epoch: 40 [31488/50000]	Loss: 0.5210	LR: 0.025000
Training Epoch: 40 [31616/50000]	Loss: 0.4584	LR: 0.025000
Training Epoch: 40 [31744/50000]	Loss: 0.6560	LR: 0.025000
Training Epoch: 40 [31872/50000]	Loss: 0.7793	LR: 0.025000
Training Epoch: 40 [32000/50000]	Loss: 0.5134	LR: 0.025000
Training Epoch: 40 [32128/50000]	Loss: 0.6364	LR: 0.025000
Training Epoch: 40 [32256/50000]	Loss: 0.6533	LR: 0.025000
Training Epoch: 40 [32384/50000]	Loss: 0.4740	LR: 0.025000
Training Epoch: 40 [32512/50000]	Loss: 0.5453	LR: 0.025000
Training Epoch: 40 [32640/50000]	Loss: 0.4775	LR: 0.025000
Training Epoch: 40 [32768/50000]	Loss: 0.5695	LR: 0.025000
Training Epoch: 40 [32896/50000]	Loss: 0.5543	LR: 0.025000
Training Epoch: 40 [33024/50000]	Loss: 0.5576	LR: 0.025000
Training Epoch: 40 [33152/50000]	Loss: 0.5538	LR: 0.025000
Training Epoch: 40 [33280/50000]	Loss: 0.8147	LR: 0.025000
Training Epoch: 40 [33408/50000]	Loss: 0.6299	LR: 0.025000
Training Epoch: 40 [33536/50000]	Loss: 0.6218	LR: 0.025000
Training Epoch: 40 [33664/50000]	Loss: 0.7059	LR: 0.025000
Training Epoch: 40 [33792/50000]	Loss: 0.6695	LR: 0.025000
Training Epoch: 40 [33920/50000]	Loss: 0.6168	LR: 0.025000
Training Epoch: 40 [34048/50000]	Loss: 0.4976	LR: 0.025000
Training Epoch: 40 [34176/50000]	Loss: 0.6449	LR: 0.025000
Training Epoch: 40 [34304/50000]	Loss: 0.6328	LR: 0.025000
Training Epoch: 40 [34432/50000]	Loss: 0.5921	LR: 0.025000
Training Epoch: 40 [34560/50000]	Loss: 0.5247	LR: 0.025000
Training Epoch: 40 [34688/50000]	Loss: 0.5410	LR: 0.025000
Training Epoch: 40 [34816/50000]	Loss: 0.4880	LR: 0.025000
Training Epoch: 40 [34944/50000]	Loss: 0.5965	LR: 0.025000
Training Epoch: 40 [35072/50000]	Loss: 0.5148	LR: 0.025000
Training Epoch: 40 [35200/50000]	Loss: 0.5743	LR: 0.025000
Training Epoch: 40 [35328/50000]	Loss: 0.6403	LR: 0.025000
Training Epoch: 40 [35456/50000]	Loss: 0.5954	LR: 0.025000
Training Epoch: 40 [35584/50000]	Loss: 0.5375	LR: 0.025000
Training Epoch: 40 [35712/50000]	Loss: 0.5867	LR: 0.025000
Training Epoch: 40 [35840/50000]	Loss: 0.5807	LR: 0.025000
Training Epoch: 40 [35968/50000]	Loss: 0.6024	LR: 0.025000
Training Epoch: 40 [36096/50000]	Loss: 0.5353	LR: 0.025000
Training Epoch: 40 [36224/50000]	Loss: 0.4765	LR: 0.025000
Training Epoch: 40 [36352/50000]	Loss: 0.5867	LR: 0.025000
Training Epoch: 40 [36480/50000]	Loss: 0.7113	LR: 0.025000
Training Epoch: 40 [36608/50000]	Loss: 0.7729	LR: 0.025000
Training Epoch: 40 [36736/50000]	Loss: 0.8039	LR: 0.025000
Training Epoch: 40 [36864/50000]	Loss: 0.6196	LR: 0.025000
Training Epoch: 40 [36992/50000]	Loss: 0.5996	LR: 0.025000
Training Epoch: 40 [37120/50000]	Loss: 0.5593	LR: 0.025000
Training Epoch: 40 [37248/50000]	Loss: 0.5567	LR: 0.025000
Training Epoch: 40 [37376/50000]	Loss: 0.6499	LR: 0.025000
Training Epoch: 40 [37504/50000]	Loss: 0.5961	LR: 0.025000
Training Epoch: 40 [37632/50000]	Loss: 0.6414	LR: 0.025000
Training Epoch: 40 [37760/50000]	Loss: 0.7944	LR: 0.025000
Training Epoch: 40 [37888/50000]	Loss: 0.7446	LR: 0.025000
Training Epoch: 40 [38016/50000]	Loss: 0.6233	LR: 0.025000
Training Epoch: 40 [38144/50000]	Loss: 0.5656	LR: 0.025000
Training Epoch: 40 [38272/50000]	Loss: 0.6083	LR: 0.025000
Training Epoch: 40 [38400/50000]	Loss: 0.5727	LR: 0.025000
Training Epoch: 40 [38528/50000]	Loss: 0.7720	LR: 0.025000
Training Epoch: 40 [38656/50000]	Loss: 0.6634	LR: 0.025000
Training Epoch: 40 [38784/50000]	Loss: 0.4564	LR: 0.025000
Training Epoch: 40 [38912/50000]	Loss: 0.6046	LR: 0.025000
Training Epoch: 40 [39040/50000]	Loss: 0.6601	LR: 0.025000
Training Epoch: 40 [39168/50000]	Loss: 0.6066	LR: 0.025000
Training Epoch: 40 [39296/50000]	Loss: 0.6480	LR: 0.025000
Training Epoch: 40 [39424/50000]	Loss: 0.6679	LR: 0.025000
Training Epoch: 40 [39552/50000]	Loss: 0.5842	LR: 0.025000
Training Epoch: 40 [39680/50000]	Loss: 0.4677	LR: 0.025000
Training Epoch: 40 [39808/50000]	Loss: 0.5866	LR: 0.025000
Training Epoch: 40 [39936/50000]	Loss: 0.4849	LR: 0.025000
Training Epoch: 40 [40064/50000]	Loss: 0.6854	LR: 0.025000
Training Epoch: 40 [40192/50000]	Loss: 0.6233	LR: 0.025000
Training Epoch: 40 [40320/50000]	Loss: 0.6945	LR: 0.025000
Training Epoch: 40 [40448/50000]	Loss: 0.5398	LR: 0.025000
Training Epoch: 40 [40576/50000]	Loss: 0.5648	LR: 0.025000
Training Epoch: 40 [40704/50000]	Loss: 0.7387	LR: 0.025000
Training Epoch: 40 [40832/50000]	Loss: 0.5410	LR: 0.025000
Training Epoch: 40 [40960/50000]	Loss: 0.6606	LR: 0.025000
Training Epoch: 40 [41088/50000]	Loss: 0.6737	LR: 0.025000
Training Epoch: 40 [41216/50000]	Loss: 0.6121	LR: 0.025000
Training Epoch: 40 [41344/50000]	Loss: 0.6561	LR: 0.025000
Training Epoch: 40 [41472/50000]	Loss: 0.6258	LR: 0.025000
Training Epoch: 40 [41600/50000]	Loss: 0.6512	LR: 0.025000
Training Epoch: 40 [41728/50000]	Loss: 0.5497	LR: 0.025000
Training Epoch: 40 [41856/50000]	Loss: 0.6075	LR: 0.025000
Training Epoch: 40 [41984/50000]	Loss: 0.4884	LR: 0.025000
Training Epoch: 40 [42112/50000]	Loss: 0.7126	LR: 0.025000
Training Epoch: 40 [42240/50000]	Loss: 0.8296	LR: 0.025000
Training Epoch: 40 [42368/50000]	Loss: 0.5406	LR: 0.025000
Training Epoch: 40 [42496/50000]	Loss: 0.5327	LR: 0.025000
Training Epoch: 40 [42624/50000]	Loss: 0.4985	LR: 0.025000
Training Epoch: 40 [42752/50000]	Loss: 0.6134	LR: 0.025000
Training Epoch: 40 [42880/50000]	Loss: 0.6782	LR: 0.025000
Training Epoch: 40 [43008/50000]	Loss: 0.5904	LR: 0.025000
Training Epoch: 40 [43136/50000]	Loss: 0.8214	LR: 0.025000
Training Epoch: 40 [43264/50000]	Loss: 0.7191	LR: 0.025000
Training Epoch: 40 [43392/50000]	Loss: 0.6389	LR: 0.025000
Training Epoch: 40 [43520/50000]	Loss: 0.6855	LR: 0.025000
Training Epoch: 40 [43648/50000]	Loss: 0.7203	LR: 0.025000
Training Epoch: 40 [43776/50000]	Loss: 0.5344	LR: 0.025000
Training Epoch: 40 [43904/50000]	Loss: 0.5875	LR: 0.025000
Training Epoch: 40 [44032/50000]	Loss: 0.5646	LR: 0.025000
Training Epoch: 40 [44160/50000]	Loss: 0.5590	LR: 0.025000
Training Epoch: 40 [44288/50000]	Loss: 0.5621	LR: 0.025000
Training Epoch: 40 [44416/50000]	Loss: 0.7191	LR: 0.025000
Training Epoch: 40 [44544/50000]	Loss: 0.4974	LR: 0.025000
Training Epoch: 40 [44672/50000]	Loss: 0.5592	LR: 0.025000
Training Epoch: 40 [44800/50000]	Loss: 0.7111	LR: 0.025000
Training Epoch: 40 [44928/50000]	Loss: 0.5529	LR: 0.025000
Training Epoch: 40 [45056/50000]	Loss: 0.6269	LR: 0.025000
Training Epoch: 40 [45184/50000]	Loss: 0.6473	LR: 0.025000
Training Epoch: 40 [45312/50000]	Loss: 0.6955	LR: 0.025000
Training Epoch: 40 [45440/50000]	Loss: 0.6141	LR: 0.025000
Training Epoch: 40 [45568/50000]	Loss: 0.5805	LR: 0.025000
Training Epoch: 40 [45696/50000]	Loss: 0.7186	LR: 0.025000
Training Epoch: 40 [45824/50000]	Loss: 0.5675	LR: 0.025000
Training Epoch: 40 [45952/50000]	Loss: 0.4706	LR: 0.025000
Training Epoch: 40 [46080/50000]	Loss: 0.5672	LR: 0.025000
Training Epoch: 40 [46208/50000]	Loss: 0.7152	LR: 0.025000
Training Epoch: 40 [46336/50000]	Loss: 0.7711	LR: 0.025000
Training Epoch: 40 [46464/50000]	Loss: 0.7458	LR: 0.025000
Training Epoch: 40 [46592/50000]	Loss: 0.6240	LR: 0.025000
Training Epoch: 40 [46720/50000]	Loss: 0.7832	LR: 0.025000
Training Epoch: 40 [46848/50000]	Loss: 0.6673	LR: 0.025000
Training Epoch: 40 [46976/50000]	Loss: 0.7634	LR: 0.025000
Training Epoch: 40 [47104/50000]	Loss: 0.7937	LR: 0.025000
Training Epoch: 40 [47232/50000]	Loss: 0.7745	LR: 0.025000
Training Epoch: 40 [47360/50000]	Loss: 0.6291	LR: 0.025000
Training Epoch: 40 [47488/50000]	Loss: 0.6527	LR: 0.025000
Training Epoch: 40 [47616/50000]	Loss: 0.5490	LR: 0.025000
Training Epoch: 40 [47744/50000]	Loss: 0.6531	LR: 0.025000
Training Epoch: 40 [47872/50000]	Loss: 0.5333	LR: 0.025000
Training Epoch: 40 [48000/50000]	Loss: 0.6044	LR: 0.025000
Training Epoch: 40 [48128/50000]	Loss: 0.7033	LR: 0.025000
Training Epoch: 40 [48256/50000]	Loss: 0.5884	LR: 0.025000
Training Epoch: 40 [48384/50000]	Loss: 0.4836	LR: 0.025000
Training Epoch: 40 [48512/50000]	Loss: 0.5619	LR: 0.025000
Training Epoch: 40 [48640/50000]	Loss: 0.5387	LR: 0.025000
Training Epoch: 40 [48768/50000]	Loss: 0.5496	LR: 0.025000
Training Epoch: 40 [48896/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 40 [49024/50000]	Loss: 0.4997	LR: 0.025000
Training Epoch: 40 [49152/50000]	Loss: 0.6494	LR: 0.025000
Training Epoch: 40 [49280/50000]	Loss: 0.6178	LR: 0.025000
Training Epoch: 40 [49408/50000]	Loss: 0.5648	LR: 0.025000
Training Epoch: 40 [49536/50000]	Loss: 0.6231	LR: 0.025000
Training Epoch: 40 [49664/50000]	Loss: 0.5573	LR: 0.025000
Training Epoch: 40 [49792/50000]	Loss: 0.5105	LR: 0.025000
Training Epoch: 40 [49920/50000]	Loss: 0.5476	LR: 0.025000
Training Epoch: 40 [50000/50000]	Loss: 0.6437	LR: 0.025000
epoch 40 training time consumed: 53.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  146802 GB |  146802 GB |
|       from large pool |  123392 KB |    1034 MB |  146658 GB |  146657 GB |
|       from small pool |   10798 KB |      13 MB |     144 GB |     144 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  146802 GB |  146802 GB |
|       from large pool |  123392 KB |    1034 MB |  146658 GB |  146657 GB |
|       from small pool |   10798 KB |      13 MB |     144 GB |     144 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   64603 GB |   64603 GB |
|       from large pool |  155136 KB |  433088 KB |   64443 GB |   64443 GB |
|       from small pool |    1490 KB |    3494 KB |     159 GB |     159 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    5664 K  |    5664 K  |
|       from large pool |      24    |      65    |    2956 K  |    2956 K  |
|       from small pool |     231    |     274    |    2708 K  |    2707 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    5664 K  |    5664 K  |
|       from large pool |      24    |      65    |    2956 K  |    2956 K  |
|       from small pool |     231    |     274    |    2708 K  |    2707 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2800 K  |    2800 K  |
|       from large pool |       9    |      14    |    1431 K  |    1431 K  |
|       from small pool |      12    |      16    |    1369 K  |    1369 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 40, Average loss: 0.0090, Accuracy: 0.6935, Time consumed:3.45s

Training Epoch: 41 [128/50000]	Loss: 0.6669	LR: 0.025000
Training Epoch: 41 [256/50000]	Loss: 0.6871	LR: 0.025000
Training Epoch: 41 [384/50000]	Loss: 0.6000	LR: 0.025000
Training Epoch: 41 [512/50000]	Loss: 0.6377	LR: 0.025000
Training Epoch: 41 [640/50000]	Loss: 0.4920	LR: 0.025000
Training Epoch: 41 [768/50000]	Loss: 0.4606	LR: 0.025000
Training Epoch: 41 [896/50000]	Loss: 0.5059	LR: 0.025000
Training Epoch: 41 [1024/50000]	Loss: 0.6266	LR: 0.025000
Training Epoch: 41 [1152/50000]	Loss: 0.5767	LR: 0.025000
Training Epoch: 41 [1280/50000]	Loss: 0.4857	LR: 0.025000
Training Epoch: 41 [1408/50000]	Loss: 0.5296	LR: 0.025000
Training Epoch: 41 [1536/50000]	Loss: 0.4452	LR: 0.025000
Training Epoch: 41 [1664/50000]	Loss: 0.4379	LR: 0.025000
Training Epoch: 41 [1792/50000]	Loss: 0.5518	LR: 0.025000
Training Epoch: 41 [1920/50000]	Loss: 0.4901	LR: 0.025000
Training Epoch: 41 [2048/50000]	Loss: 0.4523	LR: 0.025000
Training Epoch: 41 [2176/50000]	Loss: 0.5004	LR: 0.025000
Training Epoch: 41 [2304/50000]	Loss: 0.4215	LR: 0.025000
Training Epoch: 41 [2432/50000]	Loss: 0.5561	LR: 0.025000
Training Epoch: 41 [2560/50000]	Loss: 0.5604	LR: 0.025000
Training Epoch: 41 [2688/50000]	Loss: 0.5343	LR: 0.025000
Training Epoch: 41 [2816/50000]	Loss: 0.6188	LR: 0.025000
Training Epoch: 41 [2944/50000]	Loss: 0.5129	LR: 0.025000
Training Epoch: 41 [3072/50000]	Loss: 0.4834	LR: 0.025000
Training Epoch: 41 [3200/50000]	Loss: 0.5167	LR: 0.025000
Training Epoch: 41 [3328/50000]	Loss: 0.4769	LR: 0.025000
Training Epoch: 41 [3456/50000]	Loss: 0.5167	LR: 0.025000
Training Epoch: 41 [3584/50000]	Loss: 0.5829	LR: 0.025000
Training Epoch: 41 [3712/50000]	Loss: 0.5768	LR: 0.025000
Training Epoch: 41 [3840/50000]	Loss: 0.6122	LR: 0.025000
Training Epoch: 41 [3968/50000]	Loss: 0.5520	LR: 0.025000
Training Epoch: 41 [4096/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 41 [4224/50000]	Loss: 0.4521	LR: 0.025000
Training Epoch: 41 [4352/50000]	Loss: 0.7220	LR: 0.025000
Training Epoch: 41 [4480/50000]	Loss: 0.5537	LR: 0.025000
Training Epoch: 41 [4608/50000]	Loss: 0.3756	LR: 0.025000
Training Epoch: 41 [4736/50000]	Loss: 0.4296	LR: 0.025000
Training Epoch: 41 [4864/50000]	Loss: 0.5764	LR: 0.025000
Training Epoch: 41 [4992/50000]	Loss: 0.4988	LR: 0.025000
Training Epoch: 41 [5120/50000]	Loss: 0.6016	LR: 0.025000
Training Epoch: 41 [5248/50000]	Loss: 0.5974	LR: 0.025000
Training Epoch: 41 [5376/50000]	Loss: 0.4905	LR: 0.025000
Training Epoch: 41 [5504/50000]	Loss: 0.6545	LR: 0.025000
Training Epoch: 41 [5632/50000]	Loss: 0.5248	LR: 0.025000
Training Epoch: 41 [5760/50000]	Loss: 0.4180	LR: 0.025000
Training Epoch: 41 [5888/50000]	Loss: 0.4678	LR: 0.025000
Training Epoch: 41 [6016/50000]	Loss: 0.5709	LR: 0.025000
Training Epoch: 41 [6144/50000]	Loss: 0.6135	LR: 0.025000
Training Epoch: 41 [6272/50000]	Loss: 0.6094	LR: 0.025000
Training Epoch: 41 [6400/50000]	Loss: 0.4889	LR: 0.025000
Training Epoch: 41 [6528/50000]	Loss: 0.3798	LR: 0.025000
Training Epoch: 41 [6656/50000]	Loss: 0.4567	LR: 0.025000
Training Epoch: 41 [6784/50000]	Loss: 0.5621	LR: 0.025000
Training Epoch: 41 [6912/50000]	Loss: 0.5296	LR: 0.025000
Training Epoch: 41 [7040/50000]	Loss: 0.6116	LR: 0.025000
Training Epoch: 41 [7168/50000]	Loss: 0.3285	LR: 0.025000
Training Epoch: 41 [7296/50000]	Loss: 0.5848	LR: 0.025000
Training Epoch: 41 [7424/50000]	Loss: 0.4111	LR: 0.025000
Training Epoch: 41 [7552/50000]	Loss: 0.5341	LR: 0.025000
Training Epoch: 41 [7680/50000]	Loss: 0.6151	LR: 0.025000
Training Epoch: 41 [7808/50000]	Loss: 0.4558	LR: 0.025000
Training Epoch: 41 [7936/50000]	Loss: 0.5448	LR: 0.025000
Training Epoch: 41 [8064/50000]	Loss: 0.5059	LR: 0.025000
Training Epoch: 41 [8192/50000]	Loss: 0.6082	LR: 0.025000
Training Epoch: 41 [8320/50000]	Loss: 0.5358	LR: 0.025000
Training Epoch: 41 [8448/50000]	Loss: 0.6286	LR: 0.025000
Training Epoch: 41 [8576/50000]	Loss: 0.5313	LR: 0.025000
Training Epoch: 41 [8704/50000]	Loss: 0.5360	LR: 0.025000
Training Epoch: 41 [8832/50000]	Loss: 0.6444	LR: 0.025000
Training Epoch: 41 [8960/50000]	Loss: 0.5315	LR: 0.025000
Training Epoch: 41 [9088/50000]	Loss: 0.4820	LR: 0.025000
Training Epoch: 41 [9216/50000]	Loss: 0.4799	LR: 0.025000
Training Epoch: 41 [9344/50000]	Loss: 0.5539	LR: 0.025000
Training Epoch: 41 [9472/50000]	Loss: 0.4824	LR: 0.025000
Training Epoch: 41 [9600/50000]	Loss: 0.4351	LR: 0.025000
Training Epoch: 41 [9728/50000]	Loss: 0.6262	LR: 0.025000
Training Epoch: 41 [9856/50000]	Loss: 0.5903	LR: 0.025000
Training Epoch: 41 [9984/50000]	Loss: 0.4134	LR: 0.025000
Training Epoch: 41 [10112/50000]	Loss: 0.5719	LR: 0.025000
Training Epoch: 41 [10240/50000]	Loss: 0.4607	LR: 0.025000
Training Epoch: 41 [10368/50000]	Loss: 0.4565	LR: 0.025000
Training Epoch: 41 [10496/50000]	Loss: 0.5779	LR: 0.025000
Training Epoch: 41 [10624/50000]	Loss: 0.5553	LR: 0.025000
Training Epoch: 41 [10752/50000]	Loss: 0.5444	LR: 0.025000
Training Epoch: 41 [10880/50000]	Loss: 0.5245	LR: 0.025000
Training Epoch: 41 [11008/50000]	Loss: 0.6229	LR: 0.025000
Training Epoch: 41 [11136/50000]	Loss: 0.6152	LR: 0.025000
Training Epoch: 41 [11264/50000]	Loss: 0.5384	LR: 0.025000
Training Epoch: 41 [11392/50000]	Loss: 0.6078	LR: 0.025000
Training Epoch: 41 [11520/50000]	Loss: 0.5601	LR: 0.025000
Training Epoch: 41 [11648/50000]	Loss: 0.5001	LR: 0.025000
Training Epoch: 41 [11776/50000]	Loss: 0.5989	LR: 0.025000
Training Epoch: 41 [11904/50000]	Loss: 0.5498	LR: 0.025000
Training Epoch: 41 [12032/50000]	Loss: 0.5783	LR: 0.025000
Training Epoch: 41 [12160/50000]	Loss: 0.4687	LR: 0.025000
Training Epoch: 41 [12288/50000]	Loss: 0.6435	LR: 0.025000
Training Epoch: 41 [12416/50000]	Loss: 0.5905	LR: 0.025000
Training Epoch: 41 [12544/50000]	Loss: 0.6848	LR: 0.025000
Training Epoch: 41 [12672/50000]	Loss: 0.5153	LR: 0.025000
Training Epoch: 41 [12800/50000]	Loss: 0.5526	LR: 0.025000
Training Epoch: 41 [12928/50000]	Loss: 0.4186	LR: 0.025000
Training Epoch: 41 [13056/50000]	Loss: 0.3795	LR: 0.025000
Training Epoch: 41 [13184/50000]	Loss: 0.6206	LR: 0.025000
Training Epoch: 41 [13312/50000]	Loss: 0.5797	LR: 0.025000
Training Epoch: 41 [13440/50000]	Loss: 0.8064	LR: 0.025000
Training Epoch: 41 [13568/50000]	Loss: 0.5700	LR: 0.025000
Training Epoch: 41 [13696/50000]	Loss: 0.4105	LR: 0.025000
Training Epoch: 41 [13824/50000]	Loss: 0.4682	LR: 0.025000
Training Epoch: 41 [13952/50000]	Loss: 0.6121	LR: 0.025000
Training Epoch: 41 [14080/50000]	Loss: 0.5536	LR: 0.025000
Training Epoch: 41 [14208/50000]	Loss: 0.4722	LR: 0.025000
Training Epoch: 41 [14336/50000]	Loss: 0.5413	LR: 0.025000
Training Epoch: 41 [14464/50000]	Loss: 0.6736	LR: 0.025000
Training Epoch: 41 [14592/50000]	Loss: 0.5553	LR: 0.025000
Training Epoch: 41 [14720/50000]	Loss: 0.6326	LR: 0.025000
Training Epoch: 41 [14848/50000]	Loss: 0.6875	LR: 0.025000
Training Epoch: 41 [14976/50000]	Loss: 0.4995	LR: 0.025000
Training Epoch: 41 [15104/50000]	Loss: 0.4582	LR: 0.025000
Training Epoch: 41 [15232/50000]	Loss: 0.5419	LR: 0.025000
Training Epoch: 41 [15360/50000]	Loss: 0.5615	LR: 0.025000
Training Epoch: 41 [15488/50000]	Loss: 0.4082	LR: 0.025000
Training Epoch: 41 [15616/50000]	Loss: 0.5739	LR: 0.025000
Training Epoch: 41 [15744/50000]	Loss: 0.4711	LR: 0.025000
Training Epoch: 41 [15872/50000]	Loss: 0.4867	LR: 0.025000
Training Epoch: 41 [16000/50000]	Loss: 0.5502	LR: 0.025000
Training Epoch: 41 [16128/50000]	Loss: 0.4421	LR: 0.025000
Training Epoch: 41 [16256/50000]	Loss: 0.3706	LR: 0.025000
Training Epoch: 41 [16384/50000]	Loss: 0.5971	LR: 0.025000
Training Epoch: 41 [16512/50000]	Loss: 0.7696	LR: 0.025000
Training Epoch: 41 [16640/50000]	Loss: 0.6394	LR: 0.025000
Training Epoch: 41 [16768/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 41 [16896/50000]	Loss: 0.4231	LR: 0.025000
Training Epoch: 41 [17024/50000]	Loss: 0.5361	LR: 0.025000
Training Epoch: 41 [17152/50000]	Loss: 0.5534	LR: 0.025000
Training Epoch: 41 [17280/50000]	Loss: 0.6904	LR: 0.025000
Training Epoch: 41 [17408/50000]	Loss: 0.4872	LR: 0.025000
Training Epoch: 41 [17536/50000]	Loss: 0.5510	LR: 0.025000
Training Epoch: 41 [17664/50000]	Loss: 0.5617	LR: 0.025000
Training Epoch: 41 [17792/50000]	Loss: 0.4386	LR: 0.025000
Training Epoch: 41 [17920/50000]	Loss: 0.4548	LR: 0.025000
Training Epoch: 41 [18048/50000]	Loss: 0.5873	LR: 0.025000
Training Epoch: 41 [18176/50000]	Loss: 0.6137	LR: 0.025000
Training Epoch: 41 [18304/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 41 [18432/50000]	Loss: 0.5086	LR: 0.025000
Training Epoch: 41 [18560/50000]	Loss: 0.4694	LR: 0.025000
Training Epoch: 41 [18688/50000]	Loss: 0.4189	LR: 0.025000
Training Epoch: 41 [18816/50000]	Loss: 0.8466	LR: 0.025000
Training Epoch: 41 [18944/50000]	Loss: 0.6246	LR: 0.025000
Training Epoch: 41 [19072/50000]	Loss: 0.4537	LR: 0.025000
Training Epoch: 41 [19200/50000]	Loss: 0.4827	LR: 0.025000
Training Epoch: 41 [19328/50000]	Loss: 0.5740	LR: 0.025000
Training Epoch: 41 [19456/50000]	Loss: 0.5092	LR: 0.025000
Training Epoch: 41 [19584/50000]	Loss: 0.5828	LR: 0.025000
Training Epoch: 41 [19712/50000]	Loss: 0.7792	LR: 0.025000
Training Epoch: 41 [19840/50000]	Loss: 0.5714	LR: 0.025000
Training Epoch: 41 [19968/50000]	Loss: 0.4764	LR: 0.025000
Training Epoch: 41 [20096/50000]	Loss: 0.5891	LR: 0.025000
Training Epoch: 41 [20224/50000]	Loss: 0.4644	LR: 0.025000
Training Epoch: 41 [20352/50000]	Loss: 0.3559	LR: 0.025000
Training Epoch: 41 [20480/50000]	Loss: 0.5050	LR: 0.025000
Training Epoch: 41 [20608/50000]	Loss: 0.6587	LR: 0.025000
Training Epoch: 41 [20736/50000]	Loss: 0.4826	LR: 0.025000
Training Epoch: 41 [20864/50000]	Loss: 0.5636	LR: 0.025000
Training Epoch: 41 [20992/50000]	Loss: 0.5343	LR: 0.025000
Training Epoch: 41 [21120/50000]	Loss: 0.5750	LR: 0.025000
Training Epoch: 41 [21248/50000]	Loss: 0.6104	LR: 0.025000
Training Epoch: 41 [21376/50000]	Loss: 0.4548	LR: 0.025000
Training Epoch: 41 [21504/50000]	Loss: 0.5278	LR: 0.025000
Training Epoch: 41 [21632/50000]	Loss: 0.5734	LR: 0.025000
Training Epoch: 41 [21760/50000]	Loss: 0.5605	LR: 0.025000
Training Epoch: 41 [21888/50000]	Loss: 0.6088	LR: 0.025000
Training Epoch: 41 [22016/50000]	Loss: 0.5576	LR: 0.025000
Training Epoch: 41 [22144/50000]	Loss: 0.5895	LR: 0.025000
Training Epoch: 41 [22272/50000]	Loss: 0.7055	LR: 0.025000
Training Epoch: 41 [22400/50000]	Loss: 0.7201	LR: 0.025000
Training Epoch: 41 [22528/50000]	Loss: 0.5426	LR: 0.025000
Training Epoch: 41 [22656/50000]	Loss: 0.4245	LR: 0.025000
Training Epoch: 41 [22784/50000]	Loss: 0.7569	LR: 0.025000
Training Epoch: 41 [22912/50000]	Loss: 0.6512	LR: 0.025000
Training Epoch: 41 [23040/50000]	Loss: 0.4094	LR: 0.025000
Training Epoch: 41 [23168/50000]	Loss: 0.6237	LR: 0.025000
Training Epoch: 41 [23296/50000]	Loss: 0.4340	LR: 0.025000
Training Epoch: 41 [23424/50000]	Loss: 0.6558	LR: 0.025000
Training Epoch: 41 [23552/50000]	Loss: 0.6164	LR: 0.025000
Training Epoch: 41 [23680/50000]	Loss: 0.6714	LR: 0.025000
Training Epoch: 41 [23808/50000]	Loss: 0.6384	LR: 0.025000
Training Epoch: 41 [23936/50000]	Loss: 0.5933	LR: 0.025000
Training Epoch: 41 [24064/50000]	Loss: 0.4854	LR: 0.025000
Training Epoch: 41 [24192/50000]	Loss: 0.4992	LR: 0.025000
Training Epoch: 41 [24320/50000]	Loss: 0.6321	LR: 0.025000
Training Epoch: 41 [24448/50000]	Loss: 0.4873	LR: 0.025000
Training Epoch: 41 [24576/50000]	Loss: 0.6584	LR: 0.025000
Training Epoch: 41 [24704/50000]	Loss: 0.4682	LR: 0.025000
Training Epoch: 41 [24832/50000]	Loss: 0.5142	LR: 0.025000
Training Epoch: 41 [24960/50000]	Loss: 0.4712	LR: 0.025000
Training Epoch: 41 [25088/50000]	Loss: 0.7246	LR: 0.025000
Training Epoch: 41 [25216/50000]	Loss: 0.4854	LR: 0.025000
Training Epoch: 41 [25344/50000]	Loss: 0.5122	LR: 0.025000
Training Epoch: 41 [25472/50000]	Loss: 0.4833	LR: 0.025000
Training Epoch: 41 [25600/50000]	Loss: 0.8909	LR: 0.025000
Training Epoch: 41 [25728/50000]	Loss: 0.5071	LR: 0.025000
Training Epoch: 41 [25856/50000]	Loss: 0.5199	LR: 0.025000
Training Epoch: 41 [25984/50000]	Loss: 0.5060	LR: 0.025000
Training Epoch: 41 [26112/50000]	Loss: 0.7160	LR: 0.025000
Training Epoch: 41 [26240/50000]	Loss: 0.6351	LR: 0.025000
Training Epoch: 41 [26368/50000]	Loss: 0.5915	LR: 0.025000
Training Epoch: 41 [26496/50000]	Loss: 0.5797	LR: 0.025000
Training Epoch: 41 [26624/50000]	Loss: 0.7385	LR: 0.025000
Training Epoch: 41 [26752/50000]	Loss: 0.4698	LR: 0.025000
Training Epoch: 41 [26880/50000]	Loss: 0.4956	LR: 0.025000
Training Epoch: 41 [27008/50000]	Loss: 0.5780	LR: 0.025000
Training Epoch: 41 [27136/50000]	Loss: 0.6125	LR: 0.025000
Training Epoch: 41 [27264/50000]	Loss: 0.5872	LR: 0.025000
Training Epoch: 41 [27392/50000]	Loss: 0.5665	LR: 0.025000
Training Epoch: 41 [27520/50000]	Loss: 0.5407	LR: 0.025000
Training Epoch: 41 [27648/50000]	Loss: 0.6273	LR: 0.025000
Training Epoch: 41 [27776/50000]	Loss: 0.5446	LR: 0.025000
Training Epoch: 41 [27904/50000]	Loss: 0.5094	LR: 0.025000
Training Epoch: 41 [28032/50000]	Loss: 0.6409	LR: 0.025000
Training Epoch: 41 [28160/50000]	Loss: 0.6065	LR: 0.025000
Training Epoch: 41 [28288/50000]	Loss: 0.5463	LR: 0.025000
Training Epoch: 41 [28416/50000]	Loss: 0.6025	LR: 0.025000
Training Epoch: 41 [28544/50000]	Loss: 0.5735	LR: 0.025000
Training Epoch: 41 [28672/50000]	Loss: 0.5664	LR: 0.025000
Training Epoch: 41 [28800/50000]	Loss: 0.5535	LR: 0.025000
Training Epoch: 41 [28928/50000]	Loss: 0.5450	LR: 0.025000
Training Epoch: 41 [29056/50000]	Loss: 0.6435	LR: 0.025000
Training Epoch: 41 [29184/50000]	Loss: 0.5920	LR: 0.025000
Training Epoch: 41 [29312/50000]	Loss: 0.4856	LR: 0.025000
Training Epoch: 41 [29440/50000]	Loss: 0.5013	LR: 0.025000
Training Epoch: 41 [29568/50000]	Loss: 0.5964	LR: 0.025000
Training Epoch: 41 [29696/50000]	Loss: 0.5117	LR: 0.025000
Training Epoch: 41 [29824/50000]	Loss: 0.3857	LR: 0.025000
Training Epoch: 41 [29952/50000]	Loss: 0.5874	LR: 0.025000
Training Epoch: 41 [30080/50000]	Loss: 0.5667	LR: 0.025000
Training Epoch: 41 [30208/50000]	Loss: 0.5850	LR: 0.025000
Training Epoch: 41 [30336/50000]	Loss: 0.4922	LR: 0.025000
Training Epoch: 41 [30464/50000]	Loss: 0.5833	LR: 0.025000
Training Epoch: 41 [30592/50000]	Loss: 0.6113	LR: 0.025000
Training Epoch: 41 [30720/50000]	Loss: 0.5032	LR: 0.025000
Training Epoch: 41 [30848/50000]	Loss: 0.5457	LR: 0.025000
Training Epoch: 41 [30976/50000]	Loss: 0.6385	LR: 0.025000
Training Epoch: 41 [31104/50000]	Loss: 0.6786	LR: 0.025000
Training Epoch: 41 [31232/50000]	Loss: 0.7350	LR: 0.025000
Training Epoch: 41 [31360/50000]	Loss: 0.7981	LR: 0.025000
Training Epoch: 41 [31488/50000]	Loss: 0.6650	LR: 0.025000
Training Epoch: 41 [31616/50000]	Loss: 0.4939	LR: 0.025000
Training Epoch: 41 [31744/50000]	Loss: 0.6573	LR: 0.025000
Training Epoch: 41 [31872/50000]	Loss: 0.6129	LR: 0.025000
Training Epoch: 41 [32000/50000]	Loss: 0.5364	LR: 0.025000
Training Epoch: 41 [32128/50000]	Loss: 0.5948	LR: 0.025000
Training Epoch: 41 [32256/50000]	Loss: 0.7079	LR: 0.025000
Training Epoch: 41 [32384/50000]	Loss: 0.7399	LR: 0.025000
Training Epoch: 41 [32512/50000]	Loss: 0.6631	LR: 0.025000
Training Epoch: 41 [32640/50000]	Loss: 0.4881	LR: 0.025000
Training Epoch: 41 [32768/50000]	Loss: 0.6205	LR: 0.025000
Training Epoch: 41 [32896/50000]	Loss: 0.5663	LR: 0.025000
Training Epoch: 41 [33024/50000]	Loss: 0.7088	LR: 0.025000
Training Epoch: 41 [33152/50000]	Loss: 0.6107	LR: 0.025000
Training Epoch: 41 [33280/50000]	Loss: 0.6201	LR: 0.025000
Training Epoch: 41 [33408/50000]	Loss: 0.6811	LR: 0.025000
Training Epoch: 41 [33536/50000]	Loss: 0.6018	LR: 0.025000
Training Epoch: 41 [33664/50000]	Loss: 0.6054	LR: 0.025000
Training Epoch: 41 [33792/50000]	Loss: 0.6415	LR: 0.025000
Training Epoch: 41 [33920/50000]	Loss: 0.5618	LR: 0.025000
Training Epoch: 41 [34048/50000]	Loss: 0.4514	LR: 0.025000
Training Epoch: 41 [34176/50000]	Loss: 0.6537	LR: 0.025000
Training Epoch: 41 [34304/50000]	Loss: 0.5683	LR: 0.025000
Training Epoch: 41 [34432/50000]	Loss: 0.4603	LR: 0.025000
Training Epoch: 41 [34560/50000]	Loss: 0.4499	LR: 0.025000
Training Epoch: 41 [34688/50000]	Loss: 0.5889	LR: 0.025000
Training Epoch: 41 [34816/50000]	Loss: 0.6310	LR: 0.025000
Training Epoch: 41 [34944/50000]	Loss: 0.5104	LR: 0.025000
Training Epoch: 41 [35072/50000]	Loss: 0.6391	LR: 0.025000
Training Epoch: 41 [35200/50000]	Loss: 0.7910	LR: 0.025000
Training Epoch: 41 [35328/50000]	Loss: 0.6348	LR: 0.025000
Training Epoch: 41 [35456/50000]	Loss: 0.5835	LR: 0.025000
Training Epoch: 41 [35584/50000]	Loss: 0.7196	LR: 0.025000
Training Epoch: 41 [35712/50000]	Loss: 0.6673	LR: 0.025000
Training Epoch: 41 [35840/50000]	Loss: 0.7167	LR: 0.025000
Training Epoch: 41 [35968/50000]	Loss: 0.5893	LR: 0.025000
Training Epoch: 41 [36096/50000]	Loss: 0.5681	LR: 0.025000
Training Epoch: 41 [36224/50000]	Loss: 0.5368	LR: 0.025000
Training Epoch: 41 [36352/50000]	Loss: 0.4868	LR: 0.025000
Training Epoch: 41 [36480/50000]	Loss: 0.5955	LR: 0.025000
Training Epoch: 41 [36608/50000]	Loss: 0.5801	LR: 0.025000
Training Epoch: 41 [36736/50000]	Loss: 0.6710	LR: 0.025000
Training Epoch: 41 [36864/50000]	Loss: 0.4608	LR: 0.025000
Training Epoch: 41 [36992/50000]	Loss: 0.5037	LR: 0.025000
Training Epoch: 41 [37120/50000]	Loss: 0.4798	LR: 0.025000
Training Epoch: 41 [37248/50000]	Loss: 0.5338	LR: 0.025000
Training Epoch: 41 [37376/50000]	Loss: 0.4792	LR: 0.025000
Training Epoch: 41 [37504/50000]	Loss: 0.6148	LR: 0.025000
Training Epoch: 41 [37632/50000]	Loss: 0.4514	LR: 0.025000
Training Epoch: 41 [37760/50000]	Loss: 0.5138	LR: 0.025000
Training Epoch: 41 [37888/50000]	Loss: 0.6550	LR: 0.025000
Training Epoch: 41 [38016/50000]	Loss: 0.5625	LR: 0.025000
Training Epoch: 41 [38144/50000]	Loss: 0.5557	LR: 0.025000
Training Epoch: 41 [38272/50000]	Loss: 0.6298	LR: 0.025000
Training Epoch: 41 [38400/50000]	Loss: 0.5466	LR: 0.025000
Training Epoch: 41 [38528/50000]	Loss: 0.5600	LR: 0.025000
Training Epoch: 41 [38656/50000]	Loss: 0.4775	LR: 0.025000
Training Epoch: 41 [38784/50000]	Loss: 0.6344	LR: 0.025000
Training Epoch: 41 [38912/50000]	Loss: 0.4223	LR: 0.025000
Training Epoch: 41 [39040/50000]	Loss: 0.7240	LR: 0.025000
Training Epoch: 41 [39168/50000]	Loss: 0.7619	LR: 0.025000
Training Epoch: 41 [39296/50000]	Loss: 0.6625	LR: 0.025000
Training Epoch: 41 [39424/50000]	Loss: 0.6038	LR: 0.025000
Training Epoch: 41 [39552/50000]	Loss: 0.6517	LR: 0.025000
Training Epoch: 41 [39680/50000]	Loss: 0.5860	LR: 0.025000
Training Epoch: 41 [39808/50000]	Loss: 0.6013	LR: 0.025000
Training Epoch: 41 [39936/50000]	Loss: 0.7296	LR: 0.025000
Training Epoch: 41 [40064/50000]	Loss: 0.7270	LR: 0.025000
Training Epoch: 41 [40192/50000]	Loss: 0.5619	LR: 0.025000
Training Epoch: 41 [40320/50000]	Loss: 0.6446	LR: 0.025000
Training Epoch: 41 [40448/50000]	Loss: 0.6001	LR: 0.025000
Training Epoch: 41 [40576/50000]	Loss: 0.6810	LR: 0.025000
Training Epoch: 41 [40704/50000]	Loss: 0.5625	LR: 0.025000
Training Epoch: 41 [40832/50000]	Loss: 0.4463	LR: 0.025000
Training Epoch: 41 [40960/50000]	Loss: 0.5164	LR: 0.025000
Training Epoch: 41 [41088/50000]	Loss: 0.5498	LR: 0.025000
Training Epoch: 41 [41216/50000]	Loss: 0.6760	LR: 0.025000
Training Epoch: 41 [41344/50000]	Loss: 0.4739	LR: 0.025000
Training Epoch: 41 [41472/50000]	Loss: 0.5602	LR: 0.025000
Training Epoch: 41 [41600/50000]	Loss: 0.5465	LR: 0.025000
Training Epoch: 41 [41728/50000]	Loss: 0.5331	LR: 0.025000
Training Epoch: 41 [41856/50000]	Loss: 0.8271	LR: 0.025000
Training Epoch: 41 [41984/50000]	Loss: 0.5952	LR: 0.025000
Training Epoch: 41 [42112/50000]	Loss: 0.5347	LR: 0.025000
Training Epoch: 41 [42240/50000]	Loss: 0.6185	LR: 0.025000
Training Epoch: 41 [42368/50000]	Loss: 0.6573	LR: 0.025000
Training Epoch: 41 [42496/50000]	Loss: 0.6622	LR: 0.025000
Training Epoch: 41 [42624/50000]	Loss: 0.6235	LR: 0.025000
Training Epoch: 41 [42752/50000]	Loss: 0.5064	LR: 0.025000
Training Epoch: 41 [42880/50000]	Loss: 0.6516	LR: 0.025000
Training Epoch: 41 [43008/50000]	Loss: 0.6165	LR: 0.025000
Training Epoch: 41 [43136/50000]	Loss: 0.6122	LR: 0.025000
Training Epoch: 41 [43264/50000]	Loss: 0.4934	LR: 0.025000
Training Epoch: 41 [43392/50000]	Loss: 0.6640	LR: 0.025000
Training Epoch: 41 [43520/50000]	Loss: 0.6405	LR: 0.025000
Training Epoch: 41 [43648/50000]	Loss: 0.5263	LR: 0.025000
Training Epoch: 41 [43776/50000]	Loss: 0.5462	LR: 0.025000
Training Epoch: 41 [43904/50000]	Loss: 0.6040	LR: 0.025000
Training Epoch: 41 [44032/50000]	Loss: 0.5905	LR: 0.025000
Training Epoch: 41 [44160/50000]	Loss: 0.5759	LR: 0.025000
Training Epoch: 41 [44288/50000]	Loss: 0.6627	LR: 0.025000
Training Epoch: 41 [44416/50000]	Loss: 0.6225	LR: 0.025000
Training Epoch: 41 [44544/50000]	Loss: 0.6649	LR: 0.025000
Training Epoch: 41 [44672/50000]	Loss: 0.5592	LR: 0.025000
Training Epoch: 41 [44800/50000]	Loss: 0.6436	LR: 0.025000
Training Epoch: 41 [44928/50000]	Loss: 0.4710	LR: 0.025000
Training Epoch: 41 [45056/50000]	Loss: 0.6918	LR: 0.025000
Training Epoch: 41 [45184/50000]	Loss: 0.6270	LR: 0.025000
Training Epoch: 41 [45312/50000]	Loss: 0.5880	LR: 0.025000
Training Epoch: 41 [45440/50000]	Loss: 0.6957	LR: 0.025000
Training Epoch: 41 [45568/50000]	Loss: 0.5820	LR: 0.025000
Training Epoch: 41 [45696/50000]	Loss: 0.6092	LR: 0.025000
Training Epoch: 41 [45824/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 41 [45952/50000]	Loss: 0.5897	LR: 0.025000
Training Epoch: 41 [46080/50000]	Loss: 0.6090	LR: 0.025000
Training Epoch: 41 [46208/50000]	Loss: 0.5936	LR: 0.025000
Training Epoch: 41 [46336/50000]	Loss: 0.6960	LR: 0.025000
Training Epoch: 41 [46464/50000]	Loss: 0.6331	LR: 0.025000
Training Epoch: 41 [46592/50000]	Loss: 0.6760	LR: 0.025000
Training Epoch: 41 [46720/50000]	Loss: 0.7668	LR: 0.025000
Training Epoch: 41 [46848/50000]	Loss: 0.5495	LR: 0.025000
Training Epoch: 41 [46976/50000]	Loss: 0.6281	LR: 0.025000
Training Epoch: 41 [47104/50000]	Loss: 0.6895	LR: 0.025000
Training Epoch: 41 [47232/50000]	Loss: 0.6207	LR: 0.025000
Training Epoch: 41 [47360/50000]	Loss: 0.6154	LR: 0.025000
Training Epoch: 41 [47488/50000]	Loss: 0.4396	LR: 0.025000
Training Epoch: 41 [47616/50000]	Loss: 0.5538	LR: 0.025000
Training Epoch: 41 [47744/50000]	Loss: 0.4406	LR: 0.025000
Training Epoch: 41 [47872/50000]	Loss: 0.6652	LR: 0.025000
Training Epoch: 41 [48000/50000]	Loss: 0.5632	LR: 0.025000
Training Epoch: 41 [48128/50000]	Loss: 0.6966	LR: 0.025000
Training Epoch: 41 [48256/50000]	Loss: 0.5564	LR: 0.025000
Training Epoch: 41 [48384/50000]	Loss: 0.5808	LR: 0.025000
Training Epoch: 41 [48512/50000]	Loss: 0.5344	LR: 0.025000
Training Epoch: 41 [48640/50000]	Loss: 0.5792	LR: 0.025000
Training Epoch: 41 [48768/50000]	Loss: 0.5437	LR: 0.025000
Training Epoch: 41 [48896/50000]	Loss: 0.6372	LR: 0.025000
Training Epoch: 41 [49024/50000]	Loss: 0.6491	LR: 0.025000
Training Epoch: 41 [49152/50000]	Loss: 0.5837	LR: 0.025000
Training Epoch: 41 [49280/50000]	Loss: 0.4540	LR: 0.025000
Training Epoch: 41 [49408/50000]	Loss: 0.7060	LR: 0.025000
Training Epoch: 41 [49536/50000]	Loss: 0.6371	LR: 0.025000
Training Epoch: 41 [49664/50000]	Loss: 0.5588	LR: 0.025000
Training Epoch: 41 [49792/50000]	Loss: 0.6588	LR: 0.025000
Training Epoch: 41 [49920/50000]	Loss: 0.6216	LR: 0.025000
Training Epoch: 41 [50000/50000]	Loss: 0.4588	LR: 0.025000
epoch 41 training time consumed: 53.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  150472 GB |  150472 GB |
|       from large pool |  123392 KB |    1034 MB |  150324 GB |  150324 GB |
|       from small pool |   10798 KB |      13 MB |     148 GB |     148 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  150472 GB |  150472 GB |
|       from large pool |  123392 KB |    1034 MB |  150324 GB |  150324 GB |
|       from small pool |   10798 KB |      13 MB |     148 GB |     148 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   66218 GB |   66218 GB |
|       from large pool |  155136 KB |  433088 KB |   66054 GB |   66054 GB |
|       from small pool |    1490 KB |    3494 KB |     163 GB |     163 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    5806 K  |    5806 K  |
|       from large pool |      24    |      65    |    3030 K  |    3030 K  |
|       from small pool |     231    |     274    |    2775 K  |    2775 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    5806 K  |    5806 K  |
|       from large pool |      24    |      65    |    3030 K  |    3030 K  |
|       from small pool |     231    |     274    |    2775 K  |    2775 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2870 K  |    2870 K  |
|       from large pool |       9    |      14    |    1466 K  |    1466 K  |
|       from small pool |      12    |      16    |    1403 K  |    1403 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 41, Average loss: 0.0087, Accuracy: 0.7009, Time consumed:3.45s

Training Epoch: 42 [128/50000]	Loss: 0.4990	LR: 0.025000
Training Epoch: 42 [256/50000]	Loss: 0.5775	LR: 0.025000
Training Epoch: 42 [384/50000]	Loss: 0.4526	LR: 0.025000
Training Epoch: 42 [512/50000]	Loss: 0.4551	LR: 0.025000
Training Epoch: 42 [640/50000]	Loss: 0.3876	LR: 0.025000
Training Epoch: 42 [768/50000]	Loss: 0.4363	LR: 0.025000
Training Epoch: 42 [896/50000]	Loss: 0.4678	LR: 0.025000
Training Epoch: 42 [1024/50000]	Loss: 0.4221	LR: 0.025000
Training Epoch: 42 [1152/50000]	Loss: 0.4543	LR: 0.025000
Training Epoch: 42 [1280/50000]	Loss: 0.4031	LR: 0.025000
Training Epoch: 42 [1408/50000]	Loss: 0.5465	LR: 0.025000
Training Epoch: 42 [1536/50000]	Loss: 0.5136	LR: 0.025000
Training Epoch: 42 [1664/50000]	Loss: 0.4479	LR: 0.025000
Training Epoch: 42 [1792/50000]	Loss: 0.4806	LR: 0.025000
Training Epoch: 42 [1920/50000]	Loss: 0.5343	LR: 0.025000
Training Epoch: 42 [2048/50000]	Loss: 0.4147	LR: 0.025000
Training Epoch: 42 [2176/50000]	Loss: 0.4515	LR: 0.025000
Training Epoch: 42 [2304/50000]	Loss: 0.6292	LR: 0.025000
Training Epoch: 42 [2432/50000]	Loss: 0.4128	LR: 0.025000
Training Epoch: 42 [2560/50000]	Loss: 0.4269	LR: 0.025000
Training Epoch: 42 [2688/50000]	Loss: 0.4222	LR: 0.025000
Training Epoch: 42 [2816/50000]	Loss: 0.4981	LR: 0.025000
Training Epoch: 42 [2944/50000]	Loss: 0.5227	LR: 0.025000
Training Epoch: 42 [3072/50000]	Loss: 0.4133	LR: 0.025000
Training Epoch: 42 [3200/50000]	Loss: 0.3871	LR: 0.025000
Training Epoch: 42 [3328/50000]	Loss: 0.5575	LR: 0.025000
Training Epoch: 42 [3456/50000]	Loss: 0.6095	LR: 0.025000
Training Epoch: 42 [3584/50000]	Loss: 0.4702	LR: 0.025000
Training Epoch: 42 [3712/50000]	Loss: 0.4244	LR: 0.025000
Training Epoch: 42 [3840/50000]	Loss: 0.5155	LR: 0.025000
Training Epoch: 42 [3968/50000]	Loss: 0.4874	LR: 0.025000
Training Epoch: 42 [4096/50000]	Loss: 0.5513	LR: 0.025000
Training Epoch: 42 [4224/50000]	Loss: 0.4799	LR: 0.025000
Training Epoch: 42 [4352/50000]	Loss: 0.4821	LR: 0.025000
Training Epoch: 42 [4480/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 42 [4608/50000]	Loss: 0.4221	LR: 0.025000
Training Epoch: 42 [4736/50000]	Loss: 0.4946	LR: 0.025000
Training Epoch: 42 [4864/50000]	Loss: 0.6024	LR: 0.025000
Training Epoch: 42 [4992/50000]	Loss: 0.4123	LR: 0.025000
Training Epoch: 42 [5120/50000]	Loss: 0.4665	LR: 0.025000
Training Epoch: 42 [5248/50000]	Loss: 0.4170	LR: 0.025000
Training Epoch: 42 [5376/50000]	Loss: 0.6855	LR: 0.025000
Training Epoch: 42 [5504/50000]	Loss: 0.5503	LR: 0.025000
Training Epoch: 42 [5632/50000]	Loss: 0.5393	LR: 0.025000
Training Epoch: 42 [5760/50000]	Loss: 0.4232	LR: 0.025000
Training Epoch: 42 [5888/50000]	Loss: 0.4016	LR: 0.025000
Training Epoch: 42 [6016/50000]	Loss: 0.5963	LR: 0.025000
Training Epoch: 42 [6144/50000]	Loss: 0.4829	LR: 0.025000
Training Epoch: 42 [6272/50000]	Loss: 0.3840	LR: 0.025000
Training Epoch: 42 [6400/50000]	Loss: 0.5361	LR: 0.025000
Training Epoch: 42 [6528/50000]	Loss: 0.4394	LR: 0.025000
Training Epoch: 42 [6656/50000]	Loss: 0.5272	LR: 0.025000
Training Epoch: 42 [6784/50000]	Loss: 0.5135	LR: 0.025000
Training Epoch: 42 [6912/50000]	Loss: 0.5153	LR: 0.025000
Training Epoch: 42 [7040/50000]	Loss: 0.3502	LR: 0.025000
Training Epoch: 42 [7168/50000]	Loss: 0.3903	LR: 0.025000
Training Epoch: 42 [7296/50000]	Loss: 0.6041	LR: 0.025000
Training Epoch: 42 [7424/50000]	Loss: 0.5426	LR: 0.025000
Training Epoch: 42 [7552/50000]	Loss: 0.3554	LR: 0.025000
Training Epoch: 42 [7680/50000]	Loss: 0.5415	LR: 0.025000
Training Epoch: 42 [7808/50000]	Loss: 0.4844	LR: 0.025000
Training Epoch: 42 [7936/50000]	Loss: 0.4859	LR: 0.025000
Training Epoch: 42 [8064/50000]	Loss: 0.4908	LR: 0.025000
Training Epoch: 42 [8192/50000]	Loss: 0.5414	LR: 0.025000
Training Epoch: 42 [8320/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 42 [8448/50000]	Loss: 0.4506	LR: 0.025000
Training Epoch: 42 [8576/50000]	Loss: 0.4709	LR: 0.025000
Training Epoch: 42 [8704/50000]	Loss: 0.5356	LR: 0.025000
Training Epoch: 42 [8832/50000]	Loss: 0.5059	LR: 0.025000
Training Epoch: 42 [8960/50000]	Loss: 0.5394	LR: 0.025000
Training Epoch: 42 [9088/50000]	Loss: 0.4186	LR: 0.025000
Training Epoch: 42 [9216/50000]	Loss: 0.4769	LR: 0.025000
Training Epoch: 42 [9344/50000]	Loss: 0.5712	LR: 0.025000
Training Epoch: 42 [9472/50000]	Loss: 0.5188	LR: 0.025000
Training Epoch: 42 [9600/50000]	Loss: 0.5283	LR: 0.025000
Training Epoch: 42 [9728/50000]	Loss: 0.4935	LR: 0.025000
Training Epoch: 42 [9856/50000]	Loss: 0.5328	LR: 0.025000
Training Epoch: 42 [9984/50000]	Loss: 0.3791	LR: 0.025000
Training Epoch: 42 [10112/50000]	Loss: 0.4121	LR: 0.025000
Training Epoch: 42 [10240/50000]	Loss: 0.4334	LR: 0.025000
Training Epoch: 42 [10368/50000]	Loss: 0.4411	LR: 0.025000
Training Epoch: 42 [10496/50000]	Loss: 0.5340	LR: 0.025000
Training Epoch: 42 [10624/50000]	Loss: 0.6422	LR: 0.025000
Training Epoch: 42 [10752/50000]	Loss: 0.3415	LR: 0.025000
Training Epoch: 42 [10880/50000]	Loss: 0.4328	LR: 0.025000
Training Epoch: 42 [11008/50000]	Loss: 0.4607	LR: 0.025000
Training Epoch: 42 [11136/50000]	Loss: 0.4974	LR: 0.025000
Training Epoch: 42 [11264/50000]	Loss: 0.5724	LR: 0.025000
Training Epoch: 42 [11392/50000]	Loss: 0.3775	LR: 0.025000
Training Epoch: 42 [11520/50000]	Loss: 0.5873	LR: 0.025000
Training Epoch: 42 [11648/50000]	Loss: 0.6672	LR: 0.025000
Training Epoch: 42 [11776/50000]	Loss: 0.4491	LR: 0.025000
Training Epoch: 42 [11904/50000]	Loss: 0.5982	LR: 0.025000
Training Epoch: 42 [12032/50000]	Loss: 0.5836	LR: 0.025000
Training Epoch: 42 [12160/50000]	Loss: 0.4547	LR: 0.025000
Training Epoch: 42 [12288/50000]	Loss: 0.5643	LR: 0.025000
Training Epoch: 42 [12416/50000]	Loss: 0.5888	LR: 0.025000
Training Epoch: 42 [12544/50000]	Loss: 0.4923	LR: 0.025000
Training Epoch: 42 [12672/50000]	Loss: 0.5488	LR: 0.025000
Training Epoch: 42 [12800/50000]	Loss: 0.4635	LR: 0.025000
Training Epoch: 42 [12928/50000]	Loss: 0.5220	LR: 0.025000
Training Epoch: 42 [13056/50000]	Loss: 0.4469	LR: 0.025000
Training Epoch: 42 [13184/50000]	Loss: 0.4971	LR: 0.025000
Training Epoch: 42 [13312/50000]	Loss: 0.5274	LR: 0.025000
Training Epoch: 42 [13440/50000]	Loss: 0.4722	LR: 0.025000
Training Epoch: 42 [13568/50000]	Loss: 0.5003	LR: 0.025000
Training Epoch: 42 [13696/50000]	Loss: 0.5845	LR: 0.025000
Training Epoch: 42 [13824/50000]	Loss: 0.4958	LR: 0.025000
Training Epoch: 42 [13952/50000]	Loss: 0.4401	LR: 0.025000
Training Epoch: 42 [14080/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 42 [14208/50000]	Loss: 0.6847	LR: 0.025000
Training Epoch: 42 [14336/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 42 [14464/50000]	Loss: 0.6444	LR: 0.025000
Training Epoch: 42 [14592/50000]	Loss: 0.5763	LR: 0.025000
Training Epoch: 42 [14720/50000]	Loss: 0.4485	LR: 0.025000
Training Epoch: 42 [14848/50000]	Loss: 0.6037	LR: 0.025000
Training Epoch: 42 [14976/50000]	Loss: 0.5623	LR: 0.025000
Training Epoch: 42 [15104/50000]	Loss: 0.5780	LR: 0.025000
Training Epoch: 42 [15232/50000]	Loss: 0.5361	LR: 0.025000
Training Epoch: 42 [15360/50000]	Loss: 0.4768	LR: 0.025000
Training Epoch: 42 [15488/50000]	Loss: 0.5492	LR: 0.025000
Training Epoch: 42 [15616/50000]	Loss: 0.3589	LR: 0.025000
Training Epoch: 42 [15744/50000]	Loss: 0.4050	LR: 0.025000
Training Epoch: 42 [15872/50000]	Loss: 0.5020	LR: 0.025000
Training Epoch: 42 [16000/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 42 [16128/50000]	Loss: 0.5062	LR: 0.025000
Training Epoch: 42 [16256/50000]	Loss: 0.6412	LR: 0.025000
Training Epoch: 42 [16384/50000]	Loss: 0.5506	LR: 0.025000
Training Epoch: 42 [16512/50000]	Loss: 0.5704	LR: 0.025000
Training Epoch: 42 [16640/50000]	Loss: 0.4978	LR: 0.025000
Training Epoch: 42 [16768/50000]	Loss: 0.5441	LR: 0.025000
Training Epoch: 42 [16896/50000]	Loss: 0.6393	LR: 0.025000
Training Epoch: 42 [17024/50000]	Loss: 0.6097	LR: 0.025000
Training Epoch: 42 [17152/50000]	Loss: 0.5105	LR: 0.025000
Training Epoch: 42 [17280/50000]	Loss: 0.6572	LR: 0.025000
Training Epoch: 42 [17408/50000]	Loss: 0.5710	LR: 0.025000
Training Epoch: 42 [17536/50000]	Loss: 0.4865	LR: 0.025000
Training Epoch: 42 [17664/50000]	Loss: 0.5192	LR: 0.025000
Training Epoch: 42 [17792/50000]	Loss: 0.6344	LR: 0.025000
Training Epoch: 42 [17920/50000]	Loss: 0.5563	LR: 0.025000
Training Epoch: 42 [18048/50000]	Loss: 0.4780	LR: 0.025000
Training Epoch: 42 [18176/50000]	Loss: 0.4585	LR: 0.025000
Training Epoch: 42 [18304/50000]	Loss: 0.5276	LR: 0.025000
Training Epoch: 42 [18432/50000]	Loss: 0.5838	LR: 0.025000
Training Epoch: 42 [18560/50000]	Loss: 0.5241	LR: 0.025000
Training Epoch: 42 [18688/50000]	Loss: 0.5574	LR: 0.025000
Training Epoch: 42 [18816/50000]	Loss: 0.6227	LR: 0.025000
Training Epoch: 42 [18944/50000]	Loss: 0.6868	LR: 0.025000
Training Epoch: 42 [19072/50000]	Loss: 0.5167	LR: 0.025000
Training Epoch: 42 [19200/50000]	Loss: 0.5216	LR: 0.025000
Training Epoch: 42 [19328/50000]	Loss: 0.5789	LR: 0.025000
Training Epoch: 42 [19456/50000]	Loss: 0.5351	LR: 0.025000
Training Epoch: 42 [19584/50000]	Loss: 0.7299	LR: 0.025000
Training Epoch: 42 [19712/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 42 [19840/50000]	Loss: 0.5227	LR: 0.025000
Training Epoch: 42 [19968/50000]	Loss: 0.5453	LR: 0.025000
Training Epoch: 42 [20096/50000]	Loss: 0.5816	LR: 0.025000
Training Epoch: 42 [20224/50000]	Loss: 0.4614	LR: 0.025000
Training Epoch: 42 [20352/50000]	Loss: 0.6054	LR: 0.025000
Training Epoch: 42 [20480/50000]	Loss: 0.5757	LR: 0.025000
Training Epoch: 42 [20608/50000]	Loss: 0.5692	LR: 0.025000
Training Epoch: 42 [20736/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 42 [20864/50000]	Loss: 0.6509	LR: 0.025000
Training Epoch: 42 [20992/50000]	Loss: 0.3683	LR: 0.025000
Training Epoch: 42 [21120/50000]	Loss: 0.4874	LR: 0.025000
Training Epoch: 42 [21248/50000]	Loss: 0.5370	LR: 0.025000
Training Epoch: 42 [21376/50000]	Loss: 0.5676	LR: 0.025000
Training Epoch: 42 [21504/50000]	Loss: 0.5093	LR: 0.025000
Training Epoch: 42 [21632/50000]	Loss: 0.5738	LR: 0.025000
Training Epoch: 42 [21760/50000]	Loss: 0.5128	LR: 0.025000
Training Epoch: 42 [21888/50000]	Loss: 0.5985	LR: 0.025000
Training Epoch: 42 [22016/50000]	Loss: 0.5703	LR: 0.025000
Training Epoch: 42 [22144/50000]	Loss: 0.4687	LR: 0.025000
Training Epoch: 42 [22272/50000]	Loss: 0.5409	LR: 0.025000
Training Epoch: 42 [22400/50000]	Loss: 0.3666	LR: 0.025000
Training Epoch: 42 [22528/50000]	Loss: 0.4137	LR: 0.025000
Training Epoch: 42 [22656/50000]	Loss: 0.5801	LR: 0.025000
Training Epoch: 42 [22784/50000]	Loss: 0.5505	LR: 0.025000
Training Epoch: 42 [22912/50000]	Loss: 0.4432	LR: 0.025000
Training Epoch: 42 [23040/50000]	Loss: 0.4574	LR: 0.025000
Training Epoch: 42 [23168/50000]	Loss: 0.5480	LR: 0.025000
Training Epoch: 42 [23296/50000]	Loss: 0.5614	LR: 0.025000
Training Epoch: 42 [23424/50000]	Loss: 0.4726	LR: 0.025000
Training Epoch: 42 [23552/50000]	Loss: 0.5744	LR: 0.025000
Training Epoch: 42 [23680/50000]	Loss: 0.5498	LR: 0.025000
Training Epoch: 42 [23808/50000]	Loss: 0.4043	LR: 0.025000
Training Epoch: 42 [23936/50000]	Loss: 0.6092	LR: 0.025000
Training Epoch: 42 [24064/50000]	Loss: 0.5213	LR: 0.025000
Training Epoch: 42 [24192/50000]	Loss: 0.5204	LR: 0.025000
Training Epoch: 42 [24320/50000]	Loss: 0.4440	LR: 0.025000
Training Epoch: 42 [24448/50000]	Loss: 0.4483	LR: 0.025000
Training Epoch: 42 [24576/50000]	Loss: 0.5099	LR: 0.025000
Training Epoch: 42 [24704/50000]	Loss: 0.5210	LR: 0.025000
Training Epoch: 42 [24832/50000]	Loss: 0.7397	LR: 0.025000
Training Epoch: 42 [24960/50000]	Loss: 0.4933	LR: 0.025000
Training Epoch: 42 [25088/50000]	Loss: 0.5874	LR: 0.025000
Training Epoch: 42 [25216/50000]	Loss: 0.6476	LR: 0.025000
Training Epoch: 42 [25344/50000]	Loss: 0.6579	LR: 0.025000
Training Epoch: 42 [25472/50000]	Loss: 0.4421	LR: 0.025000
Training Epoch: 42 [25600/50000]	Loss: 0.4890	LR: 0.025000
Training Epoch: 42 [25728/50000]	Loss: 0.5024	LR: 0.025000
Training Epoch: 42 [25856/50000]	Loss: 0.5608	LR: 0.025000
Training Epoch: 42 [25984/50000]	Loss: 0.4676	LR: 0.025000
Training Epoch: 42 [26112/50000]	Loss: 0.4135	LR: 0.025000
Training Epoch: 42 [26240/50000]	Loss: 0.5156	LR: 0.025000
Training Epoch: 42 [26368/50000]	Loss: 0.6432	LR: 0.025000
Training Epoch: 42 [26496/50000]	Loss: 0.4608	LR: 0.025000
Training Epoch: 42 [26624/50000]	Loss: 0.6643	LR: 0.025000
Training Epoch: 42 [26752/50000]	Loss: 0.4606	LR: 0.025000
Training Epoch: 42 [26880/50000]	Loss: 0.5684	LR: 0.025000
Training Epoch: 42 [27008/50000]	Loss: 0.6368	LR: 0.025000
Training Epoch: 42 [27136/50000]	Loss: 0.4628	LR: 0.025000
Training Epoch: 42 [27264/50000]	Loss: 0.4602	LR: 0.025000
Training Epoch: 42 [27392/50000]	Loss: 0.5500	LR: 0.025000
Training Epoch: 42 [27520/50000]	Loss: 0.5926	LR: 0.025000
Training Epoch: 42 [27648/50000]	Loss: 0.4399	LR: 0.025000
Training Epoch: 42 [27776/50000]	Loss: 0.5012	LR: 0.025000
Training Epoch: 42 [27904/50000]	Loss: 0.4774	LR: 0.025000
Training Epoch: 42 [28032/50000]	Loss: 0.5172	LR: 0.025000
Training Epoch: 42 [28160/50000]	Loss: 0.5154	LR: 0.025000
Training Epoch: 42 [28288/50000]	Loss: 0.5399	LR: 0.025000
Training Epoch: 42 [28416/50000]	Loss: 0.4498	LR: 0.025000
Training Epoch: 42 [28544/50000]	Loss: 0.4503	LR: 0.025000
Training Epoch: 42 [28672/50000]	Loss: 0.7136	LR: 0.025000
Training Epoch: 42 [28800/50000]	Loss: 0.5319	LR: 0.025000
Training Epoch: 42 [28928/50000]	Loss: 0.4426	LR: 0.025000
Training Epoch: 42 [29056/50000]	Loss: 0.4133	LR: 0.025000
Training Epoch: 42 [29184/50000]	Loss: 0.5479	LR: 0.025000
Training Epoch: 42 [29312/50000]	Loss: 0.5343	LR: 0.025000
Training Epoch: 42 [29440/50000]	Loss: 0.4148	LR: 0.025000
Training Epoch: 42 [29568/50000]	Loss: 0.4939	LR: 0.025000
Training Epoch: 42 [29696/50000]	Loss: 0.6310	LR: 0.025000
Training Epoch: 42 [29824/50000]	Loss: 0.5699	LR: 0.025000
Training Epoch: 42 [29952/50000]	Loss: 0.6468	LR: 0.025000
Training Epoch: 42 [30080/50000]	Loss: 0.5733	LR: 0.025000
Training Epoch: 42 [30208/50000]	Loss: 0.5168	LR: 0.025000
Training Epoch: 42 [30336/50000]	Loss: 0.5760	LR: 0.025000
Training Epoch: 42 [30464/50000]	Loss: 0.4315	LR: 0.025000
Training Epoch: 42 [30592/50000]	Loss: 0.4665	LR: 0.025000
Training Epoch: 42 [30720/50000]	Loss: 0.5748	LR: 0.025000
Training Epoch: 42 [30848/50000]	Loss: 0.5274	LR: 0.025000
Training Epoch: 42 [30976/50000]	Loss: 0.5249	LR: 0.025000
Training Epoch: 42 [31104/50000]	Loss: 0.6919	LR: 0.025000
Training Epoch: 42 [31232/50000]	Loss: 0.5355	LR: 0.025000
Training Epoch: 42 [31360/50000]	Loss: 0.5642	LR: 0.025000
Training Epoch: 42 [31488/50000]	Loss: 0.7068	LR: 0.025000
Training Epoch: 42 [31616/50000]	Loss: 0.6475	LR: 0.025000
Training Epoch: 42 [31744/50000]	Loss: 0.6671	LR: 0.025000
Training Epoch: 42 [31872/50000]	Loss: 0.4305	LR: 0.025000
Training Epoch: 42 [32000/50000]	Loss: 0.6128	LR: 0.025000
Training Epoch: 42 [32128/50000]	Loss: 0.5233	LR: 0.025000
Training Epoch: 42 [32256/50000]	Loss: 0.5098	LR: 0.025000
Training Epoch: 42 [32384/50000]	Loss: 0.4759	LR: 0.025000
Training Epoch: 42 [32512/50000]	Loss: 0.6848	LR: 0.025000
Training Epoch: 42 [32640/50000]	Loss: 0.5717	LR: 0.025000
Training Epoch: 42 [32768/50000]	Loss: 0.7859	LR: 0.025000
Training Epoch: 42 [32896/50000]	Loss: 0.4777	LR: 0.025000
Training Epoch: 42 [33024/50000]	Loss: 0.4042	LR: 0.025000
Training Epoch: 42 [33152/50000]	Loss: 0.7491	LR: 0.025000
Training Epoch: 42 [33280/50000]	Loss: 0.5984	LR: 0.025000
Training Epoch: 42 [33408/50000]	Loss: 0.4759	LR: 0.025000
Training Epoch: 42 [33536/50000]	Loss: 0.5435	LR: 0.025000
Training Epoch: 42 [33664/50000]	Loss: 0.4742	LR: 0.025000
Training Epoch: 42 [33792/50000]	Loss: 0.6350	LR: 0.025000
Training Epoch: 42 [33920/50000]	Loss: 0.4622	LR: 0.025000
Training Epoch: 42 [34048/50000]	Loss: 0.3889	LR: 0.025000
Training Epoch: 42 [34176/50000]	Loss: 0.4829	LR: 0.025000
Training Epoch: 42 [34304/50000]	Loss: 0.5171	LR: 0.025000
Training Epoch: 42 [34432/50000]	Loss: 0.6607	LR: 0.025000
Training Epoch: 42 [34560/50000]	Loss: 0.4703	LR: 0.025000
Training Epoch: 42 [34688/50000]	Loss: 0.5995	LR: 0.025000
Training Epoch: 42 [34816/50000]	Loss: 0.7357	LR: 0.025000
Training Epoch: 42 [34944/50000]	Loss: 0.6825	LR: 0.025000
Training Epoch: 42 [35072/50000]	Loss: 0.7496	LR: 0.025000
Training Epoch: 42 [35200/50000]	Loss: 0.5581	LR: 0.025000
Training Epoch: 42 [35328/50000]	Loss: 0.6175	LR: 0.025000
Training Epoch: 42 [35456/50000]	Loss: 0.5514	LR: 0.025000
Training Epoch: 42 [35584/50000]	Loss: 0.5600	LR: 0.025000
Training Epoch: 42 [35712/50000]	Loss: 0.5833	LR: 0.025000
Training Epoch: 42 [35840/50000]	Loss: 0.6174	LR: 0.025000
Training Epoch: 42 [35968/50000]	Loss: 0.6251	LR: 0.025000
Training Epoch: 42 [36096/50000]	Loss: 0.6588	LR: 0.025000
Training Epoch: 42 [36224/50000]	Loss: 0.5549	LR: 0.025000
Training Epoch: 42 [36352/50000]	Loss: 0.6562	LR: 0.025000
Training Epoch: 42 [36480/50000]	Loss: 0.7225	LR: 0.025000
Training Epoch: 42 [36608/50000]	Loss: 0.5768	LR: 0.025000
Training Epoch: 42 [36736/50000]	Loss: 0.7681	LR: 0.025000
Training Epoch: 42 [36864/50000]	Loss: 0.6244	LR: 0.025000
Training Epoch: 42 [36992/50000]	Loss: 0.4773	LR: 0.025000
Training Epoch: 42 [37120/50000]	Loss: 0.5535	LR: 0.025000
Training Epoch: 42 [37248/50000]	Loss: 0.5967	LR: 0.025000
Training Epoch: 42 [37376/50000]	Loss: 0.6593	LR: 0.025000
Training Epoch: 42 [37504/50000]	Loss: 0.5446	LR: 0.025000
Training Epoch: 42 [37632/50000]	Loss: 0.7599	LR: 0.025000
Training Epoch: 42 [37760/50000]	Loss: 0.5173	LR: 0.025000
Training Epoch: 42 [37888/50000]	Loss: 0.7672	LR: 0.025000
Training Epoch: 42 [38016/50000]	Loss: 0.5835	LR: 0.025000
Training Epoch: 42 [38144/50000]	Loss: 0.5816	LR: 0.025000
Training Epoch: 42 [38272/50000]	Loss: 0.5126	LR: 0.025000
Training Epoch: 42 [38400/50000]	Loss: 0.4762	LR: 0.025000
Training Epoch: 42 [38528/50000]	Loss: 0.7254	LR: 0.025000
Training Epoch: 42 [38656/50000]	Loss: 0.5670	LR: 0.025000
Training Epoch: 42 [38784/50000]	Loss: 0.6524	LR: 0.025000
Training Epoch: 42 [38912/50000]	Loss: 0.4600	LR: 0.025000
Training Epoch: 42 [39040/50000]	Loss: 0.6540	LR: 0.025000
Training Epoch: 42 [39168/50000]	Loss: 0.5045	LR: 0.025000
Training Epoch: 42 [39296/50000]	Loss: 0.5865	LR: 0.025000
Training Epoch: 42 [39424/50000]	Loss: 0.5688	LR: 0.025000
Training Epoch: 42 [39552/50000]	Loss: 0.6372	LR: 0.025000
Training Epoch: 42 [39680/50000]	Loss: 0.5308	LR: 0.025000
Training Epoch: 42 [39808/50000]	Loss: 0.6328	LR: 0.025000
Training Epoch: 42 [39936/50000]	Loss: 0.4796	LR: 0.025000
Training Epoch: 42 [40064/50000]	Loss: 0.5540	LR: 0.025000
Training Epoch: 42 [40192/50000]	Loss: 0.4654	LR: 0.025000
Training Epoch: 42 [40320/50000]	Loss: 0.6727	LR: 0.025000
Training Epoch: 42 [40448/50000]	Loss: 0.6724	LR: 0.025000
Training Epoch: 42 [40576/50000]	Loss: 0.7097	LR: 0.025000
Training Epoch: 42 [40704/50000]	Loss: 0.5101	LR: 0.025000
Training Epoch: 42 [40832/50000]	Loss: 0.4748	LR: 0.025000
Training Epoch: 42 [40960/50000]	Loss: 0.7319	LR: 0.025000
Training Epoch: 42 [41088/50000]	Loss: 0.6944	LR: 0.025000
Training Epoch: 42 [41216/50000]	Loss: 0.5578	LR: 0.025000
Training Epoch: 42 [41344/50000]	Loss: 0.6632	LR: 0.025000
Training Epoch: 42 [41472/50000]	Loss: 0.5779	LR: 0.025000
Training Epoch: 42 [41600/50000]	Loss: 0.5263	LR: 0.025000
Training Epoch: 42 [41728/50000]	Loss: 0.5454	LR: 0.025000
Training Epoch: 42 [41856/50000]	Loss: 0.7087	LR: 0.025000
Training Epoch: 42 [41984/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 42 [42112/50000]	Loss: 0.7434	LR: 0.025000
Training Epoch: 42 [42240/50000]	Loss: 0.6063	LR: 0.025000
Training Epoch: 42 [42368/50000]	Loss: 0.6814	LR: 0.025000
Training Epoch: 42 [42496/50000]	Loss: 0.4433	LR: 0.025000
Training Epoch: 42 [42624/50000]	Loss: 0.6757	LR: 0.025000
Training Epoch: 42 [42752/50000]	Loss: 0.7117	LR: 0.025000
Training Epoch: 42 [42880/50000]	Loss: 0.6976	LR: 0.025000
Training Epoch: 42 [43008/50000]	Loss: 0.6481	LR: 0.025000
Training Epoch: 42 [43136/50000]	Loss: 0.5134	LR: 0.025000
Training Epoch: 42 [43264/50000]	Loss: 0.4848	LR: 0.025000
Training Epoch: 42 [43392/50000]	Loss: 0.6085	LR: 0.025000
Training Epoch: 42 [43520/50000]	Loss: 0.5596	LR: 0.025000
Training Epoch: 42 [43648/50000]	Loss: 0.6599	LR: 0.025000
Training Epoch: 42 [43776/50000]	Loss: 0.7300	LR: 0.025000
Training Epoch: 42 [43904/50000]	Loss: 0.6189	LR: 0.025000
Training Epoch: 42 [44032/50000]	Loss: 0.7501	LR: 0.025000
Training Epoch: 42 [44160/50000]	Loss: 0.7082	LR: 0.025000
Training Epoch: 42 [44288/50000]	Loss: 0.5388	LR: 0.025000
Training Epoch: 42 [44416/50000]	Loss: 0.5536	LR: 0.025000
Training Epoch: 42 [44544/50000]	Loss: 0.7256	LR: 0.025000
Training Epoch: 42 [44672/50000]	Loss: 0.5579	LR: 0.025000
Training Epoch: 42 [44800/50000]	Loss: 0.6575	LR: 0.025000
Training Epoch: 42 [44928/50000]	Loss: 0.5690	LR: 0.025000
Training Epoch: 42 [45056/50000]	Loss: 0.7277	LR: 0.025000
Training Epoch: 42 [45184/50000]	Loss: 0.6124	LR: 0.025000
Training Epoch: 42 [45312/50000]	Loss: 0.6684	LR: 0.025000
Training Epoch: 42 [45440/50000]	Loss: 0.8672	LR: 0.025000
Training Epoch: 42 [45568/50000]	Loss: 0.6697	LR: 0.025000
Training Epoch: 42 [45696/50000]	Loss: 0.6960	LR: 0.025000
Training Epoch: 42 [45824/50000]	Loss: 0.5389	LR: 0.025000
Training Epoch: 42 [45952/50000]	Loss: 0.6693	LR: 0.025000
Training Epoch: 42 [46080/50000]	Loss: 0.6760	LR: 0.025000
Training Epoch: 42 [46208/50000]	Loss: 0.5278	LR: 0.025000
Training Epoch: 42 [46336/50000]	Loss: 0.6819	LR: 0.025000
Training Epoch: 42 [46464/50000]	Loss: 0.6308	LR: 0.025000
Training Epoch: 42 [46592/50000]	Loss: 0.6059	LR: 0.025000
Training Epoch: 42 [46720/50000]	Loss: 0.4038	LR: 0.025000
Training Epoch: 42 [46848/50000]	Loss: 0.7546	LR: 0.025000
Training Epoch: 42 [46976/50000]	Loss: 0.7417	LR: 0.025000
Training Epoch: 42 [47104/50000]	Loss: 0.5009	LR: 0.025000
Training Epoch: 42 [47232/50000]	Loss: 0.7130	LR: 0.025000
Training Epoch: 42 [47360/50000]	Loss: 0.6964	LR: 0.025000
Training Epoch: 42 [47488/50000]	Loss: 0.5274	LR: 0.025000
Training Epoch: 42 [47616/50000]	Loss: 0.6833	LR: 0.025000
Training Epoch: 42 [47744/50000]	Loss: 0.5787	LR: 0.025000
Training Epoch: 42 [47872/50000]	Loss: 0.6550	LR: 0.025000
Training Epoch: 42 [48000/50000]	Loss: 0.5982	LR: 0.025000
Training Epoch: 42 [48128/50000]	Loss: 0.5725	LR: 0.025000
Training Epoch: 42 [48256/50000]	Loss: 0.5963	LR: 0.025000
Training Epoch: 42 [48384/50000]	Loss: 0.5199	LR: 0.025000
Training Epoch: 42 [48512/50000]	Loss: 0.5751	LR: 0.025000
Training Epoch: 42 [48640/50000]	Loss: 0.5479	LR: 0.025000
Training Epoch: 42 [48768/50000]	Loss: 0.6833	LR: 0.025000
Training Epoch: 42 [48896/50000]	Loss: 0.6029	LR: 0.025000
Training Epoch: 42 [49024/50000]	Loss: 0.5843	LR: 0.025000
Training Epoch: 42 [49152/50000]	Loss: 0.6002	LR: 0.025000
Training Epoch: 42 [49280/50000]	Loss: 0.4696	LR: 0.025000
Training Epoch: 42 [49408/50000]	Loss: 0.5626	LR: 0.025000
Training Epoch: 42 [49536/50000]	Loss: 0.6066	LR: 0.025000
Training Epoch: 42 [49664/50000]	Loss: 0.6975	LR: 0.025000
Training Epoch: 42 [49792/50000]	Loss: 0.4350	LR: 0.025000
Training Epoch: 42 [49920/50000]	Loss: 0.6764	LR: 0.025000
Training Epoch: 42 [50000/50000]	Loss: 0.5575	LR: 0.025000
epoch 42 training time consumed: 53.84s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  154142 GB |  154142 GB |
|       from large pool |  123392 KB |    1034 MB |  153990 GB |  153990 GB |
|       from small pool |   10798 KB |      13 MB |     151 GB |     151 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  154142 GB |  154142 GB |
|       from large pool |  123392 KB |    1034 MB |  153990 GB |  153990 GB |
|       from small pool |   10798 KB |      13 MB |     151 GB |     151 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   67833 GB |   67833 GB |
|       from large pool |  155136 KB |  433088 KB |   67665 GB |   67665 GB |
|       from small pool |    1490 KB |    3494 KB |     167 GB |     167 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    5948 K  |    5947 K  |
|       from large pool |      24    |      65    |    3104 K  |    3104 K  |
|       from small pool |     231    |     274    |    2843 K  |    2843 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    5948 K  |    5947 K  |
|       from large pool |      24    |      65    |    3104 K  |    3104 K  |
|       from small pool |     231    |     274    |    2843 K  |    2843 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    2940 K  |    2940 K  |
|       from large pool |       9    |      14    |    1502 K  |    1502 K  |
|       from small pool |      12    |      16    |    1437 K  |    1437 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 42, Average loss: 0.0097, Accuracy: 0.6791, Time consumed:3.44s

Training Epoch: 43 [128/50000]	Loss: 0.6287	LR: 0.025000
Training Epoch: 43 [256/50000]	Loss: 0.4617	LR: 0.025000
Training Epoch: 43 [384/50000]	Loss: 0.5399	LR: 0.025000
Training Epoch: 43 [512/50000]	Loss: 0.4200	LR: 0.025000
Training Epoch: 43 [640/50000]	Loss: 0.4029	LR: 0.025000
Training Epoch: 43 [768/50000]	Loss: 0.6728	LR: 0.025000
Training Epoch: 43 [896/50000]	Loss: 0.5291	LR: 0.025000
Training Epoch: 43 [1024/50000]	Loss: 0.5900	LR: 0.025000
Training Epoch: 43 [1152/50000]	Loss: 0.5903	LR: 0.025000
Training Epoch: 43 [1280/50000]	Loss: 0.4684	LR: 0.025000
Training Epoch: 43 [1408/50000]	Loss: 0.4550	LR: 0.025000
Training Epoch: 43 [1536/50000]	Loss: 0.4678	LR: 0.025000
Training Epoch: 43 [1664/50000]	Loss: 0.3858	LR: 0.025000
Training Epoch: 43 [1792/50000]	Loss: 0.4550	LR: 0.025000
Training Epoch: 43 [1920/50000]	Loss: 0.5264	LR: 0.025000
Training Epoch: 43 [2048/50000]	Loss: 0.5318	LR: 0.025000
Training Epoch: 43 [2176/50000]	Loss: 0.4488	LR: 0.025000
Training Epoch: 43 [2304/50000]	Loss: 0.3761	LR: 0.025000
Training Epoch: 43 [2432/50000]	Loss: 0.5419	LR: 0.025000
Training Epoch: 43 [2560/50000]	Loss: 0.5192	LR: 0.025000
Training Epoch: 43 [2688/50000]	Loss: 0.3805	LR: 0.025000
Training Epoch: 43 [2816/50000]	Loss: 0.5006	LR: 0.025000
Training Epoch: 43 [2944/50000]	Loss: 0.5044	LR: 0.025000
Training Epoch: 43 [3072/50000]	Loss: 0.5618	LR: 0.025000
Training Epoch: 43 [3200/50000]	Loss: 0.5509	LR: 0.025000
Training Epoch: 43 [3328/50000]	Loss: 0.4977	LR: 0.025000
Training Epoch: 43 [3456/50000]	Loss: 0.5035	LR: 0.025000
Training Epoch: 43 [3584/50000]	Loss: 0.5962	LR: 0.025000
Training Epoch: 43 [3712/50000]	Loss: 0.4837	LR: 0.025000
Training Epoch: 43 [3840/50000]	Loss: 0.3761	LR: 0.025000
Training Epoch: 43 [3968/50000]	Loss: 0.4695	LR: 0.025000
Training Epoch: 43 [4096/50000]	Loss: 0.4139	LR: 0.025000
Training Epoch: 43 [4224/50000]	Loss: 0.4761	LR: 0.025000
Training Epoch: 43 [4352/50000]	Loss: 0.4913	LR: 0.025000
Training Epoch: 43 [4480/50000]	Loss: 0.5710	LR: 0.025000
Training Epoch: 43 [4608/50000]	Loss: 0.4676	LR: 0.025000
Training Epoch: 43 [4736/50000]	Loss: 0.5053	LR: 0.025000
Training Epoch: 43 [4864/50000]	Loss: 0.4500	LR: 0.025000
Training Epoch: 43 [4992/50000]	Loss: 0.6061	LR: 0.025000
Training Epoch: 43 [5120/50000]	Loss: 0.4154	LR: 0.025000
Training Epoch: 43 [5248/50000]	Loss: 0.4310	LR: 0.025000
Training Epoch: 43 [5376/50000]	Loss: 0.5200	LR: 0.025000
Training Epoch: 43 [5504/50000]	Loss: 0.4515	LR: 0.025000
Training Epoch: 43 [5632/50000]	Loss: 0.5948	LR: 0.025000
Training Epoch: 43 [5760/50000]	Loss: 0.4185	LR: 0.025000
Training Epoch: 43 [5888/50000]	Loss: 0.5819	LR: 0.025000
Training Epoch: 43 [6016/50000]	Loss: 0.3885	LR: 0.025000
Training Epoch: 43 [6144/50000]	Loss: 0.4546	LR: 0.025000
Training Epoch: 43 [6272/50000]	Loss: 0.5650	LR: 0.025000
Training Epoch: 43 [6400/50000]	Loss: 0.3614	LR: 0.025000
Training Epoch: 43 [6528/50000]	Loss: 0.4728	LR: 0.025000
Training Epoch: 43 [6656/50000]	Loss: 0.4777	LR: 0.025000
Training Epoch: 43 [6784/50000]	Loss: 0.3896	LR: 0.025000
Training Epoch: 43 [6912/50000]	Loss: 0.5981	LR: 0.025000
Training Epoch: 43 [7040/50000]	Loss: 0.4655	LR: 0.025000
Training Epoch: 43 [7168/50000]	Loss: 0.5144	LR: 0.025000
Training Epoch: 43 [7296/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 43 [7424/50000]	Loss: 0.4953	LR: 0.025000
Training Epoch: 43 [7552/50000]	Loss: 0.4679	LR: 0.025000
Training Epoch: 43 [7680/50000]	Loss: 0.5565	LR: 0.025000
Training Epoch: 43 [7808/50000]	Loss: 0.5425	LR: 0.025000
Training Epoch: 43 [7936/50000]	Loss: 0.4025	LR: 0.025000
Training Epoch: 43 [8064/50000]	Loss: 0.3886	LR: 0.025000
Training Epoch: 43 [8192/50000]	Loss: 0.6206	LR: 0.025000
Training Epoch: 43 [8320/50000]	Loss: 0.4645	LR: 0.025000
Training Epoch: 43 [8448/50000]	Loss: 0.3452	LR: 0.025000
Training Epoch: 43 [8576/50000]	Loss: 0.5528	LR: 0.025000
Training Epoch: 43 [8704/50000]	Loss: 0.4386	LR: 0.025000
Training Epoch: 43 [8832/50000]	Loss: 0.4225	LR: 0.025000
Training Epoch: 43 [8960/50000]	Loss: 0.6046	LR: 0.025000
Training Epoch: 43 [9088/50000]	Loss: 0.5050	LR: 0.025000
Training Epoch: 43 [9216/50000]	Loss: 0.4164	LR: 0.025000
Training Epoch: 43 [9344/50000]	Loss: 0.5336	LR: 0.025000
Training Epoch: 43 [9472/50000]	Loss: 0.5049	LR: 0.025000
Training Epoch: 43 [9600/50000]	Loss: 0.3826	LR: 0.025000
Training Epoch: 43 [9728/50000]	Loss: 0.5060	LR: 0.025000
Training Epoch: 43 [9856/50000]	Loss: 0.5353	LR: 0.025000
Training Epoch: 43 [9984/50000]	Loss: 0.3697	LR: 0.025000
Training Epoch: 43 [10112/50000]	Loss: 0.5008	LR: 0.025000
Training Epoch: 43 [10240/50000]	Loss: 0.4498	LR: 0.025000
Training Epoch: 43 [10368/50000]	Loss: 0.5654	LR: 0.025000
Training Epoch: 43 [10496/50000]	Loss: 0.5546	LR: 0.025000
Training Epoch: 43 [10624/50000]	Loss: 0.4443	LR: 0.025000
Training Epoch: 43 [10752/50000]	Loss: 0.5546	LR: 0.025000
Training Epoch: 43 [10880/50000]	Loss: 0.5601	LR: 0.025000
Training Epoch: 43 [11008/50000]	Loss: 0.4937	LR: 0.025000
Training Epoch: 43 [11136/50000]	Loss: 0.6366	LR: 0.025000
Training Epoch: 43 [11264/50000]	Loss: 0.5248	LR: 0.025000
Training Epoch: 43 [11392/50000]	Loss: 0.5472	LR: 0.025000
Training Epoch: 43 [11520/50000]	Loss: 0.6053	LR: 0.025000
Training Epoch: 43 [11648/50000]	Loss: 0.4053	LR: 0.025000
Training Epoch: 43 [11776/50000]	Loss: 0.4382	LR: 0.025000
Training Epoch: 43 [11904/50000]	Loss: 0.5677	LR: 0.025000
Training Epoch: 43 [12032/50000]	Loss: 0.4497	LR: 0.025000
Training Epoch: 43 [12160/50000]	Loss: 0.4875	LR: 0.025000
Training Epoch: 43 [12288/50000]	Loss: 0.3456	LR: 0.025000
Training Epoch: 43 [12416/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 43 [12544/50000]	Loss: 0.4928	LR: 0.025000
Training Epoch: 43 [12672/50000]	Loss: 0.5732	LR: 0.025000
Training Epoch: 43 [12800/50000]	Loss: 0.4724	LR: 0.025000
Training Epoch: 43 [12928/50000]	Loss: 0.4448	LR: 0.025000
Training Epoch: 43 [13056/50000]	Loss: 0.3855	LR: 0.025000
Training Epoch: 43 [13184/50000]	Loss: 0.5288	LR: 0.025000
Training Epoch: 43 [13312/50000]	Loss: 0.4675	LR: 0.025000
Training Epoch: 43 [13440/50000]	Loss: 0.5119	LR: 0.025000
Training Epoch: 43 [13568/50000]	Loss: 0.4625	LR: 0.025000
Training Epoch: 43 [13696/50000]	Loss: 0.5268	LR: 0.025000
Training Epoch: 43 [13824/50000]	Loss: 0.4405	LR: 0.025000
Training Epoch: 43 [13952/50000]	Loss: 0.5993	LR: 0.025000
Training Epoch: 43 [14080/50000]	Loss: 0.4928	LR: 0.025000
Training Epoch: 43 [14208/50000]	Loss: 0.6039	LR: 0.025000
Training Epoch: 43 [14336/50000]	Loss: 0.4768	LR: 0.025000
Training Epoch: 43 [14464/50000]	Loss: 0.4482	LR: 0.025000
Training Epoch: 43 [14592/50000]	Loss: 0.5471	LR: 0.025000
Training Epoch: 43 [14720/50000]	Loss: 0.4682	LR: 0.025000
Training Epoch: 43 [14848/50000]	Loss: 0.5688	LR: 0.025000
Training Epoch: 43 [14976/50000]	Loss: 0.6683	LR: 0.025000
Training Epoch: 43 [15104/50000]	Loss: 0.5379	LR: 0.025000
Training Epoch: 43 [15232/50000]	Loss: 0.5686	LR: 0.025000
Training Epoch: 43 [15360/50000]	Loss: 0.4130	LR: 0.025000
Training Epoch: 43 [15488/50000]	Loss: 0.3989	LR: 0.025000
Training Epoch: 43 [15616/50000]	Loss: 0.4949	LR: 0.025000
Training Epoch: 43 [15744/50000]	Loss: 0.5914	LR: 0.025000
Training Epoch: 43 [15872/50000]	Loss: 0.4419	LR: 0.025000
Training Epoch: 43 [16000/50000]	Loss: 0.3368	LR: 0.025000
Training Epoch: 43 [16128/50000]	Loss: 0.5333	LR: 0.025000
Training Epoch: 43 [16256/50000]	Loss: 0.5990	LR: 0.025000
Training Epoch: 43 [16384/50000]	Loss: 0.6819	LR: 0.025000
Training Epoch: 43 [16512/50000]	Loss: 0.4558	LR: 0.025000
Training Epoch: 43 [16640/50000]	Loss: 0.6402	LR: 0.025000
Training Epoch: 43 [16768/50000]	Loss: 0.6008	LR: 0.025000
Training Epoch: 43 [16896/50000]	Loss: 0.5247	LR: 0.025000
Training Epoch: 43 [17024/50000]	Loss: 0.4997	LR: 0.025000
Training Epoch: 43 [17152/50000]	Loss: 0.4652	LR: 0.025000
Training Epoch: 43 [17280/50000]	Loss: 0.5162	LR: 0.025000
Training Epoch: 43 [17408/50000]	Loss: 0.3851	LR: 0.025000
Training Epoch: 43 [17536/50000]	Loss: 0.4555	LR: 0.025000
Training Epoch: 43 [17664/50000]	Loss: 0.5632	LR: 0.025000
Training Epoch: 43 [17792/50000]	Loss: 0.4058	LR: 0.025000
Training Epoch: 43 [17920/50000]	Loss: 0.6186	LR: 0.025000
Training Epoch: 43 [18048/50000]	Loss: 0.6371	LR: 0.025000
Training Epoch: 43 [18176/50000]	Loss: 0.6052	LR: 0.025000
Training Epoch: 43 [18304/50000]	Loss: 0.5378	LR: 0.025000
Training Epoch: 43 [18432/50000]	Loss: 0.3364	LR: 0.025000
Training Epoch: 43 [18560/50000]	Loss: 0.4430	LR: 0.025000
Training Epoch: 43 [18688/50000]	Loss: 0.6391	LR: 0.025000
Training Epoch: 43 [18816/50000]	Loss: 0.5271	LR: 0.025000
Training Epoch: 43 [18944/50000]	Loss: 0.4252	LR: 0.025000
Training Epoch: 43 [19072/50000]	Loss: 0.4196	LR: 0.025000
Training Epoch: 43 [19200/50000]	Loss: 0.4633	LR: 0.025000
Training Epoch: 43 [19328/50000]	Loss: 0.4094	LR: 0.025000
Training Epoch: 43 [19456/50000]	Loss: 0.4954	LR: 0.025000
Training Epoch: 43 [19584/50000]	Loss: 0.5428	LR: 0.025000
Training Epoch: 43 [19712/50000]	Loss: 0.4654	LR: 0.025000
Training Epoch: 43 [19840/50000]	Loss: 0.4041	LR: 0.025000
Training Epoch: 43 [19968/50000]	Loss: 0.5501	LR: 0.025000
Training Epoch: 43 [20096/50000]	Loss: 0.5217	LR: 0.025000
Training Epoch: 43 [20224/50000]	Loss: 0.5839	LR: 0.025000
Training Epoch: 43 [20352/50000]	Loss: 0.6807	LR: 0.025000
Training Epoch: 43 [20480/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 43 [20608/50000]	Loss: 0.5266	LR: 0.025000
Training Epoch: 43 [20736/50000]	Loss: 0.5354	LR: 0.025000
Training Epoch: 43 [20864/50000]	Loss: 0.6806	LR: 0.025000
Training Epoch: 43 [20992/50000]	Loss: 0.4294	LR: 0.025000
Training Epoch: 43 [21120/50000]	Loss: 0.3748	LR: 0.025000
Training Epoch: 43 [21248/50000]	Loss: 0.4775	LR: 0.025000
Training Epoch: 43 [21376/50000]	Loss: 0.5266	LR: 0.025000
Training Epoch: 43 [21504/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 43 [21632/50000]	Loss: 0.5196	LR: 0.025000
Training Epoch: 43 [21760/50000]	Loss: 0.5009	LR: 0.025000
Training Epoch: 43 [21888/50000]	Loss: 0.6025	LR: 0.025000
Training Epoch: 43 [22016/50000]	Loss: 0.5393	LR: 0.025000
Training Epoch: 43 [22144/50000]	Loss: 0.7538	LR: 0.025000
Training Epoch: 43 [22272/50000]	Loss: 0.4518	LR: 0.025000
Training Epoch: 43 [22400/50000]	Loss: 0.4472	LR: 0.025000
Training Epoch: 43 [22528/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 43 [22656/50000]	Loss: 0.4755	LR: 0.025000
Training Epoch: 43 [22784/50000]	Loss: 0.3957	LR: 0.025000
Training Epoch: 43 [22912/50000]	Loss: 0.5327	LR: 0.025000
Training Epoch: 43 [23040/50000]	Loss: 0.6959	LR: 0.025000
Training Epoch: 43 [23168/50000]	Loss: 0.7129	LR: 0.025000
Training Epoch: 43 [23296/50000]	Loss: 0.6016	LR: 0.025000
Training Epoch: 43 [23424/50000]	Loss: 0.5200	LR: 0.025000
Training Epoch: 43 [23552/50000]	Loss: 0.4603	LR: 0.025000
Training Epoch: 43 [23680/50000]	Loss: 0.5582	LR: 0.025000
Training Epoch: 43 [23808/50000]	Loss: 0.5064	LR: 0.025000
Training Epoch: 43 [23936/50000]	Loss: 0.4604	LR: 0.025000
Training Epoch: 43 [24064/50000]	Loss: 0.5517	LR: 0.025000
Training Epoch: 43 [24192/50000]	Loss: 0.6467	LR: 0.025000
Training Epoch: 43 [24320/50000]	Loss: 0.6436	LR: 0.025000
Training Epoch: 43 [24448/50000]	Loss: 0.6696	LR: 0.025000
Training Epoch: 43 [24576/50000]	Loss: 0.5675	LR: 0.025000
Training Epoch: 43 [24704/50000]	Loss: 0.5101	LR: 0.025000
Training Epoch: 43 [24832/50000]	Loss: 0.5104	LR: 0.025000
Training Epoch: 43 [24960/50000]	Loss: 0.5540	LR: 0.025000
Training Epoch: 43 [25088/50000]	Loss: 0.5244	LR: 0.025000
Training Epoch: 43 [25216/50000]	Loss: 0.4580	LR: 0.025000
Training Epoch: 43 [25344/50000]	Loss: 0.4836	LR: 0.025000
Training Epoch: 43 [25472/50000]	Loss: 0.5353	LR: 0.025000
Training Epoch: 43 [25600/50000]	Loss: 0.5867	LR: 0.025000
Training Epoch: 43 [25728/50000]	Loss: 0.6679	LR: 0.025000
Training Epoch: 43 [25856/50000]	Loss: 0.5882	LR: 0.025000
Training Epoch: 43 [25984/50000]	Loss: 0.7148	LR: 0.025000
Training Epoch: 43 [26112/50000]	Loss: 0.5379	LR: 0.025000
Training Epoch: 43 [26240/50000]	Loss: 0.5512	LR: 0.025000
Training Epoch: 43 [26368/50000]	Loss: 0.4881	LR: 0.025000
Training Epoch: 43 [26496/50000]	Loss: 0.7537	LR: 0.025000
Training Epoch: 43 [26624/50000]	Loss: 0.5427	LR: 0.025000
Training Epoch: 43 [26752/50000]	Loss: 0.4015	LR: 0.025000
Training Epoch: 43 [26880/50000]	Loss: 0.5939	LR: 0.025000
Training Epoch: 43 [27008/50000]	Loss: 0.6849	LR: 0.025000
Training Epoch: 43 [27136/50000]	Loss: 0.7674	LR: 0.025000
Training Epoch: 43 [27264/50000]	Loss: 0.4793	LR: 0.025000
Training Epoch: 43 [27392/50000]	Loss: 0.6136	LR: 0.025000
Training Epoch: 43 [27520/50000]	Loss: 0.5007	LR: 0.025000
Training Epoch: 43 [27648/50000]	Loss: 0.4723	LR: 0.025000
Training Epoch: 43 [27776/50000]	Loss: 0.4469	LR: 0.025000
Training Epoch: 43 [27904/50000]	Loss: 0.4775	LR: 0.025000
Training Epoch: 43 [28032/50000]	Loss: 0.5097	LR: 0.025000
Training Epoch: 43 [28160/50000]	Loss: 0.5074	LR: 0.025000
Training Epoch: 43 [28288/50000]	Loss: 0.6598	LR: 0.025000
Training Epoch: 43 [28416/50000]	Loss: 0.5107	LR: 0.025000
Training Epoch: 43 [28544/50000]	Loss: 0.4649	LR: 0.025000
Training Epoch: 43 [28672/50000]	Loss: 0.5878	LR: 0.025000
Training Epoch: 43 [28800/50000]	Loss: 0.4846	LR: 0.025000
Training Epoch: 43 [28928/50000]	Loss: 0.5106	LR: 0.025000
Training Epoch: 43 [29056/50000]	Loss: 0.5280	LR: 0.025000
Training Epoch: 43 [29184/50000]	Loss: 0.5749	LR: 0.025000
Training Epoch: 43 [29312/50000]	Loss: 0.5521	LR: 0.025000
Training Epoch: 43 [29440/50000]	Loss: 0.5024	LR: 0.025000
Training Epoch: 43 [29568/50000]	Loss: 0.4686	LR: 0.025000
Training Epoch: 43 [29696/50000]	Loss: 0.5640	LR: 0.025000
Training Epoch: 43 [29824/50000]	Loss: 0.4331	LR: 0.025000
Training Epoch: 43 [29952/50000]	Loss: 0.5083	LR: 0.025000
Training Epoch: 43 [30080/50000]	Loss: 0.5459	LR: 0.025000
Training Epoch: 43 [30208/50000]	Loss: 0.5948	LR: 0.025000
Training Epoch: 43 [30336/50000]	Loss: 0.6240	LR: 0.025000
Training Epoch: 43 [30464/50000]	Loss: 0.6206	LR: 0.025000
Training Epoch: 43 [30592/50000]	Loss: 0.6701	LR: 0.025000
Training Epoch: 43 [30720/50000]	Loss: 0.5176	LR: 0.025000
Training Epoch: 43 [30848/50000]	Loss: 0.4768	LR: 0.025000
Training Epoch: 43 [30976/50000]	Loss: 0.4336	LR: 0.025000
Training Epoch: 43 [31104/50000]	Loss: 0.7333	LR: 0.025000
Training Epoch: 43 [31232/50000]	Loss: 0.5003	LR: 0.025000
Training Epoch: 43 [31360/50000]	Loss: 0.4770	LR: 0.025000
Training Epoch: 43 [31488/50000]	Loss: 0.6036	LR: 0.025000
Training Epoch: 43 [31616/50000]	Loss: 0.6381	LR: 0.025000
Training Epoch: 43 [31744/50000]	Loss: 0.4447	LR: 0.025000
Training Epoch: 43 [31872/50000]	Loss: 0.5827	LR: 0.025000
Training Epoch: 43 [32000/50000]	Loss: 0.3539	LR: 0.025000
Training Epoch: 43 [32128/50000]	Loss: 0.6701	LR: 0.025000
Training Epoch: 43 [32256/50000]	Loss: 0.5604	LR: 0.025000
Training Epoch: 43 [32384/50000]	Loss: 0.6233	LR: 0.025000
Training Epoch: 43 [32512/50000]	Loss: 0.7021	LR: 0.025000
Training Epoch: 43 [32640/50000]	Loss: 0.5547	LR: 0.025000
Training Epoch: 43 [32768/50000]	Loss: 0.6659	LR: 0.025000
Training Epoch: 43 [32896/50000]	Loss: 0.4496	LR: 0.025000
Training Epoch: 43 [33024/50000]	Loss: 0.6341	LR: 0.025000
Training Epoch: 43 [33152/50000]	Loss: 0.5577	LR: 0.025000
Training Epoch: 43 [33280/50000]	Loss: 0.4965	LR: 0.025000
Training Epoch: 43 [33408/50000]	Loss: 0.5542	LR: 0.025000
Training Epoch: 43 [33536/50000]	Loss: 0.6292	LR: 0.025000
Training Epoch: 43 [33664/50000]	Loss: 0.6766	LR: 0.025000
Training Epoch: 43 [33792/50000]	Loss: 0.5387	LR: 0.025000
Training Epoch: 43 [33920/50000]	Loss: 0.5039	LR: 0.025000
Training Epoch: 43 [34048/50000]	Loss: 0.5244	LR: 0.025000
Training Epoch: 43 [34176/50000]	Loss: 0.5571	LR: 0.025000
Training Epoch: 43 [34304/50000]	Loss: 0.7722	LR: 0.025000
Training Epoch: 43 [34432/50000]	Loss: 0.5505	LR: 0.025000
Training Epoch: 43 [34560/50000]	Loss: 0.5626	LR: 0.025000
Training Epoch: 43 [34688/50000]	Loss: 0.5384	LR: 0.025000
Training Epoch: 43 [34816/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 43 [34944/50000]	Loss: 0.5150	LR: 0.025000
Training Epoch: 43 [35072/50000]	Loss: 0.5566	LR: 0.025000
Training Epoch: 43 [35200/50000]	Loss: 0.6523	LR: 0.025000
Training Epoch: 43 [35328/50000]	Loss: 0.6355	LR: 0.025000
Training Epoch: 43 [35456/50000]	Loss: 0.6223	LR: 0.025000
Training Epoch: 43 [35584/50000]	Loss: 0.5846	LR: 0.025000
Training Epoch: 43 [35712/50000]	Loss: 0.4375	LR: 0.025000
Training Epoch: 43 [35840/50000]	Loss: 0.5865	LR: 0.025000
Training Epoch: 43 [35968/50000]	Loss: 0.4817	LR: 0.025000
Training Epoch: 43 [36096/50000]	Loss: 0.4716	LR: 0.025000
Training Epoch: 43 [36224/50000]	Loss: 0.5706	LR: 0.025000
Training Epoch: 43 [36352/50000]	Loss: 0.7152	LR: 0.025000
Training Epoch: 43 [36480/50000]	Loss: 0.5800	LR: 0.025000
Training Epoch: 43 [36608/50000]	Loss: 0.5246	LR: 0.025000
Training Epoch: 43 [36736/50000]	Loss: 0.6637	LR: 0.025000
Training Epoch: 43 [36864/50000]	Loss: 0.5142	LR: 0.025000
Training Epoch: 43 [36992/50000]	Loss: 0.5702	LR: 0.025000
Training Epoch: 43 [37120/50000]	Loss: 0.5597	LR: 0.025000
Training Epoch: 43 [37248/50000]	Loss: 0.4844	LR: 0.025000
Training Epoch: 43 [37376/50000]	Loss: 0.4809	LR: 0.025000
Training Epoch: 43 [37504/50000]	Loss: 0.5429	LR: 0.025000
Training Epoch: 43 [37632/50000]	Loss: 0.6259	LR: 0.025000
Training Epoch: 43 [37760/50000]	Loss: 0.6013	LR: 0.025000
Training Epoch: 43 [37888/50000]	Loss: 0.6125	LR: 0.025000
Training Epoch: 43 [38016/50000]	Loss: 0.5392	LR: 0.025000
Training Epoch: 43 [38144/50000]	Loss: 0.4640	LR: 0.025000
Training Epoch: 43 [38272/50000]	Loss: 0.4013	LR: 0.025000
Training Epoch: 43 [38400/50000]	Loss: 0.5330	LR: 0.025000
Training Epoch: 43 [38528/50000]	Loss: 0.6911	LR: 0.025000
Training Epoch: 43 [38656/50000]	Loss: 0.4230	LR: 0.025000
Training Epoch: 43 [38784/50000]	Loss: 0.6854	LR: 0.025000
Training Epoch: 43 [38912/50000]	Loss: 0.6854	LR: 0.025000
Training Epoch: 43 [39040/50000]	Loss: 0.7137	LR: 0.025000
Training Epoch: 43 [39168/50000]	Loss: 0.7485	LR: 0.025000
Training Epoch: 43 [39296/50000]	Loss: 0.6649	LR: 0.025000
Training Epoch: 43 [39424/50000]	Loss: 0.5961	LR: 0.025000
Training Epoch: 43 [39552/50000]	Loss: 0.6544	LR: 0.025000
Training Epoch: 43 [39680/50000]	Loss: 0.5560	LR: 0.025000
Training Epoch: 43 [39808/50000]	Loss: 0.5266	LR: 0.025000
Training Epoch: 43 [39936/50000]	Loss: 0.5169	LR: 0.025000
Training Epoch: 43 [40064/50000]	Loss: 0.4763	LR: 0.025000
Training Epoch: 43 [40192/50000]	Loss: 0.7095	LR: 0.025000
Training Epoch: 43 [40320/50000]	Loss: 0.8587	LR: 0.025000
Training Epoch: 43 [40448/50000]	Loss: 0.5604	LR: 0.025000
Training Epoch: 43 [40576/50000]	Loss: 0.7470	LR: 0.025000
Training Epoch: 43 [40704/50000]	Loss: 0.5535	LR: 0.025000
Training Epoch: 43 [40832/50000]	Loss: 0.5233	LR: 0.025000
Training Epoch: 43 [40960/50000]	Loss: 0.5618	LR: 0.025000
Training Epoch: 43 [41088/50000]	Loss: 0.5415	LR: 0.025000
Training Epoch: 43 [41216/50000]	Loss: 0.7792	LR: 0.025000
Training Epoch: 43 [41344/50000]	Loss: 0.6778	LR: 0.025000
Training Epoch: 43 [41472/50000]	Loss: 0.4828	LR: 0.025000
Training Epoch: 43 [41600/50000]	Loss: 0.5464	LR: 0.025000
Training Epoch: 43 [41728/50000]	Loss: 0.5298	LR: 0.025000
Training Epoch: 43 [41856/50000]	Loss: 0.5098	LR: 0.025000
Training Epoch: 43 [41984/50000]	Loss: 0.5143	LR: 0.025000
Training Epoch: 43 [42112/50000]	Loss: 0.5697	LR: 0.025000
Training Epoch: 43 [42240/50000]	Loss: 0.5405	LR: 0.025000
Training Epoch: 43 [42368/50000]	Loss: 0.6610	LR: 0.025000
Training Epoch: 43 [42496/50000]	Loss: 0.6959	LR: 0.025000
Training Epoch: 43 [42624/50000]	Loss: 0.7633	LR: 0.025000
Training Epoch: 43 [42752/50000]	Loss: 0.4792	LR: 0.025000
Training Epoch: 43 [42880/50000]	Loss: 0.7103	LR: 0.025000
Training Epoch: 43 [43008/50000]	Loss: 0.7812	LR: 0.025000
Training Epoch: 43 [43136/50000]	Loss: 0.6733	LR: 0.025000
Training Epoch: 43 [43264/50000]	Loss: 0.5935	LR: 0.025000
Training Epoch: 43 [43392/50000]	Loss: 0.5842	LR: 0.025000
Training Epoch: 43 [43520/50000]	Loss: 0.5640	LR: 0.025000
Training Epoch: 43 [43648/50000]	Loss: 0.5608	LR: 0.025000
Training Epoch: 43 [43776/50000]	Loss: 0.6494	LR: 0.025000
Training Epoch: 43 [43904/50000]	Loss: 0.6073	LR: 0.025000
Training Epoch: 43 [44032/50000]	Loss: 0.6423	LR: 0.025000
Training Epoch: 43 [44160/50000]	Loss: 0.7451	LR: 0.025000
Training Epoch: 43 [44288/50000]	Loss: 0.6005	LR: 0.025000
Training Epoch: 43 [44416/50000]	Loss: 0.6403	LR: 0.025000
Training Epoch: 43 [44544/50000]	Loss: 0.7098	LR: 0.025000
Training Epoch: 43 [44672/50000]	Loss: 0.6203	LR: 0.025000
Training Epoch: 43 [44800/50000]	Loss: 0.5126	LR: 0.025000
Training Epoch: 43 [44928/50000]	Loss: 0.6172	LR: 0.025000
Training Epoch: 43 [45056/50000]	Loss: 0.7401	LR: 0.025000
Training Epoch: 43 [45184/50000]	Loss: 0.4667	LR: 0.025000
Training Epoch: 43 [45312/50000]	Loss: 0.7165	LR: 0.025000
Training Epoch: 43 [45440/50000]	Loss: 0.5313	LR: 0.025000
Training Epoch: 43 [45568/50000]	Loss: 0.5337	LR: 0.025000
Training Epoch: 43 [45696/50000]	Loss: 0.4625	LR: 0.025000
Training Epoch: 43 [45824/50000]	Loss: 0.5999	LR: 0.025000
Training Epoch: 43 [45952/50000]	Loss: 0.6077	LR: 0.025000
Training Epoch: 43 [46080/50000]	Loss: 0.4818	LR: 0.025000
Training Epoch: 43 [46208/50000]	Loss: 0.6932	LR: 0.025000
Training Epoch: 43 [46336/50000]	Loss: 0.6045	LR: 0.025000
Training Epoch: 43 [46464/50000]	Loss: 0.8071	LR: 0.025000
Training Epoch: 43 [46592/50000]	Loss: 0.6306	LR: 0.025000
Training Epoch: 43 [46720/50000]	Loss: 0.6854	LR: 0.025000
Training Epoch: 43 [46848/50000]	Loss: 0.5689	LR: 0.025000
Training Epoch: 43 [46976/50000]	Loss: 0.5601	LR: 0.025000
Training Epoch: 43 [47104/50000]	Loss: 0.5869	LR: 0.025000
Training Epoch: 43 [47232/50000]	Loss: 0.5219	LR: 0.025000
Training Epoch: 43 [47360/50000]	Loss: 0.5338	LR: 0.025000
Training Epoch: 43 [47488/50000]	Loss: 0.5854	LR: 0.025000
Training Epoch: 43 [47616/50000]	Loss: 0.5942	LR: 0.025000
Training Epoch: 43 [47744/50000]	Loss: 0.6425	LR: 0.025000
Training Epoch: 43 [47872/50000]	Loss: 0.7387	LR: 0.025000
Training Epoch: 43 [48000/50000]	Loss: 0.4652	LR: 0.025000
Training Epoch: 43 [48128/50000]	Loss: 0.5089	LR: 0.025000
Training Epoch: 43 [48256/50000]	Loss: 0.6413	LR: 0.025000
Training Epoch: 43 [48384/50000]	Loss: 0.5385	LR: 0.025000
Training Epoch: 43 [48512/50000]	Loss: 0.5221	LR: 0.025000
Training Epoch: 43 [48640/50000]	Loss: 0.6516	LR: 0.025000
Training Epoch: 43 [48768/50000]	Loss: 0.7198	LR: 0.025000
Training Epoch: 43 [48896/50000]	Loss: 0.5503	LR: 0.025000
Training Epoch: 43 [49024/50000]	Loss: 0.9559	LR: 0.025000
Training Epoch: 43 [49152/50000]	Loss: 0.5481	LR: 0.025000
Training Epoch: 43 [49280/50000]	Loss: 0.5483	LR: 0.025000
Training Epoch: 43 [49408/50000]	Loss: 0.5561	LR: 0.025000
Training Epoch: 43 [49536/50000]	Loss: 0.6262	LR: 0.025000
Training Epoch: 43 [49664/50000]	Loss: 0.5519	LR: 0.025000
Training Epoch: 43 [49792/50000]	Loss: 0.6574	LR: 0.025000
Training Epoch: 43 [49920/50000]	Loss: 0.6707	LR: 0.025000
Training Epoch: 43 [50000/50000]	Loss: 0.6908	LR: 0.025000
epoch 43 training time consumed: 53.98s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  157812 GB |  157812 GB |
|       from large pool |  123392 KB |    1034 MB |  157657 GB |  157657 GB |
|       from small pool |   10798 KB |      13 MB |     155 GB |     155 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  157812 GB |  157812 GB |
|       from large pool |  123392 KB |    1034 MB |  157657 GB |  157657 GB |
|       from small pool |   10798 KB |      13 MB |     155 GB |     155 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   69448 GB |   69448 GB |
|       from large pool |  155136 KB |  433088 KB |   69276 GB |   69276 GB |
|       from small pool |    1490 KB |    3494 KB |     171 GB |     171 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    6089 K  |    6089 K  |
|       from large pool |      24    |      65    |    3178 K  |    3178 K  |
|       from small pool |     231    |     274    |    2911 K  |    2910 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    6089 K  |    6089 K  |
|       from large pool |      24    |      65    |    3178 K  |    3178 K  |
|       from small pool |     231    |     274    |    2911 K  |    2910 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3010 K  |    3010 K  |
|       from large pool |       9    |      14    |    1538 K  |    1538 K  |
|       from small pool |      12    |      16    |    1472 K  |    1472 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 43, Average loss: 0.0098, Accuracy: 0.6775, Time consumed:3.49s

Training Epoch: 44 [128/50000]	Loss: 0.4264	LR: 0.025000
Training Epoch: 44 [256/50000]	Loss: 0.3250	LR: 0.025000
Training Epoch: 44 [384/50000]	Loss: 0.5357	LR: 0.025000
Training Epoch: 44 [512/50000]	Loss: 0.4202	LR: 0.025000
Training Epoch: 44 [640/50000]	Loss: 0.6361	LR: 0.025000
Training Epoch: 44 [768/50000]	Loss: 0.5762	LR: 0.025000
Training Epoch: 44 [896/50000]	Loss: 0.5038	LR: 0.025000
Training Epoch: 44 [1024/50000]	Loss: 0.5646	LR: 0.025000
Training Epoch: 44 [1152/50000]	Loss: 0.4945	LR: 0.025000
Training Epoch: 44 [1280/50000]	Loss: 0.4373	LR: 0.025000
Training Epoch: 44 [1408/50000]	Loss: 0.4932	LR: 0.025000
Training Epoch: 44 [1536/50000]	Loss: 0.4667	LR: 0.025000
Training Epoch: 44 [1664/50000]	Loss: 0.5490	LR: 0.025000
Training Epoch: 44 [1792/50000]	Loss: 0.4486	LR: 0.025000
Training Epoch: 44 [1920/50000]	Loss: 0.5904	LR: 0.025000
Training Epoch: 44 [2048/50000]	Loss: 0.6143	LR: 0.025000
Training Epoch: 44 [2176/50000]	Loss: 0.5315	LR: 0.025000
Training Epoch: 44 [2304/50000]	Loss: 0.4774	LR: 0.025000
Training Epoch: 44 [2432/50000]	Loss: 0.4865	LR: 0.025000
Training Epoch: 44 [2560/50000]	Loss: 0.4853	LR: 0.025000
Training Epoch: 44 [2688/50000]	Loss: 0.3831	LR: 0.025000
Training Epoch: 44 [2816/50000]	Loss: 0.4594	LR: 0.025000
Training Epoch: 44 [2944/50000]	Loss: 0.5115	LR: 0.025000
Training Epoch: 44 [3072/50000]	Loss: 0.4727	LR: 0.025000
Training Epoch: 44 [3200/50000]	Loss: 0.3225	LR: 0.025000
Training Epoch: 44 [3328/50000]	Loss: 0.7386	LR: 0.025000
Training Epoch: 44 [3456/50000]	Loss: 0.5969	LR: 0.025000
Training Epoch: 44 [3584/50000]	Loss: 0.3882	LR: 0.025000
Training Epoch: 44 [3712/50000]	Loss: 0.4711	LR: 0.025000
Training Epoch: 44 [3840/50000]	Loss: 0.6188	LR: 0.025000
Training Epoch: 44 [3968/50000]	Loss: 0.5312	LR: 0.025000
Training Epoch: 44 [4096/50000]	Loss: 0.5352	LR: 0.025000
Training Epoch: 44 [4224/50000]	Loss: 0.5413	LR: 0.025000
Training Epoch: 44 [4352/50000]	Loss: 0.3736	LR: 0.025000
Training Epoch: 44 [4480/50000]	Loss: 0.5237	LR: 0.025000
Training Epoch: 44 [4608/50000]	Loss: 0.4839	LR: 0.025000
Training Epoch: 44 [4736/50000]	Loss: 0.3492	LR: 0.025000
Training Epoch: 44 [4864/50000]	Loss: 0.4433	LR: 0.025000
Training Epoch: 44 [4992/50000]	Loss: 0.4428	LR: 0.025000
Training Epoch: 44 [5120/50000]	Loss: 0.5152	LR: 0.025000
Training Epoch: 44 [5248/50000]	Loss: 0.5745	LR: 0.025000
Training Epoch: 44 [5376/50000]	Loss: 0.5287	LR: 0.025000
Training Epoch: 44 [5504/50000]	Loss: 0.5944	LR: 0.025000
Training Epoch: 44 [5632/50000]	Loss: 0.5609	LR: 0.025000
Training Epoch: 44 [5760/50000]	Loss: 0.5254	LR: 0.025000
Training Epoch: 44 [5888/50000]	Loss: 0.6445	LR: 0.025000
Training Epoch: 44 [6016/50000]	Loss: 0.4613	LR: 0.025000
Training Epoch: 44 [6144/50000]	Loss: 0.4267	LR: 0.025000
Training Epoch: 44 [6272/50000]	Loss: 0.3578	LR: 0.025000
Training Epoch: 44 [6400/50000]	Loss: 0.3338	LR: 0.025000
Training Epoch: 44 [6528/50000]	Loss: 0.6465	LR: 0.025000
Training Epoch: 44 [6656/50000]	Loss: 0.5558	LR: 0.025000
Training Epoch: 44 [6784/50000]	Loss: 0.5775	LR: 0.025000
Training Epoch: 44 [6912/50000]	Loss: 0.4445	LR: 0.025000
Training Epoch: 44 [7040/50000]	Loss: 0.4576	LR: 0.025000
Training Epoch: 44 [7168/50000]	Loss: 0.5003	LR: 0.025000
Training Epoch: 44 [7296/50000]	Loss: 0.4592	LR: 0.025000
Training Epoch: 44 [7424/50000]	Loss: 0.4093	LR: 0.025000
Training Epoch: 44 [7552/50000]	Loss: 0.4703	LR: 0.025000
Training Epoch: 44 [7680/50000]	Loss: 0.3133	LR: 0.025000
Training Epoch: 44 [7808/50000]	Loss: 0.5407	LR: 0.025000
Training Epoch: 44 [7936/50000]	Loss: 0.5462	LR: 0.025000
Training Epoch: 44 [8064/50000]	Loss: 0.5805	LR: 0.025000
Training Epoch: 44 [8192/50000]	Loss: 0.4764	LR: 0.025000
Training Epoch: 44 [8320/50000]	Loss: 0.5206	LR: 0.025000
Training Epoch: 44 [8448/50000]	Loss: 0.4034	LR: 0.025000
Training Epoch: 44 [8576/50000]	Loss: 0.5205	LR: 0.025000
Training Epoch: 44 [8704/50000]	Loss: 0.5440	LR: 0.025000
Training Epoch: 44 [8832/50000]	Loss: 0.5395	LR: 0.025000
Training Epoch: 44 [8960/50000]	Loss: 0.4698	LR: 0.025000
Training Epoch: 44 [9088/50000]	Loss: 0.4865	LR: 0.025000
Training Epoch: 44 [9216/50000]	Loss: 0.4244	LR: 0.025000
Training Epoch: 44 [9344/50000]	Loss: 0.5179	LR: 0.025000
Training Epoch: 44 [9472/50000]	Loss: 0.5261	LR: 0.025000
Training Epoch: 44 [9600/50000]	Loss: 0.6131	LR: 0.025000
Training Epoch: 44 [9728/50000]	Loss: 0.5101	LR: 0.025000
Training Epoch: 44 [9856/50000]	Loss: 0.5579	LR: 0.025000
Training Epoch: 44 [9984/50000]	Loss: 0.4089	LR: 0.025000
Training Epoch: 44 [10112/50000]	Loss: 0.6139	LR: 0.025000
Training Epoch: 44 [10240/50000]	Loss: 0.4550	LR: 0.025000
Training Epoch: 44 [10368/50000]	Loss: 0.4681	LR: 0.025000
Training Epoch: 44 [10496/50000]	Loss: 0.4524	LR: 0.025000
Training Epoch: 44 [10624/50000]	Loss: 0.5730	LR: 0.025000
Training Epoch: 44 [10752/50000]	Loss: 0.4471	LR: 0.025000
Training Epoch: 44 [10880/50000]	Loss: 0.5258	LR: 0.025000
Training Epoch: 44 [11008/50000]	Loss: 0.4268	LR: 0.025000
Training Epoch: 44 [11136/50000]	Loss: 0.5842	LR: 0.025000
Training Epoch: 44 [11264/50000]	Loss: 0.5571	LR: 0.025000
Training Epoch: 44 [11392/50000]	Loss: 0.4908	LR: 0.025000
Training Epoch: 44 [11520/50000]	Loss: 0.4374	LR: 0.025000
Training Epoch: 44 [11648/50000]	Loss: 0.4734	LR: 0.025000
Training Epoch: 44 [11776/50000]	Loss: 0.6531	LR: 0.025000
Training Epoch: 44 [11904/50000]	Loss: 0.4642	LR: 0.025000
Training Epoch: 44 [12032/50000]	Loss: 0.4789	LR: 0.025000
Training Epoch: 44 [12160/50000]	Loss: 0.6178	LR: 0.025000
Training Epoch: 44 [12288/50000]	Loss: 0.4725	LR: 0.025000
Training Epoch: 44 [12416/50000]	Loss: 0.6008	LR: 0.025000
Training Epoch: 44 [12544/50000]	Loss: 0.3740	LR: 0.025000
Training Epoch: 44 [12672/50000]	Loss: 0.6157	LR: 0.025000
Training Epoch: 44 [12800/50000]	Loss: 0.4204	LR: 0.025000
Training Epoch: 44 [12928/50000]	Loss: 0.6087	LR: 0.025000
Training Epoch: 44 [13056/50000]	Loss: 0.5793	LR: 0.025000
Training Epoch: 44 [13184/50000]	Loss: 0.5071	LR: 0.025000
Training Epoch: 44 [13312/50000]	Loss: 0.4833	LR: 0.025000
Training Epoch: 44 [13440/50000]	Loss: 0.4400	LR: 0.025000
Training Epoch: 44 [13568/50000]	Loss: 0.4902	LR: 0.025000
Training Epoch: 44 [13696/50000]	Loss: 0.4988	LR: 0.025000
Training Epoch: 44 [13824/50000]	Loss: 0.5019	LR: 0.025000
Training Epoch: 44 [13952/50000]	Loss: 0.4197	LR: 0.025000
Training Epoch: 44 [14080/50000]	Loss: 0.5259	LR: 0.025000
Training Epoch: 44 [14208/50000]	Loss: 0.5038	LR: 0.025000
Training Epoch: 44 [14336/50000]	Loss: 0.6357	LR: 0.025000
Training Epoch: 44 [14464/50000]	Loss: 0.4974	LR: 0.025000
Training Epoch: 44 [14592/50000]	Loss: 0.4923	LR: 0.025000
Training Epoch: 44 [14720/50000]	Loss: 0.5609	LR: 0.025000
Training Epoch: 44 [14848/50000]	Loss: 0.6371	LR: 0.025000
Training Epoch: 44 [14976/50000]	Loss: 0.3865	LR: 0.025000
Training Epoch: 44 [15104/50000]	Loss: 0.5538	LR: 0.025000
Training Epoch: 44 [15232/50000]	Loss: 0.5697	LR: 0.025000
Training Epoch: 44 [15360/50000]	Loss: 0.5293	LR: 0.025000
Training Epoch: 44 [15488/50000]	Loss: 0.5935	LR: 0.025000
Training Epoch: 44 [15616/50000]	Loss: 0.6043	LR: 0.025000
Training Epoch: 44 [15744/50000]	Loss: 0.5088	LR: 0.025000
Training Epoch: 44 [15872/50000]	Loss: 0.4220	LR: 0.025000
Training Epoch: 44 [16000/50000]	Loss: 0.5513	LR: 0.025000
Training Epoch: 44 [16128/50000]	Loss: 0.4636	LR: 0.025000
Training Epoch: 44 [16256/50000]	Loss: 0.3545	LR: 0.025000
Training Epoch: 44 [16384/50000]	Loss: 0.6337	LR: 0.025000
Training Epoch: 44 [16512/50000]	Loss: 0.4292	LR: 0.025000
Training Epoch: 44 [16640/50000]	Loss: 0.4213	LR: 0.025000
Training Epoch: 44 [16768/50000]	Loss: 0.4584	LR: 0.025000
Training Epoch: 44 [16896/50000]	Loss: 0.5898	LR: 0.025000
Training Epoch: 44 [17024/50000]	Loss: 0.4618	LR: 0.025000
Training Epoch: 44 [17152/50000]	Loss: 0.4935	LR: 0.025000
Training Epoch: 44 [17280/50000]	Loss: 0.4600	LR: 0.025000
Training Epoch: 44 [17408/50000]	Loss: 0.5645	LR: 0.025000
Training Epoch: 44 [17536/50000]	Loss: 0.4479	LR: 0.025000
Training Epoch: 44 [17664/50000]	Loss: 0.5785	LR: 0.025000
Training Epoch: 44 [17792/50000]	Loss: 0.4551	LR: 0.025000
Training Epoch: 44 [17920/50000]	Loss: 0.4389	LR: 0.025000
Training Epoch: 44 [18048/50000]	Loss: 0.6128	LR: 0.025000
Training Epoch: 44 [18176/50000]	Loss: 0.4771	LR: 0.025000
Training Epoch: 44 [18304/50000]	Loss: 0.3838	LR: 0.025000
Training Epoch: 44 [18432/50000]	Loss: 0.6190	LR: 0.025000
Training Epoch: 44 [18560/50000]	Loss: 0.3871	LR: 0.025000
Training Epoch: 44 [18688/50000]	Loss: 0.4573	LR: 0.025000
Training Epoch: 44 [18816/50000]	Loss: 0.4347	LR: 0.025000
Training Epoch: 44 [18944/50000]	Loss: 0.6852	LR: 0.025000
Training Epoch: 44 [19072/50000]	Loss: 0.4575	LR: 0.025000
Training Epoch: 44 [19200/50000]	Loss: 0.5704	LR: 0.025000
Training Epoch: 44 [19328/50000]	Loss: 0.5354	LR: 0.025000
Training Epoch: 44 [19456/50000]	Loss: 0.4523	LR: 0.025000
Training Epoch: 44 [19584/50000]	Loss: 0.5899	LR: 0.025000
Training Epoch: 44 [19712/50000]	Loss: 0.5638	LR: 0.025000
Training Epoch: 44 [19840/50000]	Loss: 0.5686	LR: 0.025000
Training Epoch: 44 [19968/50000]	Loss: 0.6342	LR: 0.025000
Training Epoch: 44 [20096/50000]	Loss: 0.4668	LR: 0.025000
Training Epoch: 44 [20224/50000]	Loss: 0.5727	LR: 0.025000
Training Epoch: 44 [20352/50000]	Loss: 0.3772	LR: 0.025000
Training Epoch: 44 [20480/50000]	Loss: 0.4426	LR: 0.025000
Training Epoch: 44 [20608/50000]	Loss: 0.6447	LR: 0.025000
Training Epoch: 44 [20736/50000]	Loss: 0.7239	LR: 0.025000
Training Epoch: 44 [20864/50000]	Loss: 0.3071	LR: 0.025000
Training Epoch: 44 [20992/50000]	Loss: 0.5304	LR: 0.025000
Training Epoch: 44 [21120/50000]	Loss: 0.5977	LR: 0.025000
Training Epoch: 44 [21248/50000]	Loss: 0.6759	LR: 0.025000
Training Epoch: 44 [21376/50000]	Loss: 0.6424	LR: 0.025000
Training Epoch: 44 [21504/50000]	Loss: 0.4752	LR: 0.025000
Training Epoch: 44 [21632/50000]	Loss: 0.5329	LR: 0.025000
Training Epoch: 44 [21760/50000]	Loss: 0.5126	LR: 0.025000
Training Epoch: 44 [21888/50000]	Loss: 0.4500	LR: 0.025000
Training Epoch: 44 [22016/50000]	Loss: 0.4856	LR: 0.025000
Training Epoch: 44 [22144/50000]	Loss: 0.4544	LR: 0.025000
Training Epoch: 44 [22272/50000]	Loss: 0.5273	LR: 0.025000
Training Epoch: 44 [22400/50000]	Loss: 0.4118	LR: 0.025000
Training Epoch: 44 [22528/50000]	Loss: 0.5133	LR: 0.025000
Training Epoch: 44 [22656/50000]	Loss: 0.6164	LR: 0.025000
Training Epoch: 44 [22784/50000]	Loss: 0.5021	LR: 0.025000
Training Epoch: 44 [22912/50000]	Loss: 0.5555	LR: 0.025000
Training Epoch: 44 [23040/50000]	Loss: 0.3818	LR: 0.025000
Training Epoch: 44 [23168/50000]	Loss: 0.4735	LR: 0.025000
Training Epoch: 44 [23296/50000]	Loss: 0.6089	LR: 0.025000
Training Epoch: 44 [23424/50000]	Loss: 0.5226	LR: 0.025000
Training Epoch: 44 [23552/50000]	Loss: 0.5924	LR: 0.025000
Training Epoch: 44 [23680/50000]	Loss: 0.4390	LR: 0.025000
Training Epoch: 44 [23808/50000]	Loss: 0.4753	LR: 0.025000
Training Epoch: 44 [23936/50000]	Loss: 0.5481	LR: 0.025000
Training Epoch: 44 [24064/50000]	Loss: 0.3910	LR: 0.025000
Training Epoch: 44 [24192/50000]	Loss: 0.6244	LR: 0.025000
Training Epoch: 44 [24320/50000]	Loss: 0.4902	LR: 0.025000
Training Epoch: 44 [24448/50000]	Loss: 0.5923	LR: 0.025000
Training Epoch: 44 [24576/50000]	Loss: 0.5490	LR: 0.025000
Training Epoch: 44 [24704/50000]	Loss: 0.6141	LR: 0.025000
Training Epoch: 44 [24832/50000]	Loss: 0.5587	LR: 0.025000
Training Epoch: 44 [24960/50000]	Loss: 0.5122	LR: 0.025000
Training Epoch: 44 [25088/50000]	Loss: 0.5880	LR: 0.025000
Training Epoch: 44 [25216/50000]	Loss: 0.4421	LR: 0.025000
Training Epoch: 44 [25344/50000]	Loss: 0.5785	LR: 0.025000
Training Epoch: 44 [25472/50000]	Loss: 0.6181	LR: 0.025000
Training Epoch: 44 [25600/50000]	Loss: 0.4908	LR: 0.025000
Training Epoch: 44 [25728/50000]	Loss: 0.5146	LR: 0.025000
Training Epoch: 44 [25856/50000]	Loss: 0.5763	LR: 0.025000
Training Epoch: 44 [25984/50000]	Loss: 0.4718	LR: 0.025000
Training Epoch: 44 [26112/50000]	Loss: 0.4808	LR: 0.025000
Training Epoch: 44 [26240/50000]	Loss: 0.4573	LR: 0.025000
Training Epoch: 44 [26368/50000]	Loss: 0.5714	LR: 0.025000
Training Epoch: 44 [26496/50000]	Loss: 0.5745	LR: 0.025000
Training Epoch: 44 [26624/50000]	Loss: 0.5315	LR: 0.025000
Training Epoch: 44 [26752/50000]	Loss: 0.6065	LR: 0.025000
Training Epoch: 44 [26880/50000]	Loss: 0.6573	LR: 0.025000
Training Epoch: 44 [27008/50000]	Loss: 0.5741	LR: 0.025000
Training Epoch: 44 [27136/50000]	Loss: 0.5287	LR: 0.025000
Training Epoch: 44 [27264/50000]	Loss: 0.5218	LR: 0.025000
Training Epoch: 44 [27392/50000]	Loss: 0.5304	LR: 0.025000
Training Epoch: 44 [27520/50000]	Loss: 0.5861	LR: 0.025000
Training Epoch: 44 [27648/50000]	Loss: 0.4421	LR: 0.025000
Training Epoch: 44 [27776/50000]	Loss: 0.5285	LR: 0.025000
Training Epoch: 44 [27904/50000]	Loss: 0.4705	LR: 0.025000
Training Epoch: 44 [28032/50000]	Loss: 0.6818	LR: 0.025000
Training Epoch: 44 [28160/50000]	Loss: 0.5345	LR: 0.025000
Training Epoch: 44 [28288/50000]	Loss: 0.7473	LR: 0.025000
Training Epoch: 44 [28416/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 44 [28544/50000]	Loss: 0.5420	LR: 0.025000
Training Epoch: 44 [28672/50000]	Loss: 0.4367	LR: 0.025000
Training Epoch: 44 [28800/50000]	Loss: 0.4355	LR: 0.025000
Training Epoch: 44 [28928/50000]	Loss: 0.5875	LR: 0.025000
Training Epoch: 44 [29056/50000]	Loss: 0.6587	LR: 0.025000
Training Epoch: 44 [29184/50000]	Loss: 0.7766	LR: 0.025000
Training Epoch: 44 [29312/50000]	Loss: 0.6077	LR: 0.025000
Training Epoch: 44 [29440/50000]	Loss: 0.6369	LR: 0.025000
Training Epoch: 44 [29568/50000]	Loss: 0.3884	LR: 0.025000
Training Epoch: 44 [29696/50000]	Loss: 0.2945	LR: 0.025000
Training Epoch: 44 [29824/50000]	Loss: 0.5276	LR: 0.025000
Training Epoch: 44 [29952/50000]	Loss: 0.4998	LR: 0.025000
Training Epoch: 44 [30080/50000]	Loss: 0.5597	LR: 0.025000
Training Epoch: 44 [30208/50000]	Loss: 0.4932	LR: 0.025000
Training Epoch: 44 [30336/50000]	Loss: 0.6136	LR: 0.025000
Training Epoch: 44 [30464/50000]	Loss: 0.5282	LR: 0.025000
Training Epoch: 44 [30592/50000]	Loss: 0.3651	LR: 0.025000
Training Epoch: 44 [30720/50000]	Loss: 0.6623	LR: 0.025000
Training Epoch: 44 [30848/50000]	Loss: 0.4887	LR: 0.025000
Training Epoch: 44 [30976/50000]	Loss: 0.6406	LR: 0.025000
Training Epoch: 44 [31104/50000]	Loss: 0.5496	LR: 0.025000
Training Epoch: 44 [31232/50000]	Loss: 0.4984	LR: 0.025000
Training Epoch: 44 [31360/50000]	Loss: 0.5491	LR: 0.025000
Training Epoch: 44 [31488/50000]	Loss: 0.4951	LR: 0.025000
Training Epoch: 44 [31616/50000]	Loss: 0.5064	LR: 0.025000
Training Epoch: 44 [31744/50000]	Loss: 0.6116	LR: 0.025000
Training Epoch: 44 [31872/50000]	Loss: 0.5530	LR: 0.025000
Training Epoch: 44 [32000/50000]	Loss: 0.5021	LR: 0.025000
Training Epoch: 44 [32128/50000]	Loss: 0.6487	LR: 0.025000
Training Epoch: 44 [32256/50000]	Loss: 0.4355	LR: 0.025000
Training Epoch: 44 [32384/50000]	Loss: 0.4778	LR: 0.025000
Training Epoch: 44 [32512/50000]	Loss: 0.6174	LR: 0.025000
Training Epoch: 44 [32640/50000]	Loss: 0.5765	LR: 0.025000
Training Epoch: 44 [32768/50000]	Loss: 0.5764	LR: 0.025000
Training Epoch: 44 [32896/50000]	Loss: 0.5967	LR: 0.025000
Training Epoch: 44 [33024/50000]	Loss: 0.5900	LR: 0.025000
Training Epoch: 44 [33152/50000]	Loss: 0.5549	LR: 0.025000
Training Epoch: 44 [33280/50000]	Loss: 0.7110	LR: 0.025000
Training Epoch: 44 [33408/50000]	Loss: 0.5417	LR: 0.025000
Training Epoch: 44 [33536/50000]	Loss: 0.4227	LR: 0.025000
Training Epoch: 44 [33664/50000]	Loss: 0.6073	LR: 0.025000
Training Epoch: 44 [33792/50000]	Loss: 0.5658	LR: 0.025000
Training Epoch: 44 [33920/50000]	Loss: 0.6300	LR: 0.025000
Training Epoch: 44 [34048/50000]	Loss: 0.5732	LR: 0.025000
Training Epoch: 44 [34176/50000]	Loss: 0.6532	LR: 0.025000
Training Epoch: 44 [34304/50000]	Loss: 0.5014	LR: 0.025000
Training Epoch: 44 [34432/50000]	Loss: 0.3246	LR: 0.025000
Training Epoch: 44 [34560/50000]	Loss: 0.6054	LR: 0.025000
Training Epoch: 44 [34688/50000]	Loss: 0.5719	LR: 0.025000
Training Epoch: 44 [34816/50000]	Loss: 0.5073	LR: 0.025000
Training Epoch: 44 [34944/50000]	Loss: 0.3954	LR: 0.025000
Training Epoch: 44 [35072/50000]	Loss: 0.5410	LR: 0.025000
Training Epoch: 44 [35200/50000]	Loss: 0.5500	LR: 0.025000
Training Epoch: 44 [35328/50000]	Loss: 0.5755	LR: 0.025000
Training Epoch: 44 [35456/50000]	Loss: 0.6032	LR: 0.025000
Training Epoch: 44 [35584/50000]	Loss: 0.4560	LR: 0.025000
Training Epoch: 44 [35712/50000]	Loss: 0.6084	LR: 0.025000
Training Epoch: 44 [35840/50000]	Loss: 0.5137	LR: 0.025000
Training Epoch: 44 [35968/50000]	Loss: 0.6716	LR: 0.025000
Training Epoch: 44 [36096/50000]	Loss: 0.5635	LR: 0.025000
Training Epoch: 44 [36224/50000]	Loss: 0.5047	LR: 0.025000
Training Epoch: 44 [36352/50000]	Loss: 0.4708	LR: 0.025000
Training Epoch: 44 [36480/50000]	Loss: 0.4728	LR: 0.025000
Training Epoch: 44 [36608/50000]	Loss: 0.5793	LR: 0.025000
Training Epoch: 44 [36736/50000]	Loss: 0.6178	LR: 0.025000
Training Epoch: 44 [36864/50000]	Loss: 0.4267	LR: 0.025000
Training Epoch: 44 [36992/50000]	Loss: 0.7615	LR: 0.025000
Training Epoch: 44 [37120/50000]	Loss: 0.5743	LR: 0.025000
Training Epoch: 44 [37248/50000]	Loss: 0.6138	LR: 0.025000
Training Epoch: 44 [37376/50000]	Loss: 0.6234	LR: 0.025000
Training Epoch: 44 [37504/50000]	Loss: 0.5978	LR: 0.025000
Training Epoch: 44 [37632/50000]	Loss: 0.6005	LR: 0.025000
Training Epoch: 44 [37760/50000]	Loss: 0.7362	LR: 0.025000
Training Epoch: 44 [37888/50000]	Loss: 0.6224	LR: 0.025000
Training Epoch: 44 [38016/50000]	Loss: 0.7010	LR: 0.025000
Training Epoch: 44 [38144/50000]	Loss: 0.5879	LR: 0.025000
Training Epoch: 44 [38272/50000]	Loss: 0.5850	LR: 0.025000
Training Epoch: 44 [38400/50000]	Loss: 0.5947	LR: 0.025000
Training Epoch: 44 [38528/50000]	Loss: 0.5801	LR: 0.025000
Training Epoch: 44 [38656/50000]	Loss: 0.4915	LR: 0.025000
Training Epoch: 44 [38784/50000]	Loss: 0.5132	LR: 0.025000
Training Epoch: 44 [38912/50000]	Loss: 0.5850	LR: 0.025000
Training Epoch: 44 [39040/50000]	Loss: 0.4570	LR: 0.025000
Training Epoch: 44 [39168/50000]	Loss: 0.6463	LR: 0.025000
Training Epoch: 44 [39296/50000]	Loss: 0.5789	LR: 0.025000
Training Epoch: 44 [39424/50000]	Loss: 0.7925	LR: 0.025000
Training Epoch: 44 [39552/50000]	Loss: 0.6380	LR: 0.025000
Training Epoch: 44 [39680/50000]	Loss: 0.5850	LR: 0.025000
Training Epoch: 44 [39808/50000]	Loss: 0.7620	LR: 0.025000
Training Epoch: 44 [39936/50000]	Loss: 0.8304	LR: 0.025000
Training Epoch: 44 [40064/50000]	Loss: 0.7216	LR: 0.025000
Training Epoch: 44 [40192/50000]	Loss: 0.4746	LR: 0.025000
Training Epoch: 44 [40320/50000]	Loss: 0.5136	LR: 0.025000
Training Epoch: 44 [40448/50000]	Loss: 0.5944	LR: 0.025000
Training Epoch: 44 [40576/50000]	Loss: 0.6461	LR: 0.025000
Training Epoch: 44 [40704/50000]	Loss: 0.6339	LR: 0.025000
Training Epoch: 44 [40832/50000]	Loss: 0.6166	LR: 0.025000
Training Epoch: 44 [40960/50000]	Loss: 0.6562	LR: 0.025000
Training Epoch: 44 [41088/50000]	Loss: 0.5593	LR: 0.025000
Training Epoch: 44 [41216/50000]	Loss: 0.8327	LR: 0.025000
Training Epoch: 44 [41344/50000]	Loss: 0.6978	LR: 0.025000
Training Epoch: 44 [41472/50000]	Loss: 0.6840	LR: 0.025000
Training Epoch: 44 [41600/50000]	Loss: 0.6118	LR: 0.025000
Training Epoch: 44 [41728/50000]	Loss: 0.6192	LR: 0.025000
Training Epoch: 44 [41856/50000]	Loss: 0.6809	LR: 0.025000
Training Epoch: 44 [41984/50000]	Loss: 0.5841	LR: 0.025000
Training Epoch: 44 [42112/50000]	Loss: 0.5984	LR: 0.025000
Training Epoch: 44 [42240/50000]	Loss: 0.6347	LR: 0.025000
Training Epoch: 44 [42368/50000]	Loss: 0.6359	LR: 0.025000
Training Epoch: 44 [42496/50000]	Loss: 0.6241	LR: 0.025000
Training Epoch: 44 [42624/50000]	Loss: 0.4159	LR: 0.025000
Training Epoch: 44 [42752/50000]	Loss: 0.4809	LR: 0.025000
Training Epoch: 44 [42880/50000]	Loss: 0.6342	LR: 0.025000
Training Epoch: 44 [43008/50000]	Loss: 0.5536	LR: 0.025000
Training Epoch: 44 [43136/50000]	Loss: 0.5247	LR: 0.025000
Training Epoch: 44 [43264/50000]	Loss: 0.6938	LR: 0.025000
Training Epoch: 44 [43392/50000]	Loss: 0.6958	LR: 0.025000
Training Epoch: 44 [43520/50000]	Loss: 0.7239	LR: 0.025000
Training Epoch: 44 [43648/50000]	Loss: 0.5268	LR: 0.025000
Training Epoch: 44 [43776/50000]	Loss: 0.4873	LR: 0.025000
Training Epoch: 44 [43904/50000]	Loss: 0.5841	LR: 0.025000
Training Epoch: 44 [44032/50000]	Loss: 0.5218	LR: 0.025000
Training Epoch: 44 [44160/50000]	Loss: 0.6211	LR: 0.025000
Training Epoch: 44 [44288/50000]	Loss: 0.7465	LR: 0.025000
Training Epoch: 44 [44416/50000]	Loss: 0.4609	LR: 0.025000
Training Epoch: 44 [44544/50000]	Loss: 0.6372	LR: 0.025000
Training Epoch: 44 [44672/50000]	Loss: 0.5392	LR: 0.025000
Training Epoch: 44 [44800/50000]	Loss: 0.5255	LR: 0.025000
Training Epoch: 44 [44928/50000]	Loss: 0.5545	LR: 0.025000
Training Epoch: 44 [45056/50000]	Loss: 0.6104	LR: 0.025000
Training Epoch: 44 [45184/50000]	Loss: 0.6856	LR: 0.025000
Training Epoch: 44 [45312/50000]	Loss: 0.5424	LR: 0.025000
Training Epoch: 44 [45440/50000]	Loss: 0.6657	LR: 0.025000
Training Epoch: 44 [45568/50000]	Loss: 0.7363	LR: 0.025000
Training Epoch: 44 [45696/50000]	Loss: 0.5132	LR: 0.025000
Training Epoch: 44 [45824/50000]	Loss: 0.6639	LR: 0.025000
Training Epoch: 44 [45952/50000]	Loss: 0.6012	LR: 0.025000
Training Epoch: 44 [46080/50000]	Loss: 0.6055	LR: 0.025000
Training Epoch: 44 [46208/50000]	Loss: 0.5912	LR: 0.025000
Training Epoch: 44 [46336/50000]	Loss: 0.8598	LR: 0.025000
Training Epoch: 44 [46464/50000]	Loss: 0.5909	LR: 0.025000
Training Epoch: 44 [46592/50000]	Loss: 0.7026	LR: 0.025000
Training Epoch: 44 [46720/50000]	Loss: 0.6992	LR: 0.025000
Training Epoch: 44 [46848/50000]	Loss: 0.7943	LR: 0.025000
Training Epoch: 44 [46976/50000]	Loss: 0.5772	LR: 0.025000
Training Epoch: 44 [47104/50000]	Loss: 0.6253	LR: 0.025000
Training Epoch: 44 [47232/50000]	Loss: 0.6062	LR: 0.025000
Training Epoch: 44 [47360/50000]	Loss: 0.5568	LR: 0.025000
Training Epoch: 44 [47488/50000]	Loss: 0.5215	LR: 0.025000
Training Epoch: 44 [47616/50000]	Loss: 0.5363	LR: 0.025000
Training Epoch: 44 [47744/50000]	Loss: 0.5936	LR: 0.025000
Training Epoch: 44 [47872/50000]	Loss: 0.6900	LR: 0.025000
Training Epoch: 44 [48000/50000]	Loss: 0.7060	LR: 0.025000
Training Epoch: 44 [48128/50000]	Loss: 0.5660	LR: 0.025000
Training Epoch: 44 [48256/50000]	Loss: 0.6071	LR: 0.025000
Training Epoch: 44 [48384/50000]	Loss: 0.7442	LR: 0.025000
Training Epoch: 44 [48512/50000]	Loss: 0.5925	LR: 0.025000
Training Epoch: 44 [48640/50000]	Loss: 0.5693	LR: 0.025000
Training Epoch: 44 [48768/50000]	Loss: 0.7447	LR: 0.025000
Training Epoch: 44 [48896/50000]	Loss: 0.7847	LR: 0.025000
Training Epoch: 44 [49024/50000]	Loss: 0.8015	LR: 0.025000
Training Epoch: 44 [49152/50000]	Loss: 0.6758	LR: 0.025000
Training Epoch: 44 [49280/50000]	Loss: 0.7418	LR: 0.025000
Training Epoch: 44 [49408/50000]	Loss: 0.7785	LR: 0.025000
Training Epoch: 44 [49536/50000]	Loss: 0.5573	LR: 0.025000
Training Epoch: 44 [49664/50000]	Loss: 0.4266	LR: 0.025000
Training Epoch: 44 [49792/50000]	Loss: 0.5269	LR: 0.025000
Training Epoch: 44 [49920/50000]	Loss: 0.6187	LR: 0.025000
Training Epoch: 44 [50000/50000]	Loss: 0.6131	LR: 0.025000
epoch 44 training time consumed: 53.90s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  161482 GB |  161482 GB |
|       from large pool |  123392 KB |    1034 MB |  161323 GB |  161323 GB |
|       from small pool |   10798 KB |      13 MB |     159 GB |     159 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  161482 GB |  161482 GB |
|       from large pool |  123392 KB |    1034 MB |  161323 GB |  161323 GB |
|       from small pool |   10798 KB |      13 MB |     159 GB |     159 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   71063 GB |   71063 GB |
|       from large pool |  155136 KB |  433088 KB |   70887 GB |   70887 GB |
|       from small pool |    1490 KB |    3494 KB |     175 GB |     175 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    6231 K  |    6231 K  |
|       from large pool |      24    |      65    |    3252 K  |    3252 K  |
|       from small pool |     231    |     274    |    2978 K  |    2978 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    6231 K  |    6231 K  |
|       from large pool |      24    |      65    |    3252 K  |    3252 K  |
|       from small pool |     231    |     274    |    2978 K  |    2978 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3080 K  |    3080 K  |
|       from large pool |       9    |      14    |    1574 K  |    1574 K  |
|       from small pool |      12    |      16    |    1506 K  |    1506 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 44, Average loss: 0.0099, Accuracy: 0.6774, Time consumed:3.45s

Training Epoch: 45 [128/50000]	Loss: 0.5025	LR: 0.025000
Training Epoch: 45 [256/50000]	Loss: 0.5967	LR: 0.025000
Training Epoch: 45 [384/50000]	Loss: 0.5722	LR: 0.025000
Training Epoch: 45 [512/50000]	Loss: 0.4887	LR: 0.025000
Training Epoch: 45 [640/50000]	Loss: 0.5124	LR: 0.025000
Training Epoch: 45 [768/50000]	Loss: 0.5574	LR: 0.025000
Training Epoch: 45 [896/50000]	Loss: 0.5119	LR: 0.025000
Training Epoch: 45 [1024/50000]	Loss: 0.4978	LR: 0.025000
Training Epoch: 45 [1152/50000]	Loss: 0.5172	LR: 0.025000
Training Epoch: 45 [1280/50000]	Loss: 0.4088	LR: 0.025000
Training Epoch: 45 [1408/50000]	Loss: 0.5410	LR: 0.025000
Training Epoch: 45 [1536/50000]	Loss: 0.6834	LR: 0.025000
Training Epoch: 45 [1664/50000]	Loss: 0.4903	LR: 0.025000
Training Epoch: 45 [1792/50000]	Loss: 0.5515	LR: 0.025000
Training Epoch: 45 [1920/50000]	Loss: 0.5503	LR: 0.025000
Training Epoch: 45 [2048/50000]	Loss: 0.5193	LR: 0.025000
Training Epoch: 45 [2176/50000]	Loss: 0.3975	LR: 0.025000
Training Epoch: 45 [2304/50000]	Loss: 0.4067	LR: 0.025000
Training Epoch: 45 [2432/50000]	Loss: 0.5547	LR: 0.025000
Training Epoch: 45 [2560/50000]	Loss: 0.4739	LR: 0.025000
Training Epoch: 45 [2688/50000]	Loss: 0.6478	LR: 0.025000
Training Epoch: 45 [2816/50000]	Loss: 0.6061	LR: 0.025000
Training Epoch: 45 [2944/50000]	Loss: 0.5842	LR: 0.025000
Training Epoch: 45 [3072/50000]	Loss: 0.4657	LR: 0.025000
Training Epoch: 45 [3200/50000]	Loss: 0.5168	LR: 0.025000
Training Epoch: 45 [3328/50000]	Loss: 0.4482	LR: 0.025000
Training Epoch: 45 [3456/50000]	Loss: 0.3948	LR: 0.025000
Training Epoch: 45 [3584/50000]	Loss: 0.5165	LR: 0.025000
Training Epoch: 45 [3712/50000]	Loss: 0.4882	LR: 0.025000
Training Epoch: 45 [3840/50000]	Loss: 0.4755	LR: 0.025000
Training Epoch: 45 [3968/50000]	Loss: 0.5334	LR: 0.025000
Training Epoch: 45 [4096/50000]	Loss: 0.5164	LR: 0.025000
Training Epoch: 45 [4224/50000]	Loss: 0.5357	LR: 0.025000
Training Epoch: 45 [4352/50000]	Loss: 0.4325	LR: 0.025000
Training Epoch: 45 [4480/50000]	Loss: 0.5942	LR: 0.025000
Training Epoch: 45 [4608/50000]	Loss: 0.4927	LR: 0.025000
Training Epoch: 45 [4736/50000]	Loss: 0.3872	LR: 0.025000
Training Epoch: 45 [4864/50000]	Loss: 0.4602	LR: 0.025000
Training Epoch: 45 [4992/50000]	Loss: 0.5145	LR: 0.025000
Training Epoch: 45 [5120/50000]	Loss: 0.3835	LR: 0.025000
Training Epoch: 45 [5248/50000]	Loss: 0.5926	LR: 0.025000
Training Epoch: 45 [5376/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 45 [5504/50000]	Loss: 0.7172	LR: 0.025000
Training Epoch: 45 [5632/50000]	Loss: 0.4609	LR: 0.025000
Training Epoch: 45 [5760/50000]	Loss: 0.4688	LR: 0.025000
Training Epoch: 45 [5888/50000]	Loss: 0.4483	LR: 0.025000
Training Epoch: 45 [6016/50000]	Loss: 0.3841	LR: 0.025000
Training Epoch: 45 [6144/50000]	Loss: 0.4884	LR: 0.025000
Training Epoch: 45 [6272/50000]	Loss: 0.3771	LR: 0.025000
Training Epoch: 45 [6400/50000]	Loss: 0.3667	LR: 0.025000
Training Epoch: 45 [6528/50000]	Loss: 0.4060	LR: 0.025000
Training Epoch: 45 [6656/50000]	Loss: 0.5141	LR: 0.025000
Training Epoch: 45 [6784/50000]	Loss: 0.3602	LR: 0.025000
Training Epoch: 45 [6912/50000]	Loss: 0.4903	LR: 0.025000
Training Epoch: 45 [7040/50000]	Loss: 0.4836	LR: 0.025000
Training Epoch: 45 [7168/50000]	Loss: 0.4602	LR: 0.025000
Training Epoch: 45 [7296/50000]	Loss: 0.5273	LR: 0.025000
Training Epoch: 45 [7424/50000]	Loss: 0.3222	LR: 0.025000
Training Epoch: 45 [7552/50000]	Loss: 0.4414	LR: 0.025000
Training Epoch: 45 [7680/50000]	Loss: 0.4070	LR: 0.025000
Training Epoch: 45 [7808/50000]	Loss: 0.5792	LR: 0.025000
Training Epoch: 45 [7936/50000]	Loss: 0.7426	LR: 0.025000
Training Epoch: 45 [8064/50000]	Loss: 0.4511	LR: 0.025000
Training Epoch: 45 [8192/50000]	Loss: 0.5768	LR: 0.025000
Training Epoch: 45 [8320/50000]	Loss: 0.5358	LR: 0.025000
Training Epoch: 45 [8448/50000]	Loss: 0.4429	LR: 0.025000
Training Epoch: 45 [8576/50000]	Loss: 0.4869	LR: 0.025000
Training Epoch: 45 [8704/50000]	Loss: 0.3847	LR: 0.025000
Training Epoch: 45 [8832/50000]	Loss: 0.5619	LR: 0.025000
Training Epoch: 45 [8960/50000]	Loss: 0.4220	LR: 0.025000
Training Epoch: 45 [9088/50000]	Loss: 0.4581	LR: 0.025000
Training Epoch: 45 [9216/50000]	Loss: 0.5598	LR: 0.025000
Training Epoch: 45 [9344/50000]	Loss: 0.5024	LR: 0.025000
Training Epoch: 45 [9472/50000]	Loss: 0.5023	LR: 0.025000
Training Epoch: 45 [9600/50000]	Loss: 0.5212	LR: 0.025000
Training Epoch: 45 [9728/50000]	Loss: 0.5905	LR: 0.025000
Training Epoch: 45 [9856/50000]	Loss: 0.5289	LR: 0.025000
Training Epoch: 45 [9984/50000]	Loss: 0.4673	LR: 0.025000
Training Epoch: 45 [10112/50000]	Loss: 0.4016	LR: 0.025000
Training Epoch: 45 [10240/50000]	Loss: 0.5040	LR: 0.025000
Training Epoch: 45 [10368/50000]	Loss: 0.5636	LR: 0.025000
Training Epoch: 45 [10496/50000]	Loss: 0.4763	LR: 0.025000
Training Epoch: 45 [10624/50000]	Loss: 0.5346	LR: 0.025000
Training Epoch: 45 [10752/50000]	Loss: 0.5383	LR: 0.025000
Training Epoch: 45 [10880/50000]	Loss: 0.6656	LR: 0.025000
Training Epoch: 45 [11008/50000]	Loss: 0.6865	LR: 0.025000
Training Epoch: 45 [11136/50000]	Loss: 0.6064	LR: 0.025000
Training Epoch: 45 [11264/50000]	Loss: 0.4709	LR: 0.025000
Training Epoch: 45 [11392/50000]	Loss: 0.4184	LR: 0.025000
Training Epoch: 45 [11520/50000]	Loss: 0.5122	LR: 0.025000
Training Epoch: 45 [11648/50000]	Loss: 0.4420	LR: 0.025000
Training Epoch: 45 [11776/50000]	Loss: 0.4493	LR: 0.025000
Training Epoch: 45 [11904/50000]	Loss: 0.4443	LR: 0.025000
Training Epoch: 45 [12032/50000]	Loss: 0.3895	LR: 0.025000
Training Epoch: 45 [12160/50000]	Loss: 0.4697	LR: 0.025000
Training Epoch: 45 [12288/50000]	Loss: 0.5404	LR: 0.025000
Training Epoch: 45 [12416/50000]	Loss: 0.4580	LR: 0.025000
Training Epoch: 45 [12544/50000]	Loss: 0.5065	LR: 0.025000
Training Epoch: 45 [12672/50000]	Loss: 0.5561	LR: 0.025000
Training Epoch: 45 [12800/50000]	Loss: 0.5508	LR: 0.025000
Training Epoch: 45 [12928/50000]	Loss: 0.4679	LR: 0.025000
Training Epoch: 45 [13056/50000]	Loss: 0.5808	LR: 0.025000
Training Epoch: 45 [13184/50000]	Loss: 0.4279	LR: 0.025000
Training Epoch: 45 [13312/50000]	Loss: 0.4193	LR: 0.025000
Training Epoch: 45 [13440/50000]	Loss: 0.4820	LR: 0.025000
Training Epoch: 45 [13568/50000]	Loss: 0.5618	LR: 0.025000
Training Epoch: 45 [13696/50000]	Loss: 0.4384	LR: 0.025000
Training Epoch: 45 [13824/50000]	Loss: 0.7157	LR: 0.025000
Training Epoch: 45 [13952/50000]	Loss: 0.4998	LR: 0.025000
Training Epoch: 45 [14080/50000]	Loss: 0.4493	LR: 0.025000
Training Epoch: 45 [14208/50000]	Loss: 0.5465	LR: 0.025000
Training Epoch: 45 [14336/50000]	Loss: 0.5299	LR: 0.025000
Training Epoch: 45 [14464/50000]	Loss: 0.3757	LR: 0.025000
Training Epoch: 45 [14592/50000]	Loss: 0.4137	LR: 0.025000
Training Epoch: 45 [14720/50000]	Loss: 0.5785	LR: 0.025000
Training Epoch: 45 [14848/50000]	Loss: 0.4888	LR: 0.025000
Training Epoch: 45 [14976/50000]	Loss: 0.5911	LR: 0.025000
Training Epoch: 45 [15104/50000]	Loss: 0.4964	LR: 0.025000
Training Epoch: 45 [15232/50000]	Loss: 0.4030	LR: 0.025000
Training Epoch: 45 [15360/50000]	Loss: 0.3375	LR: 0.025000
Training Epoch: 45 [15488/50000]	Loss: 0.6021	LR: 0.025000
Training Epoch: 45 [15616/50000]	Loss: 0.5714	LR: 0.025000
Training Epoch: 45 [15744/50000]	Loss: 0.5170	LR: 0.025000
Training Epoch: 45 [15872/50000]	Loss: 0.5094	LR: 0.025000
Training Epoch: 45 [16000/50000]	Loss: 0.4183	LR: 0.025000
Training Epoch: 45 [16128/50000]	Loss: 0.5484	LR: 0.025000
Training Epoch: 45 [16256/50000]	Loss: 0.6450	LR: 0.025000
Training Epoch: 45 [16384/50000]	Loss: 0.4111	LR: 0.025000
Training Epoch: 45 [16512/50000]	Loss: 0.5589	LR: 0.025000
Training Epoch: 45 [16640/50000]	Loss: 0.5169	LR: 0.025000
Training Epoch: 45 [16768/50000]	Loss: 0.4377	LR: 0.025000
Training Epoch: 45 [16896/50000]	Loss: 0.4126	LR: 0.025000
Training Epoch: 45 [17024/50000]	Loss: 0.4433	LR: 0.025000
Training Epoch: 45 [17152/50000]	Loss: 0.4761	LR: 0.025000
Training Epoch: 45 [17280/50000]	Loss: 0.6270	LR: 0.025000
Training Epoch: 45 [17408/50000]	Loss: 0.4244	LR: 0.025000
Training Epoch: 45 [17536/50000]	Loss: 0.5845	LR: 0.025000
Training Epoch: 45 [17664/50000]	Loss: 0.5722	LR: 0.025000
Training Epoch: 45 [17792/50000]	Loss: 0.5839	LR: 0.025000
Training Epoch: 45 [17920/50000]	Loss: 0.5211	LR: 0.025000
Training Epoch: 45 [18048/50000]	Loss: 0.5561	LR: 0.025000
Training Epoch: 45 [18176/50000]	Loss: 0.5071	LR: 0.025000
Training Epoch: 45 [18304/50000]	Loss: 0.4853	LR: 0.025000
Training Epoch: 45 [18432/50000]	Loss: 0.5047	LR: 0.025000
Training Epoch: 45 [18560/50000]	Loss: 0.5834	LR: 0.025000
Training Epoch: 45 [18688/50000]	Loss: 0.5455	LR: 0.025000
Training Epoch: 45 [18816/50000]	Loss: 0.4383	LR: 0.025000
Training Epoch: 45 [18944/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 45 [19072/50000]	Loss: 0.6438	LR: 0.025000
Training Epoch: 45 [19200/50000]	Loss: 0.5350	LR: 0.025000
Training Epoch: 45 [19328/50000]	Loss: 0.4750	LR: 0.025000
Training Epoch: 45 [19456/50000]	Loss: 0.5488	LR: 0.025000
Training Epoch: 45 [19584/50000]	Loss: 0.5039	LR: 0.025000
Training Epoch: 45 [19712/50000]	Loss: 0.5955	LR: 0.025000
Training Epoch: 45 [19840/50000]	Loss: 0.5896	LR: 0.025000
Training Epoch: 45 [19968/50000]	Loss: 0.5304	LR: 0.025000
Training Epoch: 45 [20096/50000]	Loss: 0.5006	LR: 0.025000
Training Epoch: 45 [20224/50000]	Loss: 0.5503	LR: 0.025000
Training Epoch: 45 [20352/50000]	Loss: 0.6602	LR: 0.025000
Training Epoch: 45 [20480/50000]	Loss: 0.6629	LR: 0.025000
Training Epoch: 45 [20608/50000]	Loss: 0.5281	LR: 0.025000
Training Epoch: 45 [20736/50000]	Loss: 0.4307	LR: 0.025000
Training Epoch: 45 [20864/50000]	Loss: 0.6240	LR: 0.025000
Training Epoch: 45 [20992/50000]	Loss: 0.5853	LR: 0.025000
Training Epoch: 45 [21120/50000]	Loss: 0.4868	LR: 0.025000
Training Epoch: 45 [21248/50000]	Loss: 0.4373	LR: 0.025000
Training Epoch: 45 [21376/50000]	Loss: 0.5536	LR: 0.025000
Training Epoch: 45 [21504/50000]	Loss: 0.4959	LR: 0.025000
Training Epoch: 45 [21632/50000]	Loss: 0.5223	LR: 0.025000
Training Epoch: 45 [21760/50000]	Loss: 0.5907	LR: 0.025000
Training Epoch: 45 [21888/50000]	Loss: 0.4577	LR: 0.025000
Training Epoch: 45 [22016/50000]	Loss: 0.6263	LR: 0.025000
Training Epoch: 45 [22144/50000]	Loss: 0.4646	LR: 0.025000
Training Epoch: 45 [22272/50000]	Loss: 0.4804	LR: 0.025000
Training Epoch: 45 [22400/50000]	Loss: 0.6403	LR: 0.025000
Training Epoch: 45 [22528/50000]	Loss: 0.4834	LR: 0.025000
Training Epoch: 45 [22656/50000]	Loss: 0.4449	LR: 0.025000
Training Epoch: 45 [22784/50000]	Loss: 0.4807	LR: 0.025000
Training Epoch: 45 [22912/50000]	Loss: 0.4914	LR: 0.025000
Training Epoch: 45 [23040/50000]	Loss: 0.4499	LR: 0.025000
Training Epoch: 45 [23168/50000]	Loss: 0.4264	LR: 0.025000
Training Epoch: 45 [23296/50000]	Loss: 0.6072	LR: 0.025000
Training Epoch: 45 [23424/50000]	Loss: 0.6243	LR: 0.025000
Training Epoch: 45 [23552/50000]	Loss: 0.6621	LR: 0.025000
Training Epoch: 45 [23680/50000]	Loss: 0.5477	LR: 0.025000
Training Epoch: 45 [23808/50000]	Loss: 0.5281	LR: 0.025000
Training Epoch: 45 [23936/50000]	Loss: 0.5955	LR: 0.025000
Training Epoch: 45 [24064/50000]	Loss: 0.4057	LR: 0.025000
Training Epoch: 45 [24192/50000]	Loss: 0.4408	LR: 0.025000
Training Epoch: 45 [24320/50000]	Loss: 0.4914	LR: 0.025000
Training Epoch: 45 [24448/50000]	Loss: 0.5916	LR: 0.025000
Training Epoch: 45 [24576/50000]	Loss: 0.4468	LR: 0.025000
Training Epoch: 45 [24704/50000]	Loss: 0.4869	LR: 0.025000
Training Epoch: 45 [24832/50000]	Loss: 0.5689	LR: 0.025000
Training Epoch: 45 [24960/50000]	Loss: 0.5405	LR: 0.025000
Training Epoch: 45 [25088/50000]	Loss: 0.5647	LR: 0.025000
Training Epoch: 45 [25216/50000]	Loss: 0.5633	LR: 0.025000
Training Epoch: 45 [25344/50000]	Loss: 0.5143	LR: 0.025000
Training Epoch: 45 [25472/50000]	Loss: 0.4022	LR: 0.025000
Training Epoch: 45 [25600/50000]	Loss: 0.5733	LR: 0.025000
Training Epoch: 45 [25728/50000]	Loss: 0.6618	LR: 0.025000
Training Epoch: 45 [25856/50000]	Loss: 0.4921	LR: 0.025000
Training Epoch: 45 [25984/50000]	Loss: 0.6880	LR: 0.025000
Training Epoch: 45 [26112/50000]	Loss: 0.4565	LR: 0.025000
Training Epoch: 45 [26240/50000]	Loss: 0.6259	LR: 0.025000
Training Epoch: 45 [26368/50000]	Loss: 0.5046	LR: 0.025000
Training Epoch: 45 [26496/50000]	Loss: 0.4279	LR: 0.025000
Training Epoch: 45 [26624/50000]	Loss: 0.4809	LR: 0.025000
Training Epoch: 45 [26752/50000]	Loss: 0.7188	LR: 0.025000
Training Epoch: 45 [26880/50000]	Loss: 0.5756	LR: 0.025000
Training Epoch: 45 [27008/50000]	Loss: 0.5641	LR: 0.025000
Training Epoch: 45 [27136/50000]	Loss: 0.3877	LR: 0.025000
Training Epoch: 45 [27264/50000]	Loss: 0.4289	LR: 0.025000
Training Epoch: 45 [27392/50000]	Loss: 0.5716	LR: 0.025000
Training Epoch: 45 [27520/50000]	Loss: 0.4030	LR: 0.025000
Training Epoch: 45 [27648/50000]	Loss: 0.5772	LR: 0.025000
Training Epoch: 45 [27776/50000]	Loss: 0.6659	LR: 0.025000
Training Epoch: 45 [27904/50000]	Loss: 0.6488	LR: 0.025000
Training Epoch: 45 [28032/50000]	Loss: 0.6342	LR: 0.025000
Training Epoch: 45 [28160/50000]	Loss: 0.6255	LR: 0.025000
Training Epoch: 45 [28288/50000]	Loss: 0.4284	LR: 0.025000
Training Epoch: 45 [28416/50000]	Loss: 0.5400	LR: 0.025000
Training Epoch: 45 [28544/50000]	Loss: 0.4711	LR: 0.025000
Training Epoch: 45 [28672/50000]	Loss: 0.5200	LR: 0.025000
Training Epoch: 45 [28800/50000]	Loss: 0.4162	LR: 0.025000
Training Epoch: 45 [28928/50000]	Loss: 0.3956	LR: 0.025000
Training Epoch: 45 [29056/50000]	Loss: 0.8225	LR: 0.025000
Training Epoch: 45 [29184/50000]	Loss: 0.4780	LR: 0.025000
Training Epoch: 45 [29312/50000]	Loss: 0.4151	LR: 0.025000
Training Epoch: 45 [29440/50000]	Loss: 0.6484	LR: 0.025000
Training Epoch: 45 [29568/50000]	Loss: 0.5803	LR: 0.025000
Training Epoch: 45 [29696/50000]	Loss: 0.6568	LR: 0.025000
Training Epoch: 45 [29824/50000]	Loss: 0.6909	LR: 0.025000
Training Epoch: 45 [29952/50000]	Loss: 0.5529	LR: 0.025000
Training Epoch: 45 [30080/50000]	Loss: 0.7337	LR: 0.025000
Training Epoch: 45 [30208/50000]	Loss: 0.5702	LR: 0.025000
Training Epoch: 45 [30336/50000]	Loss: 0.5671	LR: 0.025000
Training Epoch: 45 [30464/50000]	Loss: 0.6147	LR: 0.025000
Training Epoch: 45 [30592/50000]	Loss: 0.4997	LR: 0.025000
Training Epoch: 45 [30720/50000]	Loss: 0.6374	LR: 0.025000
Training Epoch: 45 [30848/50000]	Loss: 0.5823	LR: 0.025000
Training Epoch: 45 [30976/50000]	Loss: 0.6873	LR: 0.025000
Training Epoch: 45 [31104/50000]	Loss: 0.6906	LR: 0.025000
Training Epoch: 45 [31232/50000]	Loss: 0.7037	LR: 0.025000
Training Epoch: 45 [31360/50000]	Loss: 0.7721	LR: 0.025000
Training Epoch: 45 [31488/50000]	Loss: 0.4595	LR: 0.025000
Training Epoch: 45 [31616/50000]	Loss: 0.5467	LR: 0.025000
Training Epoch: 45 [31744/50000]	Loss: 0.5558	LR: 0.025000
Training Epoch: 45 [31872/50000]	Loss: 0.5891	LR: 0.025000
Training Epoch: 45 [32000/50000]	Loss: 0.5263	LR: 0.025000
Training Epoch: 45 [32128/50000]	Loss: 0.7788	LR: 0.025000
Training Epoch: 45 [32256/50000]	Loss: 0.5448	LR: 0.025000
Training Epoch: 45 [32384/50000]	Loss: 0.5926	LR: 0.025000
Training Epoch: 45 [32512/50000]	Loss: 0.7462	LR: 0.025000
Training Epoch: 45 [32640/50000]	Loss: 0.5355	LR: 0.025000
Training Epoch: 45 [32768/50000]	Loss: 0.6236	LR: 0.025000
Training Epoch: 45 [32896/50000]	Loss: 0.4209	LR: 0.025000
Training Epoch: 45 [33024/50000]	Loss: 0.6307	LR: 0.025000
Training Epoch: 45 [33152/50000]	Loss: 0.5674	LR: 0.025000
Training Epoch: 45 [33280/50000]	Loss: 0.9816	LR: 0.025000
Training Epoch: 45 [33408/50000]	Loss: 0.5967	LR: 0.025000
Training Epoch: 45 [33536/50000]	Loss: 0.5529	LR: 0.025000
Training Epoch: 45 [33664/50000]	Loss: 0.5648	LR: 0.025000
Training Epoch: 45 [33792/50000]	Loss: 0.5520	LR: 0.025000
Training Epoch: 45 [33920/50000]	Loss: 0.5760	LR: 0.025000
Training Epoch: 45 [34048/50000]	Loss: 0.6019	LR: 0.025000
Training Epoch: 45 [34176/50000]	Loss: 0.6034	LR: 0.025000
Training Epoch: 45 [34304/50000]	Loss: 0.6628	LR: 0.025000
Training Epoch: 45 [34432/50000]	Loss: 0.5031	LR: 0.025000
Training Epoch: 45 [34560/50000]	Loss: 0.4844	LR: 0.025000
Training Epoch: 45 [34688/50000]	Loss: 0.6357	LR: 0.025000
Training Epoch: 45 [34816/50000]	Loss: 0.6084	LR: 0.025000
Training Epoch: 45 [34944/50000]	Loss: 0.6086	LR: 0.025000
Training Epoch: 45 [35072/50000]	Loss: 0.7564	LR: 0.025000
Training Epoch: 45 [35200/50000]	Loss: 0.6035	LR: 0.025000
Training Epoch: 45 [35328/50000]	Loss: 0.6137	LR: 0.025000
Training Epoch: 45 [35456/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 45 [35584/50000]	Loss: 0.5629	LR: 0.025000
Training Epoch: 45 [35712/50000]	Loss: 0.6647	LR: 0.025000
Training Epoch: 45 [35840/50000]	Loss: 0.6844	LR: 0.025000
Training Epoch: 45 [35968/50000]	Loss: 0.7263	LR: 0.025000
Training Epoch: 45 [36096/50000]	Loss: 0.5674	LR: 0.025000
Training Epoch: 45 [36224/50000]	Loss: 0.4694	LR: 0.025000
Training Epoch: 45 [36352/50000]	Loss: 0.6646	LR: 0.025000
Training Epoch: 45 [36480/50000]	Loss: 0.6645	LR: 0.025000
Training Epoch: 45 [36608/50000]	Loss: 0.6858	LR: 0.025000
Training Epoch: 45 [36736/50000]	Loss: 0.5889	LR: 0.025000
Training Epoch: 45 [36864/50000]	Loss: 0.7796	LR: 0.025000
Training Epoch: 45 [36992/50000]	Loss: 0.8002	LR: 0.025000
Training Epoch: 45 [37120/50000]	Loss: 0.5500	LR: 0.025000
Training Epoch: 45 [37248/50000]	Loss: 0.7737	LR: 0.025000
Training Epoch: 45 [37376/50000]	Loss: 0.7042	LR: 0.025000
Training Epoch: 45 [37504/50000]	Loss: 0.6677	LR: 0.025000
Training Epoch: 45 [37632/50000]	Loss: 0.7362	LR: 0.025000
Training Epoch: 45 [37760/50000]	Loss: 0.4865	LR: 0.025000
Training Epoch: 45 [37888/50000]	Loss: 0.5683	LR: 0.025000
Training Epoch: 45 [38016/50000]	Loss: 0.4813	LR: 0.025000
Training Epoch: 45 [38144/50000]	Loss: 0.5745	LR: 0.025000
Training Epoch: 45 [38272/50000]	Loss: 0.5728	LR: 0.025000
Training Epoch: 45 [38400/50000]	Loss: 0.6642	LR: 0.025000
Training Epoch: 45 [38528/50000]	Loss: 0.6301	LR: 0.025000
Training Epoch: 45 [38656/50000]	Loss: 0.6603	LR: 0.025000
Training Epoch: 45 [38784/50000]	Loss: 0.4034	LR: 0.025000
Training Epoch: 45 [38912/50000]	Loss: 0.6141	LR: 0.025000
Training Epoch: 45 [39040/50000]	Loss: 0.6730	LR: 0.025000
Training Epoch: 45 [39168/50000]	Loss: 0.5288	LR: 0.025000
Training Epoch: 45 [39296/50000]	Loss: 0.7652	LR: 0.025000
Training Epoch: 45 [39424/50000]	Loss: 0.7482	LR: 0.025000
Training Epoch: 45 [39552/50000]	Loss: 0.6784	LR: 0.025000
Training Epoch: 45 [39680/50000]	Loss: 0.5992	LR: 0.025000
Training Epoch: 45 [39808/50000]	Loss: 0.7277	LR: 0.025000
Training Epoch: 45 [39936/50000]	Loss: 0.5455	LR: 0.025000
Training Epoch: 45 [40064/50000]	Loss: 0.6316	LR: 0.025000
Training Epoch: 45 [40192/50000]	Loss: 0.7240	LR: 0.025000
Training Epoch: 45 [40320/50000]	Loss: 0.7670	LR: 0.025000
Training Epoch: 45 [40448/50000]	Loss: 0.7091	LR: 0.025000
Training Epoch: 45 [40576/50000]	Loss: 0.6366	LR: 0.025000
Training Epoch: 45 [40704/50000]	Loss: 0.6397	LR: 0.025000
Training Epoch: 45 [40832/50000]	Loss: 0.7137	LR: 0.025000
Training Epoch: 45 [40960/50000]	Loss: 0.4886	LR: 0.025000
Training Epoch: 45 [41088/50000]	Loss: 0.7886	LR: 0.025000
Training Epoch: 45 [41216/50000]	Loss: 0.6649	LR: 0.025000
Training Epoch: 45 [41344/50000]	Loss: 0.6139	LR: 0.025000
Training Epoch: 45 [41472/50000]	Loss: 0.6257	LR: 0.025000
Training Epoch: 45 [41600/50000]	Loss: 0.6236	LR: 0.025000
Training Epoch: 45 [41728/50000]	Loss: 0.6156	LR: 0.025000
Training Epoch: 45 [41856/50000]	Loss: 0.7529	LR: 0.025000
Training Epoch: 45 [41984/50000]	Loss: 0.7528	LR: 0.025000
Training Epoch: 45 [42112/50000]	Loss: 0.7942	LR: 0.025000
Training Epoch: 45 [42240/50000]	Loss: 0.6368	LR: 0.025000
Training Epoch: 45 [42368/50000]	Loss: 0.7062	LR: 0.025000
Training Epoch: 45 [42496/50000]	Loss: 0.7007	LR: 0.025000
Training Epoch: 45 [42624/50000]	Loss: 0.5907	LR: 0.025000
Training Epoch: 45 [42752/50000]	Loss: 0.6738	LR: 0.025000
Training Epoch: 45 [42880/50000]	Loss: 0.5762	LR: 0.025000
Training Epoch: 45 [43008/50000]	Loss: 0.6930	LR: 0.025000
Training Epoch: 45 [43136/50000]	Loss: 0.5083	LR: 0.025000
Training Epoch: 45 [43264/50000]	Loss: 0.6569	LR: 0.025000
Training Epoch: 45 [43392/50000]	Loss: 0.6365	LR: 0.025000
Training Epoch: 45 [43520/50000]	Loss: 0.5386	LR: 0.025000
Training Epoch: 45 [43648/50000]	Loss: 0.7006	LR: 0.025000
Training Epoch: 45 [43776/50000]	Loss: 0.5723	LR: 0.025000
Training Epoch: 45 [43904/50000]	Loss: 0.4579	LR: 0.025000
Training Epoch: 45 [44032/50000]	Loss: 0.7192	LR: 0.025000
Training Epoch: 45 [44160/50000]	Loss: 0.5541	LR: 0.025000
Training Epoch: 45 [44288/50000]	Loss: 0.5841	LR: 0.025000
Training Epoch: 45 [44416/50000]	Loss: 0.7973	LR: 0.025000
Training Epoch: 45 [44544/50000]	Loss: 0.7120	LR: 0.025000
Training Epoch: 45 [44672/50000]	Loss: 0.6952	LR: 0.025000
Training Epoch: 45 [44800/50000]	Loss: 0.8280	LR: 0.025000
Training Epoch: 45 [44928/50000]	Loss: 0.5544	LR: 0.025000
Training Epoch: 45 [45056/50000]	Loss: 0.6533	LR: 0.025000
Training Epoch: 45 [45184/50000]	Loss: 0.6322	LR: 0.025000
Training Epoch: 45 [45312/50000]	Loss: 0.5200	LR: 0.025000
Training Epoch: 45 [45440/50000]	Loss: 0.7936	LR: 0.025000
Training Epoch: 45 [45568/50000]	Loss: 0.5750	LR: 0.025000
Training Epoch: 45 [45696/50000]	Loss: 0.7920	LR: 0.025000
Training Epoch: 45 [45824/50000]	Loss: 0.7687	LR: 0.025000
Training Epoch: 45 [45952/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 45 [46080/50000]	Loss: 0.5683	LR: 0.025000
Training Epoch: 45 [46208/50000]	Loss: 0.6810	LR: 0.025000
Training Epoch: 45 [46336/50000]	Loss: 0.6982	LR: 0.025000
Training Epoch: 45 [46464/50000]	Loss: 0.4600	LR: 0.025000
Training Epoch: 45 [46592/50000]	Loss: 0.5751	LR: 0.025000
Training Epoch: 45 [46720/50000]	Loss: 0.6717	LR: 0.025000
Training Epoch: 45 [46848/50000]	Loss: 0.6433	LR: 0.025000
Training Epoch: 45 [46976/50000]	Loss: 0.7213	LR: 0.025000
Training Epoch: 45 [47104/50000]	Loss: 0.5376	LR: 0.025000
Training Epoch: 45 [47232/50000]	Loss: 0.5421	LR: 0.025000
Training Epoch: 45 [47360/50000]	Loss: 0.5303	LR: 0.025000
Training Epoch: 45 [47488/50000]	Loss: 0.6242	LR: 0.025000
Training Epoch: 45 [47616/50000]	Loss: 0.7308	LR: 0.025000
Training Epoch: 45 [47744/50000]	Loss: 0.5922	LR: 0.025000
Training Epoch: 45 [47872/50000]	Loss: 0.5564	LR: 0.025000
Training Epoch: 45 [48000/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 45 [48128/50000]	Loss: 0.5009	LR: 0.025000
Training Epoch: 45 [48256/50000]	Loss: 0.6325	LR: 0.025000
Training Epoch: 45 [48384/50000]	Loss: 0.7722	LR: 0.025000
Training Epoch: 45 [48512/50000]	Loss: 0.4986	LR: 0.025000
Training Epoch: 45 [48640/50000]	Loss: 0.5921	LR: 0.025000
Training Epoch: 45 [48768/50000]	Loss: 0.5348	LR: 0.025000
Training Epoch: 45 [48896/50000]	Loss: 0.5036	LR: 0.025000
Training Epoch: 45 [49024/50000]	Loss: 0.4515	LR: 0.025000
Training Epoch: 45 [49152/50000]	Loss: 0.6744	LR: 0.025000
Training Epoch: 45 [49280/50000]	Loss: 0.6607	LR: 0.025000
Training Epoch: 45 [49408/50000]	Loss: 0.5400	LR: 0.025000
Training Epoch: 45 [49536/50000]	Loss: 0.5800	LR: 0.025000
Training Epoch: 45 [49664/50000]	Loss: 0.6182	LR: 0.025000
Training Epoch: 45 [49792/50000]	Loss: 0.5132	LR: 0.025000
Training Epoch: 45 [49920/50000]	Loss: 0.6023	LR: 0.025000
Training Epoch: 45 [50000/50000]	Loss: 0.8456	LR: 0.025000
epoch 45 training time consumed: 53.93s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  165153 GB |  165152 GB |
|       from large pool |  123392 KB |    1034 MB |  164990 GB |  164990 GB |
|       from small pool |   10798 KB |      13 MB |     162 GB |     162 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  165153 GB |  165152 GB |
|       from large pool |  123392 KB |    1034 MB |  164990 GB |  164990 GB |
|       from small pool |   10798 KB |      13 MB |     162 GB |     162 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   72678 GB |   72678 GB |
|       from large pool |  155136 KB |  433088 KB |   72498 GB |   72498 GB |
|       from small pool |    1490 KB |    3494 KB |     179 GB |     179 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    6372 K  |    6372 K  |
|       from large pool |      24    |      65    |    3326 K  |    3326 K  |
|       from small pool |     231    |     274    |    3046 K  |    3046 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    6372 K  |    6372 K  |
|       from large pool |      24    |      65    |    3326 K  |    3326 K  |
|       from small pool |     231    |     274    |    3046 K  |    3046 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3150 K  |    3150 K  |
|       from large pool |       9    |      14    |    1610 K  |    1610 K  |
|       from small pool |      12    |      16    |    1540 K  |    1540 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 45, Average loss: 0.0103, Accuracy: 0.6651, Time consumed:3.44s

Training Epoch: 46 [128/50000]	Loss: 0.4713	LR: 0.025000
Training Epoch: 46 [256/50000]	Loss: 0.4392	LR: 0.025000
Training Epoch: 46 [384/50000]	Loss: 0.2699	LR: 0.025000
Training Epoch: 46 [512/50000]	Loss: 0.4377	LR: 0.025000
Training Epoch: 46 [640/50000]	Loss: 0.5109	LR: 0.025000
Training Epoch: 46 [768/50000]	Loss: 0.7024	LR: 0.025000
Training Epoch: 46 [896/50000]	Loss: 0.4577	LR: 0.025000
Training Epoch: 46 [1024/50000]	Loss: 0.4333	LR: 0.025000
Training Epoch: 46 [1152/50000]	Loss: 0.3949	LR: 0.025000
Training Epoch: 46 [1280/50000]	Loss: 0.5028	LR: 0.025000
Training Epoch: 46 [1408/50000]	Loss: 0.5367	LR: 0.025000
Training Epoch: 46 [1536/50000]	Loss: 0.4816	LR: 0.025000
Training Epoch: 46 [1664/50000]	Loss: 0.4386	LR: 0.025000
Training Epoch: 46 [1792/50000]	Loss: 0.4585	LR: 0.025000
Training Epoch: 46 [1920/50000]	Loss: 0.4917	LR: 0.025000
Training Epoch: 46 [2048/50000]	Loss: 0.4257	LR: 0.025000
Training Epoch: 46 [2176/50000]	Loss: 0.6945	LR: 0.025000
Training Epoch: 46 [2304/50000]	Loss: 0.5159	LR: 0.025000
Training Epoch: 46 [2432/50000]	Loss: 0.5627	LR: 0.025000
Training Epoch: 46 [2560/50000]	Loss: 0.6655	LR: 0.025000
Training Epoch: 46 [2688/50000]	Loss: 0.4604	LR: 0.025000
Training Epoch: 46 [2816/50000]	Loss: 0.5191	LR: 0.025000
Training Epoch: 46 [2944/50000]	Loss: 0.4080	LR: 0.025000
Training Epoch: 46 [3072/50000]	Loss: 0.4386	LR: 0.025000
Training Epoch: 46 [3200/50000]	Loss: 0.5136	LR: 0.025000
Training Epoch: 46 [3328/50000]	Loss: 0.4139	LR: 0.025000
Training Epoch: 46 [3456/50000]	Loss: 0.4949	LR: 0.025000
Training Epoch: 46 [3584/50000]	Loss: 0.4941	LR: 0.025000
Training Epoch: 46 [3712/50000]	Loss: 0.5649	LR: 0.025000
Training Epoch: 46 [3840/50000]	Loss: 0.3683	LR: 0.025000
Training Epoch: 46 [3968/50000]	Loss: 0.4807	LR: 0.025000
Training Epoch: 46 [4096/50000]	Loss: 0.5555	LR: 0.025000
Training Epoch: 46 [4224/50000]	Loss: 0.4169	LR: 0.025000
Training Epoch: 46 [4352/50000]	Loss: 0.4777	LR: 0.025000
Training Epoch: 46 [4480/50000]	Loss: 0.6505	LR: 0.025000
Training Epoch: 46 [4608/50000]	Loss: 0.6077	LR: 0.025000
Training Epoch: 46 [4736/50000]	Loss: 0.4399	LR: 0.025000
Training Epoch: 46 [4864/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 46 [4992/50000]	Loss: 0.4666	LR: 0.025000
Training Epoch: 46 [5120/50000]	Loss: 0.4793	LR: 0.025000
Training Epoch: 46 [5248/50000]	Loss: 0.4924	LR: 0.025000
Training Epoch: 46 [5376/50000]	Loss: 0.6041	LR: 0.025000
Training Epoch: 46 [5504/50000]	Loss: 0.4743	LR: 0.025000
Training Epoch: 46 [5632/50000]	Loss: 0.5332	LR: 0.025000
Training Epoch: 46 [5760/50000]	Loss: 0.6283	LR: 0.025000
Training Epoch: 46 [5888/50000]	Loss: 0.4590	LR: 0.025000
Training Epoch: 46 [6016/50000]	Loss: 0.4401	LR: 0.025000
Training Epoch: 46 [6144/50000]	Loss: 0.4635	LR: 0.025000
Training Epoch: 46 [6272/50000]	Loss: 0.3732	LR: 0.025000
Training Epoch: 46 [6400/50000]	Loss: 0.6073	LR: 0.025000
Training Epoch: 46 [6528/50000]	Loss: 0.4928	LR: 0.025000
Training Epoch: 46 [6656/50000]	Loss: 0.5968	LR: 0.025000
Training Epoch: 46 [6784/50000]	Loss: 0.4221	LR: 0.025000
Training Epoch: 46 [6912/50000]	Loss: 0.5125	LR: 0.025000
Training Epoch: 46 [7040/50000]	Loss: 0.4617	LR: 0.025000
Training Epoch: 46 [7168/50000]	Loss: 0.5974	LR: 0.025000
Training Epoch: 46 [7296/50000]	Loss: 0.5382	LR: 0.025000
Training Epoch: 46 [7424/50000]	Loss: 0.5847	LR: 0.025000
Training Epoch: 46 [7552/50000]	Loss: 0.4173	LR: 0.025000
Training Epoch: 46 [7680/50000]	Loss: 0.4748	LR: 0.025000
Training Epoch: 46 [7808/50000]	Loss: 0.6143	LR: 0.025000
Training Epoch: 46 [7936/50000]	Loss: 0.3508	LR: 0.025000
Training Epoch: 46 [8064/50000]	Loss: 0.4799	LR: 0.025000
Training Epoch: 46 [8192/50000]	Loss: 0.5303	LR: 0.025000
Training Epoch: 46 [8320/50000]	Loss: 0.5231	LR: 0.025000
Training Epoch: 46 [8448/50000]	Loss: 0.5087	LR: 0.025000
Training Epoch: 46 [8576/50000]	Loss: 0.3311	LR: 0.025000
Training Epoch: 46 [8704/50000]	Loss: 0.4771	LR: 0.025000
Training Epoch: 46 [8832/50000]	Loss: 0.6361	LR: 0.025000
Training Epoch: 46 [8960/50000]	Loss: 0.4258	LR: 0.025000
Training Epoch: 46 [9088/50000]	Loss: 0.4439	LR: 0.025000
Training Epoch: 46 [9216/50000]	Loss: 0.4923	LR: 0.025000
Training Epoch: 46 [9344/50000]	Loss: 0.5371	LR: 0.025000
Training Epoch: 46 [9472/50000]	Loss: 0.6061	LR: 0.025000
Training Epoch: 46 [9600/50000]	Loss: 0.4327	LR: 0.025000
Training Epoch: 46 [9728/50000]	Loss: 0.6320	LR: 0.025000
Training Epoch: 46 [9856/50000]	Loss: 0.4386	LR: 0.025000
Training Epoch: 46 [9984/50000]	Loss: 0.6153	LR: 0.025000
Training Epoch: 46 [10112/50000]	Loss: 0.4464	LR: 0.025000
Training Epoch: 46 [10240/50000]	Loss: 0.6782	LR: 0.025000
Training Epoch: 46 [10368/50000]	Loss: 0.5998	LR: 0.025000
Training Epoch: 46 [10496/50000]	Loss: 0.6503	LR: 0.025000
Training Epoch: 46 [10624/50000]	Loss: 0.5238	LR: 0.025000
Training Epoch: 46 [10752/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 46 [10880/50000]	Loss: 0.5304	LR: 0.025000
Training Epoch: 46 [11008/50000]	Loss: 0.4957	LR: 0.025000
Training Epoch: 46 [11136/50000]	Loss: 0.5863	LR: 0.025000
Training Epoch: 46 [11264/50000]	Loss: 0.4703	LR: 0.025000
Training Epoch: 46 [11392/50000]	Loss: 0.4168	LR: 0.025000
Training Epoch: 46 [11520/50000]	Loss: 0.5041	LR: 0.025000
Training Epoch: 46 [11648/50000]	Loss: 0.5447	LR: 0.025000
Training Epoch: 46 [11776/50000]	Loss: 0.5165	LR: 0.025000
Training Epoch: 46 [11904/50000]	Loss: 0.8402	LR: 0.025000
Training Epoch: 46 [12032/50000]	Loss: 0.5818	LR: 0.025000
Training Epoch: 46 [12160/50000]	Loss: 0.5048	LR: 0.025000
Training Epoch: 46 [12288/50000]	Loss: 0.3917	LR: 0.025000
Training Epoch: 46 [12416/50000]	Loss: 0.3846	LR: 0.025000
Training Epoch: 46 [12544/50000]	Loss: 0.5498	LR: 0.025000
Training Epoch: 46 [12672/50000]	Loss: 0.5121	LR: 0.025000
Training Epoch: 46 [12800/50000]	Loss: 0.4113	LR: 0.025000
Training Epoch: 46 [12928/50000]	Loss: 0.5751	LR: 0.025000
Training Epoch: 46 [13056/50000]	Loss: 0.5159	LR: 0.025000
Training Epoch: 46 [13184/50000]	Loss: 0.4404	LR: 0.025000
Training Epoch: 46 [13312/50000]	Loss: 0.5220	LR: 0.025000
Training Epoch: 46 [13440/50000]	Loss: 0.5262	LR: 0.025000
Training Epoch: 46 [13568/50000]	Loss: 0.6634	LR: 0.025000
Training Epoch: 46 [13696/50000]	Loss: 0.5439	LR: 0.025000
Training Epoch: 46 [13824/50000]	Loss: 0.3531	LR: 0.025000
Training Epoch: 46 [13952/50000]	Loss: 0.5124	LR: 0.025000
Training Epoch: 46 [14080/50000]	Loss: 0.5020	LR: 0.025000
Training Epoch: 46 [14208/50000]	Loss: 0.5676	LR: 0.025000
Training Epoch: 46 [14336/50000]	Loss: 0.4587	LR: 0.025000
Training Epoch: 46 [14464/50000]	Loss: 0.4676	LR: 0.025000
Training Epoch: 46 [14592/50000]	Loss: 0.5163	LR: 0.025000
Training Epoch: 46 [14720/50000]	Loss: 0.4343	LR: 0.025000
Training Epoch: 46 [14848/50000]	Loss: 0.4635	LR: 0.025000
Training Epoch: 46 [14976/50000]	Loss: 0.5570	LR: 0.025000
Training Epoch: 46 [15104/50000]	Loss: 0.4235	LR: 0.025000
Training Epoch: 46 [15232/50000]	Loss: 0.4739	LR: 0.025000
Training Epoch: 46 [15360/50000]	Loss: 0.5565	LR: 0.025000
Training Epoch: 46 [15488/50000]	Loss: 0.4432	LR: 0.025000
Training Epoch: 46 [15616/50000]	Loss: 0.5187	LR: 0.025000
Training Epoch: 46 [15744/50000]	Loss: 0.4263	LR: 0.025000
Training Epoch: 46 [15872/50000]	Loss: 0.6326	LR: 0.025000
Training Epoch: 46 [16000/50000]	Loss: 0.4467	LR: 0.025000
Training Epoch: 46 [16128/50000]	Loss: 0.4610	LR: 0.025000
Training Epoch: 46 [16256/50000]	Loss: 0.4696	LR: 0.025000
Training Epoch: 46 [16384/50000]	Loss: 0.6971	LR: 0.025000
Training Epoch: 46 [16512/50000]	Loss: 0.4628	LR: 0.025000
Training Epoch: 46 [16640/50000]	Loss: 0.4876	LR: 0.025000
Training Epoch: 46 [16768/50000]	Loss: 0.5725	LR: 0.025000
Training Epoch: 46 [16896/50000]	Loss: 0.5106	LR: 0.025000
Training Epoch: 46 [17024/50000]	Loss: 0.5095	LR: 0.025000
Training Epoch: 46 [17152/50000]	Loss: 0.3911	LR: 0.025000
Training Epoch: 46 [17280/50000]	Loss: 0.4327	LR: 0.025000
Training Epoch: 46 [17408/50000]	Loss: 0.4427	LR: 0.025000
Training Epoch: 46 [17536/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 46 [17664/50000]	Loss: 0.5456	LR: 0.025000
Training Epoch: 46 [17792/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 46 [17920/50000]	Loss: 0.4492	LR: 0.025000
Training Epoch: 46 [18048/50000]	Loss: 0.6320	LR: 0.025000
Training Epoch: 46 [18176/50000]	Loss: 0.5528	LR: 0.025000
Training Epoch: 46 [18304/50000]	Loss: 0.6501	LR: 0.025000
Training Epoch: 46 [18432/50000]	Loss: 0.4674	LR: 0.025000
Training Epoch: 46 [18560/50000]	Loss: 0.3759	LR: 0.025000
Training Epoch: 46 [18688/50000]	Loss: 0.5964	LR: 0.025000
Training Epoch: 46 [18816/50000]	Loss: 0.6431	LR: 0.025000
Training Epoch: 46 [18944/50000]	Loss: 0.5071	LR: 0.025000
Training Epoch: 46 [19072/50000]	Loss: 0.4904	LR: 0.025000
Training Epoch: 46 [19200/50000]	Loss: 0.5519	LR: 0.025000
Training Epoch: 46 [19328/50000]	Loss: 0.5362	LR: 0.025000
Training Epoch: 46 [19456/50000]	Loss: 0.4862	LR: 0.025000
Training Epoch: 46 [19584/50000]	Loss: 0.5484	LR: 0.025000
Training Epoch: 46 [19712/50000]	Loss: 0.4937	LR: 0.025000
Training Epoch: 46 [19840/50000]	Loss: 0.6306	LR: 0.025000
Training Epoch: 46 [19968/50000]	Loss: 0.5385	LR: 0.025000
Training Epoch: 46 [20096/50000]	Loss: 0.4548	LR: 0.025000
Training Epoch: 46 [20224/50000]	Loss: 0.5984	LR: 0.025000
Training Epoch: 46 [20352/50000]	Loss: 0.4828	LR: 0.025000
Training Epoch: 46 [20480/50000]	Loss: 0.5524	LR: 0.025000
Training Epoch: 46 [20608/50000]	Loss: 0.4938	LR: 0.025000
Training Epoch: 46 [20736/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 46 [20864/50000]	Loss: 0.5630	LR: 0.025000
Training Epoch: 46 [20992/50000]	Loss: 0.5313	LR: 0.025000
Training Epoch: 46 [21120/50000]	Loss: 0.5450	LR: 0.025000
Training Epoch: 46 [21248/50000]	Loss: 0.5992	LR: 0.025000
Training Epoch: 46 [21376/50000]	Loss: 0.6358	LR: 0.025000
Training Epoch: 46 [21504/50000]	Loss: 0.4584	LR: 0.025000
Training Epoch: 46 [21632/50000]	Loss: 0.4460	LR: 0.025000
Training Epoch: 46 [21760/50000]	Loss: 0.5059	LR: 0.025000
Training Epoch: 46 [21888/50000]	Loss: 0.4678	LR: 0.025000
Training Epoch: 46 [22016/50000]	Loss: 0.5396	LR: 0.025000
Training Epoch: 46 [22144/50000]	Loss: 0.6392	LR: 0.025000
Training Epoch: 46 [22272/50000]	Loss: 0.5994	LR: 0.025000
Training Epoch: 46 [22400/50000]	Loss: 0.7414	LR: 0.025000
Training Epoch: 46 [22528/50000]	Loss: 0.3669	LR: 0.025000
Training Epoch: 46 [22656/50000]	Loss: 0.5599	LR: 0.025000
Training Epoch: 46 [22784/50000]	Loss: 0.4756	LR: 0.025000
Training Epoch: 46 [22912/50000]	Loss: 0.4158	LR: 0.025000
Training Epoch: 46 [23040/50000]	Loss: 0.5320	LR: 0.025000
Training Epoch: 46 [23168/50000]	Loss: 0.6025	LR: 0.025000
Training Epoch: 46 [23296/50000]	Loss: 0.6020	LR: 0.025000
Training Epoch: 46 [23424/50000]	Loss: 0.5219	LR: 0.025000
Training Epoch: 46 [23552/50000]	Loss: 0.4113	LR: 0.025000
Training Epoch: 46 [23680/50000]	Loss: 0.4761	LR: 0.025000
Training Epoch: 46 [23808/50000]	Loss: 0.4172	LR: 0.025000
Training Epoch: 46 [23936/50000]	Loss: 0.3999	LR: 0.025000
Training Epoch: 46 [24064/50000]	Loss: 0.3761	LR: 0.025000
Training Epoch: 46 [24192/50000]	Loss: 0.6170	LR: 0.025000
Training Epoch: 46 [24320/50000]	Loss: 0.6037	LR: 0.025000
Training Epoch: 46 [24448/50000]	Loss: 0.6037	LR: 0.025000
Training Epoch: 46 [24576/50000]	Loss: 0.5163	LR: 0.025000
Training Epoch: 46 [24704/50000]	Loss: 0.3517	LR: 0.025000
Training Epoch: 46 [24832/50000]	Loss: 0.4548	LR: 0.025000
Training Epoch: 46 [24960/50000]	Loss: 0.4266	LR: 0.025000
Training Epoch: 46 [25088/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 46 [25216/50000]	Loss: 0.5709	LR: 0.025000
Training Epoch: 46 [25344/50000]	Loss: 0.7616	LR: 0.025000
Training Epoch: 46 [25472/50000]	Loss: 0.4466	LR: 0.025000
Training Epoch: 46 [25600/50000]	Loss: 0.6308	LR: 0.025000
Training Epoch: 46 [25728/50000]	Loss: 0.6688	LR: 0.025000
Training Epoch: 46 [25856/50000]	Loss: 0.5045	LR: 0.025000
Training Epoch: 46 [25984/50000]	Loss: 0.4382	LR: 0.025000
Training Epoch: 46 [26112/50000]	Loss: 0.5780	LR: 0.025000
Training Epoch: 46 [26240/50000]	Loss: 0.4153	LR: 0.025000
Training Epoch: 46 [26368/50000]	Loss: 0.6387	LR: 0.025000
Training Epoch: 46 [26496/50000]	Loss: 0.5981	LR: 0.025000
Training Epoch: 46 [26624/50000]	Loss: 0.4411	LR: 0.025000
Training Epoch: 46 [26752/50000]	Loss: 0.4812	LR: 0.025000
Training Epoch: 46 [26880/50000]	Loss: 0.6132	LR: 0.025000
Training Epoch: 46 [27008/50000]	Loss: 0.5879	LR: 0.025000
Training Epoch: 46 [27136/50000]	Loss: 0.4101	LR: 0.025000
Training Epoch: 46 [27264/50000]	Loss: 0.5217	LR: 0.025000
Training Epoch: 46 [27392/50000]	Loss: 0.4553	LR: 0.025000
Training Epoch: 46 [27520/50000]	Loss: 0.4716	LR: 0.025000
Training Epoch: 46 [27648/50000]	Loss: 0.5569	LR: 0.025000
Training Epoch: 46 [27776/50000]	Loss: 0.5014	LR: 0.025000
Training Epoch: 46 [27904/50000]	Loss: 0.6038	LR: 0.025000
Training Epoch: 46 [28032/50000]	Loss: 0.5502	LR: 0.025000
Training Epoch: 46 [28160/50000]	Loss: 0.5481	LR: 0.025000
Training Epoch: 46 [28288/50000]	Loss: 0.5365	LR: 0.025000
Training Epoch: 46 [28416/50000]	Loss: 0.5215	LR: 0.025000
Training Epoch: 46 [28544/50000]	Loss: 0.4940	LR: 0.025000
Training Epoch: 46 [28672/50000]	Loss: 0.6035	LR: 0.025000
Training Epoch: 46 [28800/50000]	Loss: 0.6948	LR: 0.025000
Training Epoch: 46 [28928/50000]	Loss: 0.6442	LR: 0.025000
Training Epoch: 46 [29056/50000]	Loss: 0.5405	LR: 0.025000
Training Epoch: 46 [29184/50000]	Loss: 0.5548	LR: 0.025000
Training Epoch: 46 [29312/50000]	Loss: 0.6024	LR: 0.025000
Training Epoch: 46 [29440/50000]	Loss: 0.5014	LR: 0.025000
Training Epoch: 46 [29568/50000]	Loss: 0.5733	LR: 0.025000
Training Epoch: 46 [29696/50000]	Loss: 0.5600	LR: 0.025000
Training Epoch: 46 [29824/50000]	Loss: 0.4844	LR: 0.025000
Training Epoch: 46 [29952/50000]	Loss: 0.6203	LR: 0.025000
Training Epoch: 46 [30080/50000]	Loss: 0.4725	LR: 0.025000
Training Epoch: 46 [30208/50000]	Loss: 0.6063	LR: 0.025000
Training Epoch: 46 [30336/50000]	Loss: 0.5471	LR: 0.025000
Training Epoch: 46 [30464/50000]	Loss: 0.5045	LR: 0.025000
Training Epoch: 46 [30592/50000]	Loss: 0.8102	LR: 0.025000
Training Epoch: 46 [30720/50000]	Loss: 0.6977	LR: 0.025000
Training Epoch: 46 [30848/50000]	Loss: 0.5681	LR: 0.025000
Training Epoch: 46 [30976/50000]	Loss: 0.5118	LR: 0.025000
Training Epoch: 46 [31104/50000]	Loss: 0.4845	LR: 0.025000
Training Epoch: 46 [31232/50000]	Loss: 0.6950	LR: 0.025000
Training Epoch: 46 [31360/50000]	Loss: 0.6918	LR: 0.025000
Training Epoch: 46 [31488/50000]	Loss: 0.5057	LR: 0.025000
Training Epoch: 46 [31616/50000]	Loss: 0.6156	LR: 0.025000
Training Epoch: 46 [31744/50000]	Loss: 0.5272	LR: 0.025000
Training Epoch: 46 [31872/50000]	Loss: 0.5349	LR: 0.025000
Training Epoch: 46 [32000/50000]	Loss: 0.7213	LR: 0.025000
Training Epoch: 46 [32128/50000]	Loss: 0.6586	LR: 0.025000
Training Epoch: 46 [32256/50000]	Loss: 0.5182	LR: 0.025000
Training Epoch: 46 [32384/50000]	Loss: 0.5222	LR: 0.025000
Training Epoch: 46 [32512/50000]	Loss: 0.5741	LR: 0.025000
Training Epoch: 46 [32640/50000]	Loss: 0.7901	LR: 0.025000
Training Epoch: 46 [32768/50000]	Loss: 0.5230	LR: 0.025000
Training Epoch: 46 [32896/50000]	Loss: 0.8402	LR: 0.025000
Training Epoch: 46 [33024/50000]	Loss: 0.5535	LR: 0.025000
Training Epoch: 46 [33152/50000]	Loss: 0.5726	LR: 0.025000
Training Epoch: 46 [33280/50000]	Loss: 0.5430	LR: 0.025000
Training Epoch: 46 [33408/50000]	Loss: 0.7230	LR: 0.025000
Training Epoch: 46 [33536/50000]	Loss: 0.6125	LR: 0.025000
Training Epoch: 46 [33664/50000]	Loss: 0.4518	LR: 0.025000
Training Epoch: 46 [33792/50000]	Loss: 0.5317	LR: 0.025000
Training Epoch: 46 [33920/50000]	Loss: 0.6605	LR: 0.025000
Training Epoch: 46 [34048/50000]	Loss: 0.5523	LR: 0.025000
Training Epoch: 46 [34176/50000]	Loss: 0.6222	LR: 0.025000
Training Epoch: 46 [34304/50000]	Loss: 0.6150	LR: 0.025000
Training Epoch: 46 [34432/50000]	Loss: 0.6457	LR: 0.025000
Training Epoch: 46 [34560/50000]	Loss: 0.5196	LR: 0.025000
Training Epoch: 46 [34688/50000]	Loss: 0.5527	LR: 0.025000
Training Epoch: 46 [34816/50000]	Loss: 0.8653	LR: 0.025000
Training Epoch: 46 [34944/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 46 [35072/50000]	Loss: 0.5308	LR: 0.025000
Training Epoch: 46 [35200/50000]	Loss: 0.5909	LR: 0.025000
Training Epoch: 46 [35328/50000]	Loss: 0.7026	LR: 0.025000
Training Epoch: 46 [35456/50000]	Loss: 0.4938	LR: 0.025000
Training Epoch: 46 [35584/50000]	Loss: 0.6499	LR: 0.025000
Training Epoch: 46 [35712/50000]	Loss: 0.5386	LR: 0.025000
Training Epoch: 46 [35840/50000]	Loss: 0.7346	LR: 0.025000
Training Epoch: 46 [35968/50000]	Loss: 0.6630	LR: 0.025000
Training Epoch: 46 [36096/50000]	Loss: 0.4970	LR: 0.025000
Training Epoch: 46 [36224/50000]	Loss: 0.5702	LR: 0.025000
Training Epoch: 46 [36352/50000]	Loss: 0.8332	LR: 0.025000
Training Epoch: 46 [36480/50000]	Loss: 0.7330	LR: 0.025000
Training Epoch: 46 [36608/50000]	Loss: 0.6840	LR: 0.025000
Training Epoch: 46 [36736/50000]	Loss: 0.6906	LR: 0.025000
Training Epoch: 46 [36864/50000]	Loss: 0.5353	LR: 0.025000
Training Epoch: 46 [36992/50000]	Loss: 0.5104	LR: 0.025000
Training Epoch: 46 [37120/50000]	Loss: 0.5263	LR: 0.025000
Training Epoch: 46 [37248/50000]	Loss: 0.5148	LR: 0.025000
Training Epoch: 46 [37376/50000]	Loss: 0.7459	LR: 0.025000
Training Epoch: 46 [37504/50000]	Loss: 0.2986	LR: 0.025000
Training Epoch: 46 [37632/50000]	Loss: 0.3618	LR: 0.025000
Training Epoch: 46 [37760/50000]	Loss: 0.5114	LR: 0.025000
Training Epoch: 46 [37888/50000]	Loss: 0.4288	LR: 0.025000
Training Epoch: 46 [38016/50000]	Loss: 0.6181	LR: 0.025000
Training Epoch: 46 [38144/50000]	Loss: 0.7177	LR: 0.025000
Training Epoch: 46 [38272/50000]	Loss: 0.4898	LR: 0.025000
Training Epoch: 46 [38400/50000]	Loss: 0.6261	LR: 0.025000
Training Epoch: 46 [38528/50000]	Loss: 0.6393	LR: 0.025000
Training Epoch: 46 [38656/50000]	Loss: 0.7074	LR: 0.025000
Training Epoch: 46 [38784/50000]	Loss: 0.5382	LR: 0.025000
Training Epoch: 46 [38912/50000]	Loss: 0.4214	LR: 0.025000
Training Epoch: 46 [39040/50000]	Loss: 0.5933	LR: 0.025000
Training Epoch: 46 [39168/50000]	Loss: 0.5584	LR: 0.025000
Training Epoch: 46 [39296/50000]	Loss: 0.5979	LR: 0.025000
Training Epoch: 46 [39424/50000]	Loss: 0.5982	LR: 0.025000
Training Epoch: 46 [39552/50000]	Loss: 0.5732	LR: 0.025000
Training Epoch: 46 [39680/50000]	Loss: 0.4368	LR: 0.025000
Training Epoch: 46 [39808/50000]	Loss: 0.5723	LR: 0.025000
Training Epoch: 46 [39936/50000]	Loss: 0.5520	LR: 0.025000
Training Epoch: 46 [40064/50000]	Loss: 0.4666	LR: 0.025000
Training Epoch: 46 [40192/50000]	Loss: 0.4103	LR: 0.025000
Training Epoch: 46 [40320/50000]	Loss: 0.5912	LR: 0.025000
Training Epoch: 46 [40448/50000]	Loss: 0.5421	LR: 0.025000
Training Epoch: 46 [40576/50000]	Loss: 0.6939	LR: 0.025000
Training Epoch: 46 [40704/50000]	Loss: 0.6536	LR: 0.025000
Training Epoch: 46 [40832/50000]	Loss: 0.5035	LR: 0.025000
Training Epoch: 46 [40960/50000]	Loss: 0.6728	LR: 0.025000
Training Epoch: 46 [41088/50000]	Loss: 0.5000	LR: 0.025000
Training Epoch: 46 [41216/50000]	Loss: 0.5989	LR: 0.025000
Training Epoch: 46 [41344/50000]	Loss: 0.5842	LR: 0.025000
Training Epoch: 46 [41472/50000]	Loss: 0.5758	LR: 0.025000
Training Epoch: 46 [41600/50000]	Loss: 0.6863	LR: 0.025000
Training Epoch: 46 [41728/50000]	Loss: 0.6011	LR: 0.025000
Training Epoch: 46 [41856/50000]	Loss: 0.4562	LR: 0.025000
Training Epoch: 46 [41984/50000]	Loss: 0.5825	LR: 0.025000
Training Epoch: 46 [42112/50000]	Loss: 0.5664	LR: 0.025000
Training Epoch: 46 [42240/50000]	Loss: 0.4781	LR: 0.025000
Training Epoch: 46 [42368/50000]	Loss: 0.4465	LR: 0.025000
Training Epoch: 46 [42496/50000]	Loss: 0.4685	LR: 0.025000
Training Epoch: 46 [42624/50000]	Loss: 0.5703	LR: 0.025000
Training Epoch: 46 [42752/50000]	Loss: 0.4160	LR: 0.025000
Training Epoch: 46 [42880/50000]	Loss: 0.5889	LR: 0.025000
Training Epoch: 46 [43008/50000]	Loss: 0.5790	LR: 0.025000
Training Epoch: 46 [43136/50000]	Loss: 0.5227	LR: 0.025000
Training Epoch: 46 [43264/50000]	Loss: 0.4891	LR: 0.025000
Training Epoch: 46 [43392/50000]	Loss: 0.6775	LR: 0.025000
Training Epoch: 46 [43520/50000]	Loss: 0.4886	LR: 0.025000
Training Epoch: 46 [43648/50000]	Loss: 0.5135	LR: 0.025000
Training Epoch: 46 [43776/50000]	Loss: 0.6725	LR: 0.025000
Training Epoch: 46 [43904/50000]	Loss: 0.5654	LR: 0.025000
Training Epoch: 46 [44032/50000]	Loss: 0.6814	LR: 0.025000
Training Epoch: 46 [44160/50000]	Loss: 0.5098	LR: 0.025000
Training Epoch: 46 [44288/50000]	Loss: 0.5801	LR: 0.025000
Training Epoch: 46 [44416/50000]	Loss: 0.6399	LR: 0.025000
Training Epoch: 46 [44544/50000]	Loss: 0.5850	LR: 0.025000
Training Epoch: 46 [44672/50000]	Loss: 0.5220	LR: 0.025000
Training Epoch: 46 [44800/50000]	Loss: 0.5249	LR: 0.025000
Training Epoch: 46 [44928/50000]	Loss: 0.6495	LR: 0.025000
Training Epoch: 46 [45056/50000]	Loss: 0.7351	LR: 0.025000
Training Epoch: 46 [45184/50000]	Loss: 0.6486	LR: 0.025000
Training Epoch: 46 [45312/50000]	Loss: 0.4205	LR: 0.025000
Training Epoch: 46 [45440/50000]	Loss: 0.5690	LR: 0.025000
Training Epoch: 46 [45568/50000]	Loss: 0.6578	LR: 0.025000
Training Epoch: 46 [45696/50000]	Loss: 0.8162	LR: 0.025000
Training Epoch: 46 [45824/50000]	Loss: 0.5773	LR: 0.025000
Training Epoch: 46 [45952/50000]	Loss: 0.6110	LR: 0.025000
Training Epoch: 46 [46080/50000]	Loss: 0.6002	LR: 0.025000
Training Epoch: 46 [46208/50000]	Loss: 0.4838	LR: 0.025000
Training Epoch: 46 [46336/50000]	Loss: 0.5572	LR: 0.025000
Training Epoch: 46 [46464/50000]	Loss: 0.6770	LR: 0.025000
Training Epoch: 46 [46592/50000]	Loss: 0.6267	LR: 0.025000
Training Epoch: 46 [46720/50000]	Loss: 0.5934	LR: 0.025000
Training Epoch: 46 [46848/50000]	Loss: 0.5937	LR: 0.025000
Training Epoch: 46 [46976/50000]	Loss: 0.6583	LR: 0.025000
Training Epoch: 46 [47104/50000]	Loss: 0.4439	LR: 0.025000
Training Epoch: 46 [47232/50000]	Loss: 0.4745	LR: 0.025000
Training Epoch: 46 [47360/50000]	Loss: 0.5913	LR: 0.025000
Training Epoch: 46 [47488/50000]	Loss: 0.5946	LR: 0.025000
Training Epoch: 46 [47616/50000]	Loss: 0.5839	LR: 0.025000
Training Epoch: 46 [47744/50000]	Loss: 0.5146	LR: 0.025000
Training Epoch: 46 [47872/50000]	Loss: 0.4843	LR: 0.025000
Training Epoch: 46 [48000/50000]	Loss: 0.5810	LR: 0.025000
Training Epoch: 46 [48128/50000]	Loss: 0.6793	LR: 0.025000
Training Epoch: 46 [48256/50000]	Loss: 0.6033	LR: 0.025000
Training Epoch: 46 [48384/50000]	Loss: 0.4588	LR: 0.025000
Training Epoch: 46 [48512/50000]	Loss: 0.8528	LR: 0.025000
Training Epoch: 46 [48640/50000]	Loss: 0.6457	LR: 0.025000
Training Epoch: 46 [48768/50000]	Loss: 0.7149	LR: 0.025000
Training Epoch: 46 [48896/50000]	Loss: 0.5453	LR: 0.025000
Training Epoch: 46 [49024/50000]	Loss: 0.5445	LR: 0.025000
Training Epoch: 46 [49152/50000]	Loss: 0.7542	LR: 0.025000
Training Epoch: 46 [49280/50000]	Loss: 0.4178	LR: 0.025000
Training Epoch: 46 [49408/50000]	Loss: 0.6416	LR: 0.025000
Training Epoch: 46 [49536/50000]	Loss: 0.5257	LR: 0.025000
Training Epoch: 46 [49664/50000]	Loss: 0.5288	LR: 0.025000
Training Epoch: 46 [49792/50000]	Loss: 0.8584	LR: 0.025000
Training Epoch: 46 [49920/50000]	Loss: 0.4939	LR: 0.025000
Training Epoch: 46 [50000/50000]	Loss: 0.8486	LR: 0.025000
epoch 46 training time consumed: 53.98s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  168823 GB |  168822 GB |
|       from large pool |  123392 KB |    1034 MB |  168656 GB |  168656 GB |
|       from small pool |   10798 KB |      13 MB |     166 GB |     166 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  168823 GB |  168822 GB |
|       from large pool |  123392 KB |    1034 MB |  168656 GB |  168656 GB |
|       from small pool |   10798 KB |      13 MB |     166 GB |     166 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   74293 GB |   74293 GB |
|       from large pool |  155136 KB |  433088 KB |   74109 GB |   74109 GB |
|       from small pool |    1490 KB |    3494 KB |     183 GB |     183 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    6514 K  |    6514 K  |
|       from large pool |      24    |      65    |    3400 K  |    3400 K  |
|       from small pool |     231    |     274    |    3114 K  |    3113 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    6514 K  |    6514 K  |
|       from large pool |      24    |      65    |    3400 K  |    3400 K  |
|       from small pool |     231    |     274    |    3114 K  |    3113 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3220 K  |    3220 K  |
|       from large pool |       9    |      14    |    1645 K  |    1645 K  |
|       from small pool |      12    |      16    |    1574 K  |    1574 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 46, Average loss: 0.0103, Accuracy: 0.6700, Time consumed:3.46s

Training Epoch: 47 [128/50000]	Loss: 0.4387	LR: 0.025000
Training Epoch: 47 [256/50000]	Loss: 0.6149	LR: 0.025000
Training Epoch: 47 [384/50000]	Loss: 0.5953	LR: 0.025000
Training Epoch: 47 [512/50000]	Loss: 0.4780	LR: 0.025000
Training Epoch: 47 [640/50000]	Loss: 0.5308	LR: 0.025000
Training Epoch: 47 [768/50000]	Loss: 0.4382	LR: 0.025000
Training Epoch: 47 [896/50000]	Loss: 0.5917	LR: 0.025000
Training Epoch: 47 [1024/50000]	Loss: 0.3096	LR: 0.025000
Training Epoch: 47 [1152/50000]	Loss: 0.6631	LR: 0.025000
Training Epoch: 47 [1280/50000]	Loss: 0.5091	LR: 0.025000
Training Epoch: 47 [1408/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 47 [1536/50000]	Loss: 0.4478	LR: 0.025000
Training Epoch: 47 [1664/50000]	Loss: 0.4880	LR: 0.025000
Training Epoch: 47 [1792/50000]	Loss: 0.5910	LR: 0.025000
Training Epoch: 47 [1920/50000]	Loss: 0.6194	LR: 0.025000
Training Epoch: 47 [2048/50000]	Loss: 0.4269	LR: 0.025000
Training Epoch: 47 [2176/50000]	Loss: 0.4099	LR: 0.025000
Training Epoch: 47 [2304/50000]	Loss: 0.4985	LR: 0.025000
Training Epoch: 47 [2432/50000]	Loss: 0.5042	LR: 0.025000
Training Epoch: 47 [2560/50000]	Loss: 0.3885	LR: 0.025000
Training Epoch: 47 [2688/50000]	Loss: 0.5894	LR: 0.025000
Training Epoch: 47 [2816/50000]	Loss: 0.5726	LR: 0.025000
Training Epoch: 47 [2944/50000]	Loss: 0.4633	LR: 0.025000
Training Epoch: 47 [3072/50000]	Loss: 0.4904	LR: 0.025000
Training Epoch: 47 [3200/50000]	Loss: 0.5663	LR: 0.025000
Training Epoch: 47 [3328/50000]	Loss: 0.4679	LR: 0.025000
Training Epoch: 47 [3456/50000]	Loss: 0.5290	LR: 0.025000
Training Epoch: 47 [3584/50000]	Loss: 0.4042	LR: 0.025000
Training Epoch: 47 [3712/50000]	Loss: 0.3976	LR: 0.025000
Training Epoch: 47 [3840/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 47 [3968/50000]	Loss: 0.6218	LR: 0.025000
Training Epoch: 47 [4096/50000]	Loss: 0.4638	LR: 0.025000
Training Epoch: 47 [4224/50000]	Loss: 0.4159	LR: 0.025000
Training Epoch: 47 [4352/50000]	Loss: 0.6230	LR: 0.025000
Training Epoch: 47 [4480/50000]	Loss: 0.5643	LR: 0.025000
Training Epoch: 47 [4608/50000]	Loss: 0.4655	LR: 0.025000
Training Epoch: 47 [4736/50000]	Loss: 0.4963	LR: 0.025000
Training Epoch: 47 [4864/50000]	Loss: 0.4973	LR: 0.025000
Training Epoch: 47 [4992/50000]	Loss: 0.3082	LR: 0.025000
Training Epoch: 47 [5120/50000]	Loss: 0.4923	LR: 0.025000
Training Epoch: 47 [5248/50000]	Loss: 0.4889	LR: 0.025000
Training Epoch: 47 [5376/50000]	Loss: 0.4074	LR: 0.025000
Training Epoch: 47 [5504/50000]	Loss: 0.5207	LR: 0.025000
Training Epoch: 47 [5632/50000]	Loss: 0.4848	LR: 0.025000
Training Epoch: 47 [5760/50000]	Loss: 0.4210	LR: 0.025000
Training Epoch: 47 [5888/50000]	Loss: 0.6000	LR: 0.025000
Training Epoch: 47 [6016/50000]	Loss: 0.5999	LR: 0.025000
Training Epoch: 47 [6144/50000]	Loss: 0.5757	LR: 0.025000
Training Epoch: 47 [6272/50000]	Loss: 0.4151	LR: 0.025000
Training Epoch: 47 [6400/50000]	Loss: 0.5670	LR: 0.025000
Training Epoch: 47 [6528/50000]	Loss: 0.4510	LR: 0.025000
Training Epoch: 47 [6656/50000]	Loss: 0.4028	LR: 0.025000
Training Epoch: 47 [6784/50000]	Loss: 0.5336	LR: 0.025000
Training Epoch: 47 [6912/50000]	Loss: 0.6456	LR: 0.025000
Training Epoch: 47 [7040/50000]	Loss: 0.5013	LR: 0.025000
Training Epoch: 47 [7168/50000]	Loss: 0.4392	LR: 0.025000
Training Epoch: 47 [7296/50000]	Loss: 0.4605	LR: 0.025000
Training Epoch: 47 [7424/50000]	Loss: 0.4957	LR: 0.025000
Training Epoch: 47 [7552/50000]	Loss: 0.4591	LR: 0.025000
Training Epoch: 47 [7680/50000]	Loss: 0.5188	LR: 0.025000
Training Epoch: 47 [7808/50000]	Loss: 0.6160	LR: 0.025000
Training Epoch: 47 [7936/50000]	Loss: 0.3982	LR: 0.025000
Training Epoch: 47 [8064/50000]	Loss: 0.4272	LR: 0.025000
Training Epoch: 47 [8192/50000]	Loss: 0.5429	LR: 0.025000
Training Epoch: 47 [8320/50000]	Loss: 0.5225	LR: 0.025000
Training Epoch: 47 [8448/50000]	Loss: 0.4835	LR: 0.025000
Training Epoch: 47 [8576/50000]	Loss: 0.5831	LR: 0.025000
Training Epoch: 47 [8704/50000]	Loss: 0.6153	LR: 0.025000
Training Epoch: 47 [8832/50000]	Loss: 0.5793	LR: 0.025000
Training Epoch: 47 [8960/50000]	Loss: 0.4940	LR: 0.025000
Training Epoch: 47 [9088/50000]	Loss: 0.6208	LR: 0.025000
Training Epoch: 47 [9216/50000]	Loss: 0.3805	LR: 0.025000
Training Epoch: 47 [9344/50000]	Loss: 0.4705	LR: 0.025000
Training Epoch: 47 [9472/50000]	Loss: 0.5603	LR: 0.025000
Training Epoch: 47 [9600/50000]	Loss: 0.5273	LR: 0.025000
Training Epoch: 47 [9728/50000]	Loss: 0.4805	LR: 0.025000
Training Epoch: 47 [9856/50000]	Loss: 0.5541	LR: 0.025000
Training Epoch: 47 [9984/50000]	Loss: 0.5260	LR: 0.025000
Training Epoch: 47 [10112/50000]	Loss: 0.5988	LR: 0.025000
Training Epoch: 47 [10240/50000]	Loss: 0.4692	LR: 0.025000
Training Epoch: 47 [10368/50000]	Loss: 0.3997	LR: 0.025000
Training Epoch: 47 [10496/50000]	Loss: 0.4552	LR: 0.025000
Training Epoch: 47 [10624/50000]	Loss: 0.5756	LR: 0.025000
Training Epoch: 47 [10752/50000]	Loss: 0.5151	LR: 0.025000
Training Epoch: 47 [10880/50000]	Loss: 0.5049	LR: 0.025000
Training Epoch: 47 [11008/50000]	Loss: 0.5221	LR: 0.025000
Training Epoch: 47 [11136/50000]	Loss: 0.5291	LR: 0.025000
Training Epoch: 47 [11264/50000]	Loss: 0.6136	LR: 0.025000
Training Epoch: 47 [11392/50000]	Loss: 0.4499	LR: 0.025000
Training Epoch: 47 [11520/50000]	Loss: 0.5431	LR: 0.025000
Training Epoch: 47 [11648/50000]	Loss: 0.5577	LR: 0.025000
Training Epoch: 47 [11776/50000]	Loss: 0.5314	LR: 0.025000
Training Epoch: 47 [11904/50000]	Loss: 0.5376	LR: 0.025000
Training Epoch: 47 [12032/50000]	Loss: 0.5168	LR: 0.025000
Training Epoch: 47 [12160/50000]	Loss: 0.4472	LR: 0.025000
Training Epoch: 47 [12288/50000]	Loss: 0.5372	LR: 0.025000
Training Epoch: 47 [12416/50000]	Loss: 0.5124	LR: 0.025000
Training Epoch: 47 [12544/50000]	Loss: 0.5899	LR: 0.025000
Training Epoch: 47 [12672/50000]	Loss: 0.5648	LR: 0.025000
Training Epoch: 47 [12800/50000]	Loss: 0.6628	LR: 0.025000
Training Epoch: 47 [12928/50000]	Loss: 0.3582	LR: 0.025000
Training Epoch: 47 [13056/50000]	Loss: 0.3294	LR: 0.025000
Training Epoch: 47 [13184/50000]	Loss: 0.6107	LR: 0.025000
Training Epoch: 47 [13312/50000]	Loss: 0.5889	LR: 0.025000
Training Epoch: 47 [13440/50000]	Loss: 0.3603	LR: 0.025000
Training Epoch: 47 [13568/50000]	Loss: 0.6358	LR: 0.025000
Training Epoch: 47 [13696/50000]	Loss: 0.6326	LR: 0.025000
Training Epoch: 47 [13824/50000]	Loss: 0.5202	LR: 0.025000
Training Epoch: 47 [13952/50000]	Loss: 0.5783	LR: 0.025000
Training Epoch: 47 [14080/50000]	Loss: 0.4671	LR: 0.025000
Training Epoch: 47 [14208/50000]	Loss: 0.5508	LR: 0.025000
Training Epoch: 47 [14336/50000]	Loss: 0.6258	LR: 0.025000
Training Epoch: 47 [14464/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 47 [14592/50000]	Loss: 0.4065	LR: 0.025000
Training Epoch: 47 [14720/50000]	Loss: 0.4506	LR: 0.025000
Training Epoch: 47 [14848/50000]	Loss: 0.5748	LR: 0.025000
Training Epoch: 47 [14976/50000]	Loss: 0.5231	LR: 0.025000
Training Epoch: 47 [15104/50000]	Loss: 0.5281	LR: 0.025000
Training Epoch: 47 [15232/50000]	Loss: 0.6304	LR: 0.025000
Training Epoch: 47 [15360/50000]	Loss: 0.4337	LR: 0.025000
Training Epoch: 47 [15488/50000]	Loss: 0.5948	LR: 0.025000
Training Epoch: 47 [15616/50000]	Loss: 0.6239	LR: 0.025000
Training Epoch: 47 [15744/50000]	Loss: 0.6090	LR: 0.025000
Training Epoch: 47 [15872/50000]	Loss: 0.4630	LR: 0.025000
Training Epoch: 47 [16000/50000]	Loss: 0.4830	LR: 0.025000
Training Epoch: 47 [16128/50000]	Loss: 0.6664	LR: 0.025000
Training Epoch: 47 [16256/50000]	Loss: 0.5428	LR: 0.025000
Training Epoch: 47 [16384/50000]	Loss: 0.5427	LR: 0.025000
Training Epoch: 47 [16512/50000]	Loss: 0.5915	LR: 0.025000
Training Epoch: 47 [16640/50000]	Loss: 0.5764	LR: 0.025000
Training Epoch: 47 [16768/50000]	Loss: 0.5453	LR: 0.025000
Training Epoch: 47 [16896/50000]	Loss: 0.4846	LR: 0.025000
Training Epoch: 47 [17024/50000]	Loss: 0.6434	LR: 0.025000
Training Epoch: 47 [17152/50000]	Loss: 0.4498	LR: 0.025000
Training Epoch: 47 [17280/50000]	Loss: 0.6466	LR: 0.025000
Training Epoch: 47 [17408/50000]	Loss: 0.4176	LR: 0.025000
Training Epoch: 47 [17536/50000]	Loss: 0.5435	LR: 0.025000
Training Epoch: 47 [17664/50000]	Loss: 0.6167	LR: 0.025000
Training Epoch: 47 [17792/50000]	Loss: 0.4955	LR: 0.025000
Training Epoch: 47 [17920/50000]	Loss: 0.6600	LR: 0.025000
Training Epoch: 47 [18048/50000]	Loss: 0.4849	LR: 0.025000
Training Epoch: 47 [18176/50000]	Loss: 0.5404	LR: 0.025000
Training Epoch: 47 [18304/50000]	Loss: 0.5024	LR: 0.025000
Training Epoch: 47 [18432/50000]	Loss: 0.5323	LR: 0.025000
Training Epoch: 47 [18560/50000]	Loss: 0.4857	LR: 0.025000
Training Epoch: 47 [18688/50000]	Loss: 0.6708	LR: 0.025000
Training Epoch: 47 [18816/50000]	Loss: 0.5070	LR: 0.025000
Training Epoch: 47 [18944/50000]	Loss: 0.4982	LR: 0.025000
Training Epoch: 47 [19072/50000]	Loss: 0.5076	LR: 0.025000
Training Epoch: 47 [19200/50000]	Loss: 0.6076	LR: 0.025000
Training Epoch: 47 [19328/50000]	Loss: 0.4033	LR: 0.025000
Training Epoch: 47 [19456/50000]	Loss: 0.4193	LR: 0.025000
Training Epoch: 47 [19584/50000]	Loss: 0.6050	LR: 0.025000
Training Epoch: 47 [19712/50000]	Loss: 0.5009	LR: 0.025000
Training Epoch: 47 [19840/50000]	Loss: 0.5280	LR: 0.025000
Training Epoch: 47 [19968/50000]	Loss: 0.5202	LR: 0.025000
Training Epoch: 47 [20096/50000]	Loss: 0.3553	LR: 0.025000
Training Epoch: 47 [20224/50000]	Loss: 0.4848	LR: 0.025000
Training Epoch: 47 [20352/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 47 [20480/50000]	Loss: 0.5579	LR: 0.025000
Training Epoch: 47 [20608/50000]	Loss: 0.6321	LR: 0.025000
Training Epoch: 47 [20736/50000]	Loss: 0.5662	LR: 0.025000
Training Epoch: 47 [20864/50000]	Loss: 0.6547	LR: 0.025000
Training Epoch: 47 [20992/50000]	Loss: 0.5006	LR: 0.025000
Training Epoch: 47 [21120/50000]	Loss: 0.6379	LR: 0.025000
Training Epoch: 47 [21248/50000]	Loss: 0.6741	LR: 0.025000
Training Epoch: 47 [21376/50000]	Loss: 0.3691	LR: 0.025000
Training Epoch: 47 [21504/50000]	Loss: 0.4803	LR: 0.025000
Training Epoch: 47 [21632/50000]	Loss: 0.6552	LR: 0.025000
Training Epoch: 47 [21760/50000]	Loss: 0.4936	LR: 0.025000
Training Epoch: 47 [21888/50000]	Loss: 0.4937	LR: 0.025000
Training Epoch: 47 [22016/50000]	Loss: 0.7285	LR: 0.025000
Training Epoch: 47 [22144/50000]	Loss: 0.6226	LR: 0.025000
Training Epoch: 47 [22272/50000]	Loss: 0.4317	LR: 0.025000
Training Epoch: 47 [22400/50000]	Loss: 0.5945	LR: 0.025000
Training Epoch: 47 [22528/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 47 [22656/50000]	Loss: 0.6943	LR: 0.025000
Training Epoch: 47 [22784/50000]	Loss: 0.5974	LR: 0.025000
Training Epoch: 47 [22912/50000]	Loss: 0.4862	LR: 0.025000
Training Epoch: 47 [23040/50000]	Loss: 0.4723	LR: 0.025000
Training Epoch: 47 [23168/50000]	Loss: 0.6032	LR: 0.025000
Training Epoch: 47 [23296/50000]	Loss: 0.7070	LR: 0.025000
Training Epoch: 47 [23424/50000]	Loss: 0.5385	LR: 0.025000
Training Epoch: 47 [23552/50000]	Loss: 0.5121	LR: 0.025000
Training Epoch: 47 [23680/50000]	Loss: 0.5692	LR: 0.025000
Training Epoch: 47 [23808/50000]	Loss: 0.6828	LR: 0.025000
Training Epoch: 47 [23936/50000]	Loss: 0.5314	LR: 0.025000
Training Epoch: 47 [24064/50000]	Loss: 0.5839	LR: 0.025000
Training Epoch: 47 [24192/50000]	Loss: 0.4809	LR: 0.025000
Training Epoch: 47 [24320/50000]	Loss: 0.7234	LR: 0.025000
Training Epoch: 47 [24448/50000]	Loss: 0.5606	LR: 0.025000
Training Epoch: 47 [24576/50000]	Loss: 0.7189	LR: 0.025000
Training Epoch: 47 [24704/50000]	Loss: 0.5260	LR: 0.025000
Training Epoch: 47 [24832/50000]	Loss: 0.5481	LR: 0.025000
Training Epoch: 47 [24960/50000]	Loss: 0.6028	LR: 0.025000
Training Epoch: 47 [25088/50000]	Loss: 0.6241	LR: 0.025000
Training Epoch: 47 [25216/50000]	Loss: 0.6642	LR: 0.025000
Training Epoch: 47 [25344/50000]	Loss: 0.5392	LR: 0.025000
Training Epoch: 47 [25472/50000]	Loss: 0.5117	LR: 0.025000
Training Epoch: 47 [25600/50000]	Loss: 0.4982	LR: 0.025000
Training Epoch: 47 [25728/50000]	Loss: 0.5078	LR: 0.025000
Training Epoch: 47 [25856/50000]	Loss: 0.6261	LR: 0.025000
Training Epoch: 47 [25984/50000]	Loss: 0.5387	LR: 0.025000
Training Epoch: 47 [26112/50000]	Loss: 0.5573	LR: 0.025000
Training Epoch: 47 [26240/50000]	Loss: 0.4383	LR: 0.025000
Training Epoch: 47 [26368/50000]	Loss: 0.5320	LR: 0.025000
Training Epoch: 47 [26496/50000]	Loss: 0.5351	LR: 0.025000
Training Epoch: 47 [26624/50000]	Loss: 0.6227	LR: 0.025000
Training Epoch: 47 [26752/50000]	Loss: 0.5883	LR: 0.025000
Training Epoch: 47 [26880/50000]	Loss: 0.7568	LR: 0.025000
Training Epoch: 47 [27008/50000]	Loss: 0.5913	LR: 0.025000
Training Epoch: 47 [27136/50000]	Loss: 0.4239	LR: 0.025000
Training Epoch: 47 [27264/50000]	Loss: 0.5873	LR: 0.025000
Training Epoch: 47 [27392/50000]	Loss: 0.5020	LR: 0.025000
Training Epoch: 47 [27520/50000]	Loss: 0.4814	LR: 0.025000
Training Epoch: 47 [27648/50000]	Loss: 0.5360	LR: 0.025000
Training Epoch: 47 [27776/50000]	Loss: 0.5741	LR: 0.025000
Training Epoch: 47 [27904/50000]	Loss: 0.6211	LR: 0.025000
Training Epoch: 47 [28032/50000]	Loss: 0.5374	LR: 0.025000
Training Epoch: 47 [28160/50000]	Loss: 0.5376	LR: 0.025000
Training Epoch: 47 [28288/50000]	Loss: 0.7945	LR: 0.025000
Training Epoch: 47 [28416/50000]	Loss: 0.5222	LR: 0.025000
Training Epoch: 47 [28544/50000]	Loss: 0.6492	LR: 0.025000
Training Epoch: 47 [28672/50000]	Loss: 0.5643	LR: 0.025000
Training Epoch: 47 [28800/50000]	Loss: 0.5520	LR: 0.025000
Training Epoch: 47 [28928/50000]	Loss: 0.5316	LR: 0.025000
Training Epoch: 47 [29056/50000]	Loss: 0.6990	LR: 0.025000
Training Epoch: 47 [29184/50000]	Loss: 0.6081	LR: 0.025000
Training Epoch: 47 [29312/50000]	Loss: 0.5613	LR: 0.025000
Training Epoch: 47 [29440/50000]	Loss: 0.5776	LR: 0.025000
Training Epoch: 47 [29568/50000]	Loss: 0.5586	LR: 0.025000
Training Epoch: 47 [29696/50000]	Loss: 0.6329	LR: 0.025000
Training Epoch: 47 [29824/50000]	Loss: 0.5478	LR: 0.025000
Training Epoch: 47 [29952/50000]	Loss: 0.5794	LR: 0.025000
Training Epoch: 47 [30080/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 47 [30208/50000]	Loss: 0.5684	LR: 0.025000
Training Epoch: 47 [30336/50000]	Loss: 0.5525	LR: 0.025000
Training Epoch: 47 [30464/50000]	Loss: 0.8048	LR: 0.025000
Training Epoch: 47 [30592/50000]	Loss: 0.5193	LR: 0.025000
Training Epoch: 47 [30720/50000]	Loss: 0.6443	LR: 0.025000
Training Epoch: 47 [30848/50000]	Loss: 0.5276	LR: 0.025000
Training Epoch: 47 [30976/50000]	Loss: 0.6721	LR: 0.025000
Training Epoch: 47 [31104/50000]	Loss: 0.5511	LR: 0.025000
Training Epoch: 47 [31232/50000]	Loss: 0.3385	LR: 0.025000
Training Epoch: 47 [31360/50000]	Loss: 0.5874	LR: 0.025000
Training Epoch: 47 [31488/50000]	Loss: 0.5465	LR: 0.025000
Training Epoch: 47 [31616/50000]	Loss: 0.4929	LR: 0.025000
Training Epoch: 47 [31744/50000]	Loss: 0.5243	LR: 0.025000
Training Epoch: 47 [31872/50000]	Loss: 0.5120	LR: 0.025000
Training Epoch: 47 [32000/50000]	Loss: 0.4843	LR: 0.025000
Training Epoch: 47 [32128/50000]	Loss: 0.7086	LR: 0.025000
Training Epoch: 47 [32256/50000]	Loss: 0.5053	LR: 0.025000
Training Epoch: 47 [32384/50000]	Loss: 0.5015	LR: 0.025000
Training Epoch: 47 [32512/50000]	Loss: 0.7416	LR: 0.025000
Training Epoch: 47 [32640/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 47 [32768/50000]	Loss: 0.5487	LR: 0.025000
Training Epoch: 47 [32896/50000]	Loss: 0.5420	LR: 0.025000
Training Epoch: 47 [33024/50000]	Loss: 0.4960	LR: 0.025000
Training Epoch: 47 [33152/50000]	Loss: 0.5934	LR: 0.025000
Training Epoch: 47 [33280/50000]	Loss: 0.4959	LR: 0.025000
Training Epoch: 47 [33408/50000]	Loss: 0.5018	LR: 0.025000
Training Epoch: 47 [33536/50000]	Loss: 0.5569	LR: 0.025000
Training Epoch: 47 [33664/50000]	Loss: 0.5224	LR: 0.025000
Training Epoch: 47 [33792/50000]	Loss: 0.5447	LR: 0.025000
Training Epoch: 47 [33920/50000]	Loss: 0.4953	LR: 0.025000
Training Epoch: 47 [34048/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 47 [34176/50000]	Loss: 0.6096	LR: 0.025000
Training Epoch: 47 [34304/50000]	Loss: 0.6171	LR: 0.025000
Training Epoch: 47 [34432/50000]	Loss: 0.4881	LR: 0.025000
Training Epoch: 47 [34560/50000]	Loss: 0.5106	LR: 0.025000
Training Epoch: 47 [34688/50000]	Loss: 0.6080	LR: 0.025000
Training Epoch: 47 [34816/50000]	Loss: 0.6110	LR: 0.025000
Training Epoch: 47 [34944/50000]	Loss: 0.5962	LR: 0.025000
Training Epoch: 47 [35072/50000]	Loss: 0.6154	LR: 0.025000
Training Epoch: 47 [35200/50000]	Loss: 0.6152	LR: 0.025000
Training Epoch: 47 [35328/50000]	Loss: 0.4342	LR: 0.025000
Training Epoch: 47 [35456/50000]	Loss: 0.6600	LR: 0.025000
Training Epoch: 47 [35584/50000]	Loss: 0.5653	LR: 0.025000
Training Epoch: 47 [35712/50000]	Loss: 0.5043	LR: 0.025000
Training Epoch: 47 [35840/50000]	Loss: 0.6343	LR: 0.025000
Training Epoch: 47 [35968/50000]	Loss: 0.5888	LR: 0.025000
Training Epoch: 47 [36096/50000]	Loss: 0.6854	LR: 0.025000
Training Epoch: 47 [36224/50000]	Loss: 0.4649	LR: 0.025000
Training Epoch: 47 [36352/50000]	Loss: 0.6799	LR: 0.025000
Training Epoch: 47 [36480/50000]	Loss: 0.6379	LR: 0.025000
Training Epoch: 47 [36608/50000]	Loss: 0.5647	LR: 0.025000
Training Epoch: 47 [36736/50000]	Loss: 0.6199	LR: 0.025000
Training Epoch: 47 [36864/50000]	Loss: 0.5251	LR: 0.025000
Training Epoch: 47 [36992/50000]	Loss: 0.6882	LR: 0.025000
Training Epoch: 47 [37120/50000]	Loss: 0.5710	LR: 0.025000
Training Epoch: 47 [37248/50000]	Loss: 0.7012	LR: 0.025000
Training Epoch: 47 [37376/50000]	Loss: 0.7627	LR: 0.025000
Training Epoch: 47 [37504/50000]	Loss: 0.6044	LR: 0.025000
Training Epoch: 47 [37632/50000]	Loss: 0.6387	LR: 0.025000
Training Epoch: 47 [37760/50000]	Loss: 0.5653	LR: 0.025000
Training Epoch: 47 [37888/50000]	Loss: 0.6617	LR: 0.025000
Training Epoch: 47 [38016/50000]	Loss: 0.5624	LR: 0.025000
Training Epoch: 47 [38144/50000]	Loss: 0.5500	LR: 0.025000
Training Epoch: 47 [38272/50000]	Loss: 0.5246	LR: 0.025000
Training Epoch: 47 [38400/50000]	Loss: 0.5954	LR: 0.025000
Training Epoch: 47 [38528/50000]	Loss: 0.6145	LR: 0.025000
Training Epoch: 47 [38656/50000]	Loss: 0.6513	LR: 0.025000
Training Epoch: 47 [38784/50000]	Loss: 0.5692	LR: 0.025000
Training Epoch: 47 [38912/50000]	Loss: 0.6167	LR: 0.025000
Training Epoch: 47 [39040/50000]	Loss: 0.5344	LR: 0.025000
Training Epoch: 47 [39168/50000]	Loss: 0.6269	LR: 0.025000
Training Epoch: 47 [39296/50000]	Loss: 0.5523	LR: 0.025000
Training Epoch: 47 [39424/50000]	Loss: 0.5818	LR: 0.025000
Training Epoch: 47 [39552/50000]	Loss: 0.7200	LR: 0.025000
Training Epoch: 47 [39680/50000]	Loss: 0.5257	LR: 0.025000
Training Epoch: 47 [39808/50000]	Loss: 0.7348	LR: 0.025000
Training Epoch: 47 [39936/50000]	Loss: 0.4663	LR: 0.025000
Training Epoch: 47 [40064/50000]	Loss: 0.6915	LR: 0.025000
Training Epoch: 47 [40192/50000]	Loss: 0.5749	LR: 0.025000
Training Epoch: 47 [40320/50000]	Loss: 0.6327	LR: 0.025000
Training Epoch: 47 [40448/50000]	Loss: 0.8024	LR: 0.025000
Training Epoch: 47 [40576/50000]	Loss: 0.6912	LR: 0.025000
Training Epoch: 47 [40704/50000]	Loss: 0.5637	LR: 0.025000
Training Epoch: 47 [40832/50000]	Loss: 0.5731	LR: 0.025000
Training Epoch: 47 [40960/50000]	Loss: 0.7055	LR: 0.025000
Training Epoch: 47 [41088/50000]	Loss: 0.4789	LR: 0.025000
Training Epoch: 47 [41216/50000]	Loss: 0.8231	LR: 0.025000
Training Epoch: 47 [41344/50000]	Loss: 0.7322	LR: 0.025000
Training Epoch: 47 [41472/50000]	Loss: 0.6376	LR: 0.025000
Training Epoch: 47 [41600/50000]	Loss: 0.6183	LR: 0.025000
Training Epoch: 47 [41728/50000]	Loss: 0.5121	LR: 0.025000
Training Epoch: 47 [41856/50000]	Loss: 0.5348	LR: 0.025000
Training Epoch: 47 [41984/50000]	Loss: 0.7236	LR: 0.025000
Training Epoch: 47 [42112/50000]	Loss: 0.6889	LR: 0.025000
Training Epoch: 47 [42240/50000]	Loss: 0.7180	LR: 0.025000
Training Epoch: 47 [42368/50000]	Loss: 0.6190	LR: 0.025000
Training Epoch: 47 [42496/50000]	Loss: 0.6567	LR: 0.025000
Training Epoch: 47 [42624/50000]	Loss: 0.5899	LR: 0.025000
Training Epoch: 47 [42752/50000]	Loss: 0.6388	LR: 0.025000
Training Epoch: 47 [42880/50000]	Loss: 0.5977	LR: 0.025000
Training Epoch: 47 [43008/50000]	Loss: 0.7509	LR: 0.025000
Training Epoch: 47 [43136/50000]	Loss: 0.5656	LR: 0.025000
Training Epoch: 47 [43264/50000]	Loss: 0.4896	LR: 0.025000
Training Epoch: 47 [43392/50000]	Loss: 0.5001	LR: 0.025000
Training Epoch: 47 [43520/50000]	Loss: 0.5339	LR: 0.025000
Training Epoch: 47 [43648/50000]	Loss: 0.6534	LR: 0.025000
Training Epoch: 47 [43776/50000]	Loss: 0.4190	LR: 0.025000
Training Epoch: 47 [43904/50000]	Loss: 0.5249	LR: 0.025000
Training Epoch: 47 [44032/50000]	Loss: 0.7110	LR: 0.025000
Training Epoch: 47 [44160/50000]	Loss: 0.5715	LR: 0.025000
Training Epoch: 47 [44288/50000]	Loss: 0.6601	LR: 0.025000
Training Epoch: 47 [44416/50000]	Loss: 0.5901	LR: 0.025000
Training Epoch: 47 [44544/50000]	Loss: 0.7596	LR: 0.025000
Training Epoch: 47 [44672/50000]	Loss: 0.6661	LR: 0.025000
Training Epoch: 47 [44800/50000]	Loss: 0.6003	LR: 0.025000
Training Epoch: 47 [44928/50000]	Loss: 0.6865	LR: 0.025000
Training Epoch: 47 [45056/50000]	Loss: 0.5765	LR: 0.025000
Training Epoch: 47 [45184/50000]	Loss: 0.4718	LR: 0.025000
Training Epoch: 47 [45312/50000]	Loss: 0.5425	LR: 0.025000
Training Epoch: 47 [45440/50000]	Loss: 0.6943	LR: 0.025000
Training Epoch: 47 [45568/50000]	Loss: 0.5123	LR: 0.025000
Training Epoch: 47 [45696/50000]	Loss: 0.6326	LR: 0.025000
Training Epoch: 47 [45824/50000]	Loss: 0.3882	LR: 0.025000
Training Epoch: 47 [45952/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 47 [46080/50000]	Loss: 0.4705	LR: 0.025000
Training Epoch: 47 [46208/50000]	Loss: 0.6833	LR: 0.025000
Training Epoch: 47 [46336/50000]	Loss: 0.3997	LR: 0.025000
Training Epoch: 47 [46464/50000]	Loss: 0.5897	LR: 0.025000
Training Epoch: 47 [46592/50000]	Loss: 0.5766	LR: 0.025000
Training Epoch: 47 [46720/50000]	Loss: 0.5050	LR: 0.025000
Training Epoch: 47 [46848/50000]	Loss: 0.6899	LR: 0.025000
Training Epoch: 47 [46976/50000]	Loss: 0.5038	LR: 0.025000
Training Epoch: 47 [47104/50000]	Loss: 0.6663	LR: 0.025000
Training Epoch: 47 [47232/50000]	Loss: 0.6226	LR: 0.025000
Training Epoch: 47 [47360/50000]	Loss: 0.7985	LR: 0.025000
Training Epoch: 47 [47488/50000]	Loss: 0.5406	LR: 0.025000
Training Epoch: 47 [47616/50000]	Loss: 0.6368	LR: 0.025000
Training Epoch: 47 [47744/50000]	Loss: 0.5372	LR: 0.025000
Training Epoch: 47 [47872/50000]	Loss: 0.5648	LR: 0.025000
Training Epoch: 47 [48000/50000]	Loss: 0.4656	LR: 0.025000
Training Epoch: 47 [48128/50000]	Loss: 0.5777	LR: 0.025000
Training Epoch: 47 [48256/50000]	Loss: 0.5566	LR: 0.025000
Training Epoch: 47 [48384/50000]	Loss: 0.5403	LR: 0.025000
Training Epoch: 47 [48512/50000]	Loss: 0.5538	LR: 0.025000
Training Epoch: 47 [48640/50000]	Loss: 0.7126	LR: 0.025000
Training Epoch: 47 [48768/50000]	Loss: 0.6041	LR: 0.025000
Training Epoch: 47 [48896/50000]	Loss: 0.7579	LR: 0.025000
Training Epoch: 47 [49024/50000]	Loss: 0.4783	LR: 0.025000
Training Epoch: 47 [49152/50000]	Loss: 0.6073	LR: 0.025000
Training Epoch: 47 [49280/50000]	Loss: 0.6387	LR: 0.025000
Training Epoch: 47 [49408/50000]	Loss: 0.4721	LR: 0.025000
Training Epoch: 47 [49536/50000]	Loss: 0.4929	LR: 0.025000
Training Epoch: 47 [49664/50000]	Loss: 0.6456	LR: 0.025000
Training Epoch: 47 [49792/50000]	Loss: 0.5382	LR: 0.025000
Training Epoch: 47 [49920/50000]	Loss: 0.6071	LR: 0.025000
Training Epoch: 47 [50000/50000]	Loss: 0.6218	LR: 0.025000
epoch 47 training time consumed: 53.99s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  172493 GB |  172493 GB |
|       from large pool |  123392 KB |    1034 MB |  172323 GB |  172323 GB |
|       from small pool |   10798 KB |      13 MB |     169 GB |     169 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  172493 GB |  172493 GB |
|       from large pool |  123392 KB |    1034 MB |  172323 GB |  172323 GB |
|       from small pool |   10798 KB |      13 MB |     169 GB |     169 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   75908 GB |   75908 GB |
|       from large pool |  155136 KB |  433088 KB |   75720 GB |   75720 GB |
|       from small pool |    1490 KB |    3494 KB |     187 GB |     187 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    6656 K  |    6655 K  |
|       from large pool |      24    |      65    |    3474 K  |    3474 K  |
|       from small pool |     231    |     274    |    3181 K  |    3181 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    6656 K  |    6655 K  |
|       from large pool |      24    |      65    |    3474 K  |    3474 K  |
|       from small pool |     231    |     274    |    3181 K  |    3181 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3290 K  |    3290 K  |
|       from large pool |       9    |      14    |    1681 K  |    1681 K  |
|       from small pool |      12    |      16    |    1608 K  |    1608 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 47, Average loss: 0.0104, Accuracy: 0.6634, Time consumed:3.46s

Training Epoch: 48 [128/50000]	Loss: 0.3562	LR: 0.025000
Training Epoch: 48 [256/50000]	Loss: 0.5091	LR: 0.025000
Training Epoch: 48 [384/50000]	Loss: 0.4580	LR: 0.025000
Training Epoch: 48 [512/50000]	Loss: 0.4663	LR: 0.025000
Training Epoch: 48 [640/50000]	Loss: 0.3873	LR: 0.025000
Training Epoch: 48 [768/50000]	Loss: 0.4484	LR: 0.025000
Training Epoch: 48 [896/50000]	Loss: 0.5548	LR: 0.025000
Training Epoch: 48 [1024/50000]	Loss: 0.3515	LR: 0.025000
Training Epoch: 48 [1152/50000]	Loss: 0.5618	LR: 0.025000
Training Epoch: 48 [1280/50000]	Loss: 0.5725	LR: 0.025000
Training Epoch: 48 [1408/50000]	Loss: 0.4271	LR: 0.025000
Training Epoch: 48 [1536/50000]	Loss: 0.4976	LR: 0.025000
Training Epoch: 48 [1664/50000]	Loss: 0.5184	LR: 0.025000
Training Epoch: 48 [1792/50000]	Loss: 0.4997	LR: 0.025000
Training Epoch: 48 [1920/50000]	Loss: 0.4729	LR: 0.025000
Training Epoch: 48 [2048/50000]	Loss: 0.4361	LR: 0.025000
Training Epoch: 48 [2176/50000]	Loss: 0.5289	LR: 0.025000
Training Epoch: 48 [2304/50000]	Loss: 0.5387	LR: 0.025000
Training Epoch: 48 [2432/50000]	Loss: 0.4409	LR: 0.025000
Training Epoch: 48 [2560/50000]	Loss: 0.4555	LR: 0.025000
Training Epoch: 48 [2688/50000]	Loss: 0.6105	LR: 0.025000
Training Epoch: 48 [2816/50000]	Loss: 0.4797	LR: 0.025000
Training Epoch: 48 [2944/50000]	Loss: 0.5905	LR: 0.025000
Training Epoch: 48 [3072/50000]	Loss: 0.6008	LR: 0.025000
Training Epoch: 48 [3200/50000]	Loss: 0.4245	LR: 0.025000
Training Epoch: 48 [3328/50000]	Loss: 0.5438	LR: 0.025000
Training Epoch: 48 [3456/50000]	Loss: 0.4846	LR: 0.025000
Training Epoch: 48 [3584/50000]	Loss: 0.3668	LR: 0.025000
Training Epoch: 48 [3712/50000]	Loss: 0.4868	LR: 0.025000
Training Epoch: 48 [3840/50000]	Loss: 0.4387	LR: 0.025000
Training Epoch: 48 [3968/50000]	Loss: 0.3924	LR: 0.025000
Training Epoch: 48 [4096/50000]	Loss: 0.4839	LR: 0.025000
Training Epoch: 48 [4224/50000]	Loss: 0.3181	LR: 0.025000
Training Epoch: 48 [4352/50000]	Loss: 0.4958	LR: 0.025000
Training Epoch: 48 [4480/50000]	Loss: 0.5003	LR: 0.025000
Training Epoch: 48 [4608/50000]	Loss: 0.5579	LR: 0.025000
Training Epoch: 48 [4736/50000]	Loss: 0.4878	LR: 0.025000
Training Epoch: 48 [4864/50000]	Loss: 0.4586	LR: 0.025000
Training Epoch: 48 [4992/50000]	Loss: 0.4443	LR: 0.025000
Training Epoch: 48 [5120/50000]	Loss: 0.6822	LR: 0.025000
Training Epoch: 48 [5248/50000]	Loss: 0.6530	LR: 0.025000
Training Epoch: 48 [5376/50000]	Loss: 0.4004	LR: 0.025000
Training Epoch: 48 [5504/50000]	Loss: 0.4639	LR: 0.025000
Training Epoch: 48 [5632/50000]	Loss: 0.4701	LR: 0.025000
Training Epoch: 48 [5760/50000]	Loss: 0.5328	LR: 0.025000
Training Epoch: 48 [5888/50000]	Loss: 0.4720	LR: 0.025000
Training Epoch: 48 [6016/50000]	Loss: 0.4639	LR: 0.025000
Training Epoch: 48 [6144/50000]	Loss: 0.5884	LR: 0.025000
Training Epoch: 48 [6272/50000]	Loss: 0.4951	LR: 0.025000
Training Epoch: 48 [6400/50000]	Loss: 0.4366	LR: 0.025000
Training Epoch: 48 [6528/50000]	Loss: 0.5211	LR: 0.025000
Training Epoch: 48 [6656/50000]	Loss: 0.3813	LR: 0.025000
Training Epoch: 48 [6784/50000]	Loss: 0.5784	LR: 0.025000
Training Epoch: 48 [6912/50000]	Loss: 0.4934	LR: 0.025000
Training Epoch: 48 [7040/50000]	Loss: 0.5029	LR: 0.025000
Training Epoch: 48 [7168/50000]	Loss: 0.4715	LR: 0.025000
Training Epoch: 48 [7296/50000]	Loss: 0.4118	LR: 0.025000
Training Epoch: 48 [7424/50000]	Loss: 0.5349	LR: 0.025000
Training Epoch: 48 [7552/50000]	Loss: 0.4568	LR: 0.025000
Training Epoch: 48 [7680/50000]	Loss: 0.4292	LR: 0.025000
Training Epoch: 48 [7808/50000]	Loss: 0.6602	LR: 0.025000
Training Epoch: 48 [7936/50000]	Loss: 0.4502	LR: 0.025000
Training Epoch: 48 [8064/50000]	Loss: 0.5290	LR: 0.025000
Training Epoch: 48 [8192/50000]	Loss: 0.6649	LR: 0.025000
Training Epoch: 48 [8320/50000]	Loss: 0.4922	LR: 0.025000
Training Epoch: 48 [8448/50000]	Loss: 0.4370	LR: 0.025000
Training Epoch: 48 [8576/50000]	Loss: 0.4648	LR: 0.025000
Training Epoch: 48 [8704/50000]	Loss: 0.5885	LR: 0.025000
Training Epoch: 48 [8832/50000]	Loss: 0.6186	LR: 0.025000
Training Epoch: 48 [8960/50000]	Loss: 0.4502	LR: 0.025000
Training Epoch: 48 [9088/50000]	Loss: 0.4870	LR: 0.025000
Training Epoch: 48 [9216/50000]	Loss: 0.5398	LR: 0.025000
Training Epoch: 48 [9344/50000]	Loss: 0.5582	LR: 0.025000
Training Epoch: 48 [9472/50000]	Loss: 0.3883	LR: 0.025000
Training Epoch: 48 [9600/50000]	Loss: 0.4902	LR: 0.025000
Training Epoch: 48 [9728/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 48 [9856/50000]	Loss: 0.6771	LR: 0.025000
Training Epoch: 48 [9984/50000]	Loss: 0.5349	LR: 0.025000
Training Epoch: 48 [10112/50000]	Loss: 0.6069	LR: 0.025000
Training Epoch: 48 [10240/50000]	Loss: 0.3998	LR: 0.025000
Training Epoch: 48 [10368/50000]	Loss: 0.5528	LR: 0.025000
Training Epoch: 48 [10496/50000]	Loss: 0.4758	LR: 0.025000
Training Epoch: 48 [10624/50000]	Loss: 0.4897	LR: 0.025000
Training Epoch: 48 [10752/50000]	Loss: 0.5838	LR: 0.025000
Training Epoch: 48 [10880/50000]	Loss: 0.5268	LR: 0.025000
Training Epoch: 48 [11008/50000]	Loss: 0.5381	LR: 0.025000
Training Epoch: 48 [11136/50000]	Loss: 0.5126	LR: 0.025000
Training Epoch: 48 [11264/50000]	Loss: 0.5136	LR: 0.025000
Training Epoch: 48 [11392/50000]	Loss: 0.5595	LR: 0.025000
Training Epoch: 48 [11520/50000]	Loss: 0.4113	LR: 0.025000
Training Epoch: 48 [11648/50000]	Loss: 0.5987	LR: 0.025000
Training Epoch: 48 [11776/50000]	Loss: 0.4809	LR: 0.025000
Training Epoch: 48 [11904/50000]	Loss: 0.5944	LR: 0.025000
Training Epoch: 48 [12032/50000]	Loss: 0.5199	LR: 0.025000
Training Epoch: 48 [12160/50000]	Loss: 0.6344	LR: 0.025000
Training Epoch: 48 [12288/50000]	Loss: 0.4486	LR: 0.025000
Training Epoch: 48 [12416/50000]	Loss: 0.6714	LR: 0.025000
Training Epoch: 48 [12544/50000]	Loss: 0.5357	LR: 0.025000
Training Epoch: 48 [12672/50000]	Loss: 0.5862	LR: 0.025000
Training Epoch: 48 [12800/50000]	Loss: 0.5255	LR: 0.025000
Training Epoch: 48 [12928/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 48 [13056/50000]	Loss: 0.5241	LR: 0.025000
Training Epoch: 48 [13184/50000]	Loss: 0.4246	LR: 0.025000
Training Epoch: 48 [13312/50000]	Loss: 0.5794	LR: 0.025000
Training Epoch: 48 [13440/50000]	Loss: 0.6429	LR: 0.025000
Training Epoch: 48 [13568/50000]	Loss: 0.5326	LR: 0.025000
Training Epoch: 48 [13696/50000]	Loss: 0.4963	LR: 0.025000
Training Epoch: 48 [13824/50000]	Loss: 0.4676	LR: 0.025000
Training Epoch: 48 [13952/50000]	Loss: 0.5446	LR: 0.025000
Training Epoch: 48 [14080/50000]	Loss: 0.3576	LR: 0.025000
Training Epoch: 48 [14208/50000]	Loss: 0.5242	LR: 0.025000
Training Epoch: 48 [14336/50000]	Loss: 0.5853	LR: 0.025000
Training Epoch: 48 [14464/50000]	Loss: 0.4953	LR: 0.025000
Training Epoch: 48 [14592/50000]	Loss: 0.4816	LR: 0.025000
Training Epoch: 48 [14720/50000]	Loss: 0.6757	LR: 0.025000
Training Epoch: 48 [14848/50000]	Loss: 0.5702	LR: 0.025000
Training Epoch: 48 [14976/50000]	Loss: 0.5560	LR: 0.025000
Training Epoch: 48 [15104/50000]	Loss: 0.5853	LR: 0.025000
Training Epoch: 48 [15232/50000]	Loss: 0.6721	LR: 0.025000
Training Epoch: 48 [15360/50000]	Loss: 0.5102	LR: 0.025000
Training Epoch: 48 [15488/50000]	Loss: 0.5473	LR: 0.025000
Training Epoch: 48 [15616/50000]	Loss: 0.5757	LR: 0.025000
Training Epoch: 48 [15744/50000]	Loss: 0.6055	LR: 0.025000
Training Epoch: 48 [15872/50000]	Loss: 0.5723	LR: 0.025000
Training Epoch: 48 [16000/50000]	Loss: 0.4658	LR: 0.025000
Training Epoch: 48 [16128/50000]	Loss: 0.6562	LR: 0.025000
Training Epoch: 48 [16256/50000]	Loss: 0.4883	LR: 0.025000
Training Epoch: 48 [16384/50000]	Loss: 0.4674	LR: 0.025000
Training Epoch: 48 [16512/50000]	Loss: 0.5747	LR: 0.025000
Training Epoch: 48 [16640/50000]	Loss: 0.5213	LR: 0.025000
Training Epoch: 48 [16768/50000]	Loss: 0.4713	LR: 0.025000
Training Epoch: 48 [16896/50000]	Loss: 0.4239	LR: 0.025000
Training Epoch: 48 [17024/50000]	Loss: 0.4452	LR: 0.025000
Training Epoch: 48 [17152/50000]	Loss: 0.7360	LR: 0.025000
Training Epoch: 48 [17280/50000]	Loss: 0.4530	LR: 0.025000
Training Epoch: 48 [17408/50000]	Loss: 0.6530	LR: 0.025000
Training Epoch: 48 [17536/50000]	Loss: 0.4666	LR: 0.025000
Training Epoch: 48 [17664/50000]	Loss: 0.4898	LR: 0.025000
Training Epoch: 48 [17792/50000]	Loss: 0.4415	LR: 0.025000
Training Epoch: 48 [17920/50000]	Loss: 0.5195	LR: 0.025000
Training Epoch: 48 [18048/50000]	Loss: 0.3459	LR: 0.025000
Training Epoch: 48 [18176/50000]	Loss: 0.5278	LR: 0.025000
Training Epoch: 48 [18304/50000]	Loss: 0.6428	LR: 0.025000
Training Epoch: 48 [18432/50000]	Loss: 0.5601	LR: 0.025000
Training Epoch: 48 [18560/50000]	Loss: 0.6432	LR: 0.025000
Training Epoch: 48 [18688/50000]	Loss: 0.7225	LR: 0.025000
Training Epoch: 48 [18816/50000]	Loss: 0.7027	LR: 0.025000
Training Epoch: 48 [18944/50000]	Loss: 0.6540	LR: 0.025000
Training Epoch: 48 [19072/50000]	Loss: 0.4657	LR: 0.025000
Training Epoch: 48 [19200/50000]	Loss: 0.5918	LR: 0.025000
Training Epoch: 48 [19328/50000]	Loss: 0.5393	LR: 0.025000
Training Epoch: 48 [19456/50000]	Loss: 0.5153	LR: 0.025000
Training Epoch: 48 [19584/50000]	Loss: 0.5491	LR: 0.025000
Training Epoch: 48 [19712/50000]	Loss: 0.5310	LR: 0.025000
Training Epoch: 48 [19840/50000]	Loss: 0.4330	LR: 0.025000
Training Epoch: 48 [19968/50000]	Loss: 0.5992	LR: 0.025000
Training Epoch: 48 [20096/50000]	Loss: 0.4458	LR: 0.025000
Training Epoch: 48 [20224/50000]	Loss: 0.4532	LR: 0.025000
Training Epoch: 48 [20352/50000]	Loss: 0.5987	LR: 0.025000
Training Epoch: 48 [20480/50000]	Loss: 0.5442	LR: 0.025000
Training Epoch: 48 [20608/50000]	Loss: 0.5769	LR: 0.025000
Training Epoch: 48 [20736/50000]	Loss: 0.6975	LR: 0.025000
Training Epoch: 48 [20864/50000]	Loss: 0.5929	LR: 0.025000
Training Epoch: 48 [20992/50000]	Loss: 0.6810	LR: 0.025000
Training Epoch: 48 [21120/50000]	Loss: 0.4667	LR: 0.025000
Training Epoch: 48 [21248/50000]	Loss: 0.5613	LR: 0.025000
Training Epoch: 48 [21376/50000]	Loss: 0.4953	LR: 0.025000
Training Epoch: 48 [21504/50000]	Loss: 0.5058	LR: 0.025000
Training Epoch: 48 [21632/50000]	Loss: 0.4682	LR: 0.025000
Training Epoch: 48 [21760/50000]	Loss: 0.6049	LR: 0.025000
Training Epoch: 48 [21888/50000]	Loss: 0.4589	LR: 0.025000
Training Epoch: 48 [22016/50000]	Loss: 0.5300	LR: 0.025000
Training Epoch: 48 [22144/50000]	Loss: 0.4904	LR: 0.025000
Training Epoch: 48 [22272/50000]	Loss: 0.4931	LR: 0.025000
Training Epoch: 48 [22400/50000]	Loss: 0.5362	LR: 0.025000
Training Epoch: 48 [22528/50000]	Loss: 0.5303	LR: 0.025000
Training Epoch: 48 [22656/50000]	Loss: 0.5279	LR: 0.025000
Training Epoch: 48 [22784/50000]	Loss: 0.5804	LR: 0.025000
Training Epoch: 48 [22912/50000]	Loss: 0.4473	LR: 0.025000
Training Epoch: 48 [23040/50000]	Loss: 0.3967	LR: 0.025000
Training Epoch: 48 [23168/50000]	Loss: 0.4916	LR: 0.025000
Training Epoch: 48 [23296/50000]	Loss: 0.5392	LR: 0.025000
Training Epoch: 48 [23424/50000]	Loss: 0.5394	LR: 0.025000
Training Epoch: 48 [23552/50000]	Loss: 0.6174	LR: 0.025000
Training Epoch: 48 [23680/50000]	Loss: 0.6125	LR: 0.025000
Training Epoch: 48 [23808/50000]	Loss: 0.5943	LR: 0.025000
Training Epoch: 48 [23936/50000]	Loss: 0.6479	LR: 0.025000
Training Epoch: 48 [24064/50000]	Loss: 0.5481	LR: 0.025000
Training Epoch: 48 [24192/50000]	Loss: 0.5436	LR: 0.025000
Training Epoch: 48 [24320/50000]	Loss: 0.5426	LR: 0.025000
Training Epoch: 48 [24448/50000]	Loss: 0.4748	LR: 0.025000
Training Epoch: 48 [24576/50000]	Loss: 0.5687	LR: 0.025000
Training Epoch: 48 [24704/50000]	Loss: 0.5860	LR: 0.025000
Training Epoch: 48 [24832/50000]	Loss: 0.6359	LR: 0.025000
Training Epoch: 48 [24960/50000]	Loss: 0.7247	LR: 0.025000
Training Epoch: 48 [25088/50000]	Loss: 0.4843	LR: 0.025000
Training Epoch: 48 [25216/50000]	Loss: 0.6560	LR: 0.025000
Training Epoch: 48 [25344/50000]	Loss: 0.6072	LR: 0.025000
Training Epoch: 48 [25472/50000]	Loss: 0.7044	LR: 0.025000
Training Epoch: 48 [25600/50000]	Loss: 0.6081	LR: 0.025000
Training Epoch: 48 [25728/50000]	Loss: 0.5327	LR: 0.025000
Training Epoch: 48 [25856/50000]	Loss: 0.5688	LR: 0.025000
Training Epoch: 48 [25984/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 48 [26112/50000]	Loss: 0.5076	LR: 0.025000
Training Epoch: 48 [26240/50000]	Loss: 0.4661	LR: 0.025000
Training Epoch: 48 [26368/50000]	Loss: 0.5667	LR: 0.025000
Training Epoch: 48 [26496/50000]	Loss: 0.6186	LR: 0.025000
Training Epoch: 48 [26624/50000]	Loss: 0.6594	LR: 0.025000
Training Epoch: 48 [26752/50000]	Loss: 0.4433	LR: 0.025000
Training Epoch: 48 [26880/50000]	Loss: 0.5283	LR: 0.025000
Training Epoch: 48 [27008/50000]	Loss: 0.5452	LR: 0.025000
Training Epoch: 48 [27136/50000]	Loss: 0.4907	LR: 0.025000
Training Epoch: 48 [27264/50000]	Loss: 0.5826	LR: 0.025000
Training Epoch: 48 [27392/50000]	Loss: 0.5758	LR: 0.025000
Training Epoch: 48 [27520/50000]	Loss: 0.5946	LR: 0.025000
Training Epoch: 48 [27648/50000]	Loss: 0.6156	LR: 0.025000
Training Epoch: 48 [27776/50000]	Loss: 0.6743	LR: 0.025000
Training Epoch: 48 [27904/50000]	Loss: 0.6125	LR: 0.025000
Training Epoch: 48 [28032/50000]	Loss: 0.7181	LR: 0.025000
Training Epoch: 48 [28160/50000]	Loss: 0.6086	LR: 0.025000
Training Epoch: 48 [28288/50000]	Loss: 0.6979	LR: 0.025000
Training Epoch: 48 [28416/50000]	Loss: 0.4603	LR: 0.025000
Training Epoch: 48 [28544/50000]	Loss: 0.6820	LR: 0.025000
Training Epoch: 48 [28672/50000]	Loss: 0.6388	LR: 0.025000
Training Epoch: 48 [28800/50000]	Loss: 0.5102	LR: 0.025000
Training Epoch: 48 [28928/50000]	Loss: 0.6483	LR: 0.025000
Training Epoch: 48 [29056/50000]	Loss: 0.4752	LR: 0.025000
Training Epoch: 48 [29184/50000]	Loss: 0.5308	LR: 0.025000
Training Epoch: 48 [29312/50000]	Loss: 0.7133	LR: 0.025000
Training Epoch: 48 [29440/50000]	Loss: 0.5305	LR: 0.025000
Training Epoch: 48 [29568/50000]	Loss: 0.5788	LR: 0.025000
Training Epoch: 48 [29696/50000]	Loss: 0.6142	LR: 0.025000
Training Epoch: 48 [29824/50000]	Loss: 0.5512	LR: 0.025000
Training Epoch: 48 [29952/50000]	Loss: 0.5169	LR: 0.025000
Training Epoch: 48 [30080/50000]	Loss: 0.5671	LR: 0.025000
Training Epoch: 48 [30208/50000]	Loss: 0.6350	LR: 0.025000
Training Epoch: 48 [30336/50000]	Loss: 0.6268	LR: 0.025000
Training Epoch: 48 [30464/50000]	Loss: 0.6359	LR: 0.025000
Training Epoch: 48 [30592/50000]	Loss: 0.5141	LR: 0.025000
Training Epoch: 48 [30720/50000]	Loss: 0.5584	LR: 0.025000
Training Epoch: 48 [30848/50000]	Loss: 0.5069	LR: 0.025000
Training Epoch: 48 [30976/50000]	Loss: 0.4034	LR: 0.025000
Training Epoch: 48 [31104/50000]	Loss: 0.6590	LR: 0.025000
Training Epoch: 48 [31232/50000]	Loss: 0.4594	LR: 0.025000
Training Epoch: 48 [31360/50000]	Loss: 0.6556	LR: 0.025000
Training Epoch: 48 [31488/50000]	Loss: 0.6331	LR: 0.025000
Training Epoch: 48 [31616/50000]	Loss: 0.6251	LR: 0.025000
Training Epoch: 48 [31744/50000]	Loss: 0.6867	LR: 0.025000
Training Epoch: 48 [31872/50000]	Loss: 0.7162	LR: 0.025000
Training Epoch: 48 [32000/50000]	Loss: 0.6013	LR: 0.025000
Training Epoch: 48 [32128/50000]	Loss: 0.5140	LR: 0.025000
Training Epoch: 48 [32256/50000]	Loss: 0.6290	LR: 0.025000
Training Epoch: 48 [32384/50000]	Loss: 0.6919	LR: 0.025000
Training Epoch: 48 [32512/50000]	Loss: 0.5904	LR: 0.025000
Training Epoch: 48 [32640/50000]	Loss: 0.6075	LR: 0.025000
Training Epoch: 48 [32768/50000]	Loss: 0.6107	LR: 0.025000
Training Epoch: 48 [32896/50000]	Loss: 0.5470	LR: 0.025000
Training Epoch: 48 [33024/50000]	Loss: 0.5600	LR: 0.025000
Training Epoch: 48 [33152/50000]	Loss: 0.5586	LR: 0.025000
Training Epoch: 48 [33280/50000]	Loss: 0.5544	LR: 0.025000
Training Epoch: 48 [33408/50000]	Loss: 0.5678	LR: 0.025000
Training Epoch: 48 [33536/50000]	Loss: 0.6032	LR: 0.025000
Training Epoch: 48 [33664/50000]	Loss: 0.5641	LR: 0.025000
Training Epoch: 48 [33792/50000]	Loss: 0.5157	LR: 0.025000
Training Epoch: 48 [33920/50000]	Loss: 0.6287	LR: 0.025000
Training Epoch: 48 [34048/50000]	Loss: 0.5588	LR: 0.025000
Training Epoch: 48 [34176/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 48 [34304/50000]	Loss: 0.6737	LR: 0.025000
Training Epoch: 48 [34432/50000]	Loss: 0.5753	LR: 0.025000
Training Epoch: 48 [34560/50000]	Loss: 0.4875	LR: 0.025000
Training Epoch: 48 [34688/50000]	Loss: 0.4914	LR: 0.025000
Training Epoch: 48 [34816/50000]	Loss: 0.5950	LR: 0.025000
Training Epoch: 48 [34944/50000]	Loss: 0.7633	LR: 0.025000
Training Epoch: 48 [35072/50000]	Loss: 0.5371	LR: 0.025000
Training Epoch: 48 [35200/50000]	Loss: 0.5846	LR: 0.025000
Training Epoch: 48 [35328/50000]	Loss: 0.6174	LR: 0.025000
Training Epoch: 48 [35456/50000]	Loss: 0.5734	LR: 0.025000
Training Epoch: 48 [35584/50000]	Loss: 0.5396	LR: 0.025000
Training Epoch: 48 [35712/50000]	Loss: 0.5796	LR: 0.025000
Training Epoch: 48 [35840/50000]	Loss: 0.4471	LR: 0.025000
Training Epoch: 48 [35968/50000]	Loss: 0.5480	LR: 0.025000
Training Epoch: 48 [36096/50000]	Loss: 0.5253	LR: 0.025000
Training Epoch: 48 [36224/50000]	Loss: 0.6775	LR: 0.025000
Training Epoch: 48 [36352/50000]	Loss: 0.6383	LR: 0.025000
Training Epoch: 48 [36480/50000]	Loss: 0.5517	LR: 0.025000
Training Epoch: 48 [36608/50000]	Loss: 0.5258	LR: 0.025000
Training Epoch: 48 [36736/50000]	Loss: 0.4614	LR: 0.025000
Training Epoch: 48 [36864/50000]	Loss: 0.5670	LR: 0.025000
Training Epoch: 48 [36992/50000]	Loss: 0.7778	LR: 0.025000
Training Epoch: 48 [37120/50000]	Loss: 0.5323	LR: 0.025000
Training Epoch: 48 [37248/50000]	Loss: 0.4377	LR: 0.025000
Training Epoch: 48 [37376/50000]	Loss: 0.5857	LR: 0.025000
Training Epoch: 48 [37504/50000]	Loss: 0.7741	LR: 0.025000
Training Epoch: 48 [37632/50000]	Loss: 0.5789	LR: 0.025000
Training Epoch: 48 [37760/50000]	Loss: 0.6607	LR: 0.025000
Training Epoch: 48 [37888/50000]	Loss: 0.5723	LR: 0.025000
Training Epoch: 48 [38016/50000]	Loss: 0.5936	LR: 0.025000
Training Epoch: 48 [38144/50000]	Loss: 0.6918	LR: 0.025000
Training Epoch: 48 [38272/50000]	Loss: 0.4789	LR: 0.025000
Training Epoch: 48 [38400/50000]	Loss: 0.5947	LR: 0.025000
Training Epoch: 48 [38528/50000]	Loss: 0.6855	LR: 0.025000
Training Epoch: 48 [38656/50000]	Loss: 0.5939	LR: 0.025000
Training Epoch: 48 [38784/50000]	Loss: 0.6267	LR: 0.025000
Training Epoch: 48 [38912/50000]	Loss: 0.7100	LR: 0.025000
Training Epoch: 48 [39040/50000]	Loss: 0.4549	LR: 0.025000
Training Epoch: 48 [39168/50000]	Loss: 0.5778	LR: 0.025000
Training Epoch: 48 [39296/50000]	Loss: 0.4020	LR: 0.025000
Training Epoch: 48 [39424/50000]	Loss: 0.4776	LR: 0.025000
Training Epoch: 48 [39552/50000]	Loss: 0.7528	LR: 0.025000
Training Epoch: 48 [39680/50000]	Loss: 0.6754	LR: 0.025000
Training Epoch: 48 [39808/50000]	Loss: 0.5974	LR: 0.025000
Training Epoch: 48 [39936/50000]	Loss: 0.5790	LR: 0.025000
Training Epoch: 48 [40064/50000]	Loss: 0.3910	LR: 0.025000
Training Epoch: 48 [40192/50000]	Loss: 0.7574	LR: 0.025000
Training Epoch: 48 [40320/50000]	Loss: 0.7696	LR: 0.025000
Training Epoch: 48 [40448/50000]	Loss: 0.5161	LR: 0.025000
Training Epoch: 48 [40576/50000]	Loss: 0.5127	LR: 0.025000
Training Epoch: 48 [40704/50000]	Loss: 0.5942	LR: 0.025000
Training Epoch: 48 [40832/50000]	Loss: 0.5069	LR: 0.025000
Training Epoch: 48 [40960/50000]	Loss: 0.6619	LR: 0.025000
Training Epoch: 48 [41088/50000]	Loss: 0.4854	LR: 0.025000
Training Epoch: 48 [41216/50000]	Loss: 0.5560	LR: 0.025000
Training Epoch: 48 [41344/50000]	Loss: 0.6225	LR: 0.025000
Training Epoch: 48 [41472/50000]	Loss: 0.5587	LR: 0.025000
Training Epoch: 48 [41600/50000]	Loss: 0.6481	LR: 0.025000
Training Epoch: 48 [41728/50000]	Loss: 0.4769	LR: 0.025000
Training Epoch: 48 [41856/50000]	Loss: 0.7016	LR: 0.025000
Training Epoch: 48 [41984/50000]	Loss: 0.5001	LR: 0.025000
Training Epoch: 48 [42112/50000]	Loss: 0.5223	LR: 0.025000
Training Epoch: 48 [42240/50000]	Loss: 0.5087	LR: 0.025000
Training Epoch: 48 [42368/50000]	Loss: 0.5576	LR: 0.025000
Training Epoch: 48 [42496/50000]	Loss: 0.6475	LR: 0.025000
Training Epoch: 48 [42624/50000]	Loss: 0.7043	LR: 0.025000
Training Epoch: 48 [42752/50000]	Loss: 0.5465	LR: 0.025000
Training Epoch: 48 [42880/50000]	Loss: 0.6840	LR: 0.025000
Training Epoch: 48 [43008/50000]	Loss: 0.5997	LR: 0.025000
Training Epoch: 48 [43136/50000]	Loss: 0.5368	LR: 0.025000
Training Epoch: 48 [43264/50000]	Loss: 0.5566	LR: 0.025000
Training Epoch: 48 [43392/50000]	Loss: 0.5975	LR: 0.025000
Training Epoch: 48 [43520/50000]	Loss: 0.4744	LR: 0.025000
Training Epoch: 48 [43648/50000]	Loss: 0.5099	LR: 0.025000
Training Epoch: 48 [43776/50000]	Loss: 0.5149	LR: 0.025000
Training Epoch: 48 [43904/50000]	Loss: 0.4736	LR: 0.025000
Training Epoch: 48 [44032/50000]	Loss: 0.7303	LR: 0.025000
Training Epoch: 48 [44160/50000]	Loss: 0.6618	LR: 0.025000
Training Epoch: 48 [44288/50000]	Loss: 0.5029	LR: 0.025000
Training Epoch: 48 [44416/50000]	Loss: 0.5728	LR: 0.025000
Training Epoch: 48 [44544/50000]	Loss: 0.5802	LR: 0.025000
Training Epoch: 48 [44672/50000]	Loss: 0.6797	LR: 0.025000
Training Epoch: 48 [44800/50000]	Loss: 0.7967	LR: 0.025000
Training Epoch: 48 [44928/50000]	Loss: 0.5938	LR: 0.025000
Training Epoch: 48 [45056/50000]	Loss: 0.6331	LR: 0.025000
Training Epoch: 48 [45184/50000]	Loss: 0.7729	LR: 0.025000
Training Epoch: 48 [45312/50000]	Loss: 0.7555	LR: 0.025000
Training Epoch: 48 [45440/50000]	Loss: 0.5746	LR: 0.025000
Training Epoch: 48 [45568/50000]	Loss: 0.6246	LR: 0.025000
Training Epoch: 48 [45696/50000]	Loss: 0.4985	LR: 0.025000
Training Epoch: 48 [45824/50000]	Loss: 0.4901	LR: 0.025000
Training Epoch: 48 [45952/50000]	Loss: 0.4843	LR: 0.025000
Training Epoch: 48 [46080/50000]	Loss: 0.6560	LR: 0.025000
Training Epoch: 48 [46208/50000]	Loss: 0.7321	LR: 0.025000
Training Epoch: 48 [46336/50000]	Loss: 0.7411	LR: 0.025000
Training Epoch: 48 [46464/50000]	Loss: 0.5259	LR: 0.025000
Training Epoch: 48 [46592/50000]	Loss: 0.4835	LR: 0.025000
Training Epoch: 48 [46720/50000]	Loss: 0.7700	LR: 0.025000
Training Epoch: 48 [46848/50000]	Loss: 0.5571	LR: 0.025000
Training Epoch: 48 [46976/50000]	Loss: 0.6586	LR: 0.025000
Training Epoch: 48 [47104/50000]	Loss: 0.6999	LR: 0.025000
Training Epoch: 48 [47232/50000]	Loss: 0.5874	LR: 0.025000
Training Epoch: 48 [47360/50000]	Loss: 0.5701	LR: 0.025000
Training Epoch: 48 [47488/50000]	Loss: 0.4583	LR: 0.025000
Training Epoch: 48 [47616/50000]	Loss: 0.5958	LR: 0.025000
Training Epoch: 48 [47744/50000]	Loss: 0.6720	LR: 0.025000
Training Epoch: 48 [47872/50000]	Loss: 0.6276	LR: 0.025000
Training Epoch: 48 [48000/50000]	Loss: 0.6295	LR: 0.025000
Training Epoch: 48 [48128/50000]	Loss: 0.6622	LR: 0.025000
Training Epoch: 48 [48256/50000]	Loss: 0.6347	LR: 0.025000
Training Epoch: 48 [48384/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 48 [48512/50000]	Loss: 0.5679	LR: 0.025000
Training Epoch: 48 [48640/50000]	Loss: 0.4160	LR: 0.025000
Training Epoch: 48 [48768/50000]	Loss: 0.6159	LR: 0.025000
Training Epoch: 48 [48896/50000]	Loss: 0.7755	LR: 0.025000
Training Epoch: 48 [49024/50000]	Loss: 0.5565	LR: 0.025000
Training Epoch: 48 [49152/50000]	Loss: 0.6331	LR: 0.025000
Training Epoch: 48 [49280/50000]	Loss: 0.5802	LR: 0.025000
Training Epoch: 48 [49408/50000]	Loss: 0.3916	LR: 0.025000
Training Epoch: 48 [49536/50000]	Loss: 0.6843	LR: 0.025000
Training Epoch: 48 [49664/50000]	Loss: 0.6400	LR: 0.025000
Training Epoch: 48 [49792/50000]	Loss: 0.5374	LR: 0.025000
Training Epoch: 48 [49920/50000]	Loss: 0.7249	LR: 0.025000
Training Epoch: 48 [50000/50000]	Loss: 0.4489	LR: 0.025000
epoch 48 training time consumed: 53.93s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  176163 GB |  176163 GB |
|       from large pool |  123392 KB |    1034 MB |  175989 GB |  175989 GB |
|       from small pool |   10798 KB |      13 MB |     173 GB |     173 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  176163 GB |  176163 GB |
|       from large pool |  123392 KB |    1034 MB |  175989 GB |  175989 GB |
|       from small pool |   10798 KB |      13 MB |     173 GB |     173 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   77523 GB |   77523 GB |
|       from large pool |  155136 KB |  433088 KB |   77331 GB |   77331 GB |
|       from small pool |    1490 KB |    3494 KB |     191 GB |     191 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    6797 K  |    6797 K  |
|       from large pool |      24    |      65    |    3548 K  |    3548 K  |
|       from small pool |     231    |     274    |    3249 K  |    3249 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    6797 K  |    6797 K  |
|       from large pool |      24    |      65    |    3548 K  |    3548 K  |
|       from small pool |     231    |     274    |    3249 K  |    3249 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3360 K  |    3360 K  |
|       from large pool |       9    |      14    |    1717 K  |    1717 K  |
|       from small pool |      12    |      16    |    1642 K  |    1642 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 48, Average loss: 0.0113, Accuracy: 0.6475, Time consumed:3.46s

Training Epoch: 49 [128/50000]	Loss: 0.4900	LR: 0.025000
Training Epoch: 49 [256/50000]	Loss: 0.3245	LR: 0.025000
Training Epoch: 49 [384/50000]	Loss: 0.5363	LR: 0.025000
Training Epoch: 49 [512/50000]	Loss: 0.6361	LR: 0.025000
Training Epoch: 49 [640/50000]	Loss: 0.5273	LR: 0.025000
Training Epoch: 49 [768/50000]	Loss: 0.4591	LR: 0.025000
Training Epoch: 49 [896/50000]	Loss: 0.3625	LR: 0.025000
Training Epoch: 49 [1024/50000]	Loss: 0.5144	LR: 0.025000
Training Epoch: 49 [1152/50000]	Loss: 0.4524	LR: 0.025000
Training Epoch: 49 [1280/50000]	Loss: 0.5663	LR: 0.025000
Training Epoch: 49 [1408/50000]	Loss: 0.4413	LR: 0.025000
Training Epoch: 49 [1536/50000]	Loss: 0.6727	LR: 0.025000
Training Epoch: 49 [1664/50000]	Loss: 0.4267	LR: 0.025000
Training Epoch: 49 [1792/50000]	Loss: 0.4505	LR: 0.025000
Training Epoch: 49 [1920/50000]	Loss: 0.4114	LR: 0.025000
Training Epoch: 49 [2048/50000]	Loss: 0.4341	LR: 0.025000
Training Epoch: 49 [2176/50000]	Loss: 0.5742	LR: 0.025000
Training Epoch: 49 [2304/50000]	Loss: 0.4488	LR: 0.025000
Training Epoch: 49 [2432/50000]	Loss: 0.6322	LR: 0.025000
Training Epoch: 49 [2560/50000]	Loss: 0.5615	LR: 0.025000
Training Epoch: 49 [2688/50000]	Loss: 0.3777	LR: 0.025000
Training Epoch: 49 [2816/50000]	Loss: 0.5761	LR: 0.025000
Training Epoch: 49 [2944/50000]	Loss: 0.4420	LR: 0.025000
Training Epoch: 49 [3072/50000]	Loss: 0.3008	LR: 0.025000
Training Epoch: 49 [3200/50000]	Loss: 0.4264	LR: 0.025000
Training Epoch: 49 [3328/50000]	Loss: 0.5303	LR: 0.025000
Training Epoch: 49 [3456/50000]	Loss: 0.4994	LR: 0.025000
Training Epoch: 49 [3584/50000]	Loss: 0.6096	LR: 0.025000
Training Epoch: 49 [3712/50000]	Loss: 0.5879	LR: 0.025000
Training Epoch: 49 [3840/50000]	Loss: 0.5422	LR: 0.025000
Training Epoch: 49 [3968/50000]	Loss: 0.4126	LR: 0.025000
Training Epoch: 49 [4096/50000]	Loss: 0.3619	LR: 0.025000
Training Epoch: 49 [4224/50000]	Loss: 0.6230	LR: 0.025000
Training Epoch: 49 [4352/50000]	Loss: 0.4378	LR: 0.025000
Training Epoch: 49 [4480/50000]	Loss: 0.5347	LR: 0.025000
Training Epoch: 49 [4608/50000]	Loss: 0.4364	LR: 0.025000
Training Epoch: 49 [4736/50000]	Loss: 0.4912	LR: 0.025000
Training Epoch: 49 [4864/50000]	Loss: 0.4637	LR: 0.025000
Training Epoch: 49 [4992/50000]	Loss: 0.3700	LR: 0.025000
Training Epoch: 49 [5120/50000]	Loss: 0.3876	LR: 0.025000
Training Epoch: 49 [5248/50000]	Loss: 0.5447	LR: 0.025000
Training Epoch: 49 [5376/50000]	Loss: 0.4376	LR: 0.025000
Training Epoch: 49 [5504/50000]	Loss: 0.4254	LR: 0.025000
Training Epoch: 49 [5632/50000]	Loss: 0.4374	LR: 0.025000
Training Epoch: 49 [5760/50000]	Loss: 0.3951	LR: 0.025000
Training Epoch: 49 [5888/50000]	Loss: 0.5281	LR: 0.025000
Training Epoch: 49 [6016/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 49 [6144/50000]	Loss: 0.3997	LR: 0.025000
Training Epoch: 49 [6272/50000]	Loss: 0.4857	LR: 0.025000
Training Epoch: 49 [6400/50000]	Loss: 0.3791	LR: 0.025000
Training Epoch: 49 [6528/50000]	Loss: 0.4507	LR: 0.025000
Training Epoch: 49 [6656/50000]	Loss: 0.5197	LR: 0.025000
Training Epoch: 49 [6784/50000]	Loss: 0.6665	LR: 0.025000
Training Epoch: 49 [6912/50000]	Loss: 0.3496	LR: 0.025000
Training Epoch: 49 [7040/50000]	Loss: 0.6058	LR: 0.025000
Training Epoch: 49 [7168/50000]	Loss: 0.5126	LR: 0.025000
Training Epoch: 49 [7296/50000]	Loss: 0.4351	LR: 0.025000
Training Epoch: 49 [7424/50000]	Loss: 0.6045	LR: 0.025000
Training Epoch: 49 [7552/50000]	Loss: 0.4527	LR: 0.025000
Training Epoch: 49 [7680/50000]	Loss: 0.4105	LR: 0.025000
Training Epoch: 49 [7808/50000]	Loss: 0.4324	LR: 0.025000
Training Epoch: 49 [7936/50000]	Loss: 0.5031	LR: 0.025000
Training Epoch: 49 [8064/50000]	Loss: 0.6337	LR: 0.025000
Training Epoch: 49 [8192/50000]	Loss: 0.4628	LR: 0.025000
Training Epoch: 49 [8320/50000]	Loss: 0.4171	LR: 0.025000
Training Epoch: 49 [8448/50000]	Loss: 0.5242	LR: 0.025000
Training Epoch: 49 [8576/50000]	Loss: 0.5300	LR: 0.025000
Training Epoch: 49 [8704/50000]	Loss: 0.4805	LR: 0.025000
Training Epoch: 49 [8832/50000]	Loss: 0.4606	LR: 0.025000
Training Epoch: 49 [8960/50000]	Loss: 0.4706	LR: 0.025000
Training Epoch: 49 [9088/50000]	Loss: 0.4761	LR: 0.025000
Training Epoch: 49 [9216/50000]	Loss: 0.5461	LR: 0.025000
Training Epoch: 49 [9344/50000]	Loss: 0.4929	LR: 0.025000
Training Epoch: 49 [9472/50000]	Loss: 0.4376	LR: 0.025000
Training Epoch: 49 [9600/50000]	Loss: 0.5019	LR: 0.025000
Training Epoch: 49 [9728/50000]	Loss: 0.5509	LR: 0.025000
Training Epoch: 49 [9856/50000]	Loss: 0.7967	LR: 0.025000
Training Epoch: 49 [9984/50000]	Loss: 0.5536	LR: 0.025000
Training Epoch: 49 [10112/50000]	Loss: 0.4526	LR: 0.025000
Training Epoch: 49 [10240/50000]	Loss: 0.4956	LR: 0.025000
Training Epoch: 49 [10368/50000]	Loss: 0.6044	LR: 0.025000
Training Epoch: 49 [10496/50000]	Loss: 0.4435	LR: 0.025000
Training Epoch: 49 [10624/50000]	Loss: 0.4541	LR: 0.025000
Training Epoch: 49 [10752/50000]	Loss: 0.4650	LR: 0.025000
Training Epoch: 49 [10880/50000]	Loss: 0.4125	LR: 0.025000
Training Epoch: 49 [11008/50000]	Loss: 0.4350	LR: 0.025000
Training Epoch: 49 [11136/50000]	Loss: 0.4408	LR: 0.025000
Training Epoch: 49 [11264/50000]	Loss: 0.4640	LR: 0.025000
Training Epoch: 49 [11392/50000]	Loss: 0.4147	LR: 0.025000
Training Epoch: 49 [11520/50000]	Loss: 0.6170	LR: 0.025000
Training Epoch: 49 [11648/50000]	Loss: 0.4525	LR: 0.025000
Training Epoch: 49 [11776/50000]	Loss: 0.4328	LR: 0.025000
Training Epoch: 49 [11904/50000]	Loss: 0.4692	LR: 0.025000
Training Epoch: 49 [12032/50000]	Loss: 0.5653	LR: 0.025000
Training Epoch: 49 [12160/50000]	Loss: 0.5857	LR: 0.025000
Training Epoch: 49 [12288/50000]	Loss: 0.4738	LR: 0.025000
Training Epoch: 49 [12416/50000]	Loss: 0.4936	LR: 0.025000
Training Epoch: 49 [12544/50000]	Loss: 0.5145	LR: 0.025000
Training Epoch: 49 [12672/50000]	Loss: 0.4940	LR: 0.025000
Training Epoch: 49 [12800/50000]	Loss: 0.6942	LR: 0.025000
Training Epoch: 49 [12928/50000]	Loss: 0.4905	LR: 0.025000
Training Epoch: 49 [13056/50000]	Loss: 0.5220	LR: 0.025000
Training Epoch: 49 [13184/50000]	Loss: 0.7853	LR: 0.025000
Training Epoch: 49 [13312/50000]	Loss: 0.5846	LR: 0.025000
Training Epoch: 49 [13440/50000]	Loss: 0.5064	LR: 0.025000
Training Epoch: 49 [13568/50000]	Loss: 0.5507	LR: 0.025000
Training Epoch: 49 [13696/50000]	Loss: 0.4588	LR: 0.025000
Training Epoch: 49 [13824/50000]	Loss: 0.4142	LR: 0.025000
Training Epoch: 49 [13952/50000]	Loss: 0.4933	LR: 0.025000
Training Epoch: 49 [14080/50000]	Loss: 0.5373	LR: 0.025000
Training Epoch: 49 [14208/50000]	Loss: 0.4321	LR: 0.025000
Training Epoch: 49 [14336/50000]	Loss: 0.5662	LR: 0.025000
Training Epoch: 49 [14464/50000]	Loss: 0.4551	LR: 0.025000
Training Epoch: 49 [14592/50000]	Loss: 0.5575	LR: 0.025000
Training Epoch: 49 [14720/50000]	Loss: 0.4507	LR: 0.025000
Training Epoch: 49 [14848/50000]	Loss: 0.4179	LR: 0.025000
Training Epoch: 49 [14976/50000]	Loss: 0.3855	LR: 0.025000
Training Epoch: 49 [15104/50000]	Loss: 0.5137	LR: 0.025000
Training Epoch: 49 [15232/50000]	Loss: 0.4530	LR: 0.025000
Training Epoch: 49 [15360/50000]	Loss: 0.4858	LR: 0.025000
Training Epoch: 49 [15488/50000]	Loss: 0.6802	LR: 0.025000
Training Epoch: 49 [15616/50000]	Loss: 0.6970	LR: 0.025000
Training Epoch: 49 [15744/50000]	Loss: 0.5090	LR: 0.025000
Training Epoch: 49 [15872/50000]	Loss: 0.5545	LR: 0.025000
Training Epoch: 49 [16000/50000]	Loss: 0.4417	LR: 0.025000
Training Epoch: 49 [16128/50000]	Loss: 0.5765	LR: 0.025000
Training Epoch: 49 [16256/50000]	Loss: 0.4819	LR: 0.025000
Training Epoch: 49 [16384/50000]	Loss: 0.4763	LR: 0.025000
Training Epoch: 49 [16512/50000]	Loss: 0.5318	LR: 0.025000
Training Epoch: 49 [16640/50000]	Loss: 0.4454	LR: 0.025000
Training Epoch: 49 [16768/50000]	Loss: 0.5159	LR: 0.025000
Training Epoch: 49 [16896/50000]	Loss: 0.6131	LR: 0.025000
Training Epoch: 49 [17024/50000]	Loss: 0.3913	LR: 0.025000
Training Epoch: 49 [17152/50000]	Loss: 0.6608	LR: 0.025000
Training Epoch: 49 [17280/50000]	Loss: 0.4125	LR: 0.025000
Training Epoch: 49 [17408/50000]	Loss: 0.4820	LR: 0.025000
Training Epoch: 49 [17536/50000]	Loss: 0.3927	LR: 0.025000
Training Epoch: 49 [17664/50000]	Loss: 0.5402	LR: 0.025000
Training Epoch: 49 [17792/50000]	Loss: 0.4298	LR: 0.025000
Training Epoch: 49 [17920/50000]	Loss: 0.4969	LR: 0.025000
Training Epoch: 49 [18048/50000]	Loss: 0.5414	LR: 0.025000
Training Epoch: 49 [18176/50000]	Loss: 0.6032	LR: 0.025000
Training Epoch: 49 [18304/50000]	Loss: 0.4844	LR: 0.025000
Training Epoch: 49 [18432/50000]	Loss: 0.5607	LR: 0.025000
Training Epoch: 49 [18560/50000]	Loss: 0.4240	LR: 0.025000
Training Epoch: 49 [18688/50000]	Loss: 0.4413	LR: 0.025000
Training Epoch: 49 [18816/50000]	Loss: 0.5716	LR: 0.025000
Training Epoch: 49 [18944/50000]	Loss: 0.4096	LR: 0.025000
Training Epoch: 49 [19072/50000]	Loss: 0.5855	LR: 0.025000
Training Epoch: 49 [19200/50000]	Loss: 0.4969	LR: 0.025000
Training Epoch: 49 [19328/50000]	Loss: 0.5890	LR: 0.025000
Training Epoch: 49 [19456/50000]	Loss: 0.5628	LR: 0.025000
Training Epoch: 49 [19584/50000]	Loss: 0.6458	LR: 0.025000
Training Epoch: 49 [19712/50000]	Loss: 0.5390	LR: 0.025000
Training Epoch: 49 [19840/50000]	Loss: 0.5333	LR: 0.025000
Training Epoch: 49 [19968/50000]	Loss: 0.6479	LR: 0.025000
Training Epoch: 49 [20096/50000]	Loss: 0.5445	LR: 0.025000
Training Epoch: 49 [20224/50000]	Loss: 0.5112	LR: 0.025000
Training Epoch: 49 [20352/50000]	Loss: 0.5608	LR: 0.025000
Training Epoch: 49 [20480/50000]	Loss: 0.5595	LR: 0.025000
Training Epoch: 49 [20608/50000]	Loss: 0.6375	LR: 0.025000
Training Epoch: 49 [20736/50000]	Loss: 0.5246	LR: 0.025000
Training Epoch: 49 [20864/50000]	Loss: 0.5563	LR: 0.025000
Training Epoch: 49 [20992/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 49 [21120/50000]	Loss: 0.6040	LR: 0.025000
Training Epoch: 49 [21248/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 49 [21376/50000]	Loss: 0.4666	LR: 0.025000
Training Epoch: 49 [21504/50000]	Loss: 0.4153	LR: 0.025000
Training Epoch: 49 [21632/50000]	Loss: 0.5137	LR: 0.025000
Training Epoch: 49 [21760/50000]	Loss: 0.4152	LR: 0.025000
Training Epoch: 49 [21888/50000]	Loss: 0.4299	LR: 0.025000
Training Epoch: 49 [22016/50000]	Loss: 0.4480	LR: 0.025000
Training Epoch: 49 [22144/50000]	Loss: 0.4728	LR: 0.025000
Training Epoch: 49 [22272/50000]	Loss: 0.4686	LR: 0.025000
Training Epoch: 49 [22400/50000]	Loss: 0.3287	LR: 0.025000
Training Epoch: 49 [22528/50000]	Loss: 0.3613	LR: 0.025000
Training Epoch: 49 [22656/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 49 [22784/50000]	Loss: 0.4588	LR: 0.025000
Training Epoch: 49 [22912/50000]	Loss: 0.6470	LR: 0.025000
Training Epoch: 49 [23040/50000]	Loss: 0.4907	LR: 0.025000
Training Epoch: 49 [23168/50000]	Loss: 0.5345	LR: 0.025000
Training Epoch: 49 [23296/50000]	Loss: 0.4733	LR: 0.025000
Training Epoch: 49 [23424/50000]	Loss: 0.6307	LR: 0.025000
Training Epoch: 49 [23552/50000]	Loss: 0.4968	LR: 0.025000
Training Epoch: 49 [23680/50000]	Loss: 0.6171	LR: 0.025000
Training Epoch: 49 [23808/50000]	Loss: 0.6077	LR: 0.025000
Training Epoch: 49 [23936/50000]	Loss: 0.6366	LR: 0.025000
Training Epoch: 49 [24064/50000]	Loss: 0.6024	LR: 0.025000
Training Epoch: 49 [24192/50000]	Loss: 0.5829	LR: 0.025000
Training Epoch: 49 [24320/50000]	Loss: 0.6231	LR: 0.025000
Training Epoch: 49 [24448/50000]	Loss: 0.5031	LR: 0.025000
Training Epoch: 49 [24576/50000]	Loss: 0.4752	LR: 0.025000
Training Epoch: 49 [24704/50000]	Loss: 0.4245	LR: 0.025000
Training Epoch: 49 [24832/50000]	Loss: 0.5970	LR: 0.025000
Training Epoch: 49 [24960/50000]	Loss: 0.5180	LR: 0.025000
Training Epoch: 49 [25088/50000]	Loss: 0.5124	LR: 0.025000
Training Epoch: 49 [25216/50000]	Loss: 0.5690	LR: 0.025000
Training Epoch: 49 [25344/50000]	Loss: 0.3804	LR: 0.025000
Training Epoch: 49 [25472/50000]	Loss: 0.5267	LR: 0.025000
Training Epoch: 49 [25600/50000]	Loss: 0.3944	LR: 0.025000
Training Epoch: 49 [25728/50000]	Loss: 0.6044	LR: 0.025000
Training Epoch: 49 [25856/50000]	Loss: 0.6202	LR: 0.025000
Training Epoch: 49 [25984/50000]	Loss: 0.5175	LR: 0.025000
Training Epoch: 49 [26112/50000]	Loss: 0.6636	LR: 0.025000
Training Epoch: 49 [26240/50000]	Loss: 0.6611	LR: 0.025000
Training Epoch: 49 [26368/50000]	Loss: 0.6528	LR: 0.025000
Training Epoch: 49 [26496/50000]	Loss: 0.4759	LR: 0.025000
Training Epoch: 49 [26624/50000]	Loss: 0.5583	LR: 0.025000
Training Epoch: 49 [26752/50000]	Loss: 0.5185	LR: 0.025000
Training Epoch: 49 [26880/50000]	Loss: 0.6169	LR: 0.025000
Training Epoch: 49 [27008/50000]	Loss: 0.7464	LR: 0.025000
Training Epoch: 49 [27136/50000]	Loss: 0.5291	LR: 0.025000
Training Epoch: 49 [27264/50000]	Loss: 0.5444	LR: 0.025000
Training Epoch: 49 [27392/50000]	Loss: 0.4609	LR: 0.025000
Training Epoch: 49 [27520/50000]	Loss: 0.4124	LR: 0.025000
Training Epoch: 49 [27648/50000]	Loss: 0.6149	LR: 0.025000
Training Epoch: 49 [27776/50000]	Loss: 0.5682	LR: 0.025000
Training Epoch: 49 [27904/50000]	Loss: 0.4595	LR: 0.025000
Training Epoch: 49 [28032/50000]	Loss: 0.5429	LR: 0.025000
Training Epoch: 49 [28160/50000]	Loss: 0.6034	LR: 0.025000
Training Epoch: 49 [28288/50000]	Loss: 0.6321	LR: 0.025000
Training Epoch: 49 [28416/50000]	Loss: 0.4877	LR: 0.025000
Training Epoch: 49 [28544/50000]	Loss: 0.5917	LR: 0.025000
Training Epoch: 49 [28672/50000]	Loss: 0.7470	LR: 0.025000
Training Epoch: 49 [28800/50000]	Loss: 0.6249	LR: 0.025000
Training Epoch: 49 [28928/50000]	Loss: 0.5776	LR: 0.025000
Training Epoch: 49 [29056/50000]	Loss: 0.5722	LR: 0.025000
Training Epoch: 49 [29184/50000]	Loss: 0.4890	LR: 0.025000
Training Epoch: 49 [29312/50000]	Loss: 0.6413	LR: 0.025000
Training Epoch: 49 [29440/50000]	Loss: 0.5210	LR: 0.025000
Training Epoch: 49 [29568/50000]	Loss: 0.6390	LR: 0.025000
Training Epoch: 49 [29696/50000]	Loss: 0.5159	LR: 0.025000
Training Epoch: 49 [29824/50000]	Loss: 0.5580	LR: 0.025000
Training Epoch: 49 [29952/50000]	Loss: 0.6008	LR: 0.025000
Training Epoch: 49 [30080/50000]	Loss: 0.6107	LR: 0.025000
Training Epoch: 49 [30208/50000]	Loss: 0.6262	LR: 0.025000
Training Epoch: 49 [30336/50000]	Loss: 0.5897	LR: 0.025000
Training Epoch: 49 [30464/50000]	Loss: 0.6322	LR: 0.025000
Training Epoch: 49 [30592/50000]	Loss: 0.3183	LR: 0.025000
Training Epoch: 49 [30720/50000]	Loss: 0.6520	LR: 0.025000
Training Epoch: 49 [30848/50000]	Loss: 0.5956	LR: 0.025000
Training Epoch: 49 [30976/50000]	Loss: 0.5677	LR: 0.025000
Training Epoch: 49 [31104/50000]	Loss: 0.4508	LR: 0.025000
Training Epoch: 49 [31232/50000]	Loss: 0.6369	LR: 0.025000
Training Epoch: 49 [31360/50000]	Loss: 0.8509	LR: 0.025000
Training Epoch: 49 [31488/50000]	Loss: 0.5381	LR: 0.025000
Training Epoch: 49 [31616/50000]	Loss: 0.6785	LR: 0.025000
Training Epoch: 49 [31744/50000]	Loss: 0.5757	LR: 0.025000
Training Epoch: 49 [31872/50000]	Loss: 0.5852	LR: 0.025000
Training Epoch: 49 [32000/50000]	Loss: 0.6823	LR: 0.025000
Training Epoch: 49 [32128/50000]	Loss: 0.5464	LR: 0.025000
Training Epoch: 49 [32256/50000]	Loss: 0.4756	LR: 0.025000
Training Epoch: 49 [32384/50000]	Loss: 0.6982	LR: 0.025000
Training Epoch: 49 [32512/50000]	Loss: 0.5916	LR: 0.025000
Training Epoch: 49 [32640/50000]	Loss: 0.6479	LR: 0.025000
Training Epoch: 49 [32768/50000]	Loss: 0.6694	LR: 0.025000
Training Epoch: 49 [32896/50000]	Loss: 0.7367	LR: 0.025000
Training Epoch: 49 [33024/50000]	Loss: 0.6339	LR: 0.025000
Training Epoch: 49 [33152/50000]	Loss: 0.6582	LR: 0.025000
Training Epoch: 49 [33280/50000]	Loss: 0.6959	LR: 0.025000
Training Epoch: 49 [33408/50000]	Loss: 0.4949	LR: 0.025000
Training Epoch: 49 [33536/50000]	Loss: 0.8124	LR: 0.025000
Training Epoch: 49 [33664/50000]	Loss: 0.5404	LR: 0.025000
Training Epoch: 49 [33792/50000]	Loss: 0.4807	LR: 0.025000
Training Epoch: 49 [33920/50000]	Loss: 0.5000	LR: 0.025000
Training Epoch: 49 [34048/50000]	Loss: 0.4404	LR: 0.025000
Training Epoch: 49 [34176/50000]	Loss: 0.5261	LR: 0.025000
Training Epoch: 49 [34304/50000]	Loss: 0.5535	LR: 0.025000
Training Epoch: 49 [34432/50000]	Loss: 0.6066	LR: 0.025000
Training Epoch: 49 [34560/50000]	Loss: 0.5778	LR: 0.025000
Training Epoch: 49 [34688/50000]	Loss: 0.4909	LR: 0.025000
Training Epoch: 49 [34816/50000]	Loss: 0.5064	LR: 0.025000
Training Epoch: 49 [34944/50000]	Loss: 0.5400	LR: 0.025000
Training Epoch: 49 [35072/50000]	Loss: 0.5604	LR: 0.025000
Training Epoch: 49 [35200/50000]	Loss: 0.5117	LR: 0.025000
Training Epoch: 49 [35328/50000]	Loss: 0.4485	LR: 0.025000
Training Epoch: 49 [35456/50000]	Loss: 0.4327	LR: 0.025000
Training Epoch: 49 [35584/50000]	Loss: 0.5629	LR: 0.025000
Training Epoch: 49 [35712/50000]	Loss: 0.4312	LR: 0.025000
Training Epoch: 49 [35840/50000]	Loss: 0.5557	LR: 0.025000
Training Epoch: 49 [35968/50000]	Loss: 0.3888	LR: 0.025000
Training Epoch: 49 [36096/50000]	Loss: 0.4886	LR: 0.025000
Training Epoch: 49 [36224/50000]	Loss: 0.7247	LR: 0.025000
Training Epoch: 49 [36352/50000]	Loss: 0.7920	LR: 0.025000
Training Epoch: 49 [36480/50000]	Loss: 0.6667	LR: 0.025000
Training Epoch: 49 [36608/50000]	Loss: 0.6035	LR: 0.025000
Training Epoch: 49 [36736/50000]	Loss: 0.8536	LR: 0.025000
Training Epoch: 49 [36864/50000]	Loss: 0.4998	LR: 0.025000
Training Epoch: 49 [36992/50000]	Loss: 0.6174	LR: 0.025000
Training Epoch: 49 [37120/50000]	Loss: 0.5073	LR: 0.025000
Training Epoch: 49 [37248/50000]	Loss: 0.5945	LR: 0.025000
Training Epoch: 49 [37376/50000]	Loss: 0.4523	LR: 0.025000
Training Epoch: 49 [37504/50000]	Loss: 0.5571	LR: 0.025000
Training Epoch: 49 [37632/50000]	Loss: 0.5079	LR: 0.025000
Training Epoch: 49 [37760/50000]	Loss: 0.5016	LR: 0.025000
Training Epoch: 49 [37888/50000]	Loss: 0.5887	LR: 0.025000
Training Epoch: 49 [38016/50000]	Loss: 0.6842	LR: 0.025000
Training Epoch: 49 [38144/50000]	Loss: 0.6327	LR: 0.025000
Training Epoch: 49 [38272/50000]	Loss: 0.5743	LR: 0.025000
Training Epoch: 49 [38400/50000]	Loss: 0.5192	LR: 0.025000
Training Epoch: 49 [38528/50000]	Loss: 0.4694	LR: 0.025000
Training Epoch: 49 [38656/50000]	Loss: 0.7689	LR: 0.025000
Training Epoch: 49 [38784/50000]	Loss: 0.5099	LR: 0.025000
Training Epoch: 49 [38912/50000]	Loss: 0.7789	LR: 0.025000
Training Epoch: 49 [39040/50000]	Loss: 0.6315	LR: 0.025000
Training Epoch: 49 [39168/50000]	Loss: 0.3615	LR: 0.025000
Training Epoch: 49 [39296/50000]	Loss: 0.6874	LR: 0.025000
Training Epoch: 49 [39424/50000]	Loss: 0.4695	LR: 0.025000
Training Epoch: 49 [39552/50000]	Loss: 0.7492	LR: 0.025000
Training Epoch: 49 [39680/50000]	Loss: 0.5335	LR: 0.025000
Training Epoch: 49 [39808/50000]	Loss: 0.5328	LR: 0.025000
Training Epoch: 49 [39936/50000]	Loss: 0.6093	LR: 0.025000
Training Epoch: 49 [40064/50000]	Loss: 0.5827	LR: 0.025000
Training Epoch: 49 [40192/50000]	Loss: 0.6753	LR: 0.025000
Training Epoch: 49 [40320/50000]	Loss: 0.4669	LR: 0.025000
Training Epoch: 49 [40448/50000]	Loss: 0.5663	LR: 0.025000
Training Epoch: 49 [40576/50000]	Loss: 0.6147	LR: 0.025000
Training Epoch: 49 [40704/50000]	Loss: 0.4470	LR: 0.025000
Training Epoch: 49 [40832/50000]	Loss: 0.6328	LR: 0.025000
Training Epoch: 49 [40960/50000]	Loss: 0.5679	LR: 0.025000
Training Epoch: 49 [41088/50000]	Loss: 0.7302	LR: 0.025000
Training Epoch: 49 [41216/50000]	Loss: 0.5845	LR: 0.025000
Training Epoch: 49 [41344/50000]	Loss: 0.5943	LR: 0.025000
Training Epoch: 49 [41472/50000]	Loss: 0.7317	LR: 0.025000
Training Epoch: 49 [41600/50000]	Loss: 0.5067	LR: 0.025000
Training Epoch: 49 [41728/50000]	Loss: 0.6809	LR: 0.025000
Training Epoch: 49 [41856/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 49 [41984/50000]	Loss: 0.4807	LR: 0.025000
Training Epoch: 49 [42112/50000]	Loss: 0.4359	LR: 0.025000
Training Epoch: 49 [42240/50000]	Loss: 0.6000	LR: 0.025000
Training Epoch: 49 [42368/50000]	Loss: 0.5957	LR: 0.025000
Training Epoch: 49 [42496/50000]	Loss: 0.4863	LR: 0.025000
Training Epoch: 49 [42624/50000]	Loss: 0.4673	LR: 0.025000
Training Epoch: 49 [42752/50000]	Loss: 0.8168	LR: 0.025000
Training Epoch: 49 [42880/50000]	Loss: 0.7411	LR: 0.025000
Training Epoch: 49 [43008/50000]	Loss: 0.6187	LR: 0.025000
Training Epoch: 49 [43136/50000]	Loss: 0.5264	LR: 0.025000
Training Epoch: 49 [43264/50000]	Loss: 0.6778	LR: 0.025000
Training Epoch: 49 [43392/50000]	Loss: 0.7050	LR: 0.025000
Training Epoch: 49 [43520/50000]	Loss: 0.7199	LR: 0.025000
Training Epoch: 49 [43648/50000]	Loss: 0.6700	LR: 0.025000
Training Epoch: 49 [43776/50000]	Loss: 0.5252	LR: 0.025000
Training Epoch: 49 [43904/50000]	Loss: 0.5855	LR: 0.025000
Training Epoch: 49 [44032/50000]	Loss: 0.5648	LR: 0.025000
Training Epoch: 49 [44160/50000]	Loss: 0.6194	LR: 0.025000
Training Epoch: 49 [44288/50000]	Loss: 0.6751	LR: 0.025000
Training Epoch: 49 [44416/50000]	Loss: 0.6055	LR: 0.025000
Training Epoch: 49 [44544/50000]	Loss: 0.5575	LR: 0.025000
Training Epoch: 49 [44672/50000]	Loss: 0.4735	LR: 0.025000
Training Epoch: 49 [44800/50000]	Loss: 0.6151	LR: 0.025000
Training Epoch: 49 [44928/50000]	Loss: 0.5301	LR: 0.025000
Training Epoch: 49 [45056/50000]	Loss: 0.6404	LR: 0.025000
Training Epoch: 49 [45184/50000]	Loss: 0.4856	LR: 0.025000
Training Epoch: 49 [45312/50000]	Loss: 0.4974	LR: 0.025000
Training Epoch: 49 [45440/50000]	Loss: 0.5118	LR: 0.025000
Training Epoch: 49 [45568/50000]	Loss: 0.7100	LR: 0.025000
Training Epoch: 49 [45696/50000]	Loss: 0.5587	LR: 0.025000
Training Epoch: 49 [45824/50000]	Loss: 0.5293	LR: 0.025000
Training Epoch: 49 [45952/50000]	Loss: 0.5322	LR: 0.025000
Training Epoch: 49 [46080/50000]	Loss: 0.5095	LR: 0.025000
Training Epoch: 49 [46208/50000]	Loss: 0.5905	LR: 0.025000
Training Epoch: 49 [46336/50000]	Loss: 0.8459	LR: 0.025000
Training Epoch: 49 [46464/50000]	Loss: 0.8661	LR: 0.025000
Training Epoch: 49 [46592/50000]	Loss: 0.6246	LR: 0.025000
Training Epoch: 49 [46720/50000]	Loss: 0.6429	LR: 0.025000
Training Epoch: 49 [46848/50000]	Loss: 0.4893	LR: 0.025000
Training Epoch: 49 [46976/50000]	Loss: 0.6382	LR: 0.025000
Training Epoch: 49 [47104/50000]	Loss: 0.6533	LR: 0.025000
Training Epoch: 49 [47232/50000]	Loss: 0.7707	LR: 0.025000
Training Epoch: 49 [47360/50000]	Loss: 0.6412	LR: 0.025000
Training Epoch: 49 [47488/50000]	Loss: 0.4987	LR: 0.025000
Training Epoch: 49 [47616/50000]	Loss: 0.4613	LR: 0.025000
Training Epoch: 49 [47744/50000]	Loss: 0.6410	LR: 0.025000
Training Epoch: 49 [47872/50000]	Loss: 0.7200	LR: 0.025000
Training Epoch: 49 [48000/50000]	Loss: 0.6379	LR: 0.025000
Training Epoch: 49 [48128/50000]	Loss: 0.4819	LR: 0.025000
Training Epoch: 49 [48256/50000]	Loss: 0.6027	LR: 0.025000
Training Epoch: 49 [48384/50000]	Loss: 0.7914	LR: 0.025000
Training Epoch: 49 [48512/50000]	Loss: 0.6608	LR: 0.025000
Training Epoch: 49 [48640/50000]	Loss: 0.7085	LR: 0.025000
Training Epoch: 49 [48768/50000]	Loss: 0.5396	LR: 0.025000
Training Epoch: 49 [48896/50000]	Loss: 0.6696	LR: 0.025000
Training Epoch: 49 [49024/50000]	Loss: 0.5745	LR: 0.025000
Training Epoch: 49 [49152/50000]	Loss: 0.4687	LR: 0.025000
Training Epoch: 49 [49280/50000]	Loss: 0.7082	LR: 0.025000
Training Epoch: 49 [49408/50000]	Loss: 0.4114	LR: 0.025000
Training Epoch: 49 [49536/50000]	Loss: 0.6701	LR: 0.025000
Training Epoch: 49 [49664/50000]	Loss: 0.5414	LR: 0.025000
Training Epoch: 49 [49792/50000]	Loss: 0.6853	LR: 0.025000
Training Epoch: 49 [49920/50000]	Loss: 0.5491	LR: 0.025000
Training Epoch: 49 [50000/50000]	Loss: 0.5264	LR: 0.025000
epoch 49 training time consumed: 53.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  179833 GB |  179833 GB |
|       from large pool |  123392 KB |    1034 MB |  179656 GB |  179655 GB |
|       from small pool |   10798 KB |      13 MB |     177 GB |     177 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  179833 GB |  179833 GB |
|       from large pool |  123392 KB |    1034 MB |  179656 GB |  179655 GB |
|       from small pool |   10798 KB |      13 MB |     177 GB |     177 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   79138 GB |   79138 GB |
|       from large pool |  155136 KB |  433088 KB |   78942 GB |   78942 GB |
|       from small pool |    1490 KB |    3494 KB |     195 GB |     195 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    6939 K  |    6939 K  |
|       from large pool |      24    |      65    |    3622 K  |    3622 K  |
|       from small pool |     231    |     274    |    3317 K  |    3317 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    6939 K  |    6939 K  |
|       from large pool |      24    |      65    |    3622 K  |    3622 K  |
|       from small pool |     231    |     274    |    3317 K  |    3317 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3430 K  |    3430 K  |
|       from large pool |       9    |      14    |    1753 K  |    1753 K  |
|       from small pool |      12    |      16    |    1676 K  |    1676 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 49, Average loss: 0.0107, Accuracy: 0.6535, Time consumed:3.46s

Training Epoch: 50 [128/50000]	Loss: 0.5220	LR: 0.025000
Training Epoch: 50 [256/50000]	Loss: 0.3899	LR: 0.025000
Training Epoch: 50 [384/50000]	Loss: 0.5981	LR: 0.025000
Training Epoch: 50 [512/50000]	Loss: 0.5443	LR: 0.025000
Training Epoch: 50 [640/50000]	Loss: 0.4461	LR: 0.025000
Training Epoch: 50 [768/50000]	Loss: 0.5405	LR: 0.025000
Training Epoch: 50 [896/50000]	Loss: 0.5758	LR: 0.025000
Training Epoch: 50 [1024/50000]	Loss: 0.4882	LR: 0.025000
Training Epoch: 50 [1152/50000]	Loss: 0.6055	LR: 0.025000
Training Epoch: 50 [1280/50000]	Loss: 0.7000	LR: 0.025000
Training Epoch: 50 [1408/50000]	Loss: 0.5631	LR: 0.025000
Training Epoch: 50 [1536/50000]	Loss: 0.4421	LR: 0.025000
Training Epoch: 50 [1664/50000]	Loss: 0.4526	LR: 0.025000
Training Epoch: 50 [1792/50000]	Loss: 0.4720	LR: 0.025000
Training Epoch: 50 [1920/50000]	Loss: 0.7186	LR: 0.025000
Training Epoch: 50 [2048/50000]	Loss: 0.6385	LR: 0.025000
Training Epoch: 50 [2176/50000]	Loss: 0.5540	LR: 0.025000
Training Epoch: 50 [2304/50000]	Loss: 0.3906	LR: 0.025000
Training Epoch: 50 [2432/50000]	Loss: 0.3802	LR: 0.025000
Training Epoch: 50 [2560/50000]	Loss: 0.5004	LR: 0.025000
Training Epoch: 50 [2688/50000]	Loss: 0.4564	LR: 0.025000
Training Epoch: 50 [2816/50000]	Loss: 0.3821	LR: 0.025000
Training Epoch: 50 [2944/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 50 [3072/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 50 [3200/50000]	Loss: 0.5272	LR: 0.025000
Training Epoch: 50 [3328/50000]	Loss: 0.5790	LR: 0.025000
Training Epoch: 50 [3456/50000]	Loss: 0.4785	LR: 0.025000
Training Epoch: 50 [3584/50000]	Loss: 0.5843	LR: 0.025000
Training Epoch: 50 [3712/50000]	Loss: 0.4722	LR: 0.025000
Training Epoch: 50 [3840/50000]	Loss: 0.4918	LR: 0.025000
Training Epoch: 50 [3968/50000]	Loss: 0.4933	LR: 0.025000
Training Epoch: 50 [4096/50000]	Loss: 0.4323	LR: 0.025000
Training Epoch: 50 [4224/50000]	Loss: 0.5461	LR: 0.025000
Training Epoch: 50 [4352/50000]	Loss: 0.6677	LR: 0.025000
Training Epoch: 50 [4480/50000]	Loss: 0.5472	LR: 0.025000
Training Epoch: 50 [4608/50000]	Loss: 0.5270	LR: 0.025000
Training Epoch: 50 [4736/50000]	Loss: 0.5738	LR: 0.025000
Training Epoch: 50 [4864/50000]	Loss: 0.3989	LR: 0.025000
Training Epoch: 50 [4992/50000]	Loss: 0.5898	LR: 0.025000
Training Epoch: 50 [5120/50000]	Loss: 0.4464	LR: 0.025000
Training Epoch: 50 [5248/50000]	Loss: 0.5681	LR: 0.025000
Training Epoch: 50 [5376/50000]	Loss: 0.4189	LR: 0.025000
Training Epoch: 50 [5504/50000]	Loss: 0.4611	LR: 0.025000
Training Epoch: 50 [5632/50000]	Loss: 0.5020	LR: 0.025000
Training Epoch: 50 [5760/50000]	Loss: 0.5197	LR: 0.025000
Training Epoch: 50 [5888/50000]	Loss: 0.4691	LR: 0.025000
Training Epoch: 50 [6016/50000]	Loss: 0.5173	LR: 0.025000
Training Epoch: 50 [6144/50000]	Loss: 0.5382	LR: 0.025000
Training Epoch: 50 [6272/50000]	Loss: 0.3870	LR: 0.025000
Training Epoch: 50 [6400/50000]	Loss: 0.5376	LR: 0.025000
Training Epoch: 50 [6528/50000]	Loss: 0.6172	LR: 0.025000
Training Epoch: 50 [6656/50000]	Loss: 0.4013	LR: 0.025000
Training Epoch: 50 [6784/50000]	Loss: 0.4079	LR: 0.025000
Training Epoch: 50 [6912/50000]	Loss: 0.5266	LR: 0.025000
Training Epoch: 50 [7040/50000]	Loss: 0.4524	LR: 0.025000
Training Epoch: 50 [7168/50000]	Loss: 0.6019	LR: 0.025000
Training Epoch: 50 [7296/50000]	Loss: 0.5586	LR: 0.025000
Training Epoch: 50 [7424/50000]	Loss: 0.5069	LR: 0.025000
Training Epoch: 50 [7552/50000]	Loss: 0.6512	LR: 0.025000
Training Epoch: 50 [7680/50000]	Loss: 0.6157	LR: 0.025000
Training Epoch: 50 [7808/50000]	Loss: 0.4881	LR: 0.025000
Training Epoch: 50 [7936/50000]	Loss: 0.4282	LR: 0.025000
Training Epoch: 50 [8064/50000]	Loss: 0.4288	LR: 0.025000
Training Epoch: 50 [8192/50000]	Loss: 0.3986	LR: 0.025000
Training Epoch: 50 [8320/50000]	Loss: 0.6815	LR: 0.025000
Training Epoch: 50 [8448/50000]	Loss: 0.5179	LR: 0.025000
Training Epoch: 50 [8576/50000]	Loss: 0.5192	LR: 0.025000
Training Epoch: 50 [8704/50000]	Loss: 0.5069	LR: 0.025000
Training Epoch: 50 [8832/50000]	Loss: 0.4143	LR: 0.025000
Training Epoch: 50 [8960/50000]	Loss: 0.5173	LR: 0.025000
Training Epoch: 50 [9088/50000]	Loss: 0.4568	LR: 0.025000
Training Epoch: 50 [9216/50000]	Loss: 0.4305	LR: 0.025000
Training Epoch: 50 [9344/50000]	Loss: 0.5296	LR: 0.025000
Training Epoch: 50 [9472/50000]	Loss: 0.4540	LR: 0.025000
Training Epoch: 50 [9600/50000]	Loss: 0.4584	LR: 0.025000
Training Epoch: 50 [9728/50000]	Loss: 0.5809	LR: 0.025000
Training Epoch: 50 [9856/50000]	Loss: 0.4919	LR: 0.025000
Training Epoch: 50 [9984/50000]	Loss: 0.5427	LR: 0.025000
Training Epoch: 50 [10112/50000]	Loss: 0.4920	LR: 0.025000
Training Epoch: 50 [10240/50000]	Loss: 0.4546	LR: 0.025000
Training Epoch: 50 [10368/50000]	Loss: 0.5838	LR: 0.025000
Training Epoch: 50 [10496/50000]	Loss: 0.5077	LR: 0.025000
Training Epoch: 50 [10624/50000]	Loss: 0.4618	LR: 0.025000
Training Epoch: 50 [10752/50000]	Loss: 0.5142	LR: 0.025000
Training Epoch: 50 [10880/50000]	Loss: 0.4941	LR: 0.025000
Training Epoch: 50 [11008/50000]	Loss: 0.6082	LR: 0.025000
Training Epoch: 50 [11136/50000]	Loss: 0.5883	LR: 0.025000
Training Epoch: 50 [11264/50000]	Loss: 0.5017	LR: 0.025000
Training Epoch: 50 [11392/50000]	Loss: 0.4448	LR: 0.025000
Training Epoch: 50 [11520/50000]	Loss: 0.6104	LR: 0.025000
Training Epoch: 50 [11648/50000]	Loss: 0.3678	LR: 0.025000
Training Epoch: 50 [11776/50000]	Loss: 0.4363	LR: 0.025000
Training Epoch: 50 [11904/50000]	Loss: 0.5896	LR: 0.025000
Training Epoch: 50 [12032/50000]	Loss: 0.5958	LR: 0.025000
Training Epoch: 50 [12160/50000]	Loss: 0.4184	LR: 0.025000
Training Epoch: 50 [12288/50000]	Loss: 0.5206	LR: 0.025000
Training Epoch: 50 [12416/50000]	Loss: 0.5365	LR: 0.025000
Training Epoch: 50 [12544/50000]	Loss: 0.5084	LR: 0.025000
Training Epoch: 50 [12672/50000]	Loss: 0.5517	LR: 0.025000
Training Epoch: 50 [12800/50000]	Loss: 0.4083	LR: 0.025000
Training Epoch: 50 [12928/50000]	Loss: 0.4351	LR: 0.025000
Training Epoch: 50 [13056/50000]	Loss: 0.5759	LR: 0.025000
Training Epoch: 50 [13184/50000]	Loss: 0.5973	LR: 0.025000
Training Epoch: 50 [13312/50000]	Loss: 0.6272	LR: 0.025000
Training Epoch: 50 [13440/50000]	Loss: 0.3847	LR: 0.025000
Training Epoch: 50 [13568/50000]	Loss: 0.6541	LR: 0.025000
Training Epoch: 50 [13696/50000]	Loss: 0.4762	LR: 0.025000
Training Epoch: 50 [13824/50000]	Loss: 0.5097	LR: 0.025000
Training Epoch: 50 [13952/50000]	Loss: 0.5584	LR: 0.025000
Training Epoch: 50 [14080/50000]	Loss: 0.6026	LR: 0.025000
Training Epoch: 50 [14208/50000]	Loss: 0.4826	LR: 0.025000
Training Epoch: 50 [14336/50000]	Loss: 0.6320	LR: 0.025000
Training Epoch: 50 [14464/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 50 [14592/50000]	Loss: 0.4354	LR: 0.025000
Training Epoch: 50 [14720/50000]	Loss: 0.4707	LR: 0.025000
Training Epoch: 50 [14848/50000]	Loss: 0.6036	LR: 0.025000
Training Epoch: 50 [14976/50000]	Loss: 0.6802	LR: 0.025000
Training Epoch: 50 [15104/50000]	Loss: 0.3829	LR: 0.025000
Training Epoch: 50 [15232/50000]	Loss: 0.4173	LR: 0.025000
Training Epoch: 50 [15360/50000]	Loss: 0.5140	LR: 0.025000
Training Epoch: 50 [15488/50000]	Loss: 0.6381	LR: 0.025000
Training Epoch: 50 [15616/50000]	Loss: 0.5276	LR: 0.025000
Training Epoch: 50 [15744/50000]	Loss: 0.5206	LR: 0.025000
Training Epoch: 50 [15872/50000]	Loss: 0.5620	LR: 0.025000
Training Epoch: 50 [16000/50000]	Loss: 0.5627	LR: 0.025000
Training Epoch: 50 [16128/50000]	Loss: 0.5852	LR: 0.025000
Training Epoch: 50 [16256/50000]	Loss: 0.5639	LR: 0.025000
Training Epoch: 50 [16384/50000]	Loss: 0.3658	LR: 0.025000
Training Epoch: 50 [16512/50000]	Loss: 0.4856	LR: 0.025000
Training Epoch: 50 [16640/50000]	Loss: 0.3772	LR: 0.025000
Training Epoch: 50 [16768/50000]	Loss: 0.5173	LR: 0.025000
Training Epoch: 50 [16896/50000]	Loss: 0.5081	LR: 0.025000
Training Epoch: 50 [17024/50000]	Loss: 0.6036	LR: 0.025000
Training Epoch: 50 [17152/50000]	Loss: 0.5317	LR: 0.025000
Training Epoch: 50 [17280/50000]	Loss: 0.5600	LR: 0.025000
Training Epoch: 50 [17408/50000]	Loss: 0.5410	LR: 0.025000
Training Epoch: 50 [17536/50000]	Loss: 0.7005	LR: 0.025000
Training Epoch: 50 [17664/50000]	Loss: 0.4159	LR: 0.025000
Training Epoch: 50 [17792/50000]	Loss: 0.4524	LR: 0.025000
Training Epoch: 50 [17920/50000]	Loss: 0.4134	LR: 0.025000
Training Epoch: 50 [18048/50000]	Loss: 0.6020	LR: 0.025000
Training Epoch: 50 [18176/50000]	Loss: 0.7247	LR: 0.025000
Training Epoch: 50 [18304/50000]	Loss: 0.4905	LR: 0.025000
Training Epoch: 50 [18432/50000]	Loss: 0.5426	LR: 0.025000
Training Epoch: 50 [18560/50000]	Loss: 0.5431	LR: 0.025000
Training Epoch: 50 [18688/50000]	Loss: 0.5114	LR: 0.025000
Training Epoch: 50 [18816/50000]	Loss: 0.4430	LR: 0.025000
Training Epoch: 50 [18944/50000]	Loss: 0.4365	LR: 0.025000
Training Epoch: 50 [19072/50000]	Loss: 0.4500	LR: 0.025000
Training Epoch: 50 [19200/50000]	Loss: 0.5930	LR: 0.025000
Training Epoch: 50 [19328/50000]	Loss: 0.6468	LR: 0.025000
Training Epoch: 50 [19456/50000]	Loss: 0.5784	LR: 0.025000
Training Epoch: 50 [19584/50000]	Loss: 0.5440	LR: 0.025000
Training Epoch: 50 [19712/50000]	Loss: 0.7290	LR: 0.025000
Training Epoch: 50 [19840/50000]	Loss: 0.5889	LR: 0.025000
Training Epoch: 50 [19968/50000]	Loss: 0.4508	LR: 0.025000
Training Epoch: 50 [20096/50000]	Loss: 0.2116	LR: 0.025000
Training Epoch: 50 [20224/50000]	Loss: 0.5780	LR: 0.025000
Training Epoch: 50 [20352/50000]	Loss: 0.7983	LR: 0.025000
Training Epoch: 50 [20480/50000]	Loss: 0.6316	LR: 0.025000
Training Epoch: 50 [20608/50000]	Loss: 0.5375	LR: 0.025000
Training Epoch: 50 [20736/50000]	Loss: 0.7252	LR: 0.025000
Training Epoch: 50 [20864/50000]	Loss: 0.3712	LR: 0.025000
Training Epoch: 50 [20992/50000]	Loss: 0.5545	LR: 0.025000
Training Epoch: 50 [21120/50000]	Loss: 0.6053	LR: 0.025000
Training Epoch: 50 [21248/50000]	Loss: 0.6823	LR: 0.025000
Training Epoch: 50 [21376/50000]	Loss: 0.5024	LR: 0.025000
Training Epoch: 50 [21504/50000]	Loss: 0.5320	LR: 0.025000
Training Epoch: 50 [21632/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 50 [21760/50000]	Loss: 0.8119	LR: 0.025000
Training Epoch: 50 [21888/50000]	Loss: 0.4577	LR: 0.025000
Training Epoch: 50 [22016/50000]	Loss: 0.6438	LR: 0.025000
Training Epoch: 50 [22144/50000]	Loss: 0.4844	LR: 0.025000
Training Epoch: 50 [22272/50000]	Loss: 0.5546	LR: 0.025000
Training Epoch: 50 [22400/50000]	Loss: 0.6009	LR: 0.025000
Training Epoch: 50 [22528/50000]	Loss: 0.4459	LR: 0.025000
Training Epoch: 50 [22656/50000]	Loss: 0.5369	LR: 0.025000
Training Epoch: 50 [22784/50000]	Loss: 0.5112	LR: 0.025000
Training Epoch: 50 [22912/50000]	Loss: 0.5656	LR: 0.025000
Training Epoch: 50 [23040/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 50 [23168/50000]	Loss: 0.6576	LR: 0.025000
Training Epoch: 50 [23296/50000]	Loss: 0.4844	LR: 0.025000
Training Epoch: 50 [23424/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 50 [23552/50000]	Loss: 0.6594	LR: 0.025000
Training Epoch: 50 [23680/50000]	Loss: 0.5368	LR: 0.025000
Training Epoch: 50 [23808/50000]	Loss: 0.4272	LR: 0.025000
Training Epoch: 50 [23936/50000]	Loss: 0.7921	LR: 0.025000
Training Epoch: 50 [24064/50000]	Loss: 0.4135	LR: 0.025000
Training Epoch: 50 [24192/50000]	Loss: 0.6175	LR: 0.025000
Training Epoch: 50 [24320/50000]	Loss: 0.4343	LR: 0.025000
Training Epoch: 50 [24448/50000]	Loss: 0.6390	LR: 0.025000
Training Epoch: 50 [24576/50000]	Loss: 0.3472	LR: 0.025000
Training Epoch: 50 [24704/50000]	Loss: 0.5129	LR: 0.025000
Training Epoch: 50 [24832/50000]	Loss: 0.5567	LR: 0.025000
Training Epoch: 50 [24960/50000]	Loss: 0.5309	LR: 0.025000
Training Epoch: 50 [25088/50000]	Loss: 0.6334	LR: 0.025000
Training Epoch: 50 [25216/50000]	Loss: 0.5153	LR: 0.025000
Training Epoch: 50 [25344/50000]	Loss: 0.4231	LR: 0.025000
Training Epoch: 50 [25472/50000]	Loss: 0.4672	LR: 0.025000
Training Epoch: 50 [25600/50000]	Loss: 0.5753	LR: 0.025000
Training Epoch: 50 [25728/50000]	Loss: 0.4929	LR: 0.025000
Training Epoch: 50 [25856/50000]	Loss: 0.5778	LR: 0.025000
Training Epoch: 50 [25984/50000]	Loss: 0.5450	LR: 0.025000
Training Epoch: 50 [26112/50000]	Loss: 0.5130	LR: 0.025000
Training Epoch: 50 [26240/50000]	Loss: 0.6152	LR: 0.025000
Training Epoch: 50 [26368/50000]	Loss: 0.4253	LR: 0.025000
Training Epoch: 50 [26496/50000]	Loss: 0.4525	LR: 0.025000
Training Epoch: 50 [26624/50000]	Loss: 0.4648	LR: 0.025000
Training Epoch: 50 [26752/50000]	Loss: 0.4876	LR: 0.025000
Training Epoch: 50 [26880/50000]	Loss: 0.5428	LR: 0.025000
Training Epoch: 50 [27008/50000]	Loss: 0.5898	LR: 0.025000
Training Epoch: 50 [27136/50000]	Loss: 0.5860	LR: 0.025000
Training Epoch: 50 [27264/50000]	Loss: 0.5224	LR: 0.025000
Training Epoch: 50 [27392/50000]	Loss: 0.6220	LR: 0.025000
Training Epoch: 50 [27520/50000]	Loss: 0.5104	LR: 0.025000
Training Epoch: 50 [27648/50000]	Loss: 0.4802	LR: 0.025000
Training Epoch: 50 [27776/50000]	Loss: 0.4646	LR: 0.025000
Training Epoch: 50 [27904/50000]	Loss: 0.5446	LR: 0.025000
Training Epoch: 50 [28032/50000]	Loss: 0.5488	LR: 0.025000
Training Epoch: 50 [28160/50000]	Loss: 0.4679	LR: 0.025000
Training Epoch: 50 [28288/50000]	Loss: 0.6764	LR: 0.025000
Training Epoch: 50 [28416/50000]	Loss: 0.4060	LR: 0.025000
Training Epoch: 50 [28544/50000]	Loss: 0.4429	LR: 0.025000
Training Epoch: 50 [28672/50000]	Loss: 0.7375	LR: 0.025000
Training Epoch: 50 [28800/50000]	Loss: 0.4533	LR: 0.025000
Training Epoch: 50 [28928/50000]	Loss: 0.5483	LR: 0.025000
Training Epoch: 50 [29056/50000]	Loss: 0.6802	LR: 0.025000
Training Epoch: 50 [29184/50000]	Loss: 0.5359	LR: 0.025000
Training Epoch: 50 [29312/50000]	Loss: 0.5129	LR: 0.025000
Training Epoch: 50 [29440/50000]	Loss: 0.6402	LR: 0.025000
Training Epoch: 50 [29568/50000]	Loss: 0.3575	LR: 0.025000
Training Epoch: 50 [29696/50000]	Loss: 0.6087	LR: 0.025000
Training Epoch: 50 [29824/50000]	Loss: 0.6230	LR: 0.025000
Training Epoch: 50 [29952/50000]	Loss: 0.4503	LR: 0.025000
Training Epoch: 50 [30080/50000]	Loss: 0.5012	LR: 0.025000
Training Epoch: 50 [30208/50000]	Loss: 0.5346	LR: 0.025000
Training Epoch: 50 [30336/50000]	Loss: 0.5159	LR: 0.025000
Training Epoch: 50 [30464/50000]	Loss: 0.6883	LR: 0.025000
Training Epoch: 50 [30592/50000]	Loss: 0.6864	LR: 0.025000
Training Epoch: 50 [30720/50000]	Loss: 0.4568	LR: 0.025000
Training Epoch: 50 [30848/50000]	Loss: 0.6187	LR: 0.025000
Training Epoch: 50 [30976/50000]	Loss: 0.4187	LR: 0.025000
Training Epoch: 50 [31104/50000]	Loss: 0.6018	LR: 0.025000
Training Epoch: 50 [31232/50000]	Loss: 0.4603	LR: 0.025000
Training Epoch: 50 [31360/50000]	Loss: 0.4223	LR: 0.025000
Training Epoch: 50 [31488/50000]	Loss: 0.6503	LR: 0.025000
Training Epoch: 50 [31616/50000]	Loss: 0.5968	LR: 0.025000
Training Epoch: 50 [31744/50000]	Loss: 0.6398	LR: 0.025000
Training Epoch: 50 [31872/50000]	Loss: 0.6298	LR: 0.025000
Training Epoch: 50 [32000/50000]	Loss: 0.5549	LR: 0.025000
Training Epoch: 50 [32128/50000]	Loss: 0.4650	LR: 0.025000
Training Epoch: 50 [32256/50000]	Loss: 0.4893	LR: 0.025000
Training Epoch: 50 [32384/50000]	Loss: 0.4804	LR: 0.025000
Training Epoch: 50 [32512/50000]	Loss: 0.5547	LR: 0.025000
Training Epoch: 50 [32640/50000]	Loss: 0.5273	LR: 0.025000
Training Epoch: 50 [32768/50000]	Loss: 0.5951	LR: 0.025000
Training Epoch: 50 [32896/50000]	Loss: 0.5773	LR: 0.025000
Training Epoch: 50 [33024/50000]	Loss: 0.4768	LR: 0.025000
Training Epoch: 50 [33152/50000]	Loss: 0.6034	LR: 0.025000
Training Epoch: 50 [33280/50000]	Loss: 0.5662	LR: 0.025000
Training Epoch: 50 [33408/50000]	Loss: 0.5795	LR: 0.025000
Training Epoch: 50 [33536/50000]	Loss: 0.4035	LR: 0.025000
Training Epoch: 50 [33664/50000]	Loss: 0.6045	LR: 0.025000
Training Epoch: 50 [33792/50000]	Loss: 0.5908	LR: 0.025000
Training Epoch: 50 [33920/50000]	Loss: 0.6518	LR: 0.025000
Training Epoch: 50 [34048/50000]	Loss: 0.6980	LR: 0.025000
Training Epoch: 50 [34176/50000]	Loss: 0.7163	LR: 0.025000
Training Epoch: 50 [34304/50000]	Loss: 0.6962	LR: 0.025000
Training Epoch: 50 [34432/50000]	Loss: 0.6261	LR: 0.025000
Training Epoch: 50 [34560/50000]	Loss: 0.6948	LR: 0.025000
Training Epoch: 50 [34688/50000]	Loss: 0.6450	LR: 0.025000
Training Epoch: 50 [34816/50000]	Loss: 0.4674	LR: 0.025000
Training Epoch: 50 [34944/50000]	Loss: 0.4525	LR: 0.025000
Training Epoch: 50 [35072/50000]	Loss: 0.3818	LR: 0.025000
Training Epoch: 50 [35200/50000]	Loss: 0.5147	LR: 0.025000
Training Epoch: 50 [35328/50000]	Loss: 0.5803	LR: 0.025000
Training Epoch: 50 [35456/50000]	Loss: 0.5112	LR: 0.025000
Training Epoch: 50 [35584/50000]	Loss: 0.5944	LR: 0.025000
Training Epoch: 50 [35712/50000]	Loss: 0.7054	LR: 0.025000
Training Epoch: 50 [35840/50000]	Loss: 0.5324	LR: 0.025000
Training Epoch: 50 [35968/50000]	Loss: 0.6460	LR: 0.025000
Training Epoch: 50 [36096/50000]	Loss: 0.6326	LR: 0.025000
Training Epoch: 50 [36224/50000]	Loss: 0.5296	LR: 0.025000
Training Epoch: 50 [36352/50000]	Loss: 0.4952	LR: 0.025000
Training Epoch: 50 [36480/50000]	Loss: 0.6121	LR: 0.025000
Training Epoch: 50 [36608/50000]	Loss: 0.6121	LR: 0.025000
Training Epoch: 50 [36736/50000]	Loss: 0.6503	LR: 0.025000
Training Epoch: 50 [36864/50000]	Loss: 0.6760	LR: 0.025000
Training Epoch: 50 [36992/50000]	Loss: 0.5217	LR: 0.025000
Training Epoch: 50 [37120/50000]	Loss: 0.6572	LR: 0.025000
Training Epoch: 50 [37248/50000]	Loss: 0.7167	LR: 0.025000
Training Epoch: 50 [37376/50000]	Loss: 0.6497	LR: 0.025000
Training Epoch: 50 [37504/50000]	Loss: 0.7896	LR: 0.025000
Training Epoch: 50 [37632/50000]	Loss: 0.4992	LR: 0.025000
Training Epoch: 50 [37760/50000]	Loss: 0.5121	LR: 0.025000
Training Epoch: 50 [37888/50000]	Loss: 0.5331	LR: 0.025000
Training Epoch: 50 [38016/50000]	Loss: 0.4836	LR: 0.025000
Training Epoch: 50 [38144/50000]	Loss: 0.5115	LR: 0.025000
Training Epoch: 50 [38272/50000]	Loss: 0.7479	LR: 0.025000
Training Epoch: 50 [38400/50000]	Loss: 0.5425	LR: 0.025000
Training Epoch: 50 [38528/50000]	Loss: 0.5540	LR: 0.025000
Training Epoch: 50 [38656/50000]	Loss: 0.6927	LR: 0.025000
Training Epoch: 50 [38784/50000]	Loss: 0.5539	LR: 0.025000
Training Epoch: 50 [38912/50000]	Loss: 0.6157	LR: 0.025000
Training Epoch: 50 [39040/50000]	Loss: 0.5599	LR: 0.025000
Training Epoch: 50 [39168/50000]	Loss: 0.5254	LR: 0.025000
Training Epoch: 50 [39296/50000]	Loss: 0.5393	LR: 0.025000
Training Epoch: 50 [39424/50000]	Loss: 0.5128	LR: 0.025000
Training Epoch: 50 [39552/50000]	Loss: 0.5743	LR: 0.025000
Training Epoch: 50 [39680/50000]	Loss: 0.6295	LR: 0.025000
Training Epoch: 50 [39808/50000]	Loss: 0.5914	LR: 0.025000
Training Epoch: 50 [39936/50000]	Loss: 0.5998	LR: 0.025000
Training Epoch: 50 [40064/50000]	Loss: 0.7771	LR: 0.025000
Training Epoch: 50 [40192/50000]	Loss: 0.6713	LR: 0.025000
Training Epoch: 50 [40320/50000]	Loss: 0.4603	LR: 0.025000
Training Epoch: 50 [40448/50000]	Loss: 0.5435	LR: 0.025000
Training Epoch: 50 [40576/50000]	Loss: 0.7353	LR: 0.025000
Training Epoch: 50 [40704/50000]	Loss: 0.5431	LR: 0.025000
Training Epoch: 50 [40832/50000]	Loss: 0.5085	LR: 0.025000
Training Epoch: 50 [40960/50000]	Loss: 0.5835	LR: 0.025000
Training Epoch: 50 [41088/50000]	Loss: 0.6692	LR: 0.025000
Training Epoch: 50 [41216/50000]	Loss: 0.5727	LR: 0.025000
Training Epoch: 50 [41344/50000]	Loss: 0.5577	LR: 0.025000
Training Epoch: 50 [41472/50000]	Loss: 0.4622	LR: 0.025000
Training Epoch: 50 [41600/50000]	Loss: 0.5113	LR: 0.025000
Training Epoch: 50 [41728/50000]	Loss: 0.4405	LR: 0.025000
Training Epoch: 50 [41856/50000]	Loss: 0.5199	LR: 0.025000
Training Epoch: 50 [41984/50000]	Loss: 0.5889	LR: 0.025000
Training Epoch: 50 [42112/50000]	Loss: 0.3822	LR: 0.025000
Training Epoch: 50 [42240/50000]	Loss: 0.7861	LR: 0.025000
Training Epoch: 50 [42368/50000]	Loss: 0.6428	LR: 0.025000
Training Epoch: 50 [42496/50000]	Loss: 0.3990	LR: 0.025000
Training Epoch: 50 [42624/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 50 [42752/50000]	Loss: 0.5311	LR: 0.025000
Training Epoch: 50 [42880/50000]	Loss: 0.5933	LR: 0.025000
Training Epoch: 50 [43008/50000]	Loss: 0.7364	LR: 0.025000
Training Epoch: 50 [43136/50000]	Loss: 0.8351	LR: 0.025000
Training Epoch: 50 [43264/50000]	Loss: 0.8128	LR: 0.025000
Training Epoch: 50 [43392/50000]	Loss: 0.4722	LR: 0.025000
Training Epoch: 50 [43520/50000]	Loss: 0.5621	LR: 0.025000
Training Epoch: 50 [43648/50000]	Loss: 0.4332	LR: 0.025000
Training Epoch: 50 [43776/50000]	Loss: 0.6003	LR: 0.025000
Training Epoch: 50 [43904/50000]	Loss: 0.6561	LR: 0.025000
Training Epoch: 50 [44032/50000]	Loss: 0.5849	LR: 0.025000
Training Epoch: 50 [44160/50000]	Loss: 0.6860	LR: 0.025000
Training Epoch: 50 [44288/50000]	Loss: 0.7069	LR: 0.025000
Training Epoch: 50 [44416/50000]	Loss: 0.4665	LR: 0.025000
Training Epoch: 50 [44544/50000]	Loss: 0.6928	LR: 0.025000
Training Epoch: 50 [44672/50000]	Loss: 0.7320	LR: 0.025000
Training Epoch: 50 [44800/50000]	Loss: 0.5336	LR: 0.025000
Training Epoch: 50 [44928/50000]	Loss: 0.5434	LR: 0.025000
Training Epoch: 50 [45056/50000]	Loss: 0.7176	LR: 0.025000
Training Epoch: 50 [45184/50000]	Loss: 0.4734	LR: 0.025000
Training Epoch: 50 [45312/50000]	Loss: 0.5465	LR: 0.025000
Training Epoch: 50 [45440/50000]	Loss: 0.6839	LR: 0.025000
Training Epoch: 50 [45568/50000]	Loss: 0.5720	LR: 0.025000
Training Epoch: 50 [45696/50000]	Loss: 0.5922	LR: 0.025000
Training Epoch: 50 [45824/50000]	Loss: 0.6691	LR: 0.025000
Training Epoch: 50 [45952/50000]	Loss: 0.7031	LR: 0.025000
Training Epoch: 50 [46080/50000]	Loss: 0.5497	LR: 0.025000
Training Epoch: 50 [46208/50000]	Loss: 0.7375	LR: 0.025000
Training Epoch: 50 [46336/50000]	Loss: 0.6106	LR: 0.025000
Training Epoch: 50 [46464/50000]	Loss: 0.5897	LR: 0.025000
Training Epoch: 50 [46592/50000]	Loss: 0.6541	LR: 0.025000
Training Epoch: 50 [46720/50000]	Loss: 0.5809	LR: 0.025000
Training Epoch: 50 [46848/50000]	Loss: 0.4590	LR: 0.025000
Training Epoch: 50 [46976/50000]	Loss: 0.7755	LR: 0.025000
Training Epoch: 50 [47104/50000]	Loss: 0.4915	LR: 0.025000
Training Epoch: 50 [47232/50000]	Loss: 0.5311	LR: 0.025000
Training Epoch: 50 [47360/50000]	Loss: 0.5780	LR: 0.025000
Training Epoch: 50 [47488/50000]	Loss: 0.5693	LR: 0.025000
Training Epoch: 50 [47616/50000]	Loss: 0.6648	LR: 0.025000
Training Epoch: 50 [47744/50000]	Loss: 0.5725	LR: 0.025000
Training Epoch: 50 [47872/50000]	Loss: 0.4997	LR: 0.025000
Training Epoch: 50 [48000/50000]	Loss: 0.7870	LR: 0.025000
Training Epoch: 50 [48128/50000]	Loss: 0.6038	LR: 0.025000
Training Epoch: 50 [48256/50000]	Loss: 0.6266	LR: 0.025000
Training Epoch: 50 [48384/50000]	Loss: 0.6907	LR: 0.025000
Training Epoch: 50 [48512/50000]	Loss: 0.5360	LR: 0.025000
Training Epoch: 50 [48640/50000]	Loss: 0.6312	LR: 0.025000
Training Epoch: 50 [48768/50000]	Loss: 0.5860	LR: 0.025000
Training Epoch: 50 [48896/50000]	Loss: 0.5958	LR: 0.025000
Training Epoch: 50 [49024/50000]	Loss: 0.6935	LR: 0.025000
Training Epoch: 50 [49152/50000]	Loss: 0.5421	LR: 0.025000
Training Epoch: 50 [49280/50000]	Loss: 0.6434	LR: 0.025000
Training Epoch: 50 [49408/50000]	Loss: 0.6122	LR: 0.025000
Training Epoch: 50 [49536/50000]	Loss: 0.7079	LR: 0.025000
Training Epoch: 50 [49664/50000]	Loss: 0.6543	LR: 0.025000
Training Epoch: 50 [49792/50000]	Loss: 0.6948	LR: 0.025000
Training Epoch: 50 [49920/50000]	Loss: 0.5389	LR: 0.025000
Training Epoch: 50 [50000/50000]	Loss: 0.7301	LR: 0.025000
epoch 50 training time consumed: 54.09s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  183503 GB |  183503 GB |
|       from large pool |  123392 KB |    1034 MB |  183322 GB |  183322 GB |
|       from small pool |   10798 KB |      13 MB |     180 GB |     180 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  183503 GB |  183503 GB |
|       from large pool |  123392 KB |    1034 MB |  183322 GB |  183322 GB |
|       from small pool |   10798 KB |      13 MB |     180 GB |     180 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   80753 GB |   80753 GB |
|       from large pool |  155136 KB |  433088 KB |   80553 GB |   80553 GB |
|       from small pool |    1490 KB |    3494 KB |     199 GB |     199 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    7081 K  |    7080 K  |
|       from large pool |      24    |      65    |    3696 K  |    3696 K  |
|       from small pool |     231    |     274    |    3384 K  |    3384 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    7081 K  |    7080 K  |
|       from large pool |      24    |      65    |    3696 K  |    3696 K  |
|       from small pool |     231    |     274    |    3384 K  |    3384 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3499 K  |    3499 K  |
|       from large pool |       9    |      14    |    1788 K  |    1788 K  |
|       from small pool |      12    |      16    |    1710 K  |    1710 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 50, Average loss: 0.0108, Accuracy: 0.6521, Time consumed:3.45s

Training Epoch: 51 [128/50000]	Loss: 0.5457	LR: 0.025000
Training Epoch: 51 [256/50000]	Loss: 0.5982	LR: 0.025000
Training Epoch: 51 [384/50000]	Loss: 0.5465	LR: 0.025000
Training Epoch: 51 [512/50000]	Loss: 0.5257	LR: 0.025000
Training Epoch: 51 [640/50000]	Loss: 0.4877	LR: 0.025000
Training Epoch: 51 [768/50000]	Loss: 0.5316	LR: 0.025000
Training Epoch: 51 [896/50000]	Loss: 0.5570	LR: 0.025000
Training Epoch: 51 [1024/50000]	Loss: 0.5948	LR: 0.025000
Training Epoch: 51 [1152/50000]	Loss: 0.4336	LR: 0.025000
Training Epoch: 51 [1280/50000]	Loss: 0.3890	LR: 0.025000
Training Epoch: 51 [1408/50000]	Loss: 0.6063	LR: 0.025000
Training Epoch: 51 [1536/50000]	Loss: 0.6285	LR: 0.025000
Training Epoch: 51 [1664/50000]	Loss: 0.6095	LR: 0.025000
Training Epoch: 51 [1792/50000]	Loss: 0.5583	LR: 0.025000
Training Epoch: 51 [1920/50000]	Loss: 0.5266	LR: 0.025000
Training Epoch: 51 [2048/50000]	Loss: 0.5271	LR: 0.025000
Training Epoch: 51 [2176/50000]	Loss: 0.4080	LR: 0.025000
Training Epoch: 51 [2304/50000]	Loss: 0.5027	LR: 0.025000
Training Epoch: 51 [2432/50000]	Loss: 0.5260	LR: 0.025000
Training Epoch: 51 [2560/50000]	Loss: 0.3651	LR: 0.025000
Training Epoch: 51 [2688/50000]	Loss: 0.4658	LR: 0.025000
Training Epoch: 51 [2816/50000]	Loss: 0.4424	LR: 0.025000
Training Epoch: 51 [2944/50000]	Loss: 0.5819	LR: 0.025000
Training Epoch: 51 [3072/50000]	Loss: 0.6371	LR: 0.025000
Training Epoch: 51 [3200/50000]	Loss: 0.5920	LR: 0.025000
Training Epoch: 51 [3328/50000]	Loss: 0.5537	LR: 0.025000
Training Epoch: 51 [3456/50000]	Loss: 0.5096	LR: 0.025000
Training Epoch: 51 [3584/50000]	Loss: 0.5218	LR: 0.025000
Training Epoch: 51 [3712/50000]	Loss: 0.4348	LR: 0.025000
Training Epoch: 51 [3840/50000]	Loss: 0.3933	LR: 0.025000
Training Epoch: 51 [3968/50000]	Loss: 0.4808	LR: 0.025000
Training Epoch: 51 [4096/50000]	Loss: 0.4026	LR: 0.025000
Training Epoch: 51 [4224/50000]	Loss: 0.4099	LR: 0.025000
Training Epoch: 51 [4352/50000]	Loss: 0.4718	LR: 0.025000
Training Epoch: 51 [4480/50000]	Loss: 0.5376	LR: 0.025000
Training Epoch: 51 [4608/50000]	Loss: 0.5310	LR: 0.025000
Training Epoch: 51 [4736/50000]	Loss: 0.5875	LR: 0.025000
Training Epoch: 51 [4864/50000]	Loss: 0.5588	LR: 0.025000
Training Epoch: 51 [4992/50000]	Loss: 0.4260	LR: 0.025000
Training Epoch: 51 [5120/50000]	Loss: 0.3816	LR: 0.025000
Training Epoch: 51 [5248/50000]	Loss: 0.5198	LR: 0.025000
Training Epoch: 51 [5376/50000]	Loss: 0.5171	LR: 0.025000
Training Epoch: 51 [5504/50000]	Loss: 0.4109	LR: 0.025000
Training Epoch: 51 [5632/50000]	Loss: 0.4526	LR: 0.025000
Training Epoch: 51 [5760/50000]	Loss: 0.6178	LR: 0.025000
Training Epoch: 51 [5888/50000]	Loss: 0.4359	LR: 0.025000
Training Epoch: 51 [6016/50000]	Loss: 0.3790	LR: 0.025000
Training Epoch: 51 [6144/50000]	Loss: 0.4093	LR: 0.025000
Training Epoch: 51 [6272/50000]	Loss: 0.3383	LR: 0.025000
Training Epoch: 51 [6400/50000]	Loss: 0.4577	LR: 0.025000
Training Epoch: 51 [6528/50000]	Loss: 0.4914	LR: 0.025000
Training Epoch: 51 [6656/50000]	Loss: 0.4206	LR: 0.025000
Training Epoch: 51 [6784/50000]	Loss: 0.6232	LR: 0.025000
Training Epoch: 51 [6912/50000]	Loss: 0.6337	LR: 0.025000
Training Epoch: 51 [7040/50000]	Loss: 0.4802	LR: 0.025000
Training Epoch: 51 [7168/50000]	Loss: 0.4236	LR: 0.025000
Training Epoch: 51 [7296/50000]	Loss: 0.5020	LR: 0.025000
Training Epoch: 51 [7424/50000]	Loss: 0.4333	LR: 0.025000
Training Epoch: 51 [7552/50000]	Loss: 0.5047	LR: 0.025000
Training Epoch: 51 [7680/50000]	Loss: 0.5044	LR: 0.025000
Training Epoch: 51 [7808/50000]	Loss: 0.5233	LR: 0.025000
Training Epoch: 51 [7936/50000]	Loss: 0.5594	LR: 0.025000
Training Epoch: 51 [8064/50000]	Loss: 0.3232	LR: 0.025000
Training Epoch: 51 [8192/50000]	Loss: 0.6512	LR: 0.025000
Training Epoch: 51 [8320/50000]	Loss: 0.4217	LR: 0.025000
Training Epoch: 51 [8448/50000]	Loss: 0.4838	LR: 0.025000
Training Epoch: 51 [8576/50000]	Loss: 0.5764	LR: 0.025000
Training Epoch: 51 [8704/50000]	Loss: 0.5518	LR: 0.025000
Training Epoch: 51 [8832/50000]	Loss: 0.4041	LR: 0.025000
Training Epoch: 51 [8960/50000]	Loss: 0.5163	LR: 0.025000
Training Epoch: 51 [9088/50000]	Loss: 0.3828	LR: 0.025000
Training Epoch: 51 [9216/50000]	Loss: 0.5220	LR: 0.025000
Training Epoch: 51 [9344/50000]	Loss: 0.4486	LR: 0.025000
Training Epoch: 51 [9472/50000]	Loss: 0.6118	LR: 0.025000
Training Epoch: 51 [9600/50000]	Loss: 0.5201	LR: 0.025000
Training Epoch: 51 [9728/50000]	Loss: 0.4381	LR: 0.025000
Training Epoch: 51 [9856/50000]	Loss: 0.4355	LR: 0.025000
Training Epoch: 51 [9984/50000]	Loss: 0.5746	LR: 0.025000
Training Epoch: 51 [10112/50000]	Loss: 0.4486	LR: 0.025000
Training Epoch: 51 [10240/50000]	Loss: 0.3411	LR: 0.025000
Training Epoch: 51 [10368/50000]	Loss: 0.5039	LR: 0.025000
Training Epoch: 51 [10496/50000]	Loss: 0.5339	LR: 0.025000
Training Epoch: 51 [10624/50000]	Loss: 0.4970	LR: 0.025000
Training Epoch: 51 [10752/50000]	Loss: 0.5596	LR: 0.025000
Training Epoch: 51 [10880/50000]	Loss: 0.4275	LR: 0.025000
Training Epoch: 51 [11008/50000]	Loss: 0.6086	LR: 0.025000
Training Epoch: 51 [11136/50000]	Loss: 0.6531	LR: 0.025000
Training Epoch: 51 [11264/50000]	Loss: 0.4493	LR: 0.025000
Training Epoch: 51 [11392/50000]	Loss: 0.4973	LR: 0.025000
Training Epoch: 51 [11520/50000]	Loss: 0.5918	LR: 0.025000
Training Epoch: 51 [11648/50000]	Loss: 0.5007	LR: 0.025000
Training Epoch: 51 [11776/50000]	Loss: 0.5183	LR: 0.025000
Training Epoch: 51 [11904/50000]	Loss: 0.4719	LR: 0.025000
Training Epoch: 51 [12032/50000]	Loss: 0.5077	LR: 0.025000
Training Epoch: 51 [12160/50000]	Loss: 0.5030	LR: 0.025000
Training Epoch: 51 [12288/50000]	Loss: 0.6279	LR: 0.025000
Training Epoch: 51 [12416/50000]	Loss: 0.5912	LR: 0.025000
Training Epoch: 51 [12544/50000]	Loss: 0.3892	LR: 0.025000
Training Epoch: 51 [12672/50000]	Loss: 0.6096	LR: 0.025000
Training Epoch: 51 [12800/50000]	Loss: 0.5432	LR: 0.025000
Training Epoch: 51 [12928/50000]	Loss: 0.5394	LR: 0.025000
Training Epoch: 51 [13056/50000]	Loss: 0.4498	LR: 0.025000
Training Epoch: 51 [13184/50000]	Loss: 0.3633	LR: 0.025000
Training Epoch: 51 [13312/50000]	Loss: 0.5589	LR: 0.025000
Training Epoch: 51 [13440/50000]	Loss: 0.4587	LR: 0.025000
Training Epoch: 51 [13568/50000]	Loss: 0.5303	LR: 0.025000
Training Epoch: 51 [13696/50000]	Loss: 0.4384	LR: 0.025000
Training Epoch: 51 [13824/50000]	Loss: 0.4607	LR: 0.025000
Training Epoch: 51 [13952/50000]	Loss: 0.5920	LR: 0.025000
Training Epoch: 51 [14080/50000]	Loss: 0.3749	LR: 0.025000
Training Epoch: 51 [14208/50000]	Loss: 0.5148	LR: 0.025000
Training Epoch: 51 [14336/50000]	Loss: 0.5341	LR: 0.025000
Training Epoch: 51 [14464/50000]	Loss: 0.4480	LR: 0.025000
Training Epoch: 51 [14592/50000]	Loss: 0.6481	LR: 0.025000
Training Epoch: 51 [14720/50000]	Loss: 0.4965	LR: 0.025000
Training Epoch: 51 [14848/50000]	Loss: 0.5842	LR: 0.025000
Training Epoch: 51 [14976/50000]	Loss: 0.6597	LR: 0.025000
Training Epoch: 51 [15104/50000]	Loss: 0.7318	LR: 0.025000
Training Epoch: 51 [15232/50000]	Loss: 0.4567	LR: 0.025000
Training Epoch: 51 [15360/50000]	Loss: 0.4661	LR: 0.025000
Training Epoch: 51 [15488/50000]	Loss: 0.6298	LR: 0.025000
Training Epoch: 51 [15616/50000]	Loss: 0.5687	LR: 0.025000
Training Epoch: 51 [15744/50000]	Loss: 0.3365	LR: 0.025000
Training Epoch: 51 [15872/50000]	Loss: 0.5658	LR: 0.025000
Training Epoch: 51 [16000/50000]	Loss: 0.5462	LR: 0.025000
Training Epoch: 51 [16128/50000]	Loss: 0.5508	LR: 0.025000
Training Epoch: 51 [16256/50000]	Loss: 0.6314	LR: 0.025000
Training Epoch: 51 [16384/50000]	Loss: 0.6347	LR: 0.025000
Training Epoch: 51 [16512/50000]	Loss: 0.7177	LR: 0.025000
Training Epoch: 51 [16640/50000]	Loss: 0.5832	LR: 0.025000
Training Epoch: 51 [16768/50000]	Loss: 0.5776	LR: 0.025000
Training Epoch: 51 [16896/50000]	Loss: 0.4210	LR: 0.025000
Training Epoch: 51 [17024/50000]	Loss: 0.5374	LR: 0.025000
Training Epoch: 51 [17152/50000]	Loss: 0.5367	LR: 0.025000
Training Epoch: 51 [17280/50000]	Loss: 0.5490	LR: 0.025000
Training Epoch: 51 [17408/50000]	Loss: 0.5032	LR: 0.025000
Training Epoch: 51 [17536/50000]	Loss: 0.5019	LR: 0.025000
Training Epoch: 51 [17664/50000]	Loss: 0.5847	LR: 0.025000
Training Epoch: 51 [17792/50000]	Loss: 0.5028	LR: 0.025000
Training Epoch: 51 [17920/50000]	Loss: 0.4326	LR: 0.025000
Training Epoch: 51 [18048/50000]	Loss: 0.5542	LR: 0.025000
Training Epoch: 51 [18176/50000]	Loss: 0.4933	LR: 0.025000
Training Epoch: 51 [18304/50000]	Loss: 0.5042	LR: 0.025000
Training Epoch: 51 [18432/50000]	Loss: 0.5671	LR: 0.025000
Training Epoch: 51 [18560/50000]	Loss: 0.6082	LR: 0.025000
Training Epoch: 51 [18688/50000]	Loss: 0.4805	LR: 0.025000
Training Epoch: 51 [18816/50000]	Loss: 0.5569	LR: 0.025000
Training Epoch: 51 [18944/50000]	Loss: 0.3607	LR: 0.025000
Training Epoch: 51 [19072/50000]	Loss: 0.6228	LR: 0.025000
Training Epoch: 51 [19200/50000]	Loss: 0.4286	LR: 0.025000
Training Epoch: 51 [19328/50000]	Loss: 0.6296	LR: 0.025000
Training Epoch: 51 [19456/50000]	Loss: 0.4214	LR: 0.025000
Training Epoch: 51 [19584/50000]	Loss: 0.6552	LR: 0.025000
Training Epoch: 51 [19712/50000]	Loss: 0.5873	LR: 0.025000
Training Epoch: 51 [19840/50000]	Loss: 0.5035	LR: 0.025000
Training Epoch: 51 [19968/50000]	Loss: 0.4292	LR: 0.025000
Training Epoch: 51 [20096/50000]	Loss: 0.5181	LR: 0.025000
Training Epoch: 51 [20224/50000]	Loss: 0.5241	LR: 0.025000
Training Epoch: 51 [20352/50000]	Loss: 0.7081	LR: 0.025000
Training Epoch: 51 [20480/50000]	Loss: 0.4509	LR: 0.025000
Training Epoch: 51 [20608/50000]	Loss: 0.6703	LR: 0.025000
Training Epoch: 51 [20736/50000]	Loss: 0.4301	LR: 0.025000
Training Epoch: 51 [20864/50000]	Loss: 0.4942	LR: 0.025000
Training Epoch: 51 [20992/50000]	Loss: 0.6992	LR: 0.025000
Training Epoch: 51 [21120/50000]	Loss: 0.5680	LR: 0.025000
Training Epoch: 51 [21248/50000]	Loss: 0.6096	LR: 0.025000
Training Epoch: 51 [21376/50000]	Loss: 0.4922	LR: 0.025000
Training Epoch: 51 [21504/50000]	Loss: 0.6119	LR: 0.025000
Training Epoch: 51 [21632/50000]	Loss: 0.4919	LR: 0.025000
Training Epoch: 51 [21760/50000]	Loss: 0.6126	LR: 0.025000
Training Epoch: 51 [21888/50000]	Loss: 0.5647	LR: 0.025000
Training Epoch: 51 [22016/50000]	Loss: 0.5487	LR: 0.025000
Training Epoch: 51 [22144/50000]	Loss: 0.4428	LR: 0.025000
Training Epoch: 51 [22272/50000]	Loss: 0.4803	LR: 0.025000
Training Epoch: 51 [22400/50000]	Loss: 0.3994	LR: 0.025000
Training Epoch: 51 [22528/50000]	Loss: 0.6177	LR: 0.025000
Training Epoch: 51 [22656/50000]	Loss: 0.4909	LR: 0.025000
Training Epoch: 51 [22784/50000]	Loss: 0.6371	LR: 0.025000
Training Epoch: 51 [22912/50000]	Loss: 0.4391	LR: 0.025000
Training Epoch: 51 [23040/50000]	Loss: 0.5126	LR: 0.025000
Training Epoch: 51 [23168/50000]	Loss: 0.5359	LR: 0.025000
Training Epoch: 51 [23296/50000]	Loss: 0.4770	LR: 0.025000
Training Epoch: 51 [23424/50000]	Loss: 0.5232	LR: 0.025000
Training Epoch: 51 [23552/50000]	Loss: 0.6108	LR: 0.025000
Training Epoch: 51 [23680/50000]	Loss: 0.4936	LR: 0.025000
Training Epoch: 51 [23808/50000]	Loss: 0.6227	LR: 0.025000
Training Epoch: 51 [23936/50000]	Loss: 0.4976	LR: 0.025000
Training Epoch: 51 [24064/50000]	Loss: 0.5698	LR: 0.025000
Training Epoch: 51 [24192/50000]	Loss: 0.5530	LR: 0.025000
Training Epoch: 51 [24320/50000]	Loss: 0.6211	LR: 0.025000
Training Epoch: 51 [24448/50000]	Loss: 0.6061	LR: 0.025000
Training Epoch: 51 [24576/50000]	Loss: 0.4916	LR: 0.025000
Training Epoch: 51 [24704/50000]	Loss: 0.6528	LR: 0.025000
Training Epoch: 51 [24832/50000]	Loss: 0.9342	LR: 0.025000
Training Epoch: 51 [24960/50000]	Loss: 0.9557	LR: 0.025000
Training Epoch: 51 [25088/50000]	Loss: 0.5321	LR: 0.025000
Training Epoch: 51 [25216/50000]	Loss: 0.6705	LR: 0.025000
Training Epoch: 51 [25344/50000]	Loss: 0.5214	LR: 0.025000
Training Epoch: 51 [25472/50000]	Loss: 0.4217	LR: 0.025000
Training Epoch: 51 [25600/50000]	Loss: 0.5908	LR: 0.025000
Training Epoch: 51 [25728/50000]	Loss: 0.6060	LR: 0.025000
Training Epoch: 51 [25856/50000]	Loss: 0.5008	LR: 0.025000
Training Epoch: 51 [25984/50000]	Loss: 0.4578	LR: 0.025000
Training Epoch: 51 [26112/50000]	Loss: 0.7145	LR: 0.025000
Training Epoch: 51 [26240/50000]	Loss: 0.6017	LR: 0.025000
Training Epoch: 51 [26368/50000]	Loss: 0.4820	LR: 0.025000
Training Epoch: 51 [26496/50000]	Loss: 0.4598	LR: 0.025000
Training Epoch: 51 [26624/50000]	Loss: 0.6088	LR: 0.025000
Training Epoch: 51 [26752/50000]	Loss: 0.4881	LR: 0.025000
Training Epoch: 51 [26880/50000]	Loss: 0.4072	LR: 0.025000
Training Epoch: 51 [27008/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 51 [27136/50000]	Loss: 0.5245	LR: 0.025000
Training Epoch: 51 [27264/50000]	Loss: 0.6540	LR: 0.025000
Training Epoch: 51 [27392/50000]	Loss: 0.7155	LR: 0.025000
Training Epoch: 51 [27520/50000]	Loss: 0.5667	LR: 0.025000
Training Epoch: 51 [27648/50000]	Loss: 0.6023	LR: 0.025000
Training Epoch: 51 [27776/50000]	Loss: 0.5815	LR: 0.025000
Training Epoch: 51 [27904/50000]	Loss: 0.7398	LR: 0.025000
Training Epoch: 51 [28032/50000]	Loss: 0.5201	LR: 0.025000
Training Epoch: 51 [28160/50000]	Loss: 0.4076	LR: 0.025000
Training Epoch: 51 [28288/50000]	Loss: 0.7215	LR: 0.025000
Training Epoch: 51 [28416/50000]	Loss: 0.5474	LR: 0.025000
Training Epoch: 51 [28544/50000]	Loss: 0.6189	LR: 0.025000
Training Epoch: 51 [28672/50000]	Loss: 0.7979	LR: 0.025000
Training Epoch: 51 [28800/50000]	Loss: 0.5517	LR: 0.025000
Training Epoch: 51 [28928/50000]	Loss: 0.5258	LR: 0.025000
Training Epoch: 51 [29056/50000]	Loss: 0.4220	LR: 0.025000
Training Epoch: 51 [29184/50000]	Loss: 0.6293	LR: 0.025000
Training Epoch: 51 [29312/50000]	Loss: 0.5341	LR: 0.025000
Training Epoch: 51 [29440/50000]	Loss: 0.5915	LR: 0.025000
Training Epoch: 51 [29568/50000]	Loss: 0.6074	LR: 0.025000
Training Epoch: 51 [29696/50000]	Loss: 0.6079	LR: 0.025000
Training Epoch: 51 [29824/50000]	Loss: 0.5017	LR: 0.025000
Training Epoch: 51 [29952/50000]	Loss: 0.4078	LR: 0.025000
Training Epoch: 51 [30080/50000]	Loss: 0.5501	LR: 0.025000
Training Epoch: 51 [30208/50000]	Loss: 0.6434	LR: 0.025000
Training Epoch: 51 [30336/50000]	Loss: 0.6430	LR: 0.025000
Training Epoch: 51 [30464/50000]	Loss: 0.5932	LR: 0.025000
Training Epoch: 51 [30592/50000]	Loss: 0.4471	LR: 0.025000
Training Epoch: 51 [30720/50000]	Loss: 0.4878	LR: 0.025000
Training Epoch: 51 [30848/50000]	Loss: 0.5255	LR: 0.025000
Training Epoch: 51 [30976/50000]	Loss: 0.5566	LR: 0.025000
Training Epoch: 51 [31104/50000]	Loss: 0.5823	LR: 0.025000
Training Epoch: 51 [31232/50000]	Loss: 0.5662	LR: 0.025000
Training Epoch: 51 [31360/50000]	Loss: 0.5736	LR: 0.025000
Training Epoch: 51 [31488/50000]	Loss: 0.6574	LR: 0.025000
Training Epoch: 51 [31616/50000]	Loss: 0.8057	LR: 0.025000
Training Epoch: 51 [31744/50000]	Loss: 0.5806	LR: 0.025000
Training Epoch: 51 [31872/50000]	Loss: 0.5546	LR: 0.025000
Training Epoch: 51 [32000/50000]	Loss: 0.5398	LR: 0.025000
Training Epoch: 51 [32128/50000]	Loss: 0.4862	LR: 0.025000
Training Epoch: 51 [32256/50000]	Loss: 0.7573	LR: 0.025000
Training Epoch: 51 [32384/50000]	Loss: 0.5692	LR: 0.025000
Training Epoch: 51 [32512/50000]	Loss: 0.7098	LR: 0.025000
Training Epoch: 51 [32640/50000]	Loss: 0.5148	LR: 0.025000
Training Epoch: 51 [32768/50000]	Loss: 0.5496	LR: 0.025000
Training Epoch: 51 [32896/50000]	Loss: 0.5386	LR: 0.025000
Training Epoch: 51 [33024/50000]	Loss: 0.4774	LR: 0.025000
Training Epoch: 51 [33152/50000]	Loss: 0.7593	LR: 0.025000
Training Epoch: 51 [33280/50000]	Loss: 0.5071	LR: 0.025000
Training Epoch: 51 [33408/50000]	Loss: 0.5805	LR: 0.025000
Training Epoch: 51 [33536/50000]	Loss: 0.4950	LR: 0.025000
Training Epoch: 51 [33664/50000]	Loss: 0.5119	LR: 0.025000
Training Epoch: 51 [33792/50000]	Loss: 0.4541	LR: 0.025000
Training Epoch: 51 [33920/50000]	Loss: 0.6162	LR: 0.025000
Training Epoch: 51 [34048/50000]	Loss: 0.4606	LR: 0.025000
Training Epoch: 51 [34176/50000]	Loss: 0.7226	LR: 0.025000
Training Epoch: 51 [34304/50000]	Loss: 0.5497	LR: 0.025000
Training Epoch: 51 [34432/50000]	Loss: 0.6349	LR: 0.025000
Training Epoch: 51 [34560/50000]	Loss: 0.3871	LR: 0.025000
Training Epoch: 51 [34688/50000]	Loss: 0.5750	LR: 0.025000
Training Epoch: 51 [34816/50000]	Loss: 0.5678	LR: 0.025000
Training Epoch: 51 [34944/50000]	Loss: 0.6321	LR: 0.025000
Training Epoch: 51 [35072/50000]	Loss: 0.4540	LR: 0.025000
Training Epoch: 51 [35200/50000]	Loss: 0.6831	LR: 0.025000
Training Epoch: 51 [35328/50000]	Loss: 0.5763	LR: 0.025000
Training Epoch: 51 [35456/50000]	Loss: 0.7445	LR: 0.025000
Training Epoch: 51 [35584/50000]	Loss: 0.6841	LR: 0.025000
Training Epoch: 51 [35712/50000]	Loss: 0.5355	LR: 0.025000
Training Epoch: 51 [35840/50000]	Loss: 0.5146	LR: 0.025000
Training Epoch: 51 [35968/50000]	Loss: 0.5577	LR: 0.025000
Training Epoch: 51 [36096/50000]	Loss: 0.6025	LR: 0.025000
Training Epoch: 51 [36224/50000]	Loss: 0.5099	LR: 0.025000
Training Epoch: 51 [36352/50000]	Loss: 0.6190	LR: 0.025000
Training Epoch: 51 [36480/50000]	Loss: 0.5579	LR: 0.025000
Training Epoch: 51 [36608/50000]	Loss: 0.7029	LR: 0.025000
Training Epoch: 51 [36736/50000]	Loss: 0.5839	LR: 0.025000
Training Epoch: 51 [36864/50000]	Loss: 0.6171	LR: 0.025000
Training Epoch: 51 [36992/50000]	Loss: 0.6937	LR: 0.025000
Training Epoch: 51 [37120/50000]	Loss: 0.5947	LR: 0.025000
Training Epoch: 51 [37248/50000]	Loss: 0.7002	LR: 0.025000
Training Epoch: 51 [37376/50000]	Loss: 0.4191	LR: 0.025000
Training Epoch: 51 [37504/50000]	Loss: 0.5565	LR: 0.025000
Training Epoch: 51 [37632/50000]	Loss: 0.7579	LR: 0.025000
Training Epoch: 51 [37760/50000]	Loss: 0.7322	LR: 0.025000
Training Epoch: 51 [37888/50000]	Loss: 0.5848	LR: 0.025000
Training Epoch: 51 [38016/50000]	Loss: 0.5912	LR: 0.025000
Training Epoch: 51 [38144/50000]	Loss: 0.4903	LR: 0.025000
Training Epoch: 51 [38272/50000]	Loss: 0.5728	LR: 0.025000
Training Epoch: 51 [38400/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 51 [38528/50000]	Loss: 0.4399	LR: 0.025000
Training Epoch: 51 [38656/50000]	Loss: 0.5845	LR: 0.025000
Training Epoch: 51 [38784/50000]	Loss: 0.6100	LR: 0.025000
Training Epoch: 51 [38912/50000]	Loss: 0.5171	LR: 0.025000
Training Epoch: 51 [39040/50000]	Loss: 0.4895	LR: 0.025000
Training Epoch: 51 [39168/50000]	Loss: 0.4169	LR: 0.025000
Training Epoch: 51 [39296/50000]	Loss: 0.3650	LR: 0.025000
Training Epoch: 51 [39424/50000]	Loss: 0.5731	LR: 0.025000
Training Epoch: 51 [39552/50000]	Loss: 0.5739	LR: 0.025000
Training Epoch: 51 [39680/50000]	Loss: 0.4781	LR: 0.025000
Training Epoch: 51 [39808/50000]	Loss: 0.5605	LR: 0.025000
Training Epoch: 51 [39936/50000]	Loss: 0.6697	LR: 0.025000
Training Epoch: 51 [40064/50000]	Loss: 0.7277	LR: 0.025000
Training Epoch: 51 [40192/50000]	Loss: 0.7144	LR: 0.025000
Training Epoch: 51 [40320/50000]	Loss: 0.5446	LR: 0.025000
Training Epoch: 51 [40448/50000]	Loss: 0.5338	LR: 0.025000
Training Epoch: 51 [40576/50000]	Loss: 0.5750	LR: 0.025000
Training Epoch: 51 [40704/50000]	Loss: 0.6281	LR: 0.025000
Training Epoch: 51 [40832/50000]	Loss: 0.6166	LR: 0.025000
Training Epoch: 51 [40960/50000]	Loss: 0.7167	LR: 0.025000
Training Epoch: 51 [41088/50000]	Loss: 0.5773	LR: 0.025000
Training Epoch: 51 [41216/50000]	Loss: 0.6547	LR: 0.025000
Training Epoch: 51 [41344/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 51 [41472/50000]	Loss: 0.5674	LR: 0.025000
Training Epoch: 51 [41600/50000]	Loss: 0.5034	LR: 0.025000
Training Epoch: 51 [41728/50000]	Loss: 0.6999	LR: 0.025000
Training Epoch: 51 [41856/50000]	Loss: 0.8168	LR: 0.025000
Training Epoch: 51 [41984/50000]	Loss: 0.6850	LR: 0.025000
Training Epoch: 51 [42112/50000]	Loss: 0.6003	LR: 0.025000
Training Epoch: 51 [42240/50000]	Loss: 0.5380	LR: 0.025000
Training Epoch: 51 [42368/50000]	Loss: 0.6924	LR: 0.025000
Training Epoch: 51 [42496/50000]	Loss: 0.5469	LR: 0.025000
Training Epoch: 51 [42624/50000]	Loss: 0.6417	LR: 0.025000
Training Epoch: 51 [42752/50000]	Loss: 0.6391	LR: 0.025000
Training Epoch: 51 [42880/50000]	Loss: 0.6281	LR: 0.025000
Training Epoch: 51 [43008/50000]	Loss: 0.5537	LR: 0.025000
Training Epoch: 51 [43136/50000]	Loss: 0.5110	LR: 0.025000
Training Epoch: 51 [43264/50000]	Loss: 0.6779	LR: 0.025000
Training Epoch: 51 [43392/50000]	Loss: 0.5117	LR: 0.025000
Training Epoch: 51 [43520/50000]	Loss: 0.6709	LR: 0.025000
Training Epoch: 51 [43648/50000]	Loss: 0.5647	LR: 0.025000
Training Epoch: 51 [43776/50000]	Loss: 0.5644	LR: 0.025000
Training Epoch: 51 [43904/50000]	Loss: 0.5467	LR: 0.025000
Training Epoch: 51 [44032/50000]	Loss: 0.4791	LR: 0.025000
Training Epoch: 51 [44160/50000]	Loss: 0.5781	LR: 0.025000
Training Epoch: 51 [44288/50000]	Loss: 0.5732	LR: 0.025000
Training Epoch: 51 [44416/50000]	Loss: 0.5046	LR: 0.025000
Training Epoch: 51 [44544/50000]	Loss: 0.5904	LR: 0.025000
Training Epoch: 51 [44672/50000]	Loss: 0.6427	LR: 0.025000
Training Epoch: 51 [44800/50000]	Loss: 0.6601	LR: 0.025000
Training Epoch: 51 [44928/50000]	Loss: 0.6376	LR: 0.025000
Training Epoch: 51 [45056/50000]	Loss: 0.6931	LR: 0.025000
Training Epoch: 51 [45184/50000]	Loss: 0.6912	LR: 0.025000
Training Epoch: 51 [45312/50000]	Loss: 0.8117	LR: 0.025000
Training Epoch: 51 [45440/50000]	Loss: 0.6389	LR: 0.025000
Training Epoch: 51 [45568/50000]	Loss: 0.5875	LR: 0.025000
Training Epoch: 51 [45696/50000]	Loss: 0.5109	LR: 0.025000
Training Epoch: 51 [45824/50000]	Loss: 0.7642	LR: 0.025000
Training Epoch: 51 [45952/50000]	Loss: 0.5100	LR: 0.025000
Training Epoch: 51 [46080/50000]	Loss: 0.6054	LR: 0.025000
Training Epoch: 51 [46208/50000]	Loss: 0.6383	LR: 0.025000
Training Epoch: 51 [46336/50000]	Loss: 0.5582	LR: 0.025000
Training Epoch: 51 [46464/50000]	Loss: 0.7212	LR: 0.025000
Training Epoch: 51 [46592/50000]	Loss: 0.5996	LR: 0.025000
Training Epoch: 51 [46720/50000]	Loss: 0.6137	LR: 0.025000
Training Epoch: 51 [46848/50000]	Loss: 0.5471	LR: 0.025000
Training Epoch: 51 [46976/50000]	Loss: 0.6038	LR: 0.025000
Training Epoch: 51 [47104/50000]	Loss: 0.6344	LR: 0.025000
Training Epoch: 51 [47232/50000]	Loss: 0.6743	LR: 0.025000
Training Epoch: 51 [47360/50000]	Loss: 0.7857	LR: 0.025000
Training Epoch: 51 [47488/50000]	Loss: 0.6769	LR: 0.025000
Training Epoch: 51 [47616/50000]	Loss: 0.8303	LR: 0.025000
Training Epoch: 51 [47744/50000]	Loss: 0.7854	LR: 0.025000
Training Epoch: 51 [47872/50000]	Loss: 0.7225	LR: 0.025000
Training Epoch: 51 [48000/50000]	Loss: 0.4800	LR: 0.025000
Training Epoch: 51 [48128/50000]	Loss: 0.6346	LR: 0.025000
Training Epoch: 51 [48256/50000]	Loss: 0.6283	LR: 0.025000
Training Epoch: 51 [48384/50000]	Loss: 0.7634	LR: 0.025000
Training Epoch: 51 [48512/50000]	Loss: 0.5241	LR: 0.025000
Training Epoch: 51 [48640/50000]	Loss: 0.5589	LR: 0.025000
Training Epoch: 51 [48768/50000]	Loss: 0.7534	LR: 0.025000
Training Epoch: 51 [48896/50000]	Loss: 0.6183	LR: 0.025000
Training Epoch: 51 [49024/50000]	Loss: 0.6865	LR: 0.025000
Training Epoch: 51 [49152/50000]	Loss: 0.6442	LR: 0.025000
Training Epoch: 51 [49280/50000]	Loss: 0.4660	LR: 0.025000
Training Epoch: 51 [49408/50000]	Loss: 0.4553	LR: 0.025000
Training Epoch: 51 [49536/50000]	Loss: 0.5772	LR: 0.025000
Training Epoch: 51 [49664/50000]	Loss: 0.6968	LR: 0.025000
Training Epoch: 51 [49792/50000]	Loss: 0.5199	LR: 0.025000
Training Epoch: 51 [49920/50000]	Loss: 0.5352	LR: 0.025000
Training Epoch: 51 [50000/50000]	Loss: 0.6961	LR: 0.025000
epoch 51 training time consumed: 53.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  187173 GB |  187173 GB |
|       from large pool |  123392 KB |    1034 MB |  186988 GB |  186988 GB |
|       from small pool |   10798 KB |      13 MB |     184 GB |     184 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  187173 GB |  187173 GB |
|       from large pool |  123392 KB |    1034 MB |  186988 GB |  186988 GB |
|       from small pool |   10798 KB |      13 MB |     184 GB |     184 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   82368 GB |   82368 GB |
|       from large pool |  155136 KB |  433088 KB |   82164 GB |   82164 GB |
|       from small pool |    1490 KB |    3494 KB |     203 GB |     203 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    7222 K  |    7222 K  |
|       from large pool |      24    |      65    |    3769 K  |    3769 K  |
|       from small pool |     231    |     274    |    3452 K  |    3452 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    7222 K  |    7222 K  |
|       from large pool |      24    |      65    |    3769 K  |    3769 K  |
|       from small pool |     231    |     274    |    3452 K  |    3452 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3569 K  |    3569 K  |
|       from large pool |       9    |      14    |    1824 K  |    1824 K  |
|       from small pool |      12    |      16    |    1745 K  |    1745 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 51, Average loss: 0.0110, Accuracy: 0.6504, Time consumed:3.47s

Training Epoch: 52 [128/50000]	Loss: 0.4958	LR: 0.025000
Training Epoch: 52 [256/50000]	Loss: 0.4585	LR: 0.025000
Training Epoch: 52 [384/50000]	Loss: 0.5656	LR: 0.025000
Training Epoch: 52 [512/50000]	Loss: 0.4869	LR: 0.025000
Training Epoch: 52 [640/50000]	Loss: 0.4969	LR: 0.025000
Training Epoch: 52 [768/50000]	Loss: 0.4111	LR: 0.025000
Training Epoch: 52 [896/50000]	Loss: 0.5753	LR: 0.025000
Training Epoch: 52 [1024/50000]	Loss: 0.4052	LR: 0.025000
Training Epoch: 52 [1152/50000]	Loss: 0.5020	LR: 0.025000
Training Epoch: 52 [1280/50000]	Loss: 0.5421	LR: 0.025000
Training Epoch: 52 [1408/50000]	Loss: 0.4498	LR: 0.025000
Training Epoch: 52 [1536/50000]	Loss: 0.5990	LR: 0.025000
Training Epoch: 52 [1664/50000]	Loss: 0.6233	LR: 0.025000
Training Epoch: 52 [1792/50000]	Loss: 0.3952	LR: 0.025000
Training Epoch: 52 [1920/50000]	Loss: 0.4952	LR: 0.025000
Training Epoch: 52 [2048/50000]	Loss: 0.4167	LR: 0.025000
Training Epoch: 52 [2176/50000]	Loss: 0.5187	LR: 0.025000
Training Epoch: 52 [2304/50000]	Loss: 0.3680	LR: 0.025000
Training Epoch: 52 [2432/50000]	Loss: 0.5855	LR: 0.025000
Training Epoch: 52 [2560/50000]	Loss: 0.4679	LR: 0.025000
Training Epoch: 52 [2688/50000]	Loss: 0.3939	LR: 0.025000
Training Epoch: 52 [2816/50000]	Loss: 0.4636	LR: 0.025000
Training Epoch: 52 [2944/50000]	Loss: 0.5182	LR: 0.025000
Training Epoch: 52 [3072/50000]	Loss: 0.4284	LR: 0.025000
Training Epoch: 52 [3200/50000]	Loss: 0.4709	LR: 0.025000
Training Epoch: 52 [3328/50000]	Loss: 0.4387	LR: 0.025000
Training Epoch: 52 [3456/50000]	Loss: 0.4949	LR: 0.025000
Training Epoch: 52 [3584/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 52 [3712/50000]	Loss: 0.4562	LR: 0.025000
Training Epoch: 52 [3840/50000]	Loss: 0.4712	LR: 0.025000
Training Epoch: 52 [3968/50000]	Loss: 0.5129	LR: 0.025000
Training Epoch: 52 [4096/50000]	Loss: 0.4674	LR: 0.025000
Training Epoch: 52 [4224/50000]	Loss: 0.4563	LR: 0.025000
Training Epoch: 52 [4352/50000]	Loss: 0.3310	LR: 0.025000
Training Epoch: 52 [4480/50000]	Loss: 0.5106	LR: 0.025000
Training Epoch: 52 [4608/50000]	Loss: 0.4400	LR: 0.025000
Training Epoch: 52 [4736/50000]	Loss: 0.6652	LR: 0.025000
Training Epoch: 52 [4864/50000]	Loss: 0.4457	LR: 0.025000
Training Epoch: 52 [4992/50000]	Loss: 0.5313	LR: 0.025000
Training Epoch: 52 [5120/50000]	Loss: 0.3978	LR: 0.025000
Training Epoch: 52 [5248/50000]	Loss: 0.4579	LR: 0.025000
Training Epoch: 52 [5376/50000]	Loss: 0.3979	LR: 0.025000
Training Epoch: 52 [5504/50000]	Loss: 0.4048	LR: 0.025000
Training Epoch: 52 [5632/50000]	Loss: 0.5523	LR: 0.025000
Training Epoch: 52 [5760/50000]	Loss: 0.5607	LR: 0.025000
Training Epoch: 52 [5888/50000]	Loss: 0.6119	LR: 0.025000
Training Epoch: 52 [6016/50000]	Loss: 0.2489	LR: 0.025000
Training Epoch: 52 [6144/50000]	Loss: 0.4577	LR: 0.025000
Training Epoch: 52 [6272/50000]	Loss: 0.4773	LR: 0.025000
Training Epoch: 52 [6400/50000]	Loss: 0.4503	LR: 0.025000
Training Epoch: 52 [6528/50000]	Loss: 0.6430	LR: 0.025000
Training Epoch: 52 [6656/50000]	Loss: 0.4946	LR: 0.025000
Training Epoch: 52 [6784/50000]	Loss: 0.3833	LR: 0.025000
Training Epoch: 52 [6912/50000]	Loss: 0.4813	LR: 0.025000
Training Epoch: 52 [7040/50000]	Loss: 0.4459	LR: 0.025000
Training Epoch: 52 [7168/50000]	Loss: 0.4760	LR: 0.025000
Training Epoch: 52 [7296/50000]	Loss: 0.4300	LR: 0.025000
Training Epoch: 52 [7424/50000]	Loss: 0.6049	LR: 0.025000
Training Epoch: 52 [7552/50000]	Loss: 0.5881	LR: 0.025000
Training Epoch: 52 [7680/50000]	Loss: 0.4888	LR: 0.025000
Training Epoch: 52 [7808/50000]	Loss: 0.4331	LR: 0.025000
Training Epoch: 52 [7936/50000]	Loss: 0.5066	LR: 0.025000
Training Epoch: 52 [8064/50000]	Loss: 0.3582	LR: 0.025000
Training Epoch: 52 [8192/50000]	Loss: 0.5237	LR: 0.025000
Training Epoch: 52 [8320/50000]	Loss: 0.5463	LR: 0.025000
Training Epoch: 52 [8448/50000]	Loss: 0.4634	LR: 0.025000
Training Epoch: 52 [8576/50000]	Loss: 0.4964	LR: 0.025000
Training Epoch: 52 [8704/50000]	Loss: 0.4919	LR: 0.025000
Training Epoch: 52 [8832/50000]	Loss: 0.4894	LR: 0.025000
Training Epoch: 52 [8960/50000]	Loss: 0.6205	LR: 0.025000
Training Epoch: 52 [9088/50000]	Loss: 0.5245	LR: 0.025000
Training Epoch: 52 [9216/50000]	Loss: 0.6861	LR: 0.025000
Training Epoch: 52 [9344/50000]	Loss: 0.4845	LR: 0.025000
Training Epoch: 52 [9472/50000]	Loss: 0.4040	LR: 0.025000
Training Epoch: 52 [9600/50000]	Loss: 0.4576	LR: 0.025000
Training Epoch: 52 [9728/50000]	Loss: 0.5575	LR: 0.025000
Training Epoch: 52 [9856/50000]	Loss: 0.4233	LR: 0.025000
Training Epoch: 52 [9984/50000]	Loss: 0.5061	LR: 0.025000
Training Epoch: 52 [10112/50000]	Loss: 0.5951	LR: 0.025000
Training Epoch: 52 [10240/50000]	Loss: 0.5548	LR: 0.025000
Training Epoch: 52 [10368/50000]	Loss: 0.5947	LR: 0.025000
Training Epoch: 52 [10496/50000]	Loss: 0.5114	LR: 0.025000
Training Epoch: 52 [10624/50000]	Loss: 0.5459	LR: 0.025000
Training Epoch: 52 [10752/50000]	Loss: 0.4821	LR: 0.025000
Training Epoch: 52 [10880/50000]	Loss: 0.5894	LR: 0.025000
Training Epoch: 52 [11008/50000]	Loss: 0.5247	LR: 0.025000
Training Epoch: 52 [11136/50000]	Loss: 0.7609	LR: 0.025000
Training Epoch: 52 [11264/50000]	Loss: 0.4522	LR: 0.025000
Training Epoch: 52 [11392/50000]	Loss: 0.4861	LR: 0.025000
Training Epoch: 52 [11520/50000]	Loss: 0.5545	LR: 0.025000
Training Epoch: 52 [11648/50000]	Loss: 0.6719	LR: 0.025000
Training Epoch: 52 [11776/50000]	Loss: 0.4484	LR: 0.025000
Training Epoch: 52 [11904/50000]	Loss: 0.5978	LR: 0.025000
Training Epoch: 52 [12032/50000]	Loss: 0.4554	LR: 0.025000
Training Epoch: 52 [12160/50000]	Loss: 0.4004	LR: 0.025000
Training Epoch: 52 [12288/50000]	Loss: 0.5974	LR: 0.025000
Training Epoch: 52 [12416/50000]	Loss: 0.4847	LR: 0.025000
Training Epoch: 52 [12544/50000]	Loss: 0.5715	LR: 0.025000
Training Epoch: 52 [12672/50000]	Loss: 0.4604	LR: 0.025000
Training Epoch: 52 [12800/50000]	Loss: 0.4799	LR: 0.025000
Training Epoch: 52 [12928/50000]	Loss: 0.5084	LR: 0.025000
Training Epoch: 52 [13056/50000]	Loss: 0.3843	LR: 0.025000
Training Epoch: 52 [13184/50000]	Loss: 0.5688	LR: 0.025000
Training Epoch: 52 [13312/50000]	Loss: 0.3745	LR: 0.025000
Training Epoch: 52 [13440/50000]	Loss: 0.7303	LR: 0.025000
Training Epoch: 52 [13568/50000]	Loss: 0.4464	LR: 0.025000
Training Epoch: 52 [13696/50000]	Loss: 0.4638	LR: 0.025000
Training Epoch: 52 [13824/50000]	Loss: 0.4081	LR: 0.025000
Training Epoch: 52 [13952/50000]	Loss: 0.4000	LR: 0.025000
Training Epoch: 52 [14080/50000]	Loss: 0.4984	LR: 0.025000
Training Epoch: 52 [14208/50000]	Loss: 0.3975	LR: 0.025000
Training Epoch: 52 [14336/50000]	Loss: 0.4781	LR: 0.025000
Training Epoch: 52 [14464/50000]	Loss: 0.4801	LR: 0.025000
Training Epoch: 52 [14592/50000]	Loss: 0.3869	LR: 0.025000
Training Epoch: 52 [14720/50000]	Loss: 0.5399	LR: 0.025000
Training Epoch: 52 [14848/50000]	Loss: 0.4174	LR: 0.025000
Training Epoch: 52 [14976/50000]	Loss: 0.5244	LR: 0.025000
Training Epoch: 52 [15104/50000]	Loss: 0.4263	LR: 0.025000
Training Epoch: 52 [15232/50000]	Loss: 0.4151	LR: 0.025000
Training Epoch: 52 [15360/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 52 [15488/50000]	Loss: 0.5842	LR: 0.025000
Training Epoch: 52 [15616/50000]	Loss: 0.4664	LR: 0.025000
Training Epoch: 52 [15744/50000]	Loss: 0.4813	LR: 0.025000
Training Epoch: 52 [15872/50000]	Loss: 0.4192	LR: 0.025000
Training Epoch: 52 [16000/50000]	Loss: 0.6220	LR: 0.025000
Training Epoch: 52 [16128/50000]	Loss: 0.4005	LR: 0.025000
Training Epoch: 52 [16256/50000]	Loss: 0.4849	LR: 0.025000
Training Epoch: 52 [16384/50000]	Loss: 0.3682	LR: 0.025000
Training Epoch: 52 [16512/50000]	Loss: 0.7062	LR: 0.025000
Training Epoch: 52 [16640/50000]	Loss: 0.5389	LR: 0.025000
Training Epoch: 52 [16768/50000]	Loss: 0.4237	LR: 0.025000
Training Epoch: 52 [16896/50000]	Loss: 0.4668	LR: 0.025000
Training Epoch: 52 [17024/50000]	Loss: 0.5150	LR: 0.025000
Training Epoch: 52 [17152/50000]	Loss: 0.4516	LR: 0.025000
Training Epoch: 52 [17280/50000]	Loss: 0.6056	LR: 0.025000
Training Epoch: 52 [17408/50000]	Loss: 0.6546	LR: 0.025000
Training Epoch: 52 [17536/50000]	Loss: 0.6018	LR: 0.025000
Training Epoch: 52 [17664/50000]	Loss: 0.5039	LR: 0.025000
Training Epoch: 52 [17792/50000]	Loss: 0.5509	LR: 0.025000
Training Epoch: 52 [17920/50000]	Loss: 0.5571	LR: 0.025000
Training Epoch: 52 [18048/50000]	Loss: 0.3791	LR: 0.025000
Training Epoch: 52 [18176/50000]	Loss: 0.6384	LR: 0.025000
Training Epoch: 52 [18304/50000]	Loss: 0.5165	LR: 0.025000
Training Epoch: 52 [18432/50000]	Loss: 0.5160	LR: 0.025000
Training Epoch: 52 [18560/50000]	Loss: 0.4229	LR: 0.025000
Training Epoch: 52 [18688/50000]	Loss: 0.4420	LR: 0.025000
Training Epoch: 52 [18816/50000]	Loss: 0.5135	LR: 0.025000
Training Epoch: 52 [18944/50000]	Loss: 0.5237	LR: 0.025000
Training Epoch: 52 [19072/50000]	Loss: 0.5599	LR: 0.025000
Training Epoch: 52 [19200/50000]	Loss: 0.6607	LR: 0.025000
Training Epoch: 52 [19328/50000]	Loss: 0.6101	LR: 0.025000
Training Epoch: 52 [19456/50000]	Loss: 0.5400	LR: 0.025000
Training Epoch: 52 [19584/50000]	Loss: 0.4210	LR: 0.025000
Training Epoch: 52 [19712/50000]	Loss: 0.5309	LR: 0.025000
Training Epoch: 52 [19840/50000]	Loss: 0.5231	LR: 0.025000
Training Epoch: 52 [19968/50000]	Loss: 0.4368	LR: 0.025000
Training Epoch: 52 [20096/50000]	Loss: 0.5231	LR: 0.025000
Training Epoch: 52 [20224/50000]	Loss: 0.6207	LR: 0.025000
Training Epoch: 52 [20352/50000]	Loss: 0.4896	LR: 0.025000
Training Epoch: 52 [20480/50000]	Loss: 0.4110	LR: 0.025000
Training Epoch: 52 [20608/50000]	Loss: 0.4791	LR: 0.025000
Training Epoch: 52 [20736/50000]	Loss: 0.5293	LR: 0.025000
Training Epoch: 52 [20864/50000]	Loss: 0.5335	LR: 0.025000
Training Epoch: 52 [20992/50000]	Loss: 0.5941	LR: 0.025000
Training Epoch: 52 [21120/50000]	Loss: 0.5362	LR: 0.025000
Training Epoch: 52 [21248/50000]	Loss: 0.4410	LR: 0.025000
Training Epoch: 52 [21376/50000]	Loss: 0.5432	LR: 0.025000
Training Epoch: 52 [21504/50000]	Loss: 0.5021	LR: 0.025000
Training Epoch: 52 [21632/50000]	Loss: 0.4069	LR: 0.025000
Training Epoch: 52 [21760/50000]	Loss: 0.5533	LR: 0.025000
Training Epoch: 52 [21888/50000]	Loss: 0.6042	LR: 0.025000
Training Epoch: 52 [22016/50000]	Loss: 0.4296	LR: 0.025000
Training Epoch: 52 [22144/50000]	Loss: 0.4354	LR: 0.025000
Training Epoch: 52 [22272/50000]	Loss: 0.5515	LR: 0.025000
Training Epoch: 52 [22400/50000]	Loss: 0.6012	LR: 0.025000
Training Epoch: 52 [22528/50000]	Loss: 0.4807	LR: 0.025000
Training Epoch: 52 [22656/50000]	Loss: 0.4120	LR: 0.025000
Training Epoch: 52 [22784/50000]	Loss: 0.5508	LR: 0.025000
Training Epoch: 52 [22912/50000]	Loss: 0.4745	LR: 0.025000
Training Epoch: 52 [23040/50000]	Loss: 0.3503	LR: 0.025000
Training Epoch: 52 [23168/50000]	Loss: 0.4164	LR: 0.025000
Training Epoch: 52 [23296/50000]	Loss: 0.5497	LR: 0.025000
Training Epoch: 52 [23424/50000]	Loss: 0.4804	LR: 0.025000
Training Epoch: 52 [23552/50000]	Loss: 0.6817	LR: 0.025000
Training Epoch: 52 [23680/50000]	Loss: 0.6252	LR: 0.025000
Training Epoch: 52 [23808/50000]	Loss: 0.7341	LR: 0.025000
Training Epoch: 52 [23936/50000]	Loss: 0.3940	LR: 0.025000
Training Epoch: 52 [24064/50000]	Loss: 0.4432	LR: 0.025000
Training Epoch: 52 [24192/50000]	Loss: 0.5402	LR: 0.025000
Training Epoch: 52 [24320/50000]	Loss: 0.3798	LR: 0.025000
Training Epoch: 52 [24448/50000]	Loss: 0.5816	LR: 0.025000
Training Epoch: 52 [24576/50000]	Loss: 0.5352	LR: 0.025000
Training Epoch: 52 [24704/50000]	Loss: 0.5843	LR: 0.025000
Training Epoch: 52 [24832/50000]	Loss: 0.5823	LR: 0.025000
Training Epoch: 52 [24960/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 52 [25088/50000]	Loss: 0.5996	LR: 0.025000
Training Epoch: 52 [25216/50000]	Loss: 0.5813	LR: 0.025000
Training Epoch: 52 [25344/50000]	Loss: 0.5431	LR: 0.025000
Training Epoch: 52 [25472/50000]	Loss: 0.6413	LR: 0.025000
Training Epoch: 52 [25600/50000]	Loss: 0.5649	LR: 0.025000
Training Epoch: 52 [25728/50000]	Loss: 0.6021	LR: 0.025000
Training Epoch: 52 [25856/50000]	Loss: 0.6031	LR: 0.025000
Training Epoch: 52 [25984/50000]	Loss: 0.6136	LR: 0.025000
Training Epoch: 52 [26112/50000]	Loss: 0.4161	LR: 0.025000
Training Epoch: 52 [26240/50000]	Loss: 0.6834	LR: 0.025000
Training Epoch: 52 [26368/50000]	Loss: 0.4120	LR: 0.025000
Training Epoch: 52 [26496/50000]	Loss: 0.6834	LR: 0.025000
Training Epoch: 52 [26624/50000]	Loss: 0.6422	LR: 0.025000
Training Epoch: 52 [26752/50000]	Loss: 0.6594	LR: 0.025000
Training Epoch: 52 [26880/50000]	Loss: 0.3467	LR: 0.025000
Training Epoch: 52 [27008/50000]	Loss: 0.4218	LR: 0.025000
Training Epoch: 52 [27136/50000]	Loss: 0.4680	LR: 0.025000
Training Epoch: 52 [27264/50000]	Loss: 0.6416	LR: 0.025000
Training Epoch: 52 [27392/50000]	Loss: 0.5112	LR: 0.025000
Training Epoch: 52 [27520/50000]	Loss: 0.5151	LR: 0.025000
Training Epoch: 52 [27648/50000]	Loss: 0.6232	LR: 0.025000
Training Epoch: 52 [27776/50000]	Loss: 0.5748	LR: 0.025000
Training Epoch: 52 [27904/50000]	Loss: 0.5914	LR: 0.025000
Training Epoch: 52 [28032/50000]	Loss: 0.5545	LR: 0.025000
Training Epoch: 52 [28160/50000]	Loss: 0.4797	LR: 0.025000
Training Epoch: 52 [28288/50000]	Loss: 0.6279	LR: 0.025000
Training Epoch: 52 [28416/50000]	Loss: 0.7174	LR: 0.025000
Training Epoch: 52 [28544/50000]	Loss: 0.6345	LR: 0.025000
Training Epoch: 52 [28672/50000]	Loss: 0.5536	LR: 0.025000
Training Epoch: 52 [28800/50000]	Loss: 0.5017	LR: 0.025000
Training Epoch: 52 [28928/50000]	Loss: 0.4567	LR: 0.025000
Training Epoch: 52 [29056/50000]	Loss: 0.7285	LR: 0.025000
Training Epoch: 52 [29184/50000]	Loss: 0.4490	LR: 0.025000
Training Epoch: 52 [29312/50000]	Loss: 0.4526	LR: 0.025000
Training Epoch: 52 [29440/50000]	Loss: 0.5391	LR: 0.025000
Training Epoch: 52 [29568/50000]	Loss: 0.5380	LR: 0.025000
Training Epoch: 52 [29696/50000]	Loss: 0.4574	LR: 0.025000
Training Epoch: 52 [29824/50000]	Loss: 0.4943	LR: 0.025000
Training Epoch: 52 [29952/50000]	Loss: 0.5867	LR: 0.025000
Training Epoch: 52 [30080/50000]	Loss: 0.4947	LR: 0.025000
Training Epoch: 52 [30208/50000]	Loss: 0.4582	LR: 0.025000
Training Epoch: 52 [30336/50000]	Loss: 0.3905	LR: 0.025000
Training Epoch: 52 [30464/50000]	Loss: 0.4239	LR: 0.025000
Training Epoch: 52 [30592/50000]	Loss: 0.6114	LR: 0.025000
Training Epoch: 52 [30720/50000]	Loss: 0.4225	LR: 0.025000
Training Epoch: 52 [30848/50000]	Loss: 0.5477	LR: 0.025000
Training Epoch: 52 [30976/50000]	Loss: 0.5414	LR: 0.025000
Training Epoch: 52 [31104/50000]	Loss: 0.6044	LR: 0.025000
Training Epoch: 52 [31232/50000]	Loss: 0.6565	LR: 0.025000
Training Epoch: 52 [31360/50000]	Loss: 0.5624	LR: 0.025000
Training Epoch: 52 [31488/50000]	Loss: 0.6101	LR: 0.025000
Training Epoch: 52 [31616/50000]	Loss: 0.3799	LR: 0.025000
Training Epoch: 52 [31744/50000]	Loss: 0.4797	LR: 0.025000
Training Epoch: 52 [31872/50000]	Loss: 0.5437	LR: 0.025000
Training Epoch: 52 [32000/50000]	Loss: 0.6042	LR: 0.025000
Training Epoch: 52 [32128/50000]	Loss: 0.4258	LR: 0.025000
Training Epoch: 52 [32256/50000]	Loss: 0.5196	LR: 0.025000
Training Epoch: 52 [32384/50000]	Loss: 0.4686	LR: 0.025000
Training Epoch: 52 [32512/50000]	Loss: 0.6690	LR: 0.025000
Training Epoch: 52 [32640/50000]	Loss: 0.5322	LR: 0.025000
Training Epoch: 52 [32768/50000]	Loss: 0.6333	LR: 0.025000
Training Epoch: 52 [32896/50000]	Loss: 0.5963	LR: 0.025000
Training Epoch: 52 [33024/50000]	Loss: 0.6165	LR: 0.025000
Training Epoch: 52 [33152/50000]	Loss: 0.4459	LR: 0.025000
Training Epoch: 52 [33280/50000]	Loss: 0.6430	LR: 0.025000
Training Epoch: 52 [33408/50000]	Loss: 0.5441	LR: 0.025000
Training Epoch: 52 [33536/50000]	Loss: 0.4553	LR: 0.025000
Training Epoch: 52 [33664/50000]	Loss: 0.6454	LR: 0.025000
Training Epoch: 52 [33792/50000]	Loss: 0.5503	LR: 0.025000
Training Epoch: 52 [33920/50000]	Loss: 0.6041	LR: 0.025000
Training Epoch: 52 [34048/50000]	Loss: 0.6269	LR: 0.025000
Training Epoch: 52 [34176/50000]	Loss: 0.4623	LR: 0.025000
Training Epoch: 52 [34304/50000]	Loss: 0.4903	LR: 0.025000
Training Epoch: 52 [34432/50000]	Loss: 0.4904	LR: 0.025000
Training Epoch: 52 [34560/50000]	Loss: 0.6599	LR: 0.025000
Training Epoch: 52 [34688/50000]	Loss: 0.5371	LR: 0.025000
Training Epoch: 52 [34816/50000]	Loss: 0.7194	LR: 0.025000
Training Epoch: 52 [34944/50000]	Loss: 0.8238	LR: 0.025000
Training Epoch: 52 [35072/50000]	Loss: 0.4918	LR: 0.025000
Training Epoch: 52 [35200/50000]	Loss: 0.7818	LR: 0.025000
Training Epoch: 52 [35328/50000]	Loss: 0.4946	LR: 0.025000
Training Epoch: 52 [35456/50000]	Loss: 0.5637	LR: 0.025000
Training Epoch: 52 [35584/50000]	Loss: 0.5883	LR: 0.025000
Training Epoch: 52 [35712/50000]	Loss: 0.4863	LR: 0.025000
Training Epoch: 52 [35840/50000]	Loss: 0.3569	LR: 0.025000
Training Epoch: 52 [35968/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 52 [36096/50000]	Loss: 0.6206	LR: 0.025000
Training Epoch: 52 [36224/50000]	Loss: 0.6262	LR: 0.025000
Training Epoch: 52 [36352/50000]	Loss: 0.4250	LR: 0.025000
Training Epoch: 52 [36480/50000]	Loss: 0.5875	LR: 0.025000
Training Epoch: 52 [36608/50000]	Loss: 0.6061	LR: 0.025000
Training Epoch: 52 [36736/50000]	Loss: 0.6189	LR: 0.025000
Training Epoch: 52 [36864/50000]	Loss: 0.5845	LR: 0.025000
Training Epoch: 52 [36992/50000]	Loss: 0.7185	LR: 0.025000
Training Epoch: 52 [37120/50000]	Loss: 0.5668	LR: 0.025000
Training Epoch: 52 [37248/50000]	Loss: 0.4732	LR: 0.025000
Training Epoch: 52 [37376/50000]	Loss: 0.6163	LR: 0.025000
Training Epoch: 52 [37504/50000]	Loss: 0.5731	LR: 0.025000
Training Epoch: 52 [37632/50000]	Loss: 0.5032	LR: 0.025000
Training Epoch: 52 [37760/50000]	Loss: 0.4298	LR: 0.025000
Training Epoch: 52 [37888/50000]	Loss: 0.3807	LR: 0.025000
Training Epoch: 52 [38016/50000]	Loss: 0.5079	LR: 0.025000
Training Epoch: 52 [38144/50000]	Loss: 0.6414	LR: 0.025000
Training Epoch: 52 [38272/50000]	Loss: 0.6393	LR: 0.025000
Training Epoch: 52 [38400/50000]	Loss: 0.6331	LR: 0.025000
Training Epoch: 52 [38528/50000]	Loss: 0.5354	LR: 0.025000
Training Epoch: 52 [38656/50000]	Loss: 0.4943	LR: 0.025000
Training Epoch: 52 [38784/50000]	Loss: 0.5786	LR: 0.025000
Training Epoch: 52 [38912/50000]	Loss: 0.5937	LR: 0.025000
Training Epoch: 52 [39040/50000]	Loss: 0.6171	LR: 0.025000
Training Epoch: 52 [39168/50000]	Loss: 0.5202	LR: 0.025000
Training Epoch: 52 [39296/50000]	Loss: 0.6624	LR: 0.025000
Training Epoch: 52 [39424/50000]	Loss: 0.5254	LR: 0.025000
Training Epoch: 52 [39552/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 52 [39680/50000]	Loss: 0.6302	LR: 0.025000
Training Epoch: 52 [39808/50000]	Loss: 0.5855	LR: 0.025000
Training Epoch: 52 [39936/50000]	Loss: 0.6459	LR: 0.025000
Training Epoch: 52 [40064/50000]	Loss: 0.4291	LR: 0.025000
Training Epoch: 52 [40192/50000]	Loss: 0.5924	LR: 0.025000
Training Epoch: 52 [40320/50000]	Loss: 0.4078	LR: 0.025000
Training Epoch: 52 [40448/50000]	Loss: 0.5024	LR: 0.025000
Training Epoch: 52 [40576/50000]	Loss: 0.4620	LR: 0.025000
Training Epoch: 52 [40704/50000]	Loss: 0.6023	LR: 0.025000
Training Epoch: 52 [40832/50000]	Loss: 0.5697	LR: 0.025000
Training Epoch: 52 [40960/50000]	Loss: 0.6262	LR: 0.025000
Training Epoch: 52 [41088/50000]	Loss: 0.5796	LR: 0.025000
Training Epoch: 52 [41216/50000]	Loss: 0.5310	LR: 0.025000
Training Epoch: 52 [41344/50000]	Loss: 0.7612	LR: 0.025000
Training Epoch: 52 [41472/50000]	Loss: 0.3827	LR: 0.025000
Training Epoch: 52 [41600/50000]	Loss: 0.6030	LR: 0.025000
Training Epoch: 52 [41728/50000]	Loss: 0.5432	LR: 0.025000
Training Epoch: 52 [41856/50000]	Loss: 0.5636	LR: 0.025000
Training Epoch: 52 [41984/50000]	Loss: 0.5052	LR: 0.025000
Training Epoch: 52 [42112/50000]	Loss: 0.7303	LR: 0.025000
Training Epoch: 52 [42240/50000]	Loss: 0.6308	LR: 0.025000
Training Epoch: 52 [42368/50000]	Loss: 0.5559	LR: 0.025000
Training Epoch: 52 [42496/50000]	Loss: 0.5014	LR: 0.025000
Training Epoch: 52 [42624/50000]	Loss: 0.5790	LR: 0.025000
Training Epoch: 52 [42752/50000]	Loss: 0.4644	LR: 0.025000
Training Epoch: 52 [42880/50000]	Loss: 0.5533	LR: 0.025000
Training Epoch: 52 [43008/50000]	Loss: 0.6472	LR: 0.025000
Training Epoch: 52 [43136/50000]	Loss: 0.6539	LR: 0.025000
Training Epoch: 52 [43264/50000]	Loss: 0.5944	LR: 0.025000
Training Epoch: 52 [43392/50000]	Loss: 0.6892	LR: 0.025000
Training Epoch: 52 [43520/50000]	Loss: 0.7753	LR: 0.025000
Training Epoch: 52 [43648/50000]	Loss: 0.6189	LR: 0.025000
Training Epoch: 52 [43776/50000]	Loss: 0.5697	LR: 0.025000
Training Epoch: 52 [43904/50000]	Loss: 0.4987	LR: 0.025000
Training Epoch: 52 [44032/50000]	Loss: 0.6156	LR: 0.025000
Training Epoch: 52 [44160/50000]	Loss: 0.3726	LR: 0.025000
Training Epoch: 52 [44288/50000]	Loss: 0.5072	LR: 0.025000
Training Epoch: 52 [44416/50000]	Loss: 0.4457	LR: 0.025000
Training Epoch: 52 [44544/50000]	Loss: 0.6707	LR: 0.025000
Training Epoch: 52 [44672/50000]	Loss: 0.5038	LR: 0.025000
Training Epoch: 52 [44800/50000]	Loss: 0.7079	LR: 0.025000
Training Epoch: 52 [44928/50000]	Loss: 0.6593	LR: 0.025000
Training Epoch: 52 [45056/50000]	Loss: 0.6087	LR: 0.025000
Training Epoch: 52 [45184/50000]	Loss: 0.6809	LR: 0.025000
Training Epoch: 52 [45312/50000]	Loss: 0.5628	LR: 0.025000
Training Epoch: 52 [45440/50000]	Loss: 0.6058	LR: 0.025000
Training Epoch: 52 [45568/50000]	Loss: 0.6483	LR: 0.025000
Training Epoch: 52 [45696/50000]	Loss: 0.5526	LR: 0.025000
Training Epoch: 52 [45824/50000]	Loss: 0.7963	LR: 0.025000
Training Epoch: 52 [45952/50000]	Loss: 0.7568	LR: 0.025000
Training Epoch: 52 [46080/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 52 [46208/50000]	Loss: 0.6590	LR: 0.025000
Training Epoch: 52 [46336/50000]	Loss: 0.6144	LR: 0.025000
Training Epoch: 52 [46464/50000]	Loss: 0.8316	LR: 0.025000
Training Epoch: 52 [46592/50000]	Loss: 0.5553	LR: 0.025000
Training Epoch: 52 [46720/50000]	Loss: 0.5866	LR: 0.025000
Training Epoch: 52 [46848/50000]	Loss: 0.5332	LR: 0.025000
Training Epoch: 52 [46976/50000]	Loss: 0.6338	LR: 0.025000
Training Epoch: 52 [47104/50000]	Loss: 0.6703	LR: 0.025000
Training Epoch: 52 [47232/50000]	Loss: 0.6179	LR: 0.025000
Training Epoch: 52 [47360/50000]	Loss: 0.6623	LR: 0.025000
Training Epoch: 52 [47488/50000]	Loss: 0.5773	LR: 0.025000
Training Epoch: 52 [47616/50000]	Loss: 0.8332	LR: 0.025000
Training Epoch: 52 [47744/50000]	Loss: 0.6399	LR: 0.025000
Training Epoch: 52 [47872/50000]	Loss: 0.7340	LR: 0.025000
Training Epoch: 52 [48000/50000]	Loss: 0.6945	LR: 0.025000
Training Epoch: 52 [48128/50000]	Loss: 0.5625	LR: 0.025000
Training Epoch: 52 [48256/50000]	Loss: 0.6117	LR: 0.025000
Training Epoch: 52 [48384/50000]	Loss: 0.7496	LR: 0.025000
Training Epoch: 52 [48512/50000]	Loss: 0.4820	LR: 0.025000
Training Epoch: 52 [48640/50000]	Loss: 0.5888	LR: 0.025000
Training Epoch: 52 [48768/50000]	Loss: 0.5683	LR: 0.025000
Training Epoch: 52 [48896/50000]	Loss: 0.4699	LR: 0.025000
Training Epoch: 52 [49024/50000]	Loss: 0.7303	LR: 0.025000
Training Epoch: 52 [49152/50000]	Loss: 0.4889	LR: 0.025000
Training Epoch: 52 [49280/50000]	Loss: 0.5776	LR: 0.025000
Training Epoch: 52 [49408/50000]	Loss: 0.6804	LR: 0.025000
Training Epoch: 52 [49536/50000]	Loss: 0.4860	LR: 0.025000
Training Epoch: 52 [49664/50000]	Loss: 0.6805	LR: 0.025000
Training Epoch: 52 [49792/50000]	Loss: 0.6689	LR: 0.025000
Training Epoch: 52 [49920/50000]	Loss: 0.6832	LR: 0.025000
Training Epoch: 52 [50000/50000]	Loss: 0.6861	LR: 0.025000
epoch 52 training time consumed: 53.98s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  190843 GB |  190843 GB |
|       from large pool |  123392 KB |    1034 MB |  190655 GB |  190655 GB |
|       from small pool |   10798 KB |      13 MB |     188 GB |     188 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  190843 GB |  190843 GB |
|       from large pool |  123392 KB |    1034 MB |  190655 GB |  190655 GB |
|       from small pool |   10798 KB |      13 MB |     188 GB |     188 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   83983 GB |   83983 GB |
|       from large pool |  155136 KB |  433088 KB |   83775 GB |   83775 GB |
|       from small pool |    1490 KB |    3494 KB |     207 GB |     207 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    7364 K  |    7363 K  |
|       from large pool |      24    |      65    |    3843 K  |    3843 K  |
|       from small pool |     231    |     274    |    3520 K  |    3520 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    7364 K  |    7363 K  |
|       from large pool |      24    |      65    |    3843 K  |    3843 K  |
|       from small pool |     231    |     274    |    3520 K  |    3520 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3639 K  |    3639 K  |
|       from large pool |       9    |      14    |    1860 K  |    1860 K  |
|       from small pool |      12    |      16    |    1779 K  |    1779 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 52, Average loss: 0.0106, Accuracy: 0.6595, Time consumed:3.45s

Training Epoch: 53 [128/50000]	Loss: 0.4705	LR: 0.025000
Training Epoch: 53 [256/50000]	Loss: 0.6025	LR: 0.025000
Training Epoch: 53 [384/50000]	Loss: 0.5166	LR: 0.025000
Training Epoch: 53 [512/50000]	Loss: 0.4375	LR: 0.025000
Training Epoch: 53 [640/50000]	Loss: 0.5672	LR: 0.025000
Training Epoch: 53 [768/50000]	Loss: 0.3727	LR: 0.025000
Training Epoch: 53 [896/50000]	Loss: 0.5793	LR: 0.025000
Training Epoch: 53 [1024/50000]	Loss: 0.4635	LR: 0.025000
Training Epoch: 53 [1152/50000]	Loss: 0.5236	LR: 0.025000
Training Epoch: 53 [1280/50000]	Loss: 0.5317	LR: 0.025000
Training Epoch: 53 [1408/50000]	Loss: 0.6961	LR: 0.025000
Training Epoch: 53 [1536/50000]	Loss: 0.5040	LR: 0.025000
Training Epoch: 53 [1664/50000]	Loss: 0.4984	LR: 0.025000
Training Epoch: 53 [1792/50000]	Loss: 0.5820	LR: 0.025000
Training Epoch: 53 [1920/50000]	Loss: 0.4219	LR: 0.025000
Training Epoch: 53 [2048/50000]	Loss: 0.4402	LR: 0.025000
Training Epoch: 53 [2176/50000]	Loss: 0.5265	LR: 0.025000
Training Epoch: 53 [2304/50000]	Loss: 0.5137	LR: 0.025000
Training Epoch: 53 [2432/50000]	Loss: 0.4491	LR: 0.025000
Training Epoch: 53 [2560/50000]	Loss: 0.4169	LR: 0.025000
Training Epoch: 53 [2688/50000]	Loss: 0.3799	LR: 0.025000
Training Epoch: 53 [2816/50000]	Loss: 0.6146	LR: 0.025000
Training Epoch: 53 [2944/50000]	Loss: 0.4776	LR: 0.025000
Training Epoch: 53 [3072/50000]	Loss: 0.5848	LR: 0.025000
Training Epoch: 53 [3200/50000]	Loss: 0.3772	LR: 0.025000
Training Epoch: 53 [3328/50000]	Loss: 0.5211	LR: 0.025000
Training Epoch: 53 [3456/50000]	Loss: 0.5476	LR: 0.025000
Training Epoch: 53 [3584/50000]	Loss: 0.4267	LR: 0.025000
Training Epoch: 53 [3712/50000]	Loss: 0.4477	LR: 0.025000
Training Epoch: 53 [3840/50000]	Loss: 0.5298	LR: 0.025000
Training Epoch: 53 [3968/50000]	Loss: 0.6363	LR: 0.025000
Training Epoch: 53 [4096/50000]	Loss: 0.6088	LR: 0.025000
Training Epoch: 53 [4224/50000]	Loss: 0.4897	LR: 0.025000
Training Epoch: 53 [4352/50000]	Loss: 0.4586	LR: 0.025000
Training Epoch: 53 [4480/50000]	Loss: 0.5273	LR: 0.025000
Training Epoch: 53 [4608/50000]	Loss: 0.4065	LR: 0.025000
Training Epoch: 53 [4736/50000]	Loss: 0.5813	LR: 0.025000
Training Epoch: 53 [4864/50000]	Loss: 0.4670	LR: 0.025000
Training Epoch: 53 [4992/50000]	Loss: 0.6180	LR: 0.025000
Training Epoch: 53 [5120/50000]	Loss: 0.4777	LR: 0.025000
Training Epoch: 53 [5248/50000]	Loss: 0.4437	LR: 0.025000
Training Epoch: 53 [5376/50000]	Loss: 0.3315	LR: 0.025000
Training Epoch: 53 [5504/50000]	Loss: 0.3581	LR: 0.025000
Training Epoch: 53 [5632/50000]	Loss: 0.4797	LR: 0.025000
Training Epoch: 53 [5760/50000]	Loss: 0.3899	LR: 0.025000
Training Epoch: 53 [5888/50000]	Loss: 0.4027	LR: 0.025000
Training Epoch: 53 [6016/50000]	Loss: 0.4898	LR: 0.025000
Training Epoch: 53 [6144/50000]	Loss: 0.4185	LR: 0.025000
Training Epoch: 53 [6272/50000]	Loss: 0.3413	LR: 0.025000
Training Epoch: 53 [6400/50000]	Loss: 0.5651	LR: 0.025000
Training Epoch: 53 [6528/50000]	Loss: 0.4864	LR: 0.025000
Training Epoch: 53 [6656/50000]	Loss: 0.4598	LR: 0.025000
Training Epoch: 53 [6784/50000]	Loss: 0.5237	LR: 0.025000
Training Epoch: 53 [6912/50000]	Loss: 0.4632	LR: 0.025000
Training Epoch: 53 [7040/50000]	Loss: 0.4911	LR: 0.025000
Training Epoch: 53 [7168/50000]	Loss: 0.5107	LR: 0.025000
Training Epoch: 53 [7296/50000]	Loss: 0.4325	LR: 0.025000
Training Epoch: 53 [7424/50000]	Loss: 0.5632	LR: 0.025000
Training Epoch: 53 [7552/50000]	Loss: 0.3492	LR: 0.025000
Training Epoch: 53 [7680/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 53 [7808/50000]	Loss: 0.3233	LR: 0.025000
Training Epoch: 53 [7936/50000]	Loss: 0.3955	LR: 0.025000
Training Epoch: 53 [8064/50000]	Loss: 0.4208	LR: 0.025000
Training Epoch: 53 [8192/50000]	Loss: 0.4378	LR: 0.025000
Training Epoch: 53 [8320/50000]	Loss: 0.5442	LR: 0.025000
Training Epoch: 53 [8448/50000]	Loss: 0.4672	LR: 0.025000
Training Epoch: 53 [8576/50000]	Loss: 0.5177	LR: 0.025000
Training Epoch: 53 [8704/50000]	Loss: 0.4511	LR: 0.025000
Training Epoch: 53 [8832/50000]	Loss: 0.3727	LR: 0.025000
Training Epoch: 53 [8960/50000]	Loss: 0.4889	LR: 0.025000
Training Epoch: 53 [9088/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 53 [9216/50000]	Loss: 0.6123	LR: 0.025000
Training Epoch: 53 [9344/50000]	Loss: 0.6274	LR: 0.025000
Training Epoch: 53 [9472/50000]	Loss: 0.5253	LR: 0.025000
Training Epoch: 53 [9600/50000]	Loss: 0.5533	LR: 0.025000
Training Epoch: 53 [9728/50000]	Loss: 0.5399	LR: 0.025000
Training Epoch: 53 [9856/50000]	Loss: 0.3773	LR: 0.025000
Training Epoch: 53 [9984/50000]	Loss: 0.3862	LR: 0.025000
Training Epoch: 53 [10112/50000]	Loss: 0.4586	LR: 0.025000
Training Epoch: 53 [10240/50000]	Loss: 0.3488	LR: 0.025000
Training Epoch: 53 [10368/50000]	Loss: 0.4685	LR: 0.025000
Training Epoch: 53 [10496/50000]	Loss: 0.5668	LR: 0.025000
Training Epoch: 53 [10624/50000]	Loss: 0.4088	LR: 0.025000
Training Epoch: 53 [10752/50000]	Loss: 0.5427	LR: 0.025000
Training Epoch: 53 [10880/50000]	Loss: 0.5365	LR: 0.025000
Training Epoch: 53 [11008/50000]	Loss: 0.5655	LR: 0.025000
Training Epoch: 53 [11136/50000]	Loss: 0.5987	LR: 0.025000
Training Epoch: 53 [11264/50000]	Loss: 0.5674	LR: 0.025000
Training Epoch: 53 [11392/50000]	Loss: 0.4415	LR: 0.025000
Training Epoch: 53 [11520/50000]	Loss: 0.5362	LR: 0.025000
Training Epoch: 53 [11648/50000]	Loss: 0.5232	LR: 0.025000
Training Epoch: 53 [11776/50000]	Loss: 0.5335	LR: 0.025000
Training Epoch: 53 [11904/50000]	Loss: 0.4035	LR: 0.025000
Training Epoch: 53 [12032/50000]	Loss: 0.4496	LR: 0.025000
Training Epoch: 53 [12160/50000]	Loss: 0.5265	LR: 0.025000
Training Epoch: 53 [12288/50000]	Loss: 0.5425	LR: 0.025000
Training Epoch: 53 [12416/50000]	Loss: 0.4477	LR: 0.025000
Training Epoch: 53 [12544/50000]	Loss: 0.4752	LR: 0.025000
Training Epoch: 53 [12672/50000]	Loss: 0.6433	LR: 0.025000
Training Epoch: 53 [12800/50000]	Loss: 0.6265	LR: 0.025000
Training Epoch: 53 [12928/50000]	Loss: 0.5724	LR: 0.025000
Training Epoch: 53 [13056/50000]	Loss: 0.4710	LR: 0.025000
Training Epoch: 53 [13184/50000]	Loss: 0.4558	LR: 0.025000
Training Epoch: 53 [13312/50000]	Loss: 0.5506	LR: 0.025000
Training Epoch: 53 [13440/50000]	Loss: 0.4660	LR: 0.025000
Training Epoch: 53 [13568/50000]	Loss: 0.4776	LR: 0.025000
Training Epoch: 53 [13696/50000]	Loss: 0.5083	LR: 0.025000
Training Epoch: 53 [13824/50000]	Loss: 0.5449	LR: 0.025000
Training Epoch: 53 [13952/50000]	Loss: 0.3908	LR: 0.025000
Training Epoch: 53 [14080/50000]	Loss: 0.7437	LR: 0.025000
Training Epoch: 53 [14208/50000]	Loss: 0.4666	LR: 0.025000
Training Epoch: 53 [14336/50000]	Loss: 0.5403	LR: 0.025000
Training Epoch: 53 [14464/50000]	Loss: 0.4986	LR: 0.025000
Training Epoch: 53 [14592/50000]	Loss: 0.6154	LR: 0.025000
Training Epoch: 53 [14720/50000]	Loss: 0.5163	LR: 0.025000
Training Epoch: 53 [14848/50000]	Loss: 0.5349	LR: 0.025000
Training Epoch: 53 [14976/50000]	Loss: 0.5652	LR: 0.025000
Training Epoch: 53 [15104/50000]	Loss: 0.4863	LR: 0.025000
Training Epoch: 53 [15232/50000]	Loss: 0.7465	LR: 0.025000
Training Epoch: 53 [15360/50000]	Loss: 0.5036	LR: 0.025000
Training Epoch: 53 [15488/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 53 [15616/50000]	Loss: 0.5353	LR: 0.025000
Training Epoch: 53 [15744/50000]	Loss: 0.5792	LR: 0.025000
Training Epoch: 53 [15872/50000]	Loss: 0.3922	LR: 0.025000
Training Epoch: 53 [16000/50000]	Loss: 0.4594	LR: 0.025000
Training Epoch: 53 [16128/50000]	Loss: 0.4847	LR: 0.025000
Training Epoch: 53 [16256/50000]	Loss: 0.4972	LR: 0.025000
Training Epoch: 53 [16384/50000]	Loss: 0.4268	LR: 0.025000
Training Epoch: 53 [16512/50000]	Loss: 0.7267	LR: 0.025000
Training Epoch: 53 [16640/50000]	Loss: 0.6096	LR: 0.025000
Training Epoch: 53 [16768/50000]	Loss: 0.5345	LR: 0.025000
Training Epoch: 53 [16896/50000]	Loss: 0.5682	LR: 0.025000
Training Epoch: 53 [17024/50000]	Loss: 0.6267	LR: 0.025000
Training Epoch: 53 [17152/50000]	Loss: 0.5859	LR: 0.025000
Training Epoch: 53 [17280/50000]	Loss: 0.5248	LR: 0.025000
Training Epoch: 53 [17408/50000]	Loss: 0.6109	LR: 0.025000
Training Epoch: 53 [17536/50000]	Loss: 0.7531	LR: 0.025000
Training Epoch: 53 [17664/50000]	Loss: 0.5727	LR: 0.025000
Training Epoch: 53 [17792/50000]	Loss: 0.4385	LR: 0.025000
Training Epoch: 53 [17920/50000]	Loss: 0.5056	LR: 0.025000
Training Epoch: 53 [18048/50000]	Loss: 0.4924	LR: 0.025000
Training Epoch: 53 [18176/50000]	Loss: 0.3875	LR: 0.025000
Training Epoch: 53 [18304/50000]	Loss: 0.5653	LR: 0.025000
Training Epoch: 53 [18432/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 53 [18560/50000]	Loss: 0.6381	LR: 0.025000
Training Epoch: 53 [18688/50000]	Loss: 0.5202	LR: 0.025000
Training Epoch: 53 [18816/50000]	Loss: 0.4392	LR: 0.025000
Training Epoch: 53 [18944/50000]	Loss: 0.6095	LR: 0.025000
Training Epoch: 53 [19072/50000]	Loss: 0.6531	LR: 0.025000
Training Epoch: 53 [19200/50000]	Loss: 0.5166	LR: 0.025000
Training Epoch: 53 [19328/50000]	Loss: 0.5747	LR: 0.025000
Training Epoch: 53 [19456/50000]	Loss: 0.6991	LR: 0.025000
Training Epoch: 53 [19584/50000]	Loss: 0.5954	LR: 0.025000
Training Epoch: 53 [19712/50000]	Loss: 0.5392	LR: 0.025000
Training Epoch: 53 [19840/50000]	Loss: 0.5699	LR: 0.025000
Training Epoch: 53 [19968/50000]	Loss: 0.4848	LR: 0.025000
Training Epoch: 53 [20096/50000]	Loss: 0.6892	LR: 0.025000
Training Epoch: 53 [20224/50000]	Loss: 0.6870	LR: 0.025000
Training Epoch: 53 [20352/50000]	Loss: 0.6362	LR: 0.025000
Training Epoch: 53 [20480/50000]	Loss: 0.6677	LR: 0.025000
Training Epoch: 53 [20608/50000]	Loss: 0.5382	LR: 0.025000
Training Epoch: 53 [20736/50000]	Loss: 0.4974	LR: 0.025000
Training Epoch: 53 [20864/50000]	Loss: 0.3390	LR: 0.025000
Training Epoch: 53 [20992/50000]	Loss: 0.4626	LR: 0.025000
Training Epoch: 53 [21120/50000]	Loss: 0.5675	LR: 0.025000
Training Epoch: 53 [21248/50000]	Loss: 0.5753	LR: 0.025000
Training Epoch: 53 [21376/50000]	Loss: 0.3780	LR: 0.025000
Training Epoch: 53 [21504/50000]	Loss: 0.8641	LR: 0.025000
Training Epoch: 53 [21632/50000]	Loss: 0.7011	LR: 0.025000
Training Epoch: 53 [21760/50000]	Loss: 0.5993	LR: 0.025000
Training Epoch: 53 [21888/50000]	Loss: 0.5185	LR: 0.025000
Training Epoch: 53 [22016/50000]	Loss: 0.5314	LR: 0.025000
Training Epoch: 53 [22144/50000]	Loss: 0.4001	LR: 0.025000
Training Epoch: 53 [22272/50000]	Loss: 0.3785	LR: 0.025000
Training Epoch: 53 [22400/50000]	Loss: 0.3821	LR: 0.025000
Training Epoch: 53 [22528/50000]	Loss: 0.5772	LR: 0.025000
Training Epoch: 53 [22656/50000]	Loss: 0.5291	LR: 0.025000
Training Epoch: 53 [22784/50000]	Loss: 0.6022	LR: 0.025000
Training Epoch: 53 [22912/50000]	Loss: 0.5304	LR: 0.025000
Training Epoch: 53 [23040/50000]	Loss: 0.3311	LR: 0.025000
Training Epoch: 53 [23168/50000]	Loss: 0.4751	LR: 0.025000
Training Epoch: 53 [23296/50000]	Loss: 0.6333	LR: 0.025000
Training Epoch: 53 [23424/50000]	Loss: 0.6191	LR: 0.025000
Training Epoch: 53 [23552/50000]	Loss: 0.5906	LR: 0.025000
Training Epoch: 53 [23680/50000]	Loss: 0.5351	LR: 0.025000
Training Epoch: 53 [23808/50000]	Loss: 0.5510	LR: 0.025000
Training Epoch: 53 [23936/50000]	Loss: 0.4347	LR: 0.025000
Training Epoch: 53 [24064/50000]	Loss: 0.5693	LR: 0.025000
Training Epoch: 53 [24192/50000]	Loss: 0.4691	LR: 0.025000
Training Epoch: 53 [24320/50000]	Loss: 0.6012	LR: 0.025000
Training Epoch: 53 [24448/50000]	Loss: 0.5118	LR: 0.025000
Training Epoch: 53 [24576/50000]	Loss: 0.5969	LR: 0.025000
Training Epoch: 53 [24704/50000]	Loss: 0.4154	LR: 0.025000
Training Epoch: 53 [24832/50000]	Loss: 0.6083	LR: 0.025000
Training Epoch: 53 [24960/50000]	Loss: 0.5143	LR: 0.025000
Training Epoch: 53 [25088/50000]	Loss: 0.4947	LR: 0.025000
Training Epoch: 53 [25216/50000]	Loss: 0.5277	LR: 0.025000
Training Epoch: 53 [25344/50000]	Loss: 0.5496	LR: 0.025000
Training Epoch: 53 [25472/50000]	Loss: 0.3731	LR: 0.025000
Training Epoch: 53 [25600/50000]	Loss: 0.5426	LR: 0.025000
Training Epoch: 53 [25728/50000]	Loss: 0.4717	LR: 0.025000
Training Epoch: 53 [25856/50000]	Loss: 0.6012	LR: 0.025000
Training Epoch: 53 [25984/50000]	Loss: 0.5555	LR: 0.025000
Training Epoch: 53 [26112/50000]	Loss: 0.5669	LR: 0.025000
Training Epoch: 53 [26240/50000]	Loss: 0.6110	LR: 0.025000
Training Epoch: 53 [26368/50000]	Loss: 0.5701	LR: 0.025000
Training Epoch: 53 [26496/50000]	Loss: 0.6518	LR: 0.025000
Training Epoch: 53 [26624/50000]	Loss: 0.4973	LR: 0.025000
Training Epoch: 53 [26752/50000]	Loss: 0.5623	LR: 0.025000
Training Epoch: 53 [26880/50000]	Loss: 0.4827	LR: 0.025000
Training Epoch: 53 [27008/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 53 [27136/50000]	Loss: 0.4906	LR: 0.025000
Training Epoch: 53 [27264/50000]	Loss: 0.3931	LR: 0.025000
Training Epoch: 53 [27392/50000]	Loss: 0.4208	LR: 0.025000
Training Epoch: 53 [27520/50000]	Loss: 0.4812	LR: 0.025000
Training Epoch: 53 [27648/50000]	Loss: 0.5188	LR: 0.025000
Training Epoch: 53 [27776/50000]	Loss: 0.4925	LR: 0.025000
Training Epoch: 53 [27904/50000]	Loss: 0.5537	LR: 0.025000
Training Epoch: 53 [28032/50000]	Loss: 0.4435	LR: 0.025000
Training Epoch: 53 [28160/50000]	Loss: 0.4671	LR: 0.025000
Training Epoch: 53 [28288/50000]	Loss: 0.5614	LR: 0.025000
Training Epoch: 53 [28416/50000]	Loss: 0.7457	LR: 0.025000
Training Epoch: 53 [28544/50000]	Loss: 0.3793	LR: 0.025000
Training Epoch: 53 [28672/50000]	Loss: 0.3499	LR: 0.025000
Training Epoch: 53 [28800/50000]	Loss: 0.6739	LR: 0.025000
Training Epoch: 53 [28928/50000]	Loss: 0.5724	LR: 0.025000
Training Epoch: 53 [29056/50000]	Loss: 0.4927	LR: 0.025000
Training Epoch: 53 [29184/50000]	Loss: 0.5604	LR: 0.025000
Training Epoch: 53 [29312/50000]	Loss: 0.5762	LR: 0.025000
Training Epoch: 53 [29440/50000]	Loss: 0.6418	LR: 0.025000
Training Epoch: 53 [29568/50000]	Loss: 0.6156	LR: 0.025000
Training Epoch: 53 [29696/50000]	Loss: 0.6021	LR: 0.025000
Training Epoch: 53 [29824/50000]	Loss: 0.5629	LR: 0.025000
Training Epoch: 53 [29952/50000]	Loss: 0.6338	LR: 0.025000
Training Epoch: 53 [30080/50000]	Loss: 0.6089	LR: 0.025000
Training Epoch: 53 [30208/50000]	Loss: 0.6595	LR: 0.025000
Training Epoch: 53 [30336/50000]	Loss: 0.5420	LR: 0.025000
Training Epoch: 53 [30464/50000]	Loss: 0.4709	LR: 0.025000
Training Epoch: 53 [30592/50000]	Loss: 0.5662	LR: 0.025000
Training Epoch: 53 [30720/50000]	Loss: 0.6146	LR: 0.025000
Training Epoch: 53 [30848/50000]	Loss: 0.7918	LR: 0.025000
Training Epoch: 53 [30976/50000]	Loss: 0.6385	LR: 0.025000
Training Epoch: 53 [31104/50000]	Loss: 0.6193	LR: 0.025000
Training Epoch: 53 [31232/50000]	Loss: 0.5096	LR: 0.025000
Training Epoch: 53 [31360/50000]	Loss: 0.6526	LR: 0.025000
Training Epoch: 53 [31488/50000]	Loss: 0.7207	LR: 0.025000
Training Epoch: 53 [31616/50000]	Loss: 0.6074	LR: 0.025000
Training Epoch: 53 [31744/50000]	Loss: 0.6093	LR: 0.025000
Training Epoch: 53 [31872/50000]	Loss: 0.5561	LR: 0.025000
Training Epoch: 53 [32000/50000]	Loss: 0.4785	LR: 0.025000
Training Epoch: 53 [32128/50000]	Loss: 0.6127	LR: 0.025000
Training Epoch: 53 [32256/50000]	Loss: 0.6992	LR: 0.025000
Training Epoch: 53 [32384/50000]	Loss: 0.4800	LR: 0.025000
Training Epoch: 53 [32512/50000]	Loss: 0.5721	LR: 0.025000
Training Epoch: 53 [32640/50000]	Loss: 0.7487	LR: 0.025000
Training Epoch: 53 [32768/50000]	Loss: 0.5345	LR: 0.025000
Training Epoch: 53 [32896/50000]	Loss: 0.5272	LR: 0.025000
Training Epoch: 53 [33024/50000]	Loss: 0.7946	LR: 0.025000
Training Epoch: 53 [33152/50000]	Loss: 0.6262	LR: 0.025000
Training Epoch: 53 [33280/50000]	Loss: 0.6288	LR: 0.025000
Training Epoch: 53 [33408/50000]	Loss: 0.3998	LR: 0.025000
Training Epoch: 53 [33536/50000]	Loss: 0.5246	LR: 0.025000
Training Epoch: 53 [33664/50000]	Loss: 0.6463	LR: 0.025000
Training Epoch: 53 [33792/50000]	Loss: 0.5984	LR: 0.025000
Training Epoch: 53 [33920/50000]	Loss: 0.6020	LR: 0.025000
Training Epoch: 53 [34048/50000]	Loss: 0.6187	LR: 0.025000
Training Epoch: 53 [34176/50000]	Loss: 0.5348	LR: 0.025000
Training Epoch: 53 [34304/50000]	Loss: 0.5330	LR: 0.025000
Training Epoch: 53 [34432/50000]	Loss: 0.7630	LR: 0.025000
Training Epoch: 53 [34560/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 53 [34688/50000]	Loss: 0.7683	LR: 0.025000
Training Epoch: 53 [34816/50000]	Loss: 0.5653	LR: 0.025000
Training Epoch: 53 [34944/50000]	Loss: 0.4113	LR: 0.025000
Training Epoch: 53 [35072/50000]	Loss: 0.6121	LR: 0.025000
Training Epoch: 53 [35200/50000]	Loss: 0.4783	LR: 0.025000
Training Epoch: 53 [35328/50000]	Loss: 0.5662	LR: 0.025000
Training Epoch: 53 [35456/50000]	Loss: 0.5726	LR: 0.025000
Training Epoch: 53 [35584/50000]	Loss: 0.5301	LR: 0.025000
Training Epoch: 53 [35712/50000]	Loss: 0.6664	LR: 0.025000
Training Epoch: 53 [35840/50000]	Loss: 0.5790	LR: 0.025000
Training Epoch: 53 [35968/50000]	Loss: 0.6041	LR: 0.025000
Training Epoch: 53 [36096/50000]	Loss: 0.6157	LR: 0.025000
Training Epoch: 53 [36224/50000]	Loss: 0.5803	LR: 0.025000
Training Epoch: 53 [36352/50000]	Loss: 0.5494	LR: 0.025000
Training Epoch: 53 [36480/50000]	Loss: 0.4759	LR: 0.025000
Training Epoch: 53 [36608/50000]	Loss: 0.5511	LR: 0.025000
Training Epoch: 53 [36736/50000]	Loss: 0.6323	LR: 0.025000
Training Epoch: 53 [36864/50000]	Loss: 0.5468	LR: 0.025000
Training Epoch: 53 [36992/50000]	Loss: 0.7406	LR: 0.025000
Training Epoch: 53 [37120/50000]	Loss: 0.6028	LR: 0.025000
Training Epoch: 53 [37248/50000]	Loss: 0.5862	LR: 0.025000
Training Epoch: 53 [37376/50000]	Loss: 0.5398	LR: 0.025000
Training Epoch: 53 [37504/50000]	Loss: 0.7171	LR: 0.025000
Training Epoch: 53 [37632/50000]	Loss: 0.6777	LR: 0.025000
Training Epoch: 53 [37760/50000]	Loss: 0.7658	LR: 0.025000
Training Epoch: 53 [37888/50000]	Loss: 0.6301	LR: 0.025000
Training Epoch: 53 [38016/50000]	Loss: 0.6370	LR: 0.025000
Training Epoch: 53 [38144/50000]	Loss: 0.5992	LR: 0.025000
Training Epoch: 53 [38272/50000]	Loss: 0.5974	LR: 0.025000
Training Epoch: 53 [38400/50000]	Loss: 0.6706	LR: 0.025000
Training Epoch: 53 [38528/50000]	Loss: 0.5038	LR: 0.025000
Training Epoch: 53 [38656/50000]	Loss: 0.6270	LR: 0.025000
Training Epoch: 53 [38784/50000]	Loss: 0.4674	LR: 0.025000
Training Epoch: 53 [38912/50000]	Loss: 0.4562	LR: 0.025000
Training Epoch: 53 [39040/50000]	Loss: 0.5457	LR: 0.025000
Training Epoch: 53 [39168/50000]	Loss: 0.6942	LR: 0.025000
Training Epoch: 53 [39296/50000]	Loss: 0.6129	LR: 0.025000
Training Epoch: 53 [39424/50000]	Loss: 0.3947	LR: 0.025000
Training Epoch: 53 [39552/50000]	Loss: 0.4625	LR: 0.025000
Training Epoch: 53 [39680/50000]	Loss: 0.7574	LR: 0.025000
Training Epoch: 53 [39808/50000]	Loss: 0.6111	LR: 0.025000
Training Epoch: 53 [39936/50000]	Loss: 0.5865	LR: 0.025000
Training Epoch: 53 [40064/50000]	Loss: 0.6964	LR: 0.025000
Training Epoch: 53 [40192/50000]	Loss: 0.5817	LR: 0.025000
Training Epoch: 53 [40320/50000]	Loss: 0.6090	LR: 0.025000
Training Epoch: 53 [40448/50000]	Loss: 0.5548	LR: 0.025000
Training Epoch: 53 [40576/50000]	Loss: 0.3216	LR: 0.025000
Training Epoch: 53 [40704/50000]	Loss: 0.6503	LR: 0.025000
Training Epoch: 53 [40832/50000]	Loss: 0.4651	LR: 0.025000
Training Epoch: 53 [40960/50000]	Loss: 0.7316	LR: 0.025000
Training Epoch: 53 [41088/50000]	Loss: 0.5293	LR: 0.025000
Training Epoch: 53 [41216/50000]	Loss: 0.6761	LR: 0.025000
Training Epoch: 53 [41344/50000]	Loss: 0.6344	LR: 0.025000
Training Epoch: 53 [41472/50000]	Loss: 0.6061	LR: 0.025000
Training Epoch: 53 [41600/50000]	Loss: 0.4394	LR: 0.025000
Training Epoch: 53 [41728/50000]	Loss: 0.6169	LR: 0.025000
Training Epoch: 53 [41856/50000]	Loss: 0.5929	LR: 0.025000
Training Epoch: 53 [41984/50000]	Loss: 0.6127	LR: 0.025000
Training Epoch: 53 [42112/50000]	Loss: 0.6596	LR: 0.025000
Training Epoch: 53 [42240/50000]	Loss: 0.6657	LR: 0.025000
Training Epoch: 53 [42368/50000]	Loss: 0.5962	LR: 0.025000
Training Epoch: 53 [42496/50000]	Loss: 0.5999	LR: 0.025000
Training Epoch: 53 [42624/50000]	Loss: 0.6877	LR: 0.025000
Training Epoch: 53 [42752/50000]	Loss: 0.4174	LR: 0.025000
Training Epoch: 53 [42880/50000]	Loss: 0.4983	LR: 0.025000
Training Epoch: 53 [43008/50000]	Loss: 0.6002	LR: 0.025000
Training Epoch: 53 [43136/50000]	Loss: 0.4525	LR: 0.025000
Training Epoch: 53 [43264/50000]	Loss: 0.5959	LR: 0.025000
Training Epoch: 53 [43392/50000]	Loss: 0.6541	LR: 0.025000
Training Epoch: 53 [43520/50000]	Loss: 0.5161	LR: 0.025000
Training Epoch: 53 [43648/50000]	Loss: 0.5837	LR: 0.025000
Training Epoch: 53 [43776/50000]	Loss: 0.5261	LR: 0.025000
Training Epoch: 53 [43904/50000]	Loss: 0.6550	LR: 0.025000
Training Epoch: 53 [44032/50000]	Loss: 0.6375	LR: 0.025000
Training Epoch: 53 [44160/50000]	Loss: 0.7268	LR: 0.025000
Training Epoch: 53 [44288/50000]	Loss: 0.6868	LR: 0.025000
Training Epoch: 53 [44416/50000]	Loss: 0.5894	LR: 0.025000
Training Epoch: 53 [44544/50000]	Loss: 0.6182	LR: 0.025000
Training Epoch: 53 [44672/50000]	Loss: 0.6436	LR: 0.025000
Training Epoch: 53 [44800/50000]	Loss: 0.5028	LR: 0.025000
Training Epoch: 53 [44928/50000]	Loss: 0.5389	LR: 0.025000
Training Epoch: 53 [45056/50000]	Loss: 0.6776	LR: 0.025000
Training Epoch: 53 [45184/50000]	Loss: 0.5830	LR: 0.025000
Training Epoch: 53 [45312/50000]	Loss: 0.5429	LR: 0.025000
Training Epoch: 53 [45440/50000]	Loss: 0.5063	LR: 0.025000
Training Epoch: 53 [45568/50000]	Loss: 0.7443	LR: 0.025000
Training Epoch: 53 [45696/50000]	Loss: 0.5130	LR: 0.025000
Training Epoch: 53 [45824/50000]	Loss: 0.5936	LR: 0.025000
Training Epoch: 53 [45952/50000]	Loss: 0.6409	LR: 0.025000
Training Epoch: 53 [46080/50000]	Loss: 0.4869	LR: 0.025000
Training Epoch: 53 [46208/50000]	Loss: 0.5716	LR: 0.025000
Training Epoch: 53 [46336/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 53 [46464/50000]	Loss: 0.5869	LR: 0.025000
Training Epoch: 53 [46592/50000]	Loss: 0.6517	LR: 0.025000
Training Epoch: 53 [46720/50000]	Loss: 0.4900	LR: 0.025000
Training Epoch: 53 [46848/50000]	Loss: 0.5899	LR: 0.025000
Training Epoch: 53 [46976/50000]	Loss: 0.6642	LR: 0.025000
Training Epoch: 53 [47104/50000]	Loss: 0.5584	LR: 0.025000
Training Epoch: 53 [47232/50000]	Loss: 0.4381	LR: 0.025000
Training Epoch: 53 [47360/50000]	Loss: 0.4553	LR: 0.025000
Training Epoch: 53 [47488/50000]	Loss: 0.5358	LR: 0.025000
Training Epoch: 53 [47616/50000]	Loss: 0.5833	LR: 0.025000
Training Epoch: 53 [47744/50000]	Loss: 0.5187	LR: 0.025000
Training Epoch: 53 [47872/50000]	Loss: 0.4801	LR: 0.025000
Training Epoch: 53 [48000/50000]	Loss: 0.6368	LR: 0.025000
Training Epoch: 53 [48128/50000]	Loss: 0.5575	LR: 0.025000
Training Epoch: 53 [48256/50000]	Loss: 0.6184	LR: 0.025000
Training Epoch: 53 [48384/50000]	Loss: 0.5851	LR: 0.025000
Training Epoch: 53 [48512/50000]	Loss: 0.5950	LR: 0.025000
Training Epoch: 53 [48640/50000]	Loss: 0.4978	LR: 0.025000
Training Epoch: 53 [48768/50000]	Loss: 0.5593	LR: 0.025000
Training Epoch: 53 [48896/50000]	Loss: 0.5374	LR: 0.025000
Training Epoch: 53 [49024/50000]	Loss: 0.4779	LR: 0.025000
Training Epoch: 53 [49152/50000]	Loss: 0.5745	LR: 0.025000
Training Epoch: 53 [49280/50000]	Loss: 0.5826	LR: 0.025000
Training Epoch: 53 [49408/50000]	Loss: 0.5015	LR: 0.025000
Training Epoch: 53 [49536/50000]	Loss: 0.5722	LR: 0.025000
Training Epoch: 53 [49664/50000]	Loss: 0.4344	LR: 0.025000
Training Epoch: 53 [49792/50000]	Loss: 0.4437	LR: 0.025000
Training Epoch: 53 [49920/50000]	Loss: 0.7570	LR: 0.025000
Training Epoch: 53 [50000/50000]	Loss: 0.7557	LR: 0.025000
epoch 53 training time consumed: 53.98s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  194513 GB |  194513 GB |
|       from large pool |  123392 KB |    1034 MB |  194321 GB |  194321 GB |
|       from small pool |   10798 KB |      13 MB |     191 GB |     191 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  194513 GB |  194513 GB |
|       from large pool |  123392 KB |    1034 MB |  194321 GB |  194321 GB |
|       from small pool |   10798 KB |      13 MB |     191 GB |     191 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   85598 GB |   85598 GB |
|       from large pool |  155136 KB |  433088 KB |   85386 GB |   85386 GB |
|       from small pool |    1490 KB |    3494 KB |     211 GB |     211 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    7505 K  |    7505 K  |
|       from large pool |      24    |      65    |    3917 K  |    3917 K  |
|       from small pool |     231    |     274    |    3588 K  |    3587 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    7505 K  |    7505 K  |
|       from large pool |      24    |      65    |    3917 K  |    3917 K  |
|       from small pool |     231    |     274    |    3588 K  |    3587 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3709 K  |    3709 K  |
|       from large pool |       9    |      14    |    1896 K  |    1896 K  |
|       from small pool |      12    |      16    |    1813 K  |    1813 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 53, Average loss: 0.0105, Accuracy: 0.6657, Time consumed:3.47s

Training Epoch: 54 [128/50000]	Loss: 0.6134	LR: 0.025000
Training Epoch: 54 [256/50000]	Loss: 0.4320	LR: 0.025000
Training Epoch: 54 [384/50000]	Loss: 0.4355	LR: 0.025000
Training Epoch: 54 [512/50000]	Loss: 0.3857	LR: 0.025000
Training Epoch: 54 [640/50000]	Loss: 0.5053	LR: 0.025000
Training Epoch: 54 [768/50000]	Loss: 0.5117	LR: 0.025000
Training Epoch: 54 [896/50000]	Loss: 0.4485	LR: 0.025000
Training Epoch: 54 [1024/50000]	Loss: 0.4694	LR: 0.025000
Training Epoch: 54 [1152/50000]	Loss: 0.4025	LR: 0.025000
Training Epoch: 54 [1280/50000]	Loss: 0.4136	LR: 0.025000
Training Epoch: 54 [1408/50000]	Loss: 0.5282	LR: 0.025000
Training Epoch: 54 [1536/50000]	Loss: 0.4181	LR: 0.025000
Training Epoch: 54 [1664/50000]	Loss: 0.4276	LR: 0.025000
Training Epoch: 54 [1792/50000]	Loss: 0.4166	LR: 0.025000
Training Epoch: 54 [1920/50000]	Loss: 0.4423	LR: 0.025000
Training Epoch: 54 [2048/50000]	Loss: 0.4556	LR: 0.025000
Training Epoch: 54 [2176/50000]	Loss: 0.6095	LR: 0.025000
Training Epoch: 54 [2304/50000]	Loss: 0.3475	LR: 0.025000
Training Epoch: 54 [2432/50000]	Loss: 0.4739	LR: 0.025000
Training Epoch: 54 [2560/50000]	Loss: 0.5141	LR: 0.025000
Training Epoch: 54 [2688/50000]	Loss: 0.4535	LR: 0.025000
Training Epoch: 54 [2816/50000]	Loss: 0.4949	LR: 0.025000
Training Epoch: 54 [2944/50000]	Loss: 0.3388	LR: 0.025000
Training Epoch: 54 [3072/50000]	Loss: 0.4606	LR: 0.025000
Training Epoch: 54 [3200/50000]	Loss: 0.4476	LR: 0.025000
Training Epoch: 54 [3328/50000]	Loss: 0.4926	LR: 0.025000
Training Epoch: 54 [3456/50000]	Loss: 0.3321	LR: 0.025000
Training Epoch: 54 [3584/50000]	Loss: 0.4760	LR: 0.025000
Training Epoch: 54 [3712/50000]	Loss: 0.5292	LR: 0.025000
Training Epoch: 54 [3840/50000]	Loss: 0.4310	LR: 0.025000
Training Epoch: 54 [3968/50000]	Loss: 0.4049	LR: 0.025000
Training Epoch: 54 [4096/50000]	Loss: 0.2921	LR: 0.025000
Training Epoch: 54 [4224/50000]	Loss: 0.4869	LR: 0.025000
Training Epoch: 54 [4352/50000]	Loss: 0.4755	LR: 0.025000
Training Epoch: 54 [4480/50000]	Loss: 0.3812	LR: 0.025000
Training Epoch: 54 [4608/50000]	Loss: 0.5152	LR: 0.025000
Training Epoch: 54 [4736/50000]	Loss: 0.5103	LR: 0.025000
Training Epoch: 54 [4864/50000]	Loss: 0.3262	LR: 0.025000
Training Epoch: 54 [4992/50000]	Loss: 0.4309	LR: 0.025000
Training Epoch: 54 [5120/50000]	Loss: 0.4275	LR: 0.025000
Training Epoch: 54 [5248/50000]	Loss: 0.4393	LR: 0.025000
Training Epoch: 54 [5376/50000]	Loss: 0.4111	LR: 0.025000
Training Epoch: 54 [5504/50000]	Loss: 0.8253	LR: 0.025000
Training Epoch: 54 [5632/50000]	Loss: 0.5608	LR: 0.025000
Training Epoch: 54 [5760/50000]	Loss: 0.2988	LR: 0.025000
Training Epoch: 54 [5888/50000]	Loss: 0.3022	LR: 0.025000
Training Epoch: 54 [6016/50000]	Loss: 0.4555	LR: 0.025000
Training Epoch: 54 [6144/50000]	Loss: 0.4483	LR: 0.025000
Training Epoch: 54 [6272/50000]	Loss: 0.4644	LR: 0.025000
Training Epoch: 54 [6400/50000]	Loss: 0.4116	LR: 0.025000
Training Epoch: 54 [6528/50000]	Loss: 0.4778	LR: 0.025000
Training Epoch: 54 [6656/50000]	Loss: 0.3827	LR: 0.025000
Training Epoch: 54 [6784/50000]	Loss: 0.4655	LR: 0.025000
Training Epoch: 54 [6912/50000]	Loss: 0.5624	LR: 0.025000
Training Epoch: 54 [7040/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 54 [7168/50000]	Loss: 0.7397	LR: 0.025000
Training Epoch: 54 [7296/50000]	Loss: 0.6062	LR: 0.025000
Training Epoch: 54 [7424/50000]	Loss: 0.4017	LR: 0.025000
Training Epoch: 54 [7552/50000]	Loss: 0.4480	LR: 0.025000
Training Epoch: 54 [7680/50000]	Loss: 0.5350	LR: 0.025000
Training Epoch: 54 [7808/50000]	Loss: 0.4192	LR: 0.025000
Training Epoch: 54 [7936/50000]	Loss: 0.4809	LR: 0.025000
Training Epoch: 54 [8064/50000]	Loss: 0.3824	LR: 0.025000
Training Epoch: 54 [8192/50000]	Loss: 0.4932	LR: 0.025000
Training Epoch: 54 [8320/50000]	Loss: 0.4184	LR: 0.025000
Training Epoch: 54 [8448/50000]	Loss: 0.4709	LR: 0.025000
Training Epoch: 54 [8576/50000]	Loss: 0.3915	LR: 0.025000
Training Epoch: 54 [8704/50000]	Loss: 0.4495	LR: 0.025000
Training Epoch: 54 [8832/50000]	Loss: 0.4784	LR: 0.025000
Training Epoch: 54 [8960/50000]	Loss: 0.4501	LR: 0.025000
Training Epoch: 54 [9088/50000]	Loss: 0.4281	LR: 0.025000
Training Epoch: 54 [9216/50000]	Loss: 0.4780	LR: 0.025000
Training Epoch: 54 [9344/50000]	Loss: 0.4151	LR: 0.025000
Training Epoch: 54 [9472/50000]	Loss: 0.4802	LR: 0.025000
Training Epoch: 54 [9600/50000]	Loss: 0.6219	LR: 0.025000
Training Epoch: 54 [9728/50000]	Loss: 0.4118	LR: 0.025000
Training Epoch: 54 [9856/50000]	Loss: 0.5323	LR: 0.025000
Training Epoch: 54 [9984/50000]	Loss: 0.3446	LR: 0.025000
Training Epoch: 54 [10112/50000]	Loss: 0.4277	LR: 0.025000
Training Epoch: 54 [10240/50000]	Loss: 0.4121	LR: 0.025000
Training Epoch: 54 [10368/50000]	Loss: 0.4671	LR: 0.025000
Training Epoch: 54 [10496/50000]	Loss: 0.4414	LR: 0.025000
Training Epoch: 54 [10624/50000]	Loss: 0.5322	LR: 0.025000
Training Epoch: 54 [10752/50000]	Loss: 0.4091	LR: 0.025000
Training Epoch: 54 [10880/50000]	Loss: 0.5509	LR: 0.025000
Training Epoch: 54 [11008/50000]	Loss: 0.6160	LR: 0.025000
Training Epoch: 54 [11136/50000]	Loss: 0.4260	LR: 0.025000
Training Epoch: 54 [11264/50000]	Loss: 0.4468	LR: 0.025000
Training Epoch: 54 [11392/50000]	Loss: 0.5763	LR: 0.025000
Training Epoch: 54 [11520/50000]	Loss: 0.3767	LR: 0.025000
Training Epoch: 54 [11648/50000]	Loss: 0.6829	LR: 0.025000
Training Epoch: 54 [11776/50000]	Loss: 0.5687	LR: 0.025000
Training Epoch: 54 [11904/50000]	Loss: 0.3834	LR: 0.025000
Training Epoch: 54 [12032/50000]	Loss: 0.5564	LR: 0.025000
Training Epoch: 54 [12160/50000]	Loss: 0.5742	LR: 0.025000
Training Epoch: 54 [12288/50000]	Loss: 0.4360	LR: 0.025000
Training Epoch: 54 [12416/50000]	Loss: 0.4927	LR: 0.025000
Training Epoch: 54 [12544/50000]	Loss: 0.5044	LR: 0.025000
Training Epoch: 54 [12672/50000]	Loss: 0.3911	LR: 0.025000
Training Epoch: 54 [12800/50000]	Loss: 0.5085	LR: 0.025000
Training Epoch: 54 [12928/50000]	Loss: 0.5298	LR: 0.025000
Training Epoch: 54 [13056/50000]	Loss: 0.4703	LR: 0.025000
Training Epoch: 54 [13184/50000]	Loss: 0.3431	LR: 0.025000
Training Epoch: 54 [13312/50000]	Loss: 0.3706	LR: 0.025000
Training Epoch: 54 [13440/50000]	Loss: 0.4169	LR: 0.025000
Training Epoch: 54 [13568/50000]	Loss: 0.4893	LR: 0.025000
Training Epoch: 54 [13696/50000]	Loss: 0.4265	LR: 0.025000
Training Epoch: 54 [13824/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 54 [13952/50000]	Loss: 0.6624	LR: 0.025000
Training Epoch: 54 [14080/50000]	Loss: 0.5702	LR: 0.025000
Training Epoch: 54 [14208/50000]	Loss: 0.4899	LR: 0.025000
Training Epoch: 54 [14336/50000]	Loss: 0.5725	LR: 0.025000
Training Epoch: 54 [14464/50000]	Loss: 0.3858	LR: 0.025000
Training Epoch: 54 [14592/50000]	Loss: 0.5317	LR: 0.025000
Training Epoch: 54 [14720/50000]	Loss: 0.4253	LR: 0.025000
Training Epoch: 54 [14848/50000]	Loss: 0.4997	LR: 0.025000
Training Epoch: 54 [14976/50000]	Loss: 0.5028	LR: 0.025000
Training Epoch: 54 [15104/50000]	Loss: 0.4360	LR: 0.025000
Training Epoch: 54 [15232/50000]	Loss: 0.6452	LR: 0.025000
Training Epoch: 54 [15360/50000]	Loss: 0.4893	LR: 0.025000
Training Epoch: 54 [15488/50000]	Loss: 0.3840	LR: 0.025000
Training Epoch: 54 [15616/50000]	Loss: 0.4154	LR: 0.025000
Training Epoch: 54 [15744/50000]	Loss: 0.5306	LR: 0.025000
Training Epoch: 54 [15872/50000]	Loss: 0.5099	LR: 0.025000
Training Epoch: 54 [16000/50000]	Loss: 0.5323	LR: 0.025000
Training Epoch: 54 [16128/50000]	Loss: 0.6045	LR: 0.025000
Training Epoch: 54 [16256/50000]	Loss: 0.6275	LR: 0.025000
Training Epoch: 54 [16384/50000]	Loss: 0.4976	LR: 0.025000
Training Epoch: 54 [16512/50000]	Loss: 0.3813	LR: 0.025000
Training Epoch: 54 [16640/50000]	Loss: 0.3988	LR: 0.025000
Training Epoch: 54 [16768/50000]	Loss: 0.5245	LR: 0.025000
Training Epoch: 54 [16896/50000]	Loss: 0.4380	LR: 0.025000
Training Epoch: 54 [17024/50000]	Loss: 0.4477	LR: 0.025000
Training Epoch: 54 [17152/50000]	Loss: 0.5654	LR: 0.025000
Training Epoch: 54 [17280/50000]	Loss: 0.6032	LR: 0.025000
Training Epoch: 54 [17408/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 54 [17536/50000]	Loss: 0.5611	LR: 0.025000
Training Epoch: 54 [17664/50000]	Loss: 0.6052	LR: 0.025000
Training Epoch: 54 [17792/50000]	Loss: 0.5309	LR: 0.025000
Training Epoch: 54 [17920/50000]	Loss: 0.4080	LR: 0.025000
Training Epoch: 54 [18048/50000]	Loss: 0.6600	LR: 0.025000
Training Epoch: 54 [18176/50000]	Loss: 0.6062	LR: 0.025000
Training Epoch: 54 [18304/50000]	Loss: 0.5988	LR: 0.025000
Training Epoch: 54 [18432/50000]	Loss: 0.5000	LR: 0.025000
Training Epoch: 54 [18560/50000]	Loss: 0.4625	LR: 0.025000
Training Epoch: 54 [18688/50000]	Loss: 0.4224	LR: 0.025000
Training Epoch: 54 [18816/50000]	Loss: 0.6742	LR: 0.025000
Training Epoch: 54 [18944/50000]	Loss: 0.5237	LR: 0.025000
Training Epoch: 54 [19072/50000]	Loss: 0.3577	LR: 0.025000
Training Epoch: 54 [19200/50000]	Loss: 0.5596	LR: 0.025000
Training Epoch: 54 [19328/50000]	Loss: 0.5278	LR: 0.025000
Training Epoch: 54 [19456/50000]	Loss: 0.5275	LR: 0.025000
Training Epoch: 54 [19584/50000]	Loss: 0.4833	LR: 0.025000
Training Epoch: 54 [19712/50000]	Loss: 0.5452	LR: 0.025000
Training Epoch: 54 [19840/50000]	Loss: 0.4503	LR: 0.025000
Training Epoch: 54 [19968/50000]	Loss: 0.6367	LR: 0.025000
Training Epoch: 54 [20096/50000]	Loss: 0.4890	LR: 0.025000
Training Epoch: 54 [20224/50000]	Loss: 0.6193	LR: 0.025000
Training Epoch: 54 [20352/50000]	Loss: 0.4876	LR: 0.025000
Training Epoch: 54 [20480/50000]	Loss: 0.6638	LR: 0.025000
Training Epoch: 54 [20608/50000]	Loss: 0.4207	LR: 0.025000
Training Epoch: 54 [20736/50000]	Loss: 0.3759	LR: 0.025000
Training Epoch: 54 [20864/50000]	Loss: 0.5375	LR: 0.025000
Training Epoch: 54 [20992/50000]	Loss: 0.4923	LR: 0.025000
Training Epoch: 54 [21120/50000]	Loss: 0.5096	LR: 0.025000
Training Epoch: 54 [21248/50000]	Loss: 0.4921	LR: 0.025000
Training Epoch: 54 [21376/50000]	Loss: 0.5031	LR: 0.025000
Training Epoch: 54 [21504/50000]	Loss: 0.4860	LR: 0.025000
Training Epoch: 54 [21632/50000]	Loss: 0.4837	LR: 0.025000
Training Epoch: 54 [21760/50000]	Loss: 0.5657	LR: 0.025000
Training Epoch: 54 [21888/50000]	Loss: 0.4632	LR: 0.025000
Training Epoch: 54 [22016/50000]	Loss: 0.5252	LR: 0.025000
Training Epoch: 54 [22144/50000]	Loss: 0.4252	LR: 0.025000
Training Epoch: 54 [22272/50000]	Loss: 0.3732	LR: 0.025000
Training Epoch: 54 [22400/50000]	Loss: 0.6304	LR: 0.025000
Training Epoch: 54 [22528/50000]	Loss: 0.4793	LR: 0.025000
Training Epoch: 54 [22656/50000]	Loss: 0.5355	LR: 0.025000
Training Epoch: 54 [22784/50000]	Loss: 0.4553	LR: 0.025000
Training Epoch: 54 [22912/50000]	Loss: 0.4606	LR: 0.025000
Training Epoch: 54 [23040/50000]	Loss: 0.5603	LR: 0.025000
Training Epoch: 54 [23168/50000]	Loss: 0.5540	LR: 0.025000
Training Epoch: 54 [23296/50000]	Loss: 0.4685	LR: 0.025000
Training Epoch: 54 [23424/50000]	Loss: 0.6583	LR: 0.025000
Training Epoch: 54 [23552/50000]	Loss: 0.5340	LR: 0.025000
Training Epoch: 54 [23680/50000]	Loss: 0.5944	LR: 0.025000
Training Epoch: 54 [23808/50000]	Loss: 0.5856	LR: 0.025000
Training Epoch: 54 [23936/50000]	Loss: 0.4135	LR: 0.025000
Training Epoch: 54 [24064/50000]	Loss: 0.5946	LR: 0.025000
Training Epoch: 54 [24192/50000]	Loss: 0.6896	LR: 0.025000
Training Epoch: 54 [24320/50000]	Loss: 0.5401	LR: 0.025000
Training Epoch: 54 [24448/50000]	Loss: 0.5794	LR: 0.025000
Training Epoch: 54 [24576/50000]	Loss: 0.4078	LR: 0.025000
Training Epoch: 54 [24704/50000]	Loss: 0.6502	LR: 0.025000
Training Epoch: 54 [24832/50000]	Loss: 0.4844	LR: 0.025000
Training Epoch: 54 [24960/50000]	Loss: 0.5553	LR: 0.025000
Training Epoch: 54 [25088/50000]	Loss: 0.6907	LR: 0.025000
Training Epoch: 54 [25216/50000]	Loss: 0.5783	LR: 0.025000
Training Epoch: 54 [25344/50000]	Loss: 0.4194	LR: 0.025000
Training Epoch: 54 [25472/50000]	Loss: 0.4715	LR: 0.025000
Training Epoch: 54 [25600/50000]	Loss: 0.5323	LR: 0.025000
Training Epoch: 54 [25728/50000]	Loss: 0.3569	LR: 0.025000
Training Epoch: 54 [25856/50000]	Loss: 0.4988	LR: 0.025000
Training Epoch: 54 [25984/50000]	Loss: 0.4983	LR: 0.025000
Training Epoch: 54 [26112/50000]	Loss: 0.5823	LR: 0.025000
Training Epoch: 54 [26240/50000]	Loss: 0.6866	LR: 0.025000
Training Epoch: 54 [26368/50000]	Loss: 0.3811	LR: 0.025000
Training Epoch: 54 [26496/50000]	Loss: 0.4225	LR: 0.025000
Training Epoch: 54 [26624/50000]	Loss: 0.4517	LR: 0.025000
Training Epoch: 54 [26752/50000]	Loss: 0.4830	LR: 0.025000
Training Epoch: 54 [26880/50000]	Loss: 0.6319	LR: 0.025000
Training Epoch: 54 [27008/50000]	Loss: 0.4768	LR: 0.025000
Training Epoch: 54 [27136/50000]	Loss: 0.6058	LR: 0.025000
Training Epoch: 54 [27264/50000]	Loss: 0.4836	LR: 0.025000
Training Epoch: 54 [27392/50000]	Loss: 0.6101	LR: 0.025000
Training Epoch: 54 [27520/50000]	Loss: 0.4377	LR: 0.025000
Training Epoch: 54 [27648/50000]	Loss: 0.6627	LR: 0.025000
Training Epoch: 54 [27776/50000]	Loss: 0.4411	LR: 0.025000
Training Epoch: 54 [27904/50000]	Loss: 0.4618	LR: 0.025000
Training Epoch: 54 [28032/50000]	Loss: 0.4416	LR: 0.025000
Training Epoch: 54 [28160/50000]	Loss: 0.5226	LR: 0.025000
Training Epoch: 54 [28288/50000]	Loss: 0.4876	LR: 0.025000
Training Epoch: 54 [28416/50000]	Loss: 0.4727	LR: 0.025000
Training Epoch: 54 [28544/50000]	Loss: 0.4594	LR: 0.025000
Training Epoch: 54 [28672/50000]	Loss: 0.4084	LR: 0.025000
Training Epoch: 54 [28800/50000]	Loss: 0.5153	LR: 0.025000
Training Epoch: 54 [28928/50000]	Loss: 0.6489	LR: 0.025000
Training Epoch: 54 [29056/50000]	Loss: 0.5143	LR: 0.025000
Training Epoch: 54 [29184/50000]	Loss: 0.5589	LR: 0.025000
Training Epoch: 54 [29312/50000]	Loss: 0.4510	LR: 0.025000
Training Epoch: 54 [29440/50000]	Loss: 0.4698	LR: 0.025000
Training Epoch: 54 [29568/50000]	Loss: 0.6434	LR: 0.025000
Training Epoch: 54 [29696/50000]	Loss: 0.5748	LR: 0.025000
Training Epoch: 54 [29824/50000]	Loss: 0.5140	LR: 0.025000
Training Epoch: 54 [29952/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 54 [30080/50000]	Loss: 0.5509	LR: 0.025000
Training Epoch: 54 [30208/50000]	Loss: 0.4982	LR: 0.025000
Training Epoch: 54 [30336/50000]	Loss: 0.5477	LR: 0.025000
Training Epoch: 54 [30464/50000]	Loss: 0.4241	LR: 0.025000
Training Epoch: 54 [30592/50000]	Loss: 0.4971	LR: 0.025000
Training Epoch: 54 [30720/50000]	Loss: 0.5137	LR: 0.025000
Training Epoch: 54 [30848/50000]	Loss: 0.4851	LR: 0.025000
Training Epoch: 54 [30976/50000]	Loss: 0.6371	LR: 0.025000
Training Epoch: 54 [31104/50000]	Loss: 0.5528	LR: 0.025000
Training Epoch: 54 [31232/50000]	Loss: 0.4926	LR: 0.025000
Training Epoch: 54 [31360/50000]	Loss: 0.6475	LR: 0.025000
Training Epoch: 54 [31488/50000]	Loss: 0.4812	LR: 0.025000
Training Epoch: 54 [31616/50000]	Loss: 0.5155	LR: 0.025000
Training Epoch: 54 [31744/50000]	Loss: 0.5785	LR: 0.025000
Training Epoch: 54 [31872/50000]	Loss: 0.5412	LR: 0.025000
Training Epoch: 54 [32000/50000]	Loss: 0.3480	LR: 0.025000
Training Epoch: 54 [32128/50000]	Loss: 0.3549	LR: 0.025000
Training Epoch: 54 [32256/50000]	Loss: 0.6038	LR: 0.025000
Training Epoch: 54 [32384/50000]	Loss: 0.6150	LR: 0.025000
Training Epoch: 54 [32512/50000]	Loss: 0.5965	LR: 0.025000
Training Epoch: 54 [32640/50000]	Loss: 0.4951	LR: 0.025000
Training Epoch: 54 [32768/50000]	Loss: 0.4682	LR: 0.025000
Training Epoch: 54 [32896/50000]	Loss: 0.5712	LR: 0.025000
Training Epoch: 54 [33024/50000]	Loss: 0.6765	LR: 0.025000
Training Epoch: 54 [33152/50000]	Loss: 0.5712	LR: 0.025000
Training Epoch: 54 [33280/50000]	Loss: 0.6249	LR: 0.025000
Training Epoch: 54 [33408/50000]	Loss: 0.6170	LR: 0.025000
Training Epoch: 54 [33536/50000]	Loss: 0.5835	LR: 0.025000
Training Epoch: 54 [33664/50000]	Loss: 0.5734	LR: 0.025000
Training Epoch: 54 [33792/50000]	Loss: 0.4731	LR: 0.025000
Training Epoch: 54 [33920/50000]	Loss: 0.5467	LR: 0.025000
Training Epoch: 54 [34048/50000]	Loss: 0.5128	LR: 0.025000
Training Epoch: 54 [34176/50000]	Loss: 0.5814	LR: 0.025000
Training Epoch: 54 [34304/50000]	Loss: 0.3827	LR: 0.025000
Training Epoch: 54 [34432/50000]	Loss: 0.5752	LR: 0.025000
Training Epoch: 54 [34560/50000]	Loss: 0.4804	LR: 0.025000
Training Epoch: 54 [34688/50000]	Loss: 0.5904	LR: 0.025000
Training Epoch: 54 [34816/50000]	Loss: 0.5775	LR: 0.025000
Training Epoch: 54 [34944/50000]	Loss: 0.6481	LR: 0.025000
Training Epoch: 54 [35072/50000]	Loss: 0.5351	LR: 0.025000
Training Epoch: 54 [35200/50000]	Loss: 0.7779	LR: 0.025000
Training Epoch: 54 [35328/50000]	Loss: 0.4165	LR: 0.025000
Training Epoch: 54 [35456/50000]	Loss: 0.5349	LR: 0.025000
Training Epoch: 54 [35584/50000]	Loss: 0.5161	LR: 0.025000
Training Epoch: 54 [35712/50000]	Loss: 0.4251	LR: 0.025000
Training Epoch: 54 [35840/50000]	Loss: 0.4857	LR: 0.025000
Training Epoch: 54 [35968/50000]	Loss: 0.4688	LR: 0.025000
Training Epoch: 54 [36096/50000]	Loss: 0.5470	LR: 0.025000
Training Epoch: 54 [36224/50000]	Loss: 0.6280	LR: 0.025000
Training Epoch: 54 [36352/50000]	Loss: 0.6018	LR: 0.025000
Training Epoch: 54 [36480/50000]	Loss: 0.4317	LR: 0.025000
Training Epoch: 54 [36608/50000]	Loss: 0.5917	LR: 0.025000
Training Epoch: 54 [36736/50000]	Loss: 0.6783	LR: 0.025000
Training Epoch: 54 [36864/50000]	Loss: 0.6715	LR: 0.025000
Training Epoch: 54 [36992/50000]	Loss: 0.5158	LR: 0.025000
Training Epoch: 54 [37120/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 54 [37248/50000]	Loss: 0.6517	LR: 0.025000
Training Epoch: 54 [37376/50000]	Loss: 0.5864	LR: 0.025000
Training Epoch: 54 [37504/50000]	Loss: 0.4510	LR: 0.025000
Training Epoch: 54 [37632/50000]	Loss: 0.6697	LR: 0.025000
Training Epoch: 54 [37760/50000]	Loss: 0.5598	LR: 0.025000
Training Epoch: 54 [37888/50000]	Loss: 0.5521	LR: 0.025000
Training Epoch: 54 [38016/50000]	Loss: 0.5135	LR: 0.025000
Training Epoch: 54 [38144/50000]	Loss: 0.5333	LR: 0.025000
Training Epoch: 54 [38272/50000]	Loss: 0.4993	LR: 0.025000
Training Epoch: 54 [38400/50000]	Loss: 0.5617	LR: 0.025000
Training Epoch: 54 [38528/50000]	Loss: 0.5076	LR: 0.025000
Training Epoch: 54 [38656/50000]	Loss: 0.5693	LR: 0.025000
Training Epoch: 54 [38784/50000]	Loss: 0.4393	LR: 0.025000
Training Epoch: 54 [38912/50000]	Loss: 0.5088	LR: 0.025000
Training Epoch: 54 [39040/50000]	Loss: 0.4635	LR: 0.025000
Training Epoch: 54 [39168/50000]	Loss: 0.4024	LR: 0.025000
Training Epoch: 54 [39296/50000]	Loss: 0.7261	LR: 0.025000
Training Epoch: 54 [39424/50000]	Loss: 0.6642	LR: 0.025000
Training Epoch: 54 [39552/50000]	Loss: 0.5880	LR: 0.025000
Training Epoch: 54 [39680/50000]	Loss: 0.4800	LR: 0.025000
Training Epoch: 54 [39808/50000]	Loss: 0.5870	LR: 0.025000
Training Epoch: 54 [39936/50000]	Loss: 0.4395	LR: 0.025000
Training Epoch: 54 [40064/50000]	Loss: 0.7169	LR: 0.025000
Training Epoch: 54 [40192/50000]	Loss: 0.4331	LR: 0.025000
Training Epoch: 54 [40320/50000]	Loss: 0.6170	LR: 0.025000
Training Epoch: 54 [40448/50000]	Loss: 0.5444	LR: 0.025000
Training Epoch: 54 [40576/50000]	Loss: 0.5859	LR: 0.025000
Training Epoch: 54 [40704/50000]	Loss: 0.6096	LR: 0.025000
Training Epoch: 54 [40832/50000]	Loss: 0.6313	LR: 0.025000
Training Epoch: 54 [40960/50000]	Loss: 0.5314	LR: 0.025000
Training Epoch: 54 [41088/50000]	Loss: 0.4793	LR: 0.025000
Training Epoch: 54 [41216/50000]	Loss: 0.5995	LR: 0.025000
Training Epoch: 54 [41344/50000]	Loss: 0.6418	LR: 0.025000
Training Epoch: 54 [41472/50000]	Loss: 0.5439	LR: 0.025000
Training Epoch: 54 [41600/50000]	Loss: 0.3970	LR: 0.025000
Training Epoch: 54 [41728/50000]	Loss: 0.4172	LR: 0.025000
Training Epoch: 54 [41856/50000]	Loss: 0.6196	LR: 0.025000
Training Epoch: 54 [41984/50000]	Loss: 0.4558	LR: 0.025000
Training Epoch: 54 [42112/50000]	Loss: 0.5255	LR: 0.025000
Training Epoch: 54 [42240/50000]	Loss: 0.4863	LR: 0.025000
Training Epoch: 54 [42368/50000]	Loss: 0.4494	LR: 0.025000
Training Epoch: 54 [42496/50000]	Loss: 0.5694	LR: 0.025000
Training Epoch: 54 [42624/50000]	Loss: 0.7014	LR: 0.025000
Training Epoch: 54 [42752/50000]	Loss: 0.3979	LR: 0.025000
Training Epoch: 54 [42880/50000]	Loss: 0.6977	LR: 0.025000
Training Epoch: 54 [43008/50000]	Loss: 0.6253	LR: 0.025000
Training Epoch: 54 [43136/50000]	Loss: 0.4484	LR: 0.025000
Training Epoch: 54 [43264/50000]	Loss: 0.4497	LR: 0.025000
Training Epoch: 54 [43392/50000]	Loss: 0.3729	LR: 0.025000
Training Epoch: 54 [43520/50000]	Loss: 0.6218	LR: 0.025000
Training Epoch: 54 [43648/50000]	Loss: 0.6078	LR: 0.025000
Training Epoch: 54 [43776/50000]	Loss: 0.7154	LR: 0.025000
Training Epoch: 54 [43904/50000]	Loss: 0.5437	LR: 0.025000
Training Epoch: 54 [44032/50000]	Loss: 0.5989	LR: 0.025000
Training Epoch: 54 [44160/50000]	Loss: 0.6424	LR: 0.025000
Training Epoch: 54 [44288/50000]	Loss: 0.5694	LR: 0.025000
Training Epoch: 54 [44416/50000]	Loss: 0.4885	LR: 0.025000
Training Epoch: 54 [44544/50000]	Loss: 0.6406	LR: 0.025000
Training Epoch: 54 [44672/50000]	Loss: 0.6209	LR: 0.025000
Training Epoch: 54 [44800/50000]	Loss: 0.6327	LR: 0.025000
Training Epoch: 54 [44928/50000]	Loss: 0.6000	LR: 0.025000
Training Epoch: 54 [45056/50000]	Loss: 0.6022	LR: 0.025000
Training Epoch: 54 [45184/50000]	Loss: 0.3822	LR: 0.025000
Training Epoch: 54 [45312/50000]	Loss: 0.7397	LR: 0.025000
Training Epoch: 54 [45440/50000]	Loss: 0.5815	LR: 0.025000
Training Epoch: 54 [45568/50000]	Loss: 0.5519	LR: 0.025000
Training Epoch: 54 [45696/50000]	Loss: 0.5948	LR: 0.025000
Training Epoch: 54 [45824/50000]	Loss: 0.7090	LR: 0.025000
Training Epoch: 54 [45952/50000]	Loss: 0.5474	LR: 0.025000
Training Epoch: 54 [46080/50000]	Loss: 0.7639	LR: 0.025000
Training Epoch: 54 [46208/50000]	Loss: 0.4802	LR: 0.025000
Training Epoch: 54 [46336/50000]	Loss: 0.6786	LR: 0.025000
Training Epoch: 54 [46464/50000]	Loss: 0.6409	LR: 0.025000
Training Epoch: 54 [46592/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 54 [46720/50000]	Loss: 0.4551	LR: 0.025000
Training Epoch: 54 [46848/50000]	Loss: 0.5828	LR: 0.025000
Training Epoch: 54 [46976/50000]	Loss: 0.5534	LR: 0.025000
Training Epoch: 54 [47104/50000]	Loss: 0.6248	LR: 0.025000
Training Epoch: 54 [47232/50000]	Loss: 0.6822	LR: 0.025000
Training Epoch: 54 [47360/50000]	Loss: 0.6152	LR: 0.025000
Training Epoch: 54 [47488/50000]	Loss: 0.6873	LR: 0.025000
Training Epoch: 54 [47616/50000]	Loss: 0.6395	LR: 0.025000
Training Epoch: 54 [47744/50000]	Loss: 0.4869	LR: 0.025000
Training Epoch: 54 [47872/50000]	Loss: 0.5766	LR: 0.025000
Training Epoch: 54 [48000/50000]	Loss: 0.8187	LR: 0.025000
Training Epoch: 54 [48128/50000]	Loss: 0.6263	LR: 0.025000
Training Epoch: 54 [48256/50000]	Loss: 0.4930	LR: 0.025000
Training Epoch: 54 [48384/50000]	Loss: 0.6019	LR: 0.025000
Training Epoch: 54 [48512/50000]	Loss: 0.7133	LR: 0.025000
Training Epoch: 54 [48640/50000]	Loss: 0.5458	LR: 0.025000
Training Epoch: 54 [48768/50000]	Loss: 0.6640	LR: 0.025000
Training Epoch: 54 [48896/50000]	Loss: 0.6729	LR: 0.025000
Training Epoch: 54 [49024/50000]	Loss: 0.6410	LR: 0.025000
Training Epoch: 54 [49152/50000]	Loss: 0.5705	LR: 0.025000
Training Epoch: 54 [49280/50000]	Loss: 0.4667	LR: 0.025000
Training Epoch: 54 [49408/50000]	Loss: 0.5372	LR: 0.025000
Training Epoch: 54 [49536/50000]	Loss: 0.5585	LR: 0.025000
Training Epoch: 54 [49664/50000]	Loss: 0.5864	LR: 0.025000
Training Epoch: 54 [49792/50000]	Loss: 0.5391	LR: 0.025000
Training Epoch: 54 [49920/50000]	Loss: 0.6635	LR: 0.025000
Training Epoch: 54 [50000/50000]	Loss: 0.5548	LR: 0.025000
epoch 54 training time consumed: 53.89s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  198183 GB |  198183 GB |
|       from large pool |  123392 KB |    1034 MB |  197988 GB |  197988 GB |
|       from small pool |   10798 KB |      13 MB |     195 GB |     195 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  198183 GB |  198183 GB |
|       from large pool |  123392 KB |    1034 MB |  197988 GB |  197988 GB |
|       from small pool |   10798 KB |      13 MB |     195 GB |     195 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   87213 GB |   87213 GB |
|       from large pool |  155136 KB |  433088 KB |   86997 GB |   86997 GB |
|       from small pool |    1490 KB |    3494 KB |     215 GB |     215 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    7647 K  |    7647 K  |
|       from large pool |      24    |      65    |    3991 K  |    3991 K  |
|       from small pool |     231    |     274    |    3655 K  |    3655 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    7647 K  |    7647 K  |
|       from large pool |      24    |      65    |    3991 K  |    3991 K  |
|       from small pool |     231    |     274    |    3655 K  |    3655 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3779 K  |    3779 K  |
|       from large pool |       9    |      14    |    1932 K  |    1932 K  |
|       from small pool |      12    |      16    |    1847 K  |    1847 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 54, Average loss: 0.0111, Accuracy: 0.6483, Time consumed:3.45s

Training Epoch: 55 [128/50000]	Loss: 0.5258	LR: 0.025000
Training Epoch: 55 [256/50000]	Loss: 0.5182	LR: 0.025000
Training Epoch: 55 [384/50000]	Loss: 0.4510	LR: 0.025000
Training Epoch: 55 [512/50000]	Loss: 0.4691	LR: 0.025000
Training Epoch: 55 [640/50000]	Loss: 0.4564	LR: 0.025000
Training Epoch: 55 [768/50000]	Loss: 0.6080	LR: 0.025000
Training Epoch: 55 [896/50000]	Loss: 0.5405	LR: 0.025000
Training Epoch: 55 [1024/50000]	Loss: 0.3164	LR: 0.025000
Training Epoch: 55 [1152/50000]	Loss: 0.5232	LR: 0.025000
Training Epoch: 55 [1280/50000]	Loss: 0.5282	LR: 0.025000
Training Epoch: 55 [1408/50000]	Loss: 0.4037	LR: 0.025000
Training Epoch: 55 [1536/50000]	Loss: 0.4289	LR: 0.025000
Training Epoch: 55 [1664/50000]	Loss: 0.5157	LR: 0.025000
Training Epoch: 55 [1792/50000]	Loss: 0.6222	LR: 0.025000
Training Epoch: 55 [1920/50000]	Loss: 0.3951	LR: 0.025000
Training Epoch: 55 [2048/50000]	Loss: 0.4414	LR: 0.025000
Training Epoch: 55 [2176/50000]	Loss: 0.4685	LR: 0.025000
Training Epoch: 55 [2304/50000]	Loss: 0.4290	LR: 0.025000
Training Epoch: 55 [2432/50000]	Loss: 0.4314	LR: 0.025000
Training Epoch: 55 [2560/50000]	Loss: 0.4589	LR: 0.025000
Training Epoch: 55 [2688/50000]	Loss: 0.5582	LR: 0.025000
Training Epoch: 55 [2816/50000]	Loss: 0.3867	LR: 0.025000
Training Epoch: 55 [2944/50000]	Loss: 0.3942	LR: 0.025000
Training Epoch: 55 [3072/50000]	Loss: 0.4803	LR: 0.025000
Training Epoch: 55 [3200/50000]	Loss: 0.3963	LR: 0.025000
Training Epoch: 55 [3328/50000]	Loss: 0.4682	LR: 0.025000
Training Epoch: 55 [3456/50000]	Loss: 0.3695	LR: 0.025000
Training Epoch: 55 [3584/50000]	Loss: 0.5661	LR: 0.025000
Training Epoch: 55 [3712/50000]	Loss: 0.6462	LR: 0.025000
Training Epoch: 55 [3840/50000]	Loss: 0.4637	LR: 0.025000
Training Epoch: 55 [3968/50000]	Loss: 0.4506	LR: 0.025000
Training Epoch: 55 [4096/50000]	Loss: 0.4795	LR: 0.025000
Training Epoch: 55 [4224/50000]	Loss: 0.4318	LR: 0.025000
Training Epoch: 55 [4352/50000]	Loss: 0.3756	LR: 0.025000
Training Epoch: 55 [4480/50000]	Loss: 0.4944	LR: 0.025000
Training Epoch: 55 [4608/50000]	Loss: 0.4154	LR: 0.025000
Training Epoch: 55 [4736/50000]	Loss: 0.4806	LR: 0.025000
Training Epoch: 55 [4864/50000]	Loss: 0.3760	LR: 0.025000
Training Epoch: 55 [4992/50000]	Loss: 0.5419	LR: 0.025000
Training Epoch: 55 [5120/50000]	Loss: 0.4509	LR: 0.025000
Training Epoch: 55 [5248/50000]	Loss: 0.4296	LR: 0.025000
Training Epoch: 55 [5376/50000]	Loss: 0.6428	LR: 0.025000
Training Epoch: 55 [5504/50000]	Loss: 0.6568	LR: 0.025000
Training Epoch: 55 [5632/50000]	Loss: 0.5584	LR: 0.025000
Training Epoch: 55 [5760/50000]	Loss: 0.3894	LR: 0.025000
Training Epoch: 55 [5888/50000]	Loss: 0.4987	LR: 0.025000
Training Epoch: 55 [6016/50000]	Loss: 0.3480	LR: 0.025000
Training Epoch: 55 [6144/50000]	Loss: 0.4464	LR: 0.025000
Training Epoch: 55 [6272/50000]	Loss: 0.5273	LR: 0.025000
Training Epoch: 55 [6400/50000]	Loss: 0.4105	LR: 0.025000
Training Epoch: 55 [6528/50000]	Loss: 0.4901	LR: 0.025000
Training Epoch: 55 [6656/50000]	Loss: 0.3943	LR: 0.025000
Training Epoch: 55 [6784/50000]	Loss: 0.3631	LR: 0.025000
Training Epoch: 55 [6912/50000]	Loss: 0.3959	LR: 0.025000
Training Epoch: 55 [7040/50000]	Loss: 0.5238	LR: 0.025000
Training Epoch: 55 [7168/50000]	Loss: 0.4523	LR: 0.025000
Training Epoch: 55 [7296/50000]	Loss: 0.3940	LR: 0.025000
Training Epoch: 55 [7424/50000]	Loss: 0.5927	LR: 0.025000
Training Epoch: 55 [7552/50000]	Loss: 0.5935	LR: 0.025000
Training Epoch: 55 [7680/50000]	Loss: 0.5607	LR: 0.025000
Training Epoch: 55 [7808/50000]	Loss: 0.5541	LR: 0.025000
Training Epoch: 55 [7936/50000]	Loss: 0.3843	LR: 0.025000
Training Epoch: 55 [8064/50000]	Loss: 0.4984	LR: 0.025000
Training Epoch: 55 [8192/50000]	Loss: 0.4349	LR: 0.025000
Training Epoch: 55 [8320/50000]	Loss: 0.4061	LR: 0.025000
Training Epoch: 55 [8448/50000]	Loss: 0.5454	LR: 0.025000
Training Epoch: 55 [8576/50000]	Loss: 0.3938	LR: 0.025000
Training Epoch: 55 [8704/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 55 [8832/50000]	Loss: 0.4052	LR: 0.025000
Training Epoch: 55 [8960/50000]	Loss: 0.5498	LR: 0.025000
Training Epoch: 55 [9088/50000]	Loss: 0.6717	LR: 0.025000
Training Epoch: 55 [9216/50000]	Loss: 0.4094	LR: 0.025000
Training Epoch: 55 [9344/50000]	Loss: 0.5522	LR: 0.025000
Training Epoch: 55 [9472/50000]	Loss: 0.4040	LR: 0.025000
Training Epoch: 55 [9600/50000]	Loss: 0.3669	LR: 0.025000
Training Epoch: 55 [9728/50000]	Loss: 0.4730	LR: 0.025000
Training Epoch: 55 [9856/50000]	Loss: 0.5495	LR: 0.025000
Training Epoch: 55 [9984/50000]	Loss: 0.4686	LR: 0.025000
Training Epoch: 55 [10112/50000]	Loss: 0.6132	LR: 0.025000
Training Epoch: 55 [10240/50000]	Loss: 0.6074	LR: 0.025000
Training Epoch: 55 [10368/50000]	Loss: 0.4642	LR: 0.025000
Training Epoch: 55 [10496/50000]	Loss: 0.3406	LR: 0.025000
Training Epoch: 55 [10624/50000]	Loss: 0.4330	LR: 0.025000
Training Epoch: 55 [10752/50000]	Loss: 0.5430	LR: 0.025000
Training Epoch: 55 [10880/50000]	Loss: 0.4266	LR: 0.025000
Training Epoch: 55 [11008/50000]	Loss: 0.4536	LR: 0.025000
Training Epoch: 55 [11136/50000]	Loss: 0.4947	LR: 0.025000
Training Epoch: 55 [11264/50000]	Loss: 0.4119	LR: 0.025000
Training Epoch: 55 [11392/50000]	Loss: 0.5346	LR: 0.025000
Training Epoch: 55 [11520/50000]	Loss: 0.5024	LR: 0.025000
Training Epoch: 55 [11648/50000]	Loss: 0.5588	LR: 0.025000
Training Epoch: 55 [11776/50000]	Loss: 0.4928	LR: 0.025000
Training Epoch: 55 [11904/50000]	Loss: 0.4515	LR: 0.025000
Training Epoch: 55 [12032/50000]	Loss: 0.5746	LR: 0.025000
Training Epoch: 55 [12160/50000]	Loss: 0.4484	LR: 0.025000
Training Epoch: 55 [12288/50000]	Loss: 0.4934	LR: 0.025000
Training Epoch: 55 [12416/50000]	Loss: 0.6461	LR: 0.025000
Training Epoch: 55 [12544/50000]	Loss: 0.4314	LR: 0.025000
Training Epoch: 55 [12672/50000]	Loss: 0.4888	LR: 0.025000
Training Epoch: 55 [12800/50000]	Loss: 0.4761	LR: 0.025000
Training Epoch: 55 [12928/50000]	Loss: 0.4748	LR: 0.025000
Training Epoch: 55 [13056/50000]	Loss: 0.4006	LR: 0.025000
Training Epoch: 55 [13184/50000]	Loss: 0.4338	LR: 0.025000
Training Epoch: 55 [13312/50000]	Loss: 0.5597	LR: 0.025000
Training Epoch: 55 [13440/50000]	Loss: 0.6115	LR: 0.025000
Training Epoch: 55 [13568/50000]	Loss: 0.4467	LR: 0.025000
Training Epoch: 55 [13696/50000]	Loss: 0.4654	LR: 0.025000
Training Epoch: 55 [13824/50000]	Loss: 0.4771	LR: 0.025000
Training Epoch: 55 [13952/50000]	Loss: 0.4754	LR: 0.025000
Training Epoch: 55 [14080/50000]	Loss: 0.4411	LR: 0.025000
Training Epoch: 55 [14208/50000]	Loss: 0.3319	LR: 0.025000
Training Epoch: 55 [14336/50000]	Loss: 0.3680	LR: 0.025000
Training Epoch: 55 [14464/50000]	Loss: 0.4804	LR: 0.025000
Training Epoch: 55 [14592/50000]	Loss: 0.5249	LR: 0.025000
Training Epoch: 55 [14720/50000]	Loss: 0.4362	LR: 0.025000
Training Epoch: 55 [14848/50000]	Loss: 0.4481	LR: 0.025000
Training Epoch: 55 [14976/50000]	Loss: 0.6124	LR: 0.025000
Training Epoch: 55 [15104/50000]	Loss: 0.5989	LR: 0.025000
Training Epoch: 55 [15232/50000]	Loss: 0.4346	LR: 0.025000
Training Epoch: 55 [15360/50000]	Loss: 0.4520	LR: 0.025000
Training Epoch: 55 [15488/50000]	Loss: 0.4435	LR: 0.025000
Training Epoch: 55 [15616/50000]	Loss: 0.4155	LR: 0.025000
Training Epoch: 55 [15744/50000]	Loss: 0.4797	LR: 0.025000
Training Epoch: 55 [15872/50000]	Loss: 0.5274	LR: 0.025000
Training Epoch: 55 [16000/50000]	Loss: 0.6042	LR: 0.025000
Training Epoch: 55 [16128/50000]	Loss: 0.4454	LR: 0.025000
Training Epoch: 55 [16256/50000]	Loss: 0.5059	LR: 0.025000
Training Epoch: 55 [16384/50000]	Loss: 0.5425	LR: 0.025000
Training Epoch: 55 [16512/50000]	Loss: 0.4705	LR: 0.025000
Training Epoch: 55 [16640/50000]	Loss: 0.4898	LR: 0.025000
Training Epoch: 55 [16768/50000]	Loss: 0.3886	LR: 0.025000
Training Epoch: 55 [16896/50000]	Loss: 0.6436	LR: 0.025000
Training Epoch: 55 [17024/50000]	Loss: 0.7155	LR: 0.025000
Training Epoch: 55 [17152/50000]	Loss: 0.6792	LR: 0.025000
Training Epoch: 55 [17280/50000]	Loss: 0.3881	LR: 0.025000
Training Epoch: 55 [17408/50000]	Loss: 0.5194	LR: 0.025000
Training Epoch: 55 [17536/50000]	Loss: 0.5937	LR: 0.025000
Training Epoch: 55 [17664/50000]	Loss: 0.5519	LR: 0.025000
Training Epoch: 55 [17792/50000]	Loss: 0.4325	LR: 0.025000
Training Epoch: 55 [17920/50000]	Loss: 0.5592	LR: 0.025000
Training Epoch: 55 [18048/50000]	Loss: 0.7188	LR: 0.025000
Training Epoch: 55 [18176/50000]	Loss: 0.4079	LR: 0.025000
Training Epoch: 55 [18304/50000]	Loss: 0.4657	LR: 0.025000
Training Epoch: 55 [18432/50000]	Loss: 0.5829	LR: 0.025000
Training Epoch: 55 [18560/50000]	Loss: 0.4235	LR: 0.025000
Training Epoch: 55 [18688/50000]	Loss: 0.4748	LR: 0.025000
Training Epoch: 55 [18816/50000]	Loss: 0.3820	LR: 0.025000
Training Epoch: 55 [18944/50000]	Loss: 0.5643	LR: 0.025000
Training Epoch: 55 [19072/50000]	Loss: 0.4297	LR: 0.025000
Training Epoch: 55 [19200/50000]	Loss: 0.4036	LR: 0.025000
Training Epoch: 55 [19328/50000]	Loss: 0.4834	LR: 0.025000
Training Epoch: 55 [19456/50000]	Loss: 0.4474	LR: 0.025000
Training Epoch: 55 [19584/50000]	Loss: 0.5135	LR: 0.025000
Training Epoch: 55 [19712/50000]	Loss: 0.5609	LR: 0.025000
Training Epoch: 55 [19840/50000]	Loss: 0.4642	LR: 0.025000
Training Epoch: 55 [19968/50000]	Loss: 0.6138	LR: 0.025000
Training Epoch: 55 [20096/50000]	Loss: 0.5056	LR: 0.025000
Training Epoch: 55 [20224/50000]	Loss: 0.4776	LR: 0.025000
Training Epoch: 55 [20352/50000]	Loss: 0.5518	LR: 0.025000
Training Epoch: 55 [20480/50000]	Loss: 0.3829	LR: 0.025000
Training Epoch: 55 [20608/50000]	Loss: 0.4268	LR: 0.025000
Training Epoch: 55 [20736/50000]	Loss: 0.4822	LR: 0.025000
Training Epoch: 55 [20864/50000]	Loss: 0.4996	LR: 0.025000
Training Epoch: 55 [20992/50000]	Loss: 0.4697	LR: 0.025000
Training Epoch: 55 [21120/50000]	Loss: 0.4836	LR: 0.025000
Training Epoch: 55 [21248/50000]	Loss: 0.7050	LR: 0.025000
Training Epoch: 55 [21376/50000]	Loss: 0.5562	LR: 0.025000
Training Epoch: 55 [21504/50000]	Loss: 0.4346	LR: 0.025000
Training Epoch: 55 [21632/50000]	Loss: 0.6234	LR: 0.025000
Training Epoch: 55 [21760/50000]	Loss: 0.4968	LR: 0.025000
Training Epoch: 55 [21888/50000]	Loss: 0.3662	LR: 0.025000
Training Epoch: 55 [22016/50000]	Loss: 0.5504	LR: 0.025000
Training Epoch: 55 [22144/50000]	Loss: 0.4480	LR: 0.025000
Training Epoch: 55 [22272/50000]	Loss: 0.5111	LR: 0.025000
Training Epoch: 55 [22400/50000]	Loss: 0.5418	LR: 0.025000
Training Epoch: 55 [22528/50000]	Loss: 0.4739	LR: 0.025000
Training Epoch: 55 [22656/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 55 [22784/50000]	Loss: 0.5288	LR: 0.025000
Training Epoch: 55 [22912/50000]	Loss: 0.6764	LR: 0.025000
Training Epoch: 55 [23040/50000]	Loss: 0.5555	LR: 0.025000
Training Epoch: 55 [23168/50000]	Loss: 0.4745	LR: 0.025000
Training Epoch: 55 [23296/50000]	Loss: 0.5912	LR: 0.025000
Training Epoch: 55 [23424/50000]	Loss: 0.5358	LR: 0.025000
Training Epoch: 55 [23552/50000]	Loss: 0.4377	LR: 0.025000
Training Epoch: 55 [23680/50000]	Loss: 0.5289	LR: 0.025000
Training Epoch: 55 [23808/50000]	Loss: 0.5467	LR: 0.025000
Training Epoch: 55 [23936/50000]	Loss: 0.5558	LR: 0.025000
Training Epoch: 55 [24064/50000]	Loss: 0.4702	LR: 0.025000
Training Epoch: 55 [24192/50000]	Loss: 0.5403	LR: 0.025000
Training Epoch: 55 [24320/50000]	Loss: 0.4237	LR: 0.025000
Training Epoch: 55 [24448/50000]	Loss: 0.5443	LR: 0.025000
Training Epoch: 55 [24576/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 55 [24704/50000]	Loss: 0.4997	LR: 0.025000
Training Epoch: 55 [24832/50000]	Loss: 0.4862	LR: 0.025000
Training Epoch: 55 [24960/50000]	Loss: 0.3310	LR: 0.025000
Training Epoch: 55 [25088/50000]	Loss: 0.5481	LR: 0.025000
Training Epoch: 55 [25216/50000]	Loss: 0.6415	LR: 0.025000
Training Epoch: 55 [25344/50000]	Loss: 0.4747	LR: 0.025000
Training Epoch: 55 [25472/50000]	Loss: 0.3995	LR: 0.025000
Training Epoch: 55 [25600/50000]	Loss: 0.5690	LR: 0.025000
Training Epoch: 55 [25728/50000]	Loss: 0.4955	LR: 0.025000
Training Epoch: 55 [25856/50000]	Loss: 0.5557	LR: 0.025000
Training Epoch: 55 [25984/50000]	Loss: 0.5845	LR: 0.025000
Training Epoch: 55 [26112/50000]	Loss: 0.4759	LR: 0.025000
Training Epoch: 55 [26240/50000]	Loss: 0.5076	LR: 0.025000
Training Epoch: 55 [26368/50000]	Loss: 0.5391	LR: 0.025000
Training Epoch: 55 [26496/50000]	Loss: 0.5226	LR: 0.025000
Training Epoch: 55 [26624/50000]	Loss: 0.4577	LR: 0.025000
Training Epoch: 55 [26752/50000]	Loss: 0.5410	LR: 0.025000
Training Epoch: 55 [26880/50000]	Loss: 0.5022	LR: 0.025000
Training Epoch: 55 [27008/50000]	Loss: 0.4719	LR: 0.025000
Training Epoch: 55 [27136/50000]	Loss: 0.5101	LR: 0.025000
Training Epoch: 55 [27264/50000]	Loss: 0.5581	LR: 0.025000
Training Epoch: 55 [27392/50000]	Loss: 0.4974	LR: 0.025000
Training Epoch: 55 [27520/50000]	Loss: 0.4728	LR: 0.025000
Training Epoch: 55 [27648/50000]	Loss: 0.6933	LR: 0.025000
Training Epoch: 55 [27776/50000]	Loss: 0.6181	LR: 0.025000
Training Epoch: 55 [27904/50000]	Loss: 0.4933	LR: 0.025000
Training Epoch: 55 [28032/50000]	Loss: 0.5656	LR: 0.025000
Training Epoch: 55 [28160/50000]	Loss: 0.6803	LR: 0.025000
Training Epoch: 55 [28288/50000]	Loss: 0.6328	LR: 0.025000
Training Epoch: 55 [28416/50000]	Loss: 0.5558	LR: 0.025000
Training Epoch: 55 [28544/50000]	Loss: 0.6120	LR: 0.025000
Training Epoch: 55 [28672/50000]	Loss: 0.4816	LR: 0.025000
Training Epoch: 55 [28800/50000]	Loss: 0.5517	LR: 0.025000
Training Epoch: 55 [28928/50000]	Loss: 0.8512	LR: 0.025000
Training Epoch: 55 [29056/50000]	Loss: 0.5300	LR: 0.025000
Training Epoch: 55 [29184/50000]	Loss: 0.7892	LR: 0.025000
Training Epoch: 55 [29312/50000]	Loss: 0.5651	LR: 0.025000
Training Epoch: 55 [29440/50000]	Loss: 0.6036	LR: 0.025000
Training Epoch: 55 [29568/50000]	Loss: 0.6206	LR: 0.025000
Training Epoch: 55 [29696/50000]	Loss: 0.6675	LR: 0.025000
Training Epoch: 55 [29824/50000]	Loss: 0.5528	LR: 0.025000
Training Epoch: 55 [29952/50000]	Loss: 0.6122	LR: 0.025000
Training Epoch: 55 [30080/50000]	Loss: 0.6030	LR: 0.025000
Training Epoch: 55 [30208/50000]	Loss: 0.6879	LR: 0.025000
Training Epoch: 55 [30336/50000]	Loss: 0.6190	LR: 0.025000
Training Epoch: 55 [30464/50000]	Loss: 0.5361	LR: 0.025000
Training Epoch: 55 [30592/50000]	Loss: 0.5958	LR: 0.025000
Training Epoch: 55 [30720/50000]	Loss: 0.4224	LR: 0.025000
Training Epoch: 55 [30848/50000]	Loss: 0.3775	LR: 0.025000
Training Epoch: 55 [30976/50000]	Loss: 0.4629	LR: 0.025000
Training Epoch: 55 [31104/50000]	Loss: 0.4579	LR: 0.025000
Training Epoch: 55 [31232/50000]	Loss: 0.5345	LR: 0.025000
Training Epoch: 55 [31360/50000]	Loss: 0.5326	LR: 0.025000
Training Epoch: 55 [31488/50000]	Loss: 0.4407	LR: 0.025000
Training Epoch: 55 [31616/50000]	Loss: 0.5562	LR: 0.025000
Training Epoch: 55 [31744/50000]	Loss: 0.6059	LR: 0.025000
Training Epoch: 55 [31872/50000]	Loss: 0.5304	LR: 0.025000
Training Epoch: 55 [32000/50000]	Loss: 0.6780	LR: 0.025000
Training Epoch: 55 [32128/50000]	Loss: 0.6878	LR: 0.025000
Training Epoch: 55 [32256/50000]	Loss: 0.5264	LR: 0.025000
Training Epoch: 55 [32384/50000]	Loss: 0.4952	LR: 0.025000
Training Epoch: 55 [32512/50000]	Loss: 0.5807	LR: 0.025000
Training Epoch: 55 [32640/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 55 [32768/50000]	Loss: 0.5525	LR: 0.025000
Training Epoch: 55 [32896/50000]	Loss: 0.5586	LR: 0.025000
Training Epoch: 55 [33024/50000]	Loss: 0.4937	LR: 0.025000
Training Epoch: 55 [33152/50000]	Loss: 0.5153	LR: 0.025000
Training Epoch: 55 [33280/50000]	Loss: 0.5778	LR: 0.025000
Training Epoch: 55 [33408/50000]	Loss: 0.4514	LR: 0.025000
Training Epoch: 55 [33536/50000]	Loss: 0.7139	LR: 0.025000
Training Epoch: 55 [33664/50000]	Loss: 0.5541	LR: 0.025000
Training Epoch: 55 [33792/50000]	Loss: 0.4734	LR: 0.025000
Training Epoch: 55 [33920/50000]	Loss: 0.5932	LR: 0.025000
Training Epoch: 55 [34048/50000]	Loss: 0.4733	LR: 0.025000
Training Epoch: 55 [34176/50000]	Loss: 0.4869	LR: 0.025000
Training Epoch: 55 [34304/50000]	Loss: 0.5390	LR: 0.025000
Training Epoch: 55 [34432/50000]	Loss: 0.6609	LR: 0.025000
Training Epoch: 55 [34560/50000]	Loss: 0.5413	LR: 0.025000
Training Epoch: 55 [34688/50000]	Loss: 0.4081	LR: 0.025000
Training Epoch: 55 [34816/50000]	Loss: 0.5429	LR: 0.025000
Training Epoch: 55 [34944/50000]	Loss: 0.4549	LR: 0.025000
Training Epoch: 55 [35072/50000]	Loss: 0.5673	LR: 0.025000
Training Epoch: 55 [35200/50000]	Loss: 0.4775	LR: 0.025000
Training Epoch: 55 [35328/50000]	Loss: 0.6361	LR: 0.025000
Training Epoch: 55 [35456/50000]	Loss: 0.6129	LR: 0.025000
Training Epoch: 55 [35584/50000]	Loss: 0.4900	LR: 0.025000
Training Epoch: 55 [35712/50000]	Loss: 0.4793	LR: 0.025000
Training Epoch: 55 [35840/50000]	Loss: 0.6225	LR: 0.025000
Training Epoch: 55 [35968/50000]	Loss: 0.5278	LR: 0.025000
Training Epoch: 55 [36096/50000]	Loss: 0.4370	LR: 0.025000
Training Epoch: 55 [36224/50000]	Loss: 0.4495	LR: 0.025000
Training Epoch: 55 [36352/50000]	Loss: 0.5786	LR: 0.025000
Training Epoch: 55 [36480/50000]	Loss: 0.6181	LR: 0.025000
Training Epoch: 55 [36608/50000]	Loss: 0.6007	LR: 0.025000
Training Epoch: 55 [36736/50000]	Loss: 0.6255	LR: 0.025000
Training Epoch: 55 [36864/50000]	Loss: 0.4544	LR: 0.025000
Training Epoch: 55 [36992/50000]	Loss: 0.5485	LR: 0.025000
Training Epoch: 55 [37120/50000]	Loss: 0.5613	LR: 0.025000
Training Epoch: 55 [37248/50000]	Loss: 0.5018	LR: 0.025000
Training Epoch: 55 [37376/50000]	Loss: 0.5959	LR: 0.025000
Training Epoch: 55 [37504/50000]	Loss: 0.6391	LR: 0.025000
Training Epoch: 55 [37632/50000]	Loss: 0.4649	LR: 0.025000
Training Epoch: 55 [37760/50000]	Loss: 0.6893	LR: 0.025000
Training Epoch: 55 [37888/50000]	Loss: 0.6046	LR: 0.025000
Training Epoch: 55 [38016/50000]	Loss: 0.4639	LR: 0.025000
Training Epoch: 55 [38144/50000]	Loss: 0.6184	LR: 0.025000
Training Epoch: 55 [38272/50000]	Loss: 0.6808	LR: 0.025000
Training Epoch: 55 [38400/50000]	Loss: 0.5284	LR: 0.025000
Training Epoch: 55 [38528/50000]	Loss: 0.5493	LR: 0.025000
Training Epoch: 55 [38656/50000]	Loss: 0.3791	LR: 0.025000
Training Epoch: 55 [38784/50000]	Loss: 0.6156	LR: 0.025000
Training Epoch: 55 [38912/50000]	Loss: 0.6003	LR: 0.025000
Training Epoch: 55 [39040/50000]	Loss: 0.3979	LR: 0.025000
Training Epoch: 55 [39168/50000]	Loss: 0.5904	LR: 0.025000
Training Epoch: 55 [39296/50000]	Loss: 0.5363	LR: 0.025000
Training Epoch: 55 [39424/50000]	Loss: 0.6244	LR: 0.025000
Training Epoch: 55 [39552/50000]	Loss: 0.4641	LR: 0.025000
Training Epoch: 55 [39680/50000]	Loss: 0.5115	LR: 0.025000
Training Epoch: 55 [39808/50000]	Loss: 0.6343	LR: 0.025000
Training Epoch: 55 [39936/50000]	Loss: 0.5354	LR: 0.025000
Training Epoch: 55 [40064/50000]	Loss: 0.5494	LR: 0.025000
Training Epoch: 55 [40192/50000]	Loss: 0.5810	LR: 0.025000
Training Epoch: 55 [40320/50000]	Loss: 0.5620	LR: 0.025000
Training Epoch: 55 [40448/50000]	Loss: 0.6199	LR: 0.025000
Training Epoch: 55 [40576/50000]	Loss: 0.6112	LR: 0.025000
Training Epoch: 55 [40704/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 55 [40832/50000]	Loss: 0.5469	LR: 0.025000
Training Epoch: 55 [40960/50000]	Loss: 0.6797	LR: 0.025000
Training Epoch: 55 [41088/50000]	Loss: 0.5813	LR: 0.025000
Training Epoch: 55 [41216/50000]	Loss: 0.6087	LR: 0.025000
Training Epoch: 55 [41344/50000]	Loss: 0.5440	LR: 0.025000
Training Epoch: 55 [41472/50000]	Loss: 0.5928	LR: 0.025000
Training Epoch: 55 [41600/50000]	Loss: 0.4314	LR: 0.025000
Training Epoch: 55 [41728/50000]	Loss: 0.6621	LR: 0.025000
Training Epoch: 55 [41856/50000]	Loss: 0.6272	LR: 0.025000
Training Epoch: 55 [41984/50000]	Loss: 0.5075	LR: 0.025000
Training Epoch: 55 [42112/50000]	Loss: 0.6611	LR: 0.025000
Training Epoch: 55 [42240/50000]	Loss: 0.4613	LR: 0.025000
Training Epoch: 55 [42368/50000]	Loss: 0.6405	LR: 0.025000
Training Epoch: 55 [42496/50000]	Loss: 0.5244	LR: 0.025000
Training Epoch: 55 [42624/50000]	Loss: 0.4411	LR: 0.025000
Training Epoch: 55 [42752/50000]	Loss: 0.5941	LR: 0.025000
Training Epoch: 55 [42880/50000]	Loss: 0.5568	LR: 0.025000
Training Epoch: 55 [43008/50000]	Loss: 0.5340	LR: 0.025000
Training Epoch: 55 [43136/50000]	Loss: 0.4368	LR: 0.025000
Training Epoch: 55 [43264/50000]	Loss: 0.5635	LR: 0.025000
Training Epoch: 55 [43392/50000]	Loss: 0.4532	LR: 0.025000
Training Epoch: 55 [43520/50000]	Loss: 0.4333	LR: 0.025000
Training Epoch: 55 [43648/50000]	Loss: 0.5923	LR: 0.025000
Training Epoch: 55 [43776/50000]	Loss: 0.3371	LR: 0.025000
Training Epoch: 55 [43904/50000]	Loss: 0.6356	LR: 0.025000
Training Epoch: 55 [44032/50000]	Loss: 0.5991	LR: 0.025000
Training Epoch: 55 [44160/50000]	Loss: 0.6458	LR: 0.025000
Training Epoch: 55 [44288/50000]	Loss: 0.3727	LR: 0.025000
Training Epoch: 55 [44416/50000]	Loss: 0.5795	LR: 0.025000
Training Epoch: 55 [44544/50000]	Loss: 0.6184	LR: 0.025000
Training Epoch: 55 [44672/50000]	Loss: 0.6243	LR: 0.025000
Training Epoch: 55 [44800/50000]	Loss: 0.5546	LR: 0.025000
Training Epoch: 55 [44928/50000]	Loss: 0.5329	LR: 0.025000
Training Epoch: 55 [45056/50000]	Loss: 0.4244	LR: 0.025000
Training Epoch: 55 [45184/50000]	Loss: 0.5614	LR: 0.025000
Training Epoch: 55 [45312/50000]	Loss: 0.5725	LR: 0.025000
Training Epoch: 55 [45440/50000]	Loss: 0.4255	LR: 0.025000
Training Epoch: 55 [45568/50000]	Loss: 0.4049	LR: 0.025000
Training Epoch: 55 [45696/50000]	Loss: 0.5572	LR: 0.025000
Training Epoch: 55 [45824/50000]	Loss: 0.5415	LR: 0.025000
Training Epoch: 55 [45952/50000]	Loss: 0.4297	LR: 0.025000
Training Epoch: 55 [46080/50000]	Loss: 0.5001	LR: 0.025000
Training Epoch: 55 [46208/50000]	Loss: 0.6774	LR: 0.025000
Training Epoch: 55 [46336/50000]	Loss: 0.7382	LR: 0.025000
Training Epoch: 55 [46464/50000]	Loss: 0.7034	LR: 0.025000
Training Epoch: 55 [46592/50000]	Loss: 0.5245	LR: 0.025000
Training Epoch: 55 [46720/50000]	Loss: 0.5302	LR: 0.025000
Training Epoch: 55 [46848/50000]	Loss: 0.6246	LR: 0.025000
Training Epoch: 55 [46976/50000]	Loss: 0.4884	LR: 0.025000
Training Epoch: 55 [47104/50000]	Loss: 0.5887	LR: 0.025000
Training Epoch: 55 [47232/50000]	Loss: 0.6265	LR: 0.025000
Training Epoch: 55 [47360/50000]	Loss: 0.4605	LR: 0.025000
Training Epoch: 55 [47488/50000]	Loss: 0.5080	LR: 0.025000
Training Epoch: 55 [47616/50000]	Loss: 0.5129	LR: 0.025000
Training Epoch: 55 [47744/50000]	Loss: 0.5046	LR: 0.025000
Training Epoch: 55 [47872/50000]	Loss: 0.5334	LR: 0.025000
Training Epoch: 55 [48000/50000]	Loss: 0.4795	LR: 0.025000
Training Epoch: 55 [48128/50000]	Loss: 0.5493	LR: 0.025000
Training Epoch: 55 [48256/50000]	Loss: 0.3953	LR: 0.025000
Training Epoch: 55 [48384/50000]	Loss: 0.6320	LR: 0.025000
Training Epoch: 55 [48512/50000]	Loss: 0.5935	LR: 0.025000
Training Epoch: 55 [48640/50000]	Loss: 0.4838	LR: 0.025000
Training Epoch: 55 [48768/50000]	Loss: 0.5781	LR: 0.025000
Training Epoch: 55 [48896/50000]	Loss: 0.5291	LR: 0.025000
Training Epoch: 55 [49024/50000]	Loss: 0.5603	LR: 0.025000
Training Epoch: 55 [49152/50000]	Loss: 0.6850	LR: 0.025000
Training Epoch: 55 [49280/50000]	Loss: 0.7090	LR: 0.025000
Training Epoch: 55 [49408/50000]	Loss: 0.6984	LR: 0.025000
Training Epoch: 55 [49536/50000]	Loss: 0.6999	LR: 0.025000
Training Epoch: 55 [49664/50000]	Loss: 0.4899	LR: 0.025000
Training Epoch: 55 [49792/50000]	Loss: 0.4865	LR: 0.025000
Training Epoch: 55 [49920/50000]	Loss: 0.4663	LR: 0.025000
Training Epoch: 55 [50000/50000]	Loss: 0.4280	LR: 0.025000
epoch 55 training time consumed: 53.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  201853 GB |  201853 GB |
|       from large pool |  123392 KB |    1034 MB |  201654 GB |  201654 GB |
|       from small pool |   10798 KB |      13 MB |     198 GB |     198 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  201853 GB |  201853 GB |
|       from large pool |  123392 KB |    1034 MB |  201654 GB |  201654 GB |
|       from small pool |   10798 KB |      13 MB |     198 GB |     198 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   88828 GB |   88828 GB |
|       from large pool |  155136 KB |  433088 KB |   88608 GB |   88608 GB |
|       from small pool |    1490 KB |    3494 KB |     219 GB |     219 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    7789 K  |    7788 K  |
|       from large pool |      24    |      65    |    4065 K  |    4065 K  |
|       from small pool |     231    |     274    |    3723 K  |    3723 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    7789 K  |    7788 K  |
|       from large pool |      24    |      65    |    4065 K  |    4065 K  |
|       from small pool |     231    |     274    |    3723 K  |    3723 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3849 K  |    3849 K  |
|       from large pool |       9    |      14    |    1967 K  |    1967 K  |
|       from small pool |      12    |      16    |    1881 K  |    1881 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 55, Average loss: 0.0103, Accuracy: 0.6763, Time consumed:3.47s

Training Epoch: 56 [128/50000]	Loss: 0.4835	LR: 0.025000
Training Epoch: 56 [256/50000]	Loss: 0.5929	LR: 0.025000
Training Epoch: 56 [384/50000]	Loss: 0.4422	LR: 0.025000
Training Epoch: 56 [512/50000]	Loss: 0.5886	LR: 0.025000
Training Epoch: 56 [640/50000]	Loss: 0.4305	LR: 0.025000
Training Epoch: 56 [768/50000]	Loss: 0.4869	LR: 0.025000
Training Epoch: 56 [896/50000]	Loss: 0.5178	LR: 0.025000
Training Epoch: 56 [1024/50000]	Loss: 0.4446	LR: 0.025000
Training Epoch: 56 [1152/50000]	Loss: 0.5338	LR: 0.025000
Training Epoch: 56 [1280/50000]	Loss: 0.4841	LR: 0.025000
Training Epoch: 56 [1408/50000]	Loss: 0.4341	LR: 0.025000
Training Epoch: 56 [1536/50000]	Loss: 0.4820	LR: 0.025000
Training Epoch: 56 [1664/50000]	Loss: 0.3439	LR: 0.025000
Training Epoch: 56 [1792/50000]	Loss: 0.4718	LR: 0.025000
Training Epoch: 56 [1920/50000]	Loss: 0.4850	LR: 0.025000
Training Epoch: 56 [2048/50000]	Loss: 0.4277	LR: 0.025000
Training Epoch: 56 [2176/50000]	Loss: 0.3193	LR: 0.025000
Training Epoch: 56 [2304/50000]	Loss: 0.4615	LR: 0.025000
Training Epoch: 56 [2432/50000]	Loss: 0.4177	LR: 0.025000
Training Epoch: 56 [2560/50000]	Loss: 0.4291	LR: 0.025000
Training Epoch: 56 [2688/50000]	Loss: 0.3671	LR: 0.025000
Training Epoch: 56 [2816/50000]	Loss: 0.3878	LR: 0.025000
Training Epoch: 56 [2944/50000]	Loss: 0.5495	LR: 0.025000
Training Epoch: 56 [3072/50000]	Loss: 0.4905	LR: 0.025000
Training Epoch: 56 [3200/50000]	Loss: 0.3538	LR: 0.025000
Training Epoch: 56 [3328/50000]	Loss: 0.3658	LR: 0.025000
Training Epoch: 56 [3456/50000]	Loss: 0.3927	LR: 0.025000
Training Epoch: 56 [3584/50000]	Loss: 0.5217	LR: 0.025000
Training Epoch: 56 [3712/50000]	Loss: 0.4881	LR: 0.025000
Training Epoch: 56 [3840/50000]	Loss: 0.3585	LR: 0.025000
Training Epoch: 56 [3968/50000]	Loss: 0.6114	LR: 0.025000
Training Epoch: 56 [4096/50000]	Loss: 0.3304	LR: 0.025000
Training Epoch: 56 [4224/50000]	Loss: 0.4104	LR: 0.025000
Training Epoch: 56 [4352/50000]	Loss: 0.5807	LR: 0.025000
Training Epoch: 56 [4480/50000]	Loss: 0.5786	LR: 0.025000
Training Epoch: 56 [4608/50000]	Loss: 0.4479	LR: 0.025000
Training Epoch: 56 [4736/50000]	Loss: 0.3796	LR: 0.025000
Training Epoch: 56 [4864/50000]	Loss: 0.4276	LR: 0.025000
Training Epoch: 56 [4992/50000]	Loss: 0.3873	LR: 0.025000
Training Epoch: 56 [5120/50000]	Loss: 0.5926	LR: 0.025000
Training Epoch: 56 [5248/50000]	Loss: 0.3873	LR: 0.025000
Training Epoch: 56 [5376/50000]	Loss: 0.3733	LR: 0.025000
Training Epoch: 56 [5504/50000]	Loss: 0.3667	LR: 0.025000
Training Epoch: 56 [5632/50000]	Loss: 0.4676	LR: 0.025000
Training Epoch: 56 [5760/50000]	Loss: 0.6562	LR: 0.025000
Training Epoch: 56 [5888/50000]	Loss: 0.3250	LR: 0.025000
Training Epoch: 56 [6016/50000]	Loss: 0.3982	LR: 0.025000
Training Epoch: 56 [6144/50000]	Loss: 0.4322	LR: 0.025000
Training Epoch: 56 [6272/50000]	Loss: 0.5708	LR: 0.025000
Training Epoch: 56 [6400/50000]	Loss: 0.4598	LR: 0.025000
Training Epoch: 56 [6528/50000]	Loss: 0.4367	LR: 0.025000
Training Epoch: 56 [6656/50000]	Loss: 0.4956	LR: 0.025000
Training Epoch: 56 [6784/50000]	Loss: 0.5595	LR: 0.025000
Training Epoch: 56 [6912/50000]	Loss: 0.5202	LR: 0.025000
Training Epoch: 56 [7040/50000]	Loss: 0.5218	LR: 0.025000
Training Epoch: 56 [7168/50000]	Loss: 0.4732	LR: 0.025000
Training Epoch: 56 [7296/50000]	Loss: 0.4156	LR: 0.025000
Training Epoch: 56 [7424/50000]	Loss: 0.4605	LR: 0.025000
Training Epoch: 56 [7552/50000]	Loss: 0.4932	LR: 0.025000
Training Epoch: 56 [7680/50000]	Loss: 0.3616	LR: 0.025000
Training Epoch: 56 [7808/50000]	Loss: 0.5476	LR: 0.025000
Training Epoch: 56 [7936/50000]	Loss: 0.4031	LR: 0.025000
Training Epoch: 56 [8064/50000]	Loss: 0.4883	LR: 0.025000
Training Epoch: 56 [8192/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 56 [8320/50000]	Loss: 0.5286	LR: 0.025000
Training Epoch: 56 [8448/50000]	Loss: 0.4603	LR: 0.025000
Training Epoch: 56 [8576/50000]	Loss: 0.4182	LR: 0.025000
Training Epoch: 56 [8704/50000]	Loss: 0.4225	LR: 0.025000
Training Epoch: 56 [8832/50000]	Loss: 0.5223	LR: 0.025000
Training Epoch: 56 [8960/50000]	Loss: 0.4186	LR: 0.025000
Training Epoch: 56 [9088/50000]	Loss: 0.5621	LR: 0.025000
Training Epoch: 56 [9216/50000]	Loss: 0.3875	LR: 0.025000
Training Epoch: 56 [9344/50000]	Loss: 0.3983	LR: 0.025000
Training Epoch: 56 [9472/50000]	Loss: 0.4347	LR: 0.025000
Training Epoch: 56 [9600/50000]	Loss: 0.5777	LR: 0.025000
Training Epoch: 56 [9728/50000]	Loss: 0.4824	LR: 0.025000
Training Epoch: 56 [9856/50000]	Loss: 0.5289	LR: 0.025000
Training Epoch: 56 [9984/50000]	Loss: 0.6478	LR: 0.025000
Training Epoch: 56 [10112/50000]	Loss: 0.2670	LR: 0.025000
Training Epoch: 56 [10240/50000]	Loss: 0.4461	LR: 0.025000
Training Epoch: 56 [10368/50000]	Loss: 0.3730	LR: 0.025000
Training Epoch: 56 [10496/50000]	Loss: 0.4005	LR: 0.025000
Training Epoch: 56 [10624/50000]	Loss: 0.5329	LR: 0.025000
Training Epoch: 56 [10752/50000]	Loss: 0.5095	LR: 0.025000
Training Epoch: 56 [10880/50000]	Loss: 0.5116	LR: 0.025000
Training Epoch: 56 [11008/50000]	Loss: 0.3806	LR: 0.025000
Training Epoch: 56 [11136/50000]	Loss: 0.4860	LR: 0.025000
Training Epoch: 56 [11264/50000]	Loss: 0.7256	LR: 0.025000
Training Epoch: 56 [11392/50000]	Loss: 0.5338	LR: 0.025000
Training Epoch: 56 [11520/50000]	Loss: 0.5277	LR: 0.025000
Training Epoch: 56 [11648/50000]	Loss: 0.5666	LR: 0.025000
Training Epoch: 56 [11776/50000]	Loss: 0.3293	LR: 0.025000
Training Epoch: 56 [11904/50000]	Loss: 0.5136	LR: 0.025000
Training Epoch: 56 [12032/50000]	Loss: 0.5603	LR: 0.025000
Training Epoch: 56 [12160/50000]	Loss: 0.3908	LR: 0.025000
Training Epoch: 56 [12288/50000]	Loss: 0.3738	LR: 0.025000
Training Epoch: 56 [12416/50000]	Loss: 0.3183	LR: 0.025000
Training Epoch: 56 [12544/50000]	Loss: 0.4985	LR: 0.025000
Training Epoch: 56 [12672/50000]	Loss: 0.6317	LR: 0.025000
Training Epoch: 56 [12800/50000]	Loss: 0.5247	LR: 0.025000
Training Epoch: 56 [12928/50000]	Loss: 0.4792	LR: 0.025000
Training Epoch: 56 [13056/50000]	Loss: 0.3278	LR: 0.025000
Training Epoch: 56 [13184/50000]	Loss: 0.4748	LR: 0.025000
Training Epoch: 56 [13312/50000]	Loss: 0.3387	LR: 0.025000
Training Epoch: 56 [13440/50000]	Loss: 0.5459	LR: 0.025000
Training Epoch: 56 [13568/50000]	Loss: 0.5055	LR: 0.025000
Training Epoch: 56 [13696/50000]	Loss: 0.3401	LR: 0.025000
Training Epoch: 56 [13824/50000]	Loss: 0.5570	LR: 0.025000
Training Epoch: 56 [13952/50000]	Loss: 0.4386	LR: 0.025000
Training Epoch: 56 [14080/50000]	Loss: 0.5188	LR: 0.025000
Training Epoch: 56 [14208/50000]	Loss: 0.4334	LR: 0.025000
Training Epoch: 56 [14336/50000]	Loss: 0.3881	LR: 0.025000
Training Epoch: 56 [14464/50000]	Loss: 0.5404	LR: 0.025000
Training Epoch: 56 [14592/50000]	Loss: 0.5556	LR: 0.025000
Training Epoch: 56 [14720/50000]	Loss: 0.3301	LR: 0.025000
Training Epoch: 56 [14848/50000]	Loss: 0.7235	LR: 0.025000
Training Epoch: 56 [14976/50000]	Loss: 0.5629	LR: 0.025000
Training Epoch: 56 [15104/50000]	Loss: 0.6177	LR: 0.025000
Training Epoch: 56 [15232/50000]	Loss: 0.6030	LR: 0.025000
Training Epoch: 56 [15360/50000]	Loss: 0.5880	LR: 0.025000
Training Epoch: 56 [15488/50000]	Loss: 0.6030	LR: 0.025000
Training Epoch: 56 [15616/50000]	Loss: 0.4139	LR: 0.025000
Training Epoch: 56 [15744/50000]	Loss: 0.4899	LR: 0.025000
Training Epoch: 56 [15872/50000]	Loss: 0.4978	LR: 0.025000
Training Epoch: 56 [16000/50000]	Loss: 0.4827	LR: 0.025000
Training Epoch: 56 [16128/50000]	Loss: 0.4844	LR: 0.025000
Training Epoch: 56 [16256/50000]	Loss: 0.6586	LR: 0.025000
Training Epoch: 56 [16384/50000]	Loss: 0.4264	LR: 0.025000
Training Epoch: 56 [16512/50000]	Loss: 0.4021	LR: 0.025000
Training Epoch: 56 [16640/50000]	Loss: 0.4794	LR: 0.025000
Training Epoch: 56 [16768/50000]	Loss: 0.4585	LR: 0.025000
Training Epoch: 56 [16896/50000]	Loss: 0.6461	LR: 0.025000
Training Epoch: 56 [17024/50000]	Loss: 0.4446	LR: 0.025000
Training Epoch: 56 [17152/50000]	Loss: 0.5799	LR: 0.025000
Training Epoch: 56 [17280/50000]	Loss: 0.3519	LR: 0.025000
Training Epoch: 56 [17408/50000]	Loss: 0.4427	LR: 0.025000
Training Epoch: 56 [17536/50000]	Loss: 0.3409	LR: 0.025000
Training Epoch: 56 [17664/50000]	Loss: 0.4622	LR: 0.025000
Training Epoch: 56 [17792/50000]	Loss: 0.5727	LR: 0.025000
Training Epoch: 56 [17920/50000]	Loss: 0.6513	LR: 0.025000
Training Epoch: 56 [18048/50000]	Loss: 0.5474	LR: 0.025000
Training Epoch: 56 [18176/50000]	Loss: 0.6274	LR: 0.025000
Training Epoch: 56 [18304/50000]	Loss: 0.5474	LR: 0.025000
Training Epoch: 56 [18432/50000]	Loss: 0.5211	LR: 0.025000
Training Epoch: 56 [18560/50000]	Loss: 0.5604	LR: 0.025000
Training Epoch: 56 [18688/50000]	Loss: 0.5633	LR: 0.025000
Training Epoch: 56 [18816/50000]	Loss: 0.3761	LR: 0.025000
Training Epoch: 56 [18944/50000]	Loss: 0.5522	LR: 0.025000
Training Epoch: 56 [19072/50000]	Loss: 0.4850	LR: 0.025000
Training Epoch: 56 [19200/50000]	Loss: 0.4260	LR: 0.025000
Training Epoch: 56 [19328/50000]	Loss: 0.5135	LR: 0.025000
Training Epoch: 56 [19456/50000]	Loss: 0.6292	LR: 0.025000
Training Epoch: 56 [19584/50000]	Loss: 0.4395	LR: 0.025000
Training Epoch: 56 [19712/50000]	Loss: 0.6149	LR: 0.025000
Training Epoch: 56 [19840/50000]	Loss: 0.5180	LR: 0.025000
Training Epoch: 56 [19968/50000]	Loss: 0.4932	LR: 0.025000
Training Epoch: 56 [20096/50000]	Loss: 0.5074	LR: 0.025000
Training Epoch: 56 [20224/50000]	Loss: 0.5219	LR: 0.025000
Training Epoch: 56 [20352/50000]	Loss: 0.6508	LR: 0.025000
Training Epoch: 56 [20480/50000]	Loss: 0.4286	LR: 0.025000
Training Epoch: 56 [20608/50000]	Loss: 0.5780	LR: 0.025000
Training Epoch: 56 [20736/50000]	Loss: 0.7473	LR: 0.025000
Training Epoch: 56 [20864/50000]	Loss: 0.5070	LR: 0.025000
Training Epoch: 56 [20992/50000]	Loss: 0.5493	LR: 0.025000
Training Epoch: 56 [21120/50000]	Loss: 0.4738	LR: 0.025000
Training Epoch: 56 [21248/50000]	Loss: 0.5649	LR: 0.025000
Training Epoch: 56 [21376/50000]	Loss: 0.4792	LR: 0.025000
Training Epoch: 56 [21504/50000]	Loss: 0.6939	LR: 0.025000
Training Epoch: 56 [21632/50000]	Loss: 0.4270	LR: 0.025000
Training Epoch: 56 [21760/50000]	Loss: 0.5476	LR: 0.025000
Training Epoch: 56 [21888/50000]	Loss: 0.4976	LR: 0.025000
Training Epoch: 56 [22016/50000]	Loss: 0.4818	LR: 0.025000
Training Epoch: 56 [22144/50000]	Loss: 0.4754	LR: 0.025000
Training Epoch: 56 [22272/50000]	Loss: 0.4887	LR: 0.025000
Training Epoch: 56 [22400/50000]	Loss: 0.4447	LR: 0.025000
Training Epoch: 56 [22528/50000]	Loss: 0.4791	LR: 0.025000
Training Epoch: 56 [22656/50000]	Loss: 0.5293	LR: 0.025000
Training Epoch: 56 [22784/50000]	Loss: 0.4915	LR: 0.025000
Training Epoch: 56 [22912/50000]	Loss: 0.4207	LR: 0.025000
Training Epoch: 56 [23040/50000]	Loss: 0.6289	LR: 0.025000
Training Epoch: 56 [23168/50000]	Loss: 0.5699	LR: 0.025000
Training Epoch: 56 [23296/50000]	Loss: 0.5823	LR: 0.025000
Training Epoch: 56 [23424/50000]	Loss: 0.4217	LR: 0.025000
Training Epoch: 56 [23552/50000]	Loss: 0.4371	LR: 0.025000
Training Epoch: 56 [23680/50000]	Loss: 0.6406	LR: 0.025000
Training Epoch: 56 [23808/50000]	Loss: 0.6231	LR: 0.025000
Training Epoch: 56 [23936/50000]	Loss: 0.6080	LR: 0.025000
Training Epoch: 56 [24064/50000]	Loss: 0.6100	LR: 0.025000
Training Epoch: 56 [24192/50000]	Loss: 0.5828	LR: 0.025000
Training Epoch: 56 [24320/50000]	Loss: 0.6264	LR: 0.025000
Training Epoch: 56 [24448/50000]	Loss: 0.3689	LR: 0.025000
Training Epoch: 56 [24576/50000]	Loss: 0.6188	LR: 0.025000
Training Epoch: 56 [24704/50000]	Loss: 0.5260	LR: 0.025000
Training Epoch: 56 [24832/50000]	Loss: 0.5492	LR: 0.025000
Training Epoch: 56 [24960/50000]	Loss: 0.4255	LR: 0.025000
Training Epoch: 56 [25088/50000]	Loss: 0.3529	LR: 0.025000
Training Epoch: 56 [25216/50000]	Loss: 0.3972	LR: 0.025000
Training Epoch: 56 [25344/50000]	Loss: 0.4141	LR: 0.025000
Training Epoch: 56 [25472/50000]	Loss: 0.5046	LR: 0.025000
Training Epoch: 56 [25600/50000]	Loss: 0.6934	LR: 0.025000
Training Epoch: 56 [25728/50000]	Loss: 0.5648	LR: 0.025000
Training Epoch: 56 [25856/50000]	Loss: 0.3425	LR: 0.025000
Training Epoch: 56 [25984/50000]	Loss: 0.6092	LR: 0.025000
Training Epoch: 56 [26112/50000]	Loss: 0.7460	LR: 0.025000
Training Epoch: 56 [26240/50000]	Loss: 0.6348	LR: 0.025000
Training Epoch: 56 [26368/50000]	Loss: 0.4592	LR: 0.025000
Training Epoch: 56 [26496/50000]	Loss: 0.6139	LR: 0.025000
Training Epoch: 56 [26624/50000]	Loss: 0.6599	LR: 0.025000
Training Epoch: 56 [26752/50000]	Loss: 0.5055	LR: 0.025000
Training Epoch: 56 [26880/50000]	Loss: 0.5081	LR: 0.025000
Training Epoch: 56 [27008/50000]	Loss: 0.5585	LR: 0.025000
Training Epoch: 56 [27136/50000]	Loss: 0.5329	LR: 0.025000
Training Epoch: 56 [27264/50000]	Loss: 0.5232	LR: 0.025000
Training Epoch: 56 [27392/50000]	Loss: 0.5782	LR: 0.025000
Training Epoch: 56 [27520/50000]	Loss: 0.5953	LR: 0.025000
Training Epoch: 56 [27648/50000]	Loss: 0.5196	LR: 0.025000
Training Epoch: 56 [27776/50000]	Loss: 0.4974	LR: 0.025000
Training Epoch: 56 [27904/50000]	Loss: 0.5618	LR: 0.025000
Training Epoch: 56 [28032/50000]	Loss: 0.5087	LR: 0.025000
Training Epoch: 56 [28160/50000]	Loss: 0.5338	LR: 0.025000
Training Epoch: 56 [28288/50000]	Loss: 0.5372	LR: 0.025000
Training Epoch: 56 [28416/50000]	Loss: 0.3314	LR: 0.025000
Training Epoch: 56 [28544/50000]	Loss: 0.5207	LR: 0.025000
Training Epoch: 56 [28672/50000]	Loss: 0.6329	LR: 0.025000
Training Epoch: 56 [28800/50000]	Loss: 0.6527	LR: 0.025000
Training Epoch: 56 [28928/50000]	Loss: 0.4733	LR: 0.025000
Training Epoch: 56 [29056/50000]	Loss: 0.5158	LR: 0.025000
Training Epoch: 56 [29184/50000]	Loss: 0.5000	LR: 0.025000
Training Epoch: 56 [29312/50000]	Loss: 0.4845	LR: 0.025000
Training Epoch: 56 [29440/50000]	Loss: 0.6112	LR: 0.025000
Training Epoch: 56 [29568/50000]	Loss: 0.4302	LR: 0.025000
Training Epoch: 56 [29696/50000]	Loss: 0.5143	LR: 0.025000
Training Epoch: 56 [29824/50000]	Loss: 0.4755	LR: 0.025000
Training Epoch: 56 [29952/50000]	Loss: 0.6528	LR: 0.025000
Training Epoch: 56 [30080/50000]	Loss: 0.4649	LR: 0.025000
Training Epoch: 56 [30208/50000]	Loss: 0.3810	LR: 0.025000
Training Epoch: 56 [30336/50000]	Loss: 0.5348	LR: 0.025000
Training Epoch: 56 [30464/50000]	Loss: 0.4906	LR: 0.025000
Training Epoch: 56 [30592/50000]	Loss: 0.6876	LR: 0.025000
Training Epoch: 56 [30720/50000]	Loss: 0.5106	LR: 0.025000
Training Epoch: 56 [30848/50000]	Loss: 0.6641	LR: 0.025000
Training Epoch: 56 [30976/50000]	Loss: 0.4618	LR: 0.025000
Training Epoch: 56 [31104/50000]	Loss: 0.5050	LR: 0.025000
Training Epoch: 56 [31232/50000]	Loss: 0.5286	LR: 0.025000
Training Epoch: 56 [31360/50000]	Loss: 0.6415	LR: 0.025000
Training Epoch: 56 [31488/50000]	Loss: 0.6251	LR: 0.025000
Training Epoch: 56 [31616/50000]	Loss: 0.5560	LR: 0.025000
Training Epoch: 56 [31744/50000]	Loss: 0.6213	LR: 0.025000
Training Epoch: 56 [31872/50000]	Loss: 0.4307	LR: 0.025000
Training Epoch: 56 [32000/50000]	Loss: 0.4157	LR: 0.025000
Training Epoch: 56 [32128/50000]	Loss: 0.4795	LR: 0.025000
Training Epoch: 56 [32256/50000]	Loss: 0.5591	LR: 0.025000
Training Epoch: 56 [32384/50000]	Loss: 0.6750	LR: 0.025000
Training Epoch: 56 [32512/50000]	Loss: 0.5268	LR: 0.025000
Training Epoch: 56 [32640/50000]	Loss: 0.6743	LR: 0.025000
Training Epoch: 56 [32768/50000]	Loss: 0.4263	LR: 0.025000
Training Epoch: 56 [32896/50000]	Loss: 0.4053	LR: 0.025000
Training Epoch: 56 [33024/50000]	Loss: 0.6131	LR: 0.025000
Training Epoch: 56 [33152/50000]	Loss: 0.5096	LR: 0.025000
Training Epoch: 56 [33280/50000]	Loss: 0.4871	LR: 0.025000
Training Epoch: 56 [33408/50000]	Loss: 0.6225	LR: 0.025000
Training Epoch: 56 [33536/50000]	Loss: 0.4447	LR: 0.025000
Training Epoch: 56 [33664/50000]	Loss: 0.4984	LR: 0.025000
Training Epoch: 56 [33792/50000]	Loss: 0.3891	LR: 0.025000
Training Epoch: 56 [33920/50000]	Loss: 0.4169	LR: 0.025000
Training Epoch: 56 [34048/50000]	Loss: 0.6145	LR: 0.025000
Training Epoch: 56 [34176/50000]	Loss: 0.5858	LR: 0.025000
Training Epoch: 56 [34304/50000]	Loss: 0.4111	LR: 0.025000
Training Epoch: 56 [34432/50000]	Loss: 0.6443	LR: 0.025000
Training Epoch: 56 [34560/50000]	Loss: 0.6003	LR: 0.025000
Training Epoch: 56 [34688/50000]	Loss: 0.6226	LR: 0.025000
Training Epoch: 56 [34816/50000]	Loss: 0.6811	LR: 0.025000
Training Epoch: 56 [34944/50000]	Loss: 0.5718	LR: 0.025000
Training Epoch: 56 [35072/50000]	Loss: 0.4931	LR: 0.025000
Training Epoch: 56 [35200/50000]	Loss: 0.7150	LR: 0.025000
Training Epoch: 56 [35328/50000]	Loss: 0.6335	LR: 0.025000
Training Epoch: 56 [35456/50000]	Loss: 0.5925	LR: 0.025000
Training Epoch: 56 [35584/50000]	Loss: 0.5283	LR: 0.025000
Training Epoch: 56 [35712/50000]	Loss: 0.3924	LR: 0.025000
Training Epoch: 56 [35840/50000]	Loss: 0.6525	LR: 0.025000
Training Epoch: 56 [35968/50000]	Loss: 0.5384	LR: 0.025000
Training Epoch: 56 [36096/50000]	Loss: 0.6568	LR: 0.025000
Training Epoch: 56 [36224/50000]	Loss: 0.5633	LR: 0.025000
Training Epoch: 56 [36352/50000]	Loss: 0.4908	LR: 0.025000
Training Epoch: 56 [36480/50000]	Loss: 0.5717	LR: 0.025000
Training Epoch: 56 [36608/50000]	Loss: 0.3972	LR: 0.025000
Training Epoch: 56 [36736/50000]	Loss: 0.6297	LR: 0.025000
Training Epoch: 56 [36864/50000]	Loss: 0.5746	LR: 0.025000
Training Epoch: 56 [36992/50000]	Loss: 0.6954	LR: 0.025000
Training Epoch: 56 [37120/50000]	Loss: 0.4979	LR: 0.025000
Training Epoch: 56 [37248/50000]	Loss: 0.5984	LR: 0.025000
Training Epoch: 56 [37376/50000]	Loss: 0.5285	LR: 0.025000
Training Epoch: 56 [37504/50000]	Loss: 0.6305	LR: 0.025000
Training Epoch: 56 [37632/50000]	Loss: 0.4612	LR: 0.025000
Training Epoch: 56 [37760/50000]	Loss: 0.6785	LR: 0.025000
Training Epoch: 56 [37888/50000]	Loss: 0.4642	LR: 0.025000
Training Epoch: 56 [38016/50000]	Loss: 0.5871	LR: 0.025000
Training Epoch: 56 [38144/50000]	Loss: 0.5394	LR: 0.025000
Training Epoch: 56 [38272/50000]	Loss: 0.5829	LR: 0.025000
Training Epoch: 56 [38400/50000]	Loss: 0.6406	LR: 0.025000
Training Epoch: 56 [38528/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 56 [38656/50000]	Loss: 0.8006	LR: 0.025000
Training Epoch: 56 [38784/50000]	Loss: 0.5783	LR: 0.025000
Training Epoch: 56 [38912/50000]	Loss: 0.5949	LR: 0.025000
Training Epoch: 56 [39040/50000]	Loss: 0.5200	LR: 0.025000
Training Epoch: 56 [39168/50000]	Loss: 0.4667	LR: 0.025000
Training Epoch: 56 [39296/50000]	Loss: 0.5549	LR: 0.025000
Training Epoch: 56 [39424/50000]	Loss: 0.6571	LR: 0.025000
Training Epoch: 56 [39552/50000]	Loss: 0.5598	LR: 0.025000
Training Epoch: 56 [39680/50000]	Loss: 0.4678	LR: 0.025000
Training Epoch: 56 [39808/50000]	Loss: 0.5163	LR: 0.025000
Training Epoch: 56 [39936/50000]	Loss: 0.6311	LR: 0.025000
Training Epoch: 56 [40064/50000]	Loss: 0.4684	LR: 0.025000
Training Epoch: 56 [40192/50000]	Loss: 0.5762	LR: 0.025000
Training Epoch: 56 [40320/50000]	Loss: 0.6132	LR: 0.025000
Training Epoch: 56 [40448/50000]	Loss: 0.5938	LR: 0.025000
Training Epoch: 56 [40576/50000]	Loss: 0.6387	LR: 0.025000
Training Epoch: 56 [40704/50000]	Loss: 0.7410	LR: 0.025000
Training Epoch: 56 [40832/50000]	Loss: 0.5128	LR: 0.025000
Training Epoch: 56 [40960/50000]	Loss: 0.4394	LR: 0.025000
Training Epoch: 56 [41088/50000]	Loss: 0.6354	LR: 0.025000
Training Epoch: 56 [41216/50000]	Loss: 0.6145	LR: 0.025000
Training Epoch: 56 [41344/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 56 [41472/50000]	Loss: 0.6580	LR: 0.025000
Training Epoch: 56 [41600/50000]	Loss: 0.5563	LR: 0.025000
Training Epoch: 56 [41728/50000]	Loss: 0.5868	LR: 0.025000
Training Epoch: 56 [41856/50000]	Loss: 0.6114	LR: 0.025000
Training Epoch: 56 [41984/50000]	Loss: 0.6073	LR: 0.025000
Training Epoch: 56 [42112/50000]	Loss: 0.5807	LR: 0.025000
Training Epoch: 56 [42240/50000]	Loss: 0.5453	LR: 0.025000
Training Epoch: 56 [42368/50000]	Loss: 0.5903	LR: 0.025000
Training Epoch: 56 [42496/50000]	Loss: 0.4354	LR: 0.025000
Training Epoch: 56 [42624/50000]	Loss: 0.5700	LR: 0.025000
Training Epoch: 56 [42752/50000]	Loss: 0.7675	LR: 0.025000
Training Epoch: 56 [42880/50000]	Loss: 0.6618	LR: 0.025000
Training Epoch: 56 [43008/50000]	Loss: 0.7265	LR: 0.025000
Training Epoch: 56 [43136/50000]	Loss: 0.5756	LR: 0.025000
Training Epoch: 56 [43264/50000]	Loss: 0.6257	LR: 0.025000
Training Epoch: 56 [43392/50000]	Loss: 0.5794	LR: 0.025000
Training Epoch: 56 [43520/50000]	Loss: 0.4849	LR: 0.025000
Training Epoch: 56 [43648/50000]	Loss: 0.5259	LR: 0.025000
Training Epoch: 56 [43776/50000]	Loss: 0.5526	LR: 0.025000
Training Epoch: 56 [43904/50000]	Loss: 0.5506	LR: 0.025000
Training Epoch: 56 [44032/50000]	Loss: 0.5400	LR: 0.025000
Training Epoch: 56 [44160/50000]	Loss: 0.5963	LR: 0.025000
Training Epoch: 56 [44288/50000]	Loss: 0.4878	LR: 0.025000
Training Epoch: 56 [44416/50000]	Loss: 0.5822	LR: 0.025000
Training Epoch: 56 [44544/50000]	Loss: 0.4446	LR: 0.025000
Training Epoch: 56 [44672/50000]	Loss: 0.5696	LR: 0.025000
Training Epoch: 56 [44800/50000]	Loss: 0.5436	LR: 0.025000
Training Epoch: 56 [44928/50000]	Loss: 0.5114	LR: 0.025000
Training Epoch: 56 [45056/50000]	Loss: 0.5961	LR: 0.025000
Training Epoch: 56 [45184/50000]	Loss: 0.6772	LR: 0.025000
Training Epoch: 56 [45312/50000]	Loss: 0.5605	LR: 0.025000
Training Epoch: 56 [45440/50000]	Loss: 0.6899	LR: 0.025000
Training Epoch: 56 [45568/50000]	Loss: 0.4344	LR: 0.025000
Training Epoch: 56 [45696/50000]	Loss: 0.5766	LR: 0.025000
Training Epoch: 56 [45824/50000]	Loss: 0.5323	LR: 0.025000
Training Epoch: 56 [45952/50000]	Loss: 0.4250	LR: 0.025000
Training Epoch: 56 [46080/50000]	Loss: 0.5665	LR: 0.025000
Training Epoch: 56 [46208/50000]	Loss: 0.5620	LR: 0.025000
Training Epoch: 56 [46336/50000]	Loss: 0.6529	LR: 0.025000
Training Epoch: 56 [46464/50000]	Loss: 0.7113	LR: 0.025000
Training Epoch: 56 [46592/50000]	Loss: 0.3860	LR: 0.025000
Training Epoch: 56 [46720/50000]	Loss: 0.7491	LR: 0.025000
Training Epoch: 56 [46848/50000]	Loss: 0.4648	LR: 0.025000
Training Epoch: 56 [46976/50000]	Loss: 0.6260	LR: 0.025000
Training Epoch: 56 [47104/50000]	Loss: 0.5101	LR: 0.025000
Training Epoch: 56 [47232/50000]	Loss: 0.4608	LR: 0.025000
Training Epoch: 56 [47360/50000]	Loss: 0.6094	LR: 0.025000
Training Epoch: 56 [47488/50000]	Loss: 0.6130	LR: 0.025000
Training Epoch: 56 [47616/50000]	Loss: 0.6868	LR: 0.025000
Training Epoch: 56 [47744/50000]	Loss: 0.5772	LR: 0.025000
Training Epoch: 56 [47872/50000]	Loss: 0.6834	LR: 0.025000
Training Epoch: 56 [48000/50000]	Loss: 0.7753	LR: 0.025000
Training Epoch: 56 [48128/50000]	Loss: 0.3853	LR: 0.025000
Training Epoch: 56 [48256/50000]	Loss: 0.4543	LR: 0.025000
Training Epoch: 56 [48384/50000]	Loss: 0.5914	LR: 0.025000
Training Epoch: 56 [48512/50000]	Loss: 0.5394	LR: 0.025000
Training Epoch: 56 [48640/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 56 [48768/50000]	Loss: 0.7212	LR: 0.025000
Training Epoch: 56 [48896/50000]	Loss: 0.4870	LR: 0.025000
Training Epoch: 56 [49024/50000]	Loss: 0.7414	LR: 0.025000
Training Epoch: 56 [49152/50000]	Loss: 0.6667	LR: 0.025000
Training Epoch: 56 [49280/50000]	Loss: 0.4272	LR: 0.025000
Training Epoch: 56 [49408/50000]	Loss: 0.6126	LR: 0.025000
Training Epoch: 56 [49536/50000]	Loss: 0.7197	LR: 0.025000
Training Epoch: 56 [49664/50000]	Loss: 0.4927	LR: 0.025000
Training Epoch: 56 [49792/50000]	Loss: 0.7180	LR: 0.025000
Training Epoch: 56 [49920/50000]	Loss: 0.6145	LR: 0.025000
Training Epoch: 56 [50000/50000]	Loss: 0.6500	LR: 0.025000
epoch 56 training time consumed: 54.07s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  205523 GB |  205523 GB |
|       from large pool |  123392 KB |    1034 MB |  205321 GB |  205320 GB |
|       from small pool |   10798 KB |      13 MB |     202 GB |     202 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  205523 GB |  205523 GB |
|       from large pool |  123392 KB |    1034 MB |  205321 GB |  205320 GB |
|       from small pool |   10798 KB |      13 MB |     202 GB |     202 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   90443 GB |   90443 GB |
|       from large pool |  155136 KB |  433088 KB |   90219 GB |   90219 GB |
|       from small pool |    1490 KB |    3494 KB |     223 GB |     223 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    7930 K  |    7930 K  |
|       from large pool |      24    |      65    |    4139 K  |    4139 K  |
|       from small pool |     231    |     274    |    3791 K  |    3790 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    7930 K  |    7930 K  |
|       from large pool |      24    |      65    |    4139 K  |    4139 K  |
|       from small pool |     231    |     274    |    3791 K  |    3790 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3919 K  |    3919 K  |
|       from large pool |       9    |      14    |    2003 K  |    2003 K  |
|       from small pool |      12    |      16    |    1915 K  |    1915 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 56, Average loss: 0.0108, Accuracy: 0.6681, Time consumed:3.46s

Training Epoch: 57 [128/50000]	Loss: 0.4926	LR: 0.025000
Training Epoch: 57 [256/50000]	Loss: 0.4373	LR: 0.025000
Training Epoch: 57 [384/50000]	Loss: 0.5014	LR: 0.025000
Training Epoch: 57 [512/50000]	Loss: 0.4897	LR: 0.025000
Training Epoch: 57 [640/50000]	Loss: 0.4540	LR: 0.025000
Training Epoch: 57 [768/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 57 [896/50000]	Loss: 0.3924	LR: 0.025000
Training Epoch: 57 [1024/50000]	Loss: 0.4377	LR: 0.025000
Training Epoch: 57 [1152/50000]	Loss: 0.6321	LR: 0.025000
Training Epoch: 57 [1280/50000]	Loss: 0.4596	LR: 0.025000
Training Epoch: 57 [1408/50000]	Loss: 0.4643	LR: 0.025000
Training Epoch: 57 [1536/50000]	Loss: 0.5717	LR: 0.025000
Training Epoch: 57 [1664/50000]	Loss: 0.4719	LR: 0.025000
Training Epoch: 57 [1792/50000]	Loss: 0.4265	LR: 0.025000
Training Epoch: 57 [1920/50000]	Loss: 0.4949	LR: 0.025000
Training Epoch: 57 [2048/50000]	Loss: 0.5219	LR: 0.025000
Training Epoch: 57 [2176/50000]	Loss: 0.5166	LR: 0.025000
Training Epoch: 57 [2304/50000]	Loss: 0.5225	LR: 0.025000
Training Epoch: 57 [2432/50000]	Loss: 0.3719	LR: 0.025000
Training Epoch: 57 [2560/50000]	Loss: 0.4116	LR: 0.025000
Training Epoch: 57 [2688/50000]	Loss: 0.2771	LR: 0.025000
Training Epoch: 57 [2816/50000]	Loss: 0.4251	LR: 0.025000
Training Epoch: 57 [2944/50000]	Loss: 0.3363	LR: 0.025000
Training Epoch: 57 [3072/50000]	Loss: 0.4535	LR: 0.025000
Training Epoch: 57 [3200/50000]	Loss: 0.3692	LR: 0.025000
Training Epoch: 57 [3328/50000]	Loss: 0.5119	LR: 0.025000
Training Epoch: 57 [3456/50000]	Loss: 0.4732	LR: 0.025000
Training Epoch: 57 [3584/50000]	Loss: 0.5033	LR: 0.025000
Training Epoch: 57 [3712/50000]	Loss: 0.4081	LR: 0.025000
Training Epoch: 57 [3840/50000]	Loss: 0.4329	LR: 0.025000
Training Epoch: 57 [3968/50000]	Loss: 0.5013	LR: 0.025000
Training Epoch: 57 [4096/50000]	Loss: 0.4767	LR: 0.025000
Training Epoch: 57 [4224/50000]	Loss: 0.4137	LR: 0.025000
Training Epoch: 57 [4352/50000]	Loss: 0.4362	LR: 0.025000
Training Epoch: 57 [4480/50000]	Loss: 0.3878	LR: 0.025000
Training Epoch: 57 [4608/50000]	Loss: 0.4049	LR: 0.025000
Training Epoch: 57 [4736/50000]	Loss: 0.5411	LR: 0.025000
Training Epoch: 57 [4864/50000]	Loss: 0.5639	LR: 0.025000
Training Epoch: 57 [4992/50000]	Loss: 0.5409	LR: 0.025000
Training Epoch: 57 [5120/50000]	Loss: 0.4010	LR: 0.025000
Training Epoch: 57 [5248/50000]	Loss: 0.4910	LR: 0.025000
Training Epoch: 57 [5376/50000]	Loss: 0.4871	LR: 0.025000
Training Epoch: 57 [5504/50000]	Loss: 0.5400	LR: 0.025000
Training Epoch: 57 [5632/50000]	Loss: 0.4808	LR: 0.025000
Training Epoch: 57 [5760/50000]	Loss: 0.4056	LR: 0.025000
Training Epoch: 57 [5888/50000]	Loss: 0.5204	LR: 0.025000
Training Epoch: 57 [6016/50000]	Loss: 0.4766	LR: 0.025000
Training Epoch: 57 [6144/50000]	Loss: 0.5335	LR: 0.025000
Training Epoch: 57 [6272/50000]	Loss: 0.4113	LR: 0.025000
Training Epoch: 57 [6400/50000]	Loss: 0.4392	LR: 0.025000
Training Epoch: 57 [6528/50000]	Loss: 0.4099	LR: 0.025000
Training Epoch: 57 [6656/50000]	Loss: 0.4695	LR: 0.025000
Training Epoch: 57 [6784/50000]	Loss: 0.4527	LR: 0.025000
Training Epoch: 57 [6912/50000]	Loss: 0.4613	LR: 0.025000
Training Epoch: 57 [7040/50000]	Loss: 0.5846	LR: 0.025000
Training Epoch: 57 [7168/50000]	Loss: 0.5747	LR: 0.025000
Training Epoch: 57 [7296/50000]	Loss: 0.4049	LR: 0.025000
Training Epoch: 57 [7424/50000]	Loss: 0.3062	LR: 0.025000
Training Epoch: 57 [7552/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 57 [7680/50000]	Loss: 0.3921	LR: 0.025000
Training Epoch: 57 [7808/50000]	Loss: 0.5589	LR: 0.025000
Training Epoch: 57 [7936/50000]	Loss: 0.3327	LR: 0.025000
Training Epoch: 57 [8064/50000]	Loss: 0.4234	LR: 0.025000
Training Epoch: 57 [8192/50000]	Loss: 0.5623	LR: 0.025000
Training Epoch: 57 [8320/50000]	Loss: 0.4984	LR: 0.025000
Training Epoch: 57 [8448/50000]	Loss: 0.4682	LR: 0.025000
Training Epoch: 57 [8576/50000]	Loss: 0.4275	LR: 0.025000
Training Epoch: 57 [8704/50000]	Loss: 0.5497	LR: 0.025000
Training Epoch: 57 [8832/50000]	Loss: 0.5270	LR: 0.025000
Training Epoch: 57 [8960/50000]	Loss: 0.4574	LR: 0.025000
Training Epoch: 57 [9088/50000]	Loss: 0.4739	LR: 0.025000
Training Epoch: 57 [9216/50000]	Loss: 0.4070	LR: 0.025000
Training Epoch: 57 [9344/50000]	Loss: 0.3795	LR: 0.025000
Training Epoch: 57 [9472/50000]	Loss: 0.4394	LR: 0.025000
Training Epoch: 57 [9600/50000]	Loss: 0.5011	LR: 0.025000
Training Epoch: 57 [9728/50000]	Loss: 0.3544	LR: 0.025000
Training Epoch: 57 [9856/50000]	Loss: 0.3591	LR: 0.025000
Training Epoch: 57 [9984/50000]	Loss: 0.4972	LR: 0.025000
Training Epoch: 57 [10112/50000]	Loss: 0.4090	LR: 0.025000
Training Epoch: 57 [10240/50000]	Loss: 0.4024	LR: 0.025000
Training Epoch: 57 [10368/50000]	Loss: 0.5025	LR: 0.025000
Training Epoch: 57 [10496/50000]	Loss: 0.4400	LR: 0.025000
Training Epoch: 57 [10624/50000]	Loss: 0.4046	LR: 0.025000
Training Epoch: 57 [10752/50000]	Loss: 0.4118	LR: 0.025000
Training Epoch: 57 [10880/50000]	Loss: 0.3925	LR: 0.025000
Training Epoch: 57 [11008/50000]	Loss: 0.5010	LR: 0.025000
Training Epoch: 57 [11136/50000]	Loss: 0.4849	LR: 0.025000
Training Epoch: 57 [11264/50000]	Loss: 0.3142	LR: 0.025000
Training Epoch: 57 [11392/50000]	Loss: 0.5700	LR: 0.025000
Training Epoch: 57 [11520/50000]	Loss: 0.5608	LR: 0.025000
Training Epoch: 57 [11648/50000]	Loss: 0.4162	LR: 0.025000
Training Epoch: 57 [11776/50000]	Loss: 0.4617	LR: 0.025000
Training Epoch: 57 [11904/50000]	Loss: 0.4157	LR: 0.025000
Training Epoch: 57 [12032/50000]	Loss: 0.4603	LR: 0.025000
Training Epoch: 57 [12160/50000]	Loss: 0.4903	LR: 0.025000
Training Epoch: 57 [12288/50000]	Loss: 0.5099	LR: 0.025000
Training Epoch: 57 [12416/50000]	Loss: 0.4233	LR: 0.025000
Training Epoch: 57 [12544/50000]	Loss: 0.4838	LR: 0.025000
Training Epoch: 57 [12672/50000]	Loss: 0.5826	LR: 0.025000
Training Epoch: 57 [12800/50000]	Loss: 0.3324	LR: 0.025000
Training Epoch: 57 [12928/50000]	Loss: 0.3810	LR: 0.025000
Training Epoch: 57 [13056/50000]	Loss: 0.4336	LR: 0.025000
Training Epoch: 57 [13184/50000]	Loss: 0.4892	LR: 0.025000
Training Epoch: 57 [13312/50000]	Loss: 0.4938	LR: 0.025000
Training Epoch: 57 [13440/50000]	Loss: 0.4666	LR: 0.025000
Training Epoch: 57 [13568/50000]	Loss: 0.5709	LR: 0.025000
Training Epoch: 57 [13696/50000]	Loss: 0.4696	LR: 0.025000
Training Epoch: 57 [13824/50000]	Loss: 0.4461	LR: 0.025000
Training Epoch: 57 [13952/50000]	Loss: 0.4344	LR: 0.025000
Training Epoch: 57 [14080/50000]	Loss: 0.4440	LR: 0.025000
Training Epoch: 57 [14208/50000]	Loss: 0.6036	LR: 0.025000
Training Epoch: 57 [14336/50000]	Loss: 0.3917	LR: 0.025000
Training Epoch: 57 [14464/50000]	Loss: 0.3739	LR: 0.025000
Training Epoch: 57 [14592/50000]	Loss: 0.3869	LR: 0.025000
Training Epoch: 57 [14720/50000]	Loss: 0.4296	LR: 0.025000
Training Epoch: 57 [14848/50000]	Loss: 0.3357	LR: 0.025000
Training Epoch: 57 [14976/50000]	Loss: 0.3921	LR: 0.025000
Training Epoch: 57 [15104/50000]	Loss: 0.5747	LR: 0.025000
Training Epoch: 57 [15232/50000]	Loss: 0.3409	LR: 0.025000
Training Epoch: 57 [15360/50000]	Loss: 0.3973	LR: 0.025000
Training Epoch: 57 [15488/50000]	Loss: 0.5383	LR: 0.025000
Training Epoch: 57 [15616/50000]	Loss: 0.4838	LR: 0.025000
Training Epoch: 57 [15744/50000]	Loss: 0.3984	LR: 0.025000
Training Epoch: 57 [15872/50000]	Loss: 0.4812	LR: 0.025000
Training Epoch: 57 [16000/50000]	Loss: 0.5291	LR: 0.025000
Training Epoch: 57 [16128/50000]	Loss: 0.6002	LR: 0.025000
Training Epoch: 57 [16256/50000]	Loss: 0.4154	LR: 0.025000
Training Epoch: 57 [16384/50000]	Loss: 0.5540	LR: 0.025000
Training Epoch: 57 [16512/50000]	Loss: 0.6731	LR: 0.025000
Training Epoch: 57 [16640/50000]	Loss: 0.5894	LR: 0.025000
Training Epoch: 57 [16768/50000]	Loss: 0.7328	LR: 0.025000
Training Epoch: 57 [16896/50000]	Loss: 0.4365	LR: 0.025000
Training Epoch: 57 [17024/50000]	Loss: 0.5414	LR: 0.025000
Training Epoch: 57 [17152/50000]	Loss: 0.4296	LR: 0.025000
Training Epoch: 57 [17280/50000]	Loss: 0.4247	LR: 0.025000
Training Epoch: 57 [17408/50000]	Loss: 0.4528	LR: 0.025000
Training Epoch: 57 [17536/50000]	Loss: 0.4781	LR: 0.025000
Training Epoch: 57 [17664/50000]	Loss: 0.3781	LR: 0.025000
Training Epoch: 57 [17792/50000]	Loss: 0.4828	LR: 0.025000
Training Epoch: 57 [17920/50000]	Loss: 0.4463	LR: 0.025000
Training Epoch: 57 [18048/50000]	Loss: 0.5157	LR: 0.025000
Training Epoch: 57 [18176/50000]	Loss: 0.5278	LR: 0.025000
Training Epoch: 57 [18304/50000]	Loss: 0.3874	LR: 0.025000
Training Epoch: 57 [18432/50000]	Loss: 0.4823	LR: 0.025000
Training Epoch: 57 [18560/50000]	Loss: 0.3556	LR: 0.025000
Training Epoch: 57 [18688/50000]	Loss: 0.4853	LR: 0.025000
Training Epoch: 57 [18816/50000]	Loss: 0.5028	LR: 0.025000
Training Epoch: 57 [18944/50000]	Loss: 0.6503	LR: 0.025000
Training Epoch: 57 [19072/50000]	Loss: 0.5202	LR: 0.025000
Training Epoch: 57 [19200/50000]	Loss: 0.5095	LR: 0.025000
Training Epoch: 57 [19328/50000]	Loss: 0.5625	LR: 0.025000
Training Epoch: 57 [19456/50000]	Loss: 0.5392	LR: 0.025000
Training Epoch: 57 [19584/50000]	Loss: 0.4201	LR: 0.025000
Training Epoch: 57 [19712/50000]	Loss: 0.4809	LR: 0.025000
Training Epoch: 57 [19840/50000]	Loss: 0.6318	LR: 0.025000
Training Epoch: 57 [19968/50000]	Loss: 0.6710	LR: 0.025000
Training Epoch: 57 [20096/50000]	Loss: 0.4109	LR: 0.025000
Training Epoch: 57 [20224/50000]	Loss: 0.5637	LR: 0.025000
Training Epoch: 57 [20352/50000]	Loss: 0.5325	LR: 0.025000
Training Epoch: 57 [20480/50000]	Loss: 0.5719	LR: 0.025000
Training Epoch: 57 [20608/50000]	Loss: 0.6017	LR: 0.025000
Training Epoch: 57 [20736/50000]	Loss: 0.4325	LR: 0.025000
Training Epoch: 57 [20864/50000]	Loss: 0.4633	LR: 0.025000
Training Epoch: 57 [20992/50000]	Loss: 0.5535	LR: 0.025000
Training Epoch: 57 [21120/50000]	Loss: 0.4650	LR: 0.025000
Training Epoch: 57 [21248/50000]	Loss: 0.6942	LR: 0.025000
Training Epoch: 57 [21376/50000]	Loss: 0.4529	LR: 0.025000
Training Epoch: 57 [21504/50000]	Loss: 0.4119	LR: 0.025000
Training Epoch: 57 [21632/50000]	Loss: 0.5878	LR: 0.025000
Training Epoch: 57 [21760/50000]	Loss: 0.5754	LR: 0.025000
Training Epoch: 57 [21888/50000]	Loss: 0.5046	LR: 0.025000
Training Epoch: 57 [22016/50000]	Loss: 0.6430	LR: 0.025000
Training Epoch: 57 [22144/50000]	Loss: 0.6131	LR: 0.025000
Training Epoch: 57 [22272/50000]	Loss: 0.7288	LR: 0.025000
Training Epoch: 57 [22400/50000]	Loss: 0.5003	LR: 0.025000
Training Epoch: 57 [22528/50000]	Loss: 0.4228	LR: 0.025000
Training Epoch: 57 [22656/50000]	Loss: 0.4814	LR: 0.025000
Training Epoch: 57 [22784/50000]	Loss: 0.5086	LR: 0.025000
Training Epoch: 57 [22912/50000]	Loss: 0.6237	LR: 0.025000
Training Epoch: 57 [23040/50000]	Loss: 0.5666	LR: 0.025000
Training Epoch: 57 [23168/50000]	Loss: 0.5009	LR: 0.025000
Training Epoch: 57 [23296/50000]	Loss: 0.4016	LR: 0.025000
Training Epoch: 57 [23424/50000]	Loss: 0.4951	LR: 0.025000
Training Epoch: 57 [23552/50000]	Loss: 0.5103	LR: 0.025000
Training Epoch: 57 [23680/50000]	Loss: 0.5624	LR: 0.025000
Training Epoch: 57 [23808/50000]	Loss: 0.5394	LR: 0.025000
Training Epoch: 57 [23936/50000]	Loss: 0.5106	LR: 0.025000
Training Epoch: 57 [24064/50000]	Loss: 0.5793	LR: 0.025000
Training Epoch: 57 [24192/50000]	Loss: 0.6011	LR: 0.025000
Training Epoch: 57 [24320/50000]	Loss: 0.5480	LR: 0.025000
Training Epoch: 57 [24448/50000]	Loss: 0.5061	LR: 0.025000
Training Epoch: 57 [24576/50000]	Loss: 0.4594	LR: 0.025000
Training Epoch: 57 [24704/50000]	Loss: 0.5443	LR: 0.025000
Training Epoch: 57 [24832/50000]	Loss: 0.5093	LR: 0.025000
Training Epoch: 57 [24960/50000]	Loss: 0.7856	LR: 0.025000
Training Epoch: 57 [25088/50000]	Loss: 0.5990	LR: 0.025000
Training Epoch: 57 [25216/50000]	Loss: 0.6309	LR: 0.025000
Training Epoch: 57 [25344/50000]	Loss: 0.5248	LR: 0.025000
Training Epoch: 57 [25472/50000]	Loss: 0.4907	LR: 0.025000
Training Epoch: 57 [25600/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 57 [25728/50000]	Loss: 0.6458	LR: 0.025000
Training Epoch: 57 [25856/50000]	Loss: 0.4513	LR: 0.025000
Training Epoch: 57 [25984/50000]	Loss: 0.4863	LR: 0.025000
Training Epoch: 57 [26112/50000]	Loss: 0.4907	LR: 0.025000
Training Epoch: 57 [26240/50000]	Loss: 0.6565	LR: 0.025000
Training Epoch: 57 [26368/50000]	Loss: 0.6240	LR: 0.025000
Training Epoch: 57 [26496/50000]	Loss: 0.5638	LR: 0.025000
Training Epoch: 57 [26624/50000]	Loss: 0.5926	LR: 0.025000
Training Epoch: 57 [26752/50000]	Loss: 0.4091	LR: 0.025000
Training Epoch: 57 [26880/50000]	Loss: 0.5129	LR: 0.025000
Training Epoch: 57 [27008/50000]	Loss: 0.5044	LR: 0.025000
Training Epoch: 57 [27136/50000]	Loss: 0.6405	LR: 0.025000
Training Epoch: 57 [27264/50000]	Loss: 0.6138	LR: 0.025000
Training Epoch: 57 [27392/50000]	Loss: 0.6656	LR: 0.025000
Training Epoch: 57 [27520/50000]	Loss: 0.5479	LR: 0.025000
Training Epoch: 57 [27648/50000]	Loss: 0.5320	LR: 0.025000
Training Epoch: 57 [27776/50000]	Loss: 0.5093	LR: 0.025000
Training Epoch: 57 [27904/50000]	Loss: 0.6703	LR: 0.025000
Training Epoch: 57 [28032/50000]	Loss: 0.5272	LR: 0.025000
Training Epoch: 57 [28160/50000]	Loss: 0.4341	LR: 0.025000
Training Epoch: 57 [28288/50000]	Loss: 0.6035	LR: 0.025000
Training Epoch: 57 [28416/50000]	Loss: 0.5764	LR: 0.025000
Training Epoch: 57 [28544/50000]	Loss: 0.5438	LR: 0.025000
Training Epoch: 57 [28672/50000]	Loss: 0.6013	LR: 0.025000
Training Epoch: 57 [28800/50000]	Loss: 0.5425	LR: 0.025000
Training Epoch: 57 [28928/50000]	Loss: 0.5557	LR: 0.025000
Training Epoch: 57 [29056/50000]	Loss: 0.5525	LR: 0.025000
Training Epoch: 57 [29184/50000]	Loss: 0.4781	LR: 0.025000
Training Epoch: 57 [29312/50000]	Loss: 0.4955	LR: 0.025000
Training Epoch: 57 [29440/50000]	Loss: 0.6067	LR: 0.025000
Training Epoch: 57 [29568/50000]	Loss: 0.6107	LR: 0.025000
Training Epoch: 57 [29696/50000]	Loss: 0.3902	LR: 0.025000
Training Epoch: 57 [29824/50000]	Loss: 0.6640	LR: 0.025000
Training Epoch: 57 [29952/50000]	Loss: 0.5431	LR: 0.025000
Training Epoch: 57 [30080/50000]	Loss: 0.4804	LR: 0.025000
Training Epoch: 57 [30208/50000]	Loss: 0.6593	LR: 0.025000
Training Epoch: 57 [30336/50000]	Loss: 0.5006	LR: 0.025000
Training Epoch: 57 [30464/50000]	Loss: 0.4938	LR: 0.025000
Training Epoch: 57 [30592/50000]	Loss: 0.5202	LR: 0.025000
Training Epoch: 57 [30720/50000]	Loss: 0.6252	LR: 0.025000
Training Epoch: 57 [30848/50000]	Loss: 0.4605	LR: 0.025000
Training Epoch: 57 [30976/50000]	Loss: 0.5888	LR: 0.025000
Training Epoch: 57 [31104/50000]	Loss: 0.5020	LR: 0.025000
Training Epoch: 57 [31232/50000]	Loss: 0.4662	LR: 0.025000
Training Epoch: 57 [31360/50000]	Loss: 0.4693	LR: 0.025000
Training Epoch: 57 [31488/50000]	Loss: 0.4321	LR: 0.025000
Training Epoch: 57 [31616/50000]	Loss: 0.4404	LR: 0.025000
Training Epoch: 57 [31744/50000]	Loss: 0.4788	LR: 0.025000
Training Epoch: 57 [31872/50000]	Loss: 0.6709	LR: 0.025000
Training Epoch: 57 [32000/50000]	Loss: 0.5017	LR: 0.025000
Training Epoch: 57 [32128/50000]	Loss: 0.4959	LR: 0.025000
Training Epoch: 57 [32256/50000]	Loss: 0.4318	LR: 0.025000
Training Epoch: 57 [32384/50000]	Loss: 0.5235	LR: 0.025000
Training Epoch: 57 [32512/50000]	Loss: 0.4422	LR: 0.025000
Training Epoch: 57 [32640/50000]	Loss: 0.4356	LR: 0.025000
Training Epoch: 57 [32768/50000]	Loss: 0.4482	LR: 0.025000
Training Epoch: 57 [32896/50000]	Loss: 0.7042	LR: 0.025000
Training Epoch: 57 [33024/50000]	Loss: 0.4323	LR: 0.025000
Training Epoch: 57 [33152/50000]	Loss: 0.5524	LR: 0.025000
Training Epoch: 57 [33280/50000]	Loss: 0.5420	LR: 0.025000
Training Epoch: 57 [33408/50000]	Loss: 0.5577	LR: 0.025000
Training Epoch: 57 [33536/50000]	Loss: 0.5365	LR: 0.025000
Training Epoch: 57 [33664/50000]	Loss: 0.5933	LR: 0.025000
Training Epoch: 57 [33792/50000]	Loss: 0.6028	LR: 0.025000
Training Epoch: 57 [33920/50000]	Loss: 0.4730	LR: 0.025000
Training Epoch: 57 [34048/50000]	Loss: 0.6075	LR: 0.025000
Training Epoch: 57 [34176/50000]	Loss: 0.6025	LR: 0.025000
Training Epoch: 57 [34304/50000]	Loss: 0.5939	LR: 0.025000
Training Epoch: 57 [34432/50000]	Loss: 0.4837	LR: 0.025000
Training Epoch: 57 [34560/50000]	Loss: 0.4071	LR: 0.025000
Training Epoch: 57 [34688/50000]	Loss: 0.5248	LR: 0.025000
Training Epoch: 57 [34816/50000]	Loss: 0.5685	LR: 0.025000
Training Epoch: 57 [34944/50000]	Loss: 0.5782	LR: 0.025000
Training Epoch: 57 [35072/50000]	Loss: 0.5175	LR: 0.025000
Training Epoch: 57 [35200/50000]	Loss: 0.6926	LR: 0.025000
Training Epoch: 57 [35328/50000]	Loss: 0.5719	LR: 0.025000
Training Epoch: 57 [35456/50000]	Loss: 0.5523	LR: 0.025000
Training Epoch: 57 [35584/50000]	Loss: 0.5462	LR: 0.025000
Training Epoch: 57 [35712/50000]	Loss: 0.6337	LR: 0.025000
Training Epoch: 57 [35840/50000]	Loss: 0.5938	LR: 0.025000
Training Epoch: 57 [35968/50000]	Loss: 0.6181	LR: 0.025000
Training Epoch: 57 [36096/50000]	Loss: 0.5141	LR: 0.025000
Training Epoch: 57 [36224/50000]	Loss: 0.5059	LR: 0.025000
Training Epoch: 57 [36352/50000]	Loss: 0.4139	LR: 0.025000
Training Epoch: 57 [36480/50000]	Loss: 0.3614	LR: 0.025000
Training Epoch: 57 [36608/50000]	Loss: 0.3901	LR: 0.025000
Training Epoch: 57 [36736/50000]	Loss: 0.5346	LR: 0.025000
Training Epoch: 57 [36864/50000]	Loss: 0.6323	LR: 0.025000
Training Epoch: 57 [36992/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 57 [37120/50000]	Loss: 0.5470	LR: 0.025000
Training Epoch: 57 [37248/50000]	Loss: 0.4923	LR: 0.025000
Training Epoch: 57 [37376/50000]	Loss: 0.5565	LR: 0.025000
Training Epoch: 57 [37504/50000]	Loss: 0.5145	LR: 0.025000
Training Epoch: 57 [37632/50000]	Loss: 0.5721	LR: 0.025000
Training Epoch: 57 [37760/50000]	Loss: 0.4687	LR: 0.025000
Training Epoch: 57 [37888/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 57 [38016/50000]	Loss: 0.5181	LR: 0.025000
Training Epoch: 57 [38144/50000]	Loss: 0.6430	LR: 0.025000
Training Epoch: 57 [38272/50000]	Loss: 0.5903	LR: 0.025000
Training Epoch: 57 [38400/50000]	Loss: 0.6694	LR: 0.025000
Training Epoch: 57 [38528/50000]	Loss: 0.5419	LR: 0.025000
Training Epoch: 57 [38656/50000]	Loss: 0.6496	LR: 0.025000
Training Epoch: 57 [38784/50000]	Loss: 0.5450	LR: 0.025000
Training Epoch: 57 [38912/50000]	Loss: 0.5900	LR: 0.025000
Training Epoch: 57 [39040/50000]	Loss: 0.4769	LR: 0.025000
Training Epoch: 57 [39168/50000]	Loss: 0.5241	LR: 0.025000
Training Epoch: 57 [39296/50000]	Loss: 0.6099	LR: 0.025000
Training Epoch: 57 [39424/50000]	Loss: 0.6078	LR: 0.025000
Training Epoch: 57 [39552/50000]	Loss: 0.4161	LR: 0.025000
Training Epoch: 57 [39680/50000]	Loss: 0.5083	LR: 0.025000
Training Epoch: 57 [39808/50000]	Loss: 0.5393	LR: 0.025000
Training Epoch: 57 [39936/50000]	Loss: 0.5879	LR: 0.025000
Training Epoch: 57 [40064/50000]	Loss: 0.5946	LR: 0.025000
Training Epoch: 57 [40192/50000]	Loss: 0.5327	LR: 0.025000
Training Epoch: 57 [40320/50000]	Loss: 0.5813	LR: 0.025000
Training Epoch: 57 [40448/50000]	Loss: 0.4556	LR: 0.025000
Training Epoch: 57 [40576/50000]	Loss: 0.5297	LR: 0.025000
Training Epoch: 57 [40704/50000]	Loss: 0.3758	LR: 0.025000
Training Epoch: 57 [40832/50000]	Loss: 0.6206	LR: 0.025000
Training Epoch: 57 [40960/50000]	Loss: 0.6687	LR: 0.025000
Training Epoch: 57 [41088/50000]	Loss: 0.4034	LR: 0.025000
Training Epoch: 57 [41216/50000]	Loss: 0.4818	LR: 0.025000
Training Epoch: 57 [41344/50000]	Loss: 0.5069	LR: 0.025000
Training Epoch: 57 [41472/50000]	Loss: 0.6092	LR: 0.025000
Training Epoch: 57 [41600/50000]	Loss: 0.6053	LR: 0.025000
Training Epoch: 57 [41728/50000]	Loss: 0.7526	LR: 0.025000
Training Epoch: 57 [41856/50000]	Loss: 0.6850	LR: 0.025000
Training Epoch: 57 [41984/50000]	Loss: 0.5332	LR: 0.025000
Training Epoch: 57 [42112/50000]	Loss: 0.6067	LR: 0.025000
Training Epoch: 57 [42240/50000]	Loss: 0.5246	LR: 0.025000
Training Epoch: 57 [42368/50000]	Loss: 0.6426	LR: 0.025000
Training Epoch: 57 [42496/50000]	Loss: 0.6542	LR: 0.025000
Training Epoch: 57 [42624/50000]	Loss: 0.6504	LR: 0.025000
Training Epoch: 57 [42752/50000]	Loss: 0.4613	LR: 0.025000
Training Epoch: 57 [42880/50000]	Loss: 0.5428	LR: 0.025000
Training Epoch: 57 [43008/50000]	Loss: 0.6157	LR: 0.025000
Training Epoch: 57 [43136/50000]	Loss: 0.5974	LR: 0.025000
Training Epoch: 57 [43264/50000]	Loss: 0.4267	LR: 0.025000
Training Epoch: 57 [43392/50000]	Loss: 0.6840	LR: 0.025000
Training Epoch: 57 [43520/50000]	Loss: 0.5930	LR: 0.025000
Training Epoch: 57 [43648/50000]	Loss: 0.5699	LR: 0.025000
Training Epoch: 57 [43776/50000]	Loss: 0.4379	LR: 0.025000
Training Epoch: 57 [43904/50000]	Loss: 0.6195	LR: 0.025000
Training Epoch: 57 [44032/50000]	Loss: 0.6828	LR: 0.025000
Training Epoch: 57 [44160/50000]	Loss: 0.5473	LR: 0.025000
Training Epoch: 57 [44288/50000]	Loss: 0.4921	LR: 0.025000
Training Epoch: 57 [44416/50000]	Loss: 0.5693	LR: 0.025000
Training Epoch: 57 [44544/50000]	Loss: 0.5830	LR: 0.025000
Training Epoch: 57 [44672/50000]	Loss: 0.5612	LR: 0.025000
Training Epoch: 57 [44800/50000]	Loss: 0.6592	LR: 0.025000
Training Epoch: 57 [44928/50000]	Loss: 0.6656	LR: 0.025000
Training Epoch: 57 [45056/50000]	Loss: 0.4676	LR: 0.025000
Training Epoch: 57 [45184/50000]	Loss: 0.5702	LR: 0.025000
Training Epoch: 57 [45312/50000]	Loss: 0.6069	LR: 0.025000
Training Epoch: 57 [45440/50000]	Loss: 0.5660	LR: 0.025000
Training Epoch: 57 [45568/50000]	Loss: 0.5189	LR: 0.025000
Training Epoch: 57 [45696/50000]	Loss: 0.5930	LR: 0.025000
Training Epoch: 57 [45824/50000]	Loss: 0.7792	LR: 0.025000
Training Epoch: 57 [45952/50000]	Loss: 0.6001	LR: 0.025000
Training Epoch: 57 [46080/50000]	Loss: 0.4817	LR: 0.025000
Training Epoch: 57 [46208/50000]	Loss: 0.6309	LR: 0.025000
Training Epoch: 57 [46336/50000]	Loss: 0.5066	LR: 0.025000
Training Epoch: 57 [46464/50000]	Loss: 0.6027	LR: 0.025000
Training Epoch: 57 [46592/50000]	Loss: 0.5989	LR: 0.025000
Training Epoch: 57 [46720/50000]	Loss: 0.5298	LR: 0.025000
Training Epoch: 57 [46848/50000]	Loss: 0.5606	LR: 0.025000
Training Epoch: 57 [46976/50000]	Loss: 0.6060	LR: 0.025000
Training Epoch: 57 [47104/50000]	Loss: 0.5757	LR: 0.025000
Training Epoch: 57 [47232/50000]	Loss: 0.7131	LR: 0.025000
Training Epoch: 57 [47360/50000]	Loss: 0.4716	LR: 0.025000
Training Epoch: 57 [47488/50000]	Loss: 0.5107	LR: 0.025000
Training Epoch: 57 [47616/50000]	Loss: 0.6052	LR: 0.025000
Training Epoch: 57 [47744/50000]	Loss: 0.5389	LR: 0.025000
Training Epoch: 57 [47872/50000]	Loss: 0.3985	LR: 0.025000
Training Epoch: 57 [48000/50000]	Loss: 0.5115	LR: 0.025000
Training Epoch: 57 [48128/50000]	Loss: 0.6376	LR: 0.025000
Training Epoch: 57 [48256/50000]	Loss: 0.5071	LR: 0.025000
Training Epoch: 57 [48384/50000]	Loss: 0.5905	LR: 0.025000
Training Epoch: 57 [48512/50000]	Loss: 0.6293	LR: 0.025000
Training Epoch: 57 [48640/50000]	Loss: 0.5534	LR: 0.025000
Training Epoch: 57 [48768/50000]	Loss: 0.6406	LR: 0.025000
Training Epoch: 57 [48896/50000]	Loss: 0.7363	LR: 0.025000
Training Epoch: 57 [49024/50000]	Loss: 0.5385	LR: 0.025000
Training Epoch: 57 [49152/50000]	Loss: 0.6717	LR: 0.025000
Training Epoch: 57 [49280/50000]	Loss: 0.6024	LR: 0.025000
Training Epoch: 57 [49408/50000]	Loss: 0.7976	LR: 0.025000
Training Epoch: 57 [49536/50000]	Loss: 0.5805	LR: 0.025000
Training Epoch: 57 [49664/50000]	Loss: 0.4293	LR: 0.025000
Training Epoch: 57 [49792/50000]	Loss: 0.5826	LR: 0.025000
Training Epoch: 57 [49920/50000]	Loss: 0.5905	LR: 0.025000
Training Epoch: 57 [50000/50000]	Loss: 0.4952	LR: 0.025000
epoch 57 training time consumed: 53.96s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  209193 GB |  209193 GB |
|       from large pool |  123392 KB |    1034 MB |  208987 GB |  208987 GB |
|       from small pool |   10798 KB |      13 MB |     206 GB |     206 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  209193 GB |  209193 GB |
|       from large pool |  123392 KB |    1034 MB |  208987 GB |  208987 GB |
|       from small pool |   10798 KB |      13 MB |     206 GB |     206 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   92058 GB |   92058 GB |
|       from large pool |  155136 KB |  433088 KB |   91830 GB |   91830 GB |
|       from small pool |    1490 KB |    3494 KB |     227 GB |     227 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    8072 K  |    8072 K  |
|       from large pool |      24    |      65    |    4213 K  |    4213 K  |
|       from small pool |     231    |     274    |    3858 K  |    3858 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    8072 K  |    8072 K  |
|       from large pool |      24    |      65    |    4213 K  |    4213 K  |
|       from small pool |     231    |     274    |    3858 K  |    3858 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    3989 K  |    3989 K  |
|       from large pool |       9    |      14    |    2039 K  |    2039 K  |
|       from small pool |      12    |      16    |    1949 K  |    1949 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 57, Average loss: 0.0104, Accuracy: 0.6745, Time consumed:3.46s

Training Epoch: 58 [128/50000]	Loss: 0.5019	LR: 0.025000
Training Epoch: 58 [256/50000]	Loss: 0.4780	LR: 0.025000
Training Epoch: 58 [384/50000]	Loss: 0.5088	LR: 0.025000
Training Epoch: 58 [512/50000]	Loss: 0.4652	LR: 0.025000
Training Epoch: 58 [640/50000]	Loss: 0.4205	LR: 0.025000
Training Epoch: 58 [768/50000]	Loss: 0.5094	LR: 0.025000
Training Epoch: 58 [896/50000]	Loss: 0.3934	LR: 0.025000
Training Epoch: 58 [1024/50000]	Loss: 0.3984	LR: 0.025000
Training Epoch: 58 [1152/50000]	Loss: 0.4822	LR: 0.025000
Training Epoch: 58 [1280/50000]	Loss: 0.3980	LR: 0.025000
Training Epoch: 58 [1408/50000]	Loss: 0.4581	LR: 0.025000
Training Epoch: 58 [1536/50000]	Loss: 0.4982	LR: 0.025000
Training Epoch: 58 [1664/50000]	Loss: 0.3690	LR: 0.025000
Training Epoch: 58 [1792/50000]	Loss: 0.4279	LR: 0.025000
Training Epoch: 58 [1920/50000]	Loss: 0.4269	LR: 0.025000
Training Epoch: 58 [2048/50000]	Loss: 0.3797	LR: 0.025000
Training Epoch: 58 [2176/50000]	Loss: 0.3992	LR: 0.025000
Training Epoch: 58 [2304/50000]	Loss: 0.4234	LR: 0.025000
Training Epoch: 58 [2432/50000]	Loss: 0.4518	LR: 0.025000
Training Epoch: 58 [2560/50000]	Loss: 0.4673	LR: 0.025000
Training Epoch: 58 [2688/50000]	Loss: 0.4621	LR: 0.025000
Training Epoch: 58 [2816/50000]	Loss: 0.5271	LR: 0.025000
Training Epoch: 58 [2944/50000]	Loss: 0.5115	LR: 0.025000
Training Epoch: 58 [3072/50000]	Loss: 0.5843	LR: 0.025000
Training Epoch: 58 [3200/50000]	Loss: 0.3859	LR: 0.025000
Training Epoch: 58 [3328/50000]	Loss: 0.3493	LR: 0.025000
Training Epoch: 58 [3456/50000]	Loss: 0.4758	LR: 0.025000
Training Epoch: 58 [3584/50000]	Loss: 0.4985	LR: 0.025000
Training Epoch: 58 [3712/50000]	Loss: 0.5205	LR: 0.025000
Training Epoch: 58 [3840/50000]	Loss: 0.2663	LR: 0.025000
Training Epoch: 58 [3968/50000]	Loss: 0.4748	LR: 0.025000
Training Epoch: 58 [4096/50000]	Loss: 0.4520	LR: 0.025000
Training Epoch: 58 [4224/50000]	Loss: 0.3422	LR: 0.025000
Training Epoch: 58 [4352/50000]	Loss: 0.4522	LR: 0.025000
Training Epoch: 58 [4480/50000]	Loss: 0.4163	LR: 0.025000
Training Epoch: 58 [4608/50000]	Loss: 0.3910	LR: 0.025000
Training Epoch: 58 [4736/50000]	Loss: 0.4734	LR: 0.025000
Training Epoch: 58 [4864/50000]	Loss: 0.3604	LR: 0.025000
Training Epoch: 58 [4992/50000]	Loss: 0.4632	LR: 0.025000
Training Epoch: 58 [5120/50000]	Loss: 0.5269	LR: 0.025000
Training Epoch: 58 [5248/50000]	Loss: 0.4354	LR: 0.025000
Training Epoch: 58 [5376/50000]	Loss: 0.4038	LR: 0.025000
Training Epoch: 58 [5504/50000]	Loss: 0.5531	LR: 0.025000
Training Epoch: 58 [5632/50000]	Loss: 0.4862	LR: 0.025000
Training Epoch: 58 [5760/50000]	Loss: 0.5226	LR: 0.025000
Training Epoch: 58 [5888/50000]	Loss: 0.4078	LR: 0.025000
Training Epoch: 58 [6016/50000]	Loss: 0.5693	LR: 0.025000
Training Epoch: 58 [6144/50000]	Loss: 0.6319	LR: 0.025000
Training Epoch: 58 [6272/50000]	Loss: 0.4486	LR: 0.025000
Training Epoch: 58 [6400/50000]	Loss: 0.4290	LR: 0.025000
Training Epoch: 58 [6528/50000]	Loss: 0.3472	LR: 0.025000
Training Epoch: 58 [6656/50000]	Loss: 0.4369	LR: 0.025000
Training Epoch: 58 [6784/50000]	Loss: 0.4674	LR: 0.025000
Training Epoch: 58 [6912/50000]	Loss: 0.5273	LR: 0.025000
Training Epoch: 58 [7040/50000]	Loss: 0.5351	LR: 0.025000
Training Epoch: 58 [7168/50000]	Loss: 0.4549	LR: 0.025000
Training Epoch: 58 [7296/50000]	Loss: 0.4332	LR: 0.025000
Training Epoch: 58 [7424/50000]	Loss: 0.5725	LR: 0.025000
Training Epoch: 58 [7552/50000]	Loss: 0.4838	LR: 0.025000
Training Epoch: 58 [7680/50000]	Loss: 0.3451	LR: 0.025000
Training Epoch: 58 [7808/50000]	Loss: 0.4851	LR: 0.025000
Training Epoch: 58 [7936/50000]	Loss: 0.4841	LR: 0.025000
Training Epoch: 58 [8064/50000]	Loss: 0.5535	LR: 0.025000
Training Epoch: 58 [8192/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 58 [8320/50000]	Loss: 0.3510	LR: 0.025000
Training Epoch: 58 [8448/50000]	Loss: 0.5171	LR: 0.025000
Training Epoch: 58 [8576/50000]	Loss: 0.4720	LR: 0.025000
Training Epoch: 58 [8704/50000]	Loss: 0.4098	LR: 0.025000
Training Epoch: 58 [8832/50000]	Loss: 0.4365	LR: 0.025000
Training Epoch: 58 [8960/50000]	Loss: 0.3557	LR: 0.025000
Training Epoch: 58 [9088/50000]	Loss: 0.3493	LR: 0.025000
Training Epoch: 58 [9216/50000]	Loss: 0.4938	LR: 0.025000
Training Epoch: 58 [9344/50000]	Loss: 0.3359	LR: 0.025000
Training Epoch: 58 [9472/50000]	Loss: 0.4019	LR: 0.025000
Training Epoch: 58 [9600/50000]	Loss: 0.3497	LR: 0.025000
Training Epoch: 58 [9728/50000]	Loss: 0.4273	LR: 0.025000
Training Epoch: 58 [9856/50000]	Loss: 0.4622	LR: 0.025000
Training Epoch: 58 [9984/50000]	Loss: 0.4544	LR: 0.025000
Training Epoch: 58 [10112/50000]	Loss: 0.3769	LR: 0.025000
Training Epoch: 58 [10240/50000]	Loss: 0.5287	LR: 0.025000
Training Epoch: 58 [10368/50000]	Loss: 0.5021	LR: 0.025000
Training Epoch: 58 [10496/50000]	Loss: 0.4245	LR: 0.025000
Training Epoch: 58 [10624/50000]	Loss: 0.4274	LR: 0.025000
Training Epoch: 58 [10752/50000]	Loss: 0.4338	LR: 0.025000
Training Epoch: 58 [10880/50000]	Loss: 0.3106	LR: 0.025000
Training Epoch: 58 [11008/50000]	Loss: 0.4051	LR: 0.025000
Training Epoch: 58 [11136/50000]	Loss: 0.5626	LR: 0.025000
Training Epoch: 58 [11264/50000]	Loss: 0.4733	LR: 0.025000
Training Epoch: 58 [11392/50000]	Loss: 0.4423	LR: 0.025000
Training Epoch: 58 [11520/50000]	Loss: 0.5646	LR: 0.025000
Training Epoch: 58 [11648/50000]	Loss: 0.4423	LR: 0.025000
Training Epoch: 58 [11776/50000]	Loss: 0.4398	LR: 0.025000
Training Epoch: 58 [11904/50000]	Loss: 0.5268	LR: 0.025000
Training Epoch: 58 [12032/50000]	Loss: 0.5333	LR: 0.025000
Training Epoch: 58 [12160/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 58 [12288/50000]	Loss: 0.5159	LR: 0.025000
Training Epoch: 58 [12416/50000]	Loss: 0.3967	LR: 0.025000
Training Epoch: 58 [12544/50000]	Loss: 0.4592	LR: 0.025000
Training Epoch: 58 [12672/50000]	Loss: 0.5469	LR: 0.025000
Training Epoch: 58 [12800/50000]	Loss: 0.2793	LR: 0.025000
Training Epoch: 58 [12928/50000]	Loss: 0.4929	LR: 0.025000
Training Epoch: 58 [13056/50000]	Loss: 0.3389	LR: 0.025000
Training Epoch: 58 [13184/50000]	Loss: 0.4048	LR: 0.025000
Training Epoch: 58 [13312/50000]	Loss: 0.3997	LR: 0.025000
Training Epoch: 58 [13440/50000]	Loss: 0.4445	LR: 0.025000
Training Epoch: 58 [13568/50000]	Loss: 0.6592	LR: 0.025000
Training Epoch: 58 [13696/50000]	Loss: 0.4200	LR: 0.025000
Training Epoch: 58 [13824/50000]	Loss: 0.4702	LR: 0.025000
Training Epoch: 58 [13952/50000]	Loss: 0.5911	LR: 0.025000
Training Epoch: 58 [14080/50000]	Loss: 0.4974	LR: 0.025000
Training Epoch: 58 [14208/50000]	Loss: 0.4003	LR: 0.025000
Training Epoch: 58 [14336/50000]	Loss: 0.5339	LR: 0.025000
Training Epoch: 58 [14464/50000]	Loss: 0.4244	LR: 0.025000
Training Epoch: 58 [14592/50000]	Loss: 0.5359	LR: 0.025000
Training Epoch: 58 [14720/50000]	Loss: 0.4780	LR: 0.025000
Training Epoch: 58 [14848/50000]	Loss: 0.4965	LR: 0.025000
Training Epoch: 58 [14976/50000]	Loss: 0.4511	LR: 0.025000
Training Epoch: 58 [15104/50000]	Loss: 0.4512	LR: 0.025000
Training Epoch: 58 [15232/50000]	Loss: 0.5029	LR: 0.025000
Training Epoch: 58 [15360/50000]	Loss: 0.5107	LR: 0.025000
Training Epoch: 58 [15488/50000]	Loss: 0.4254	LR: 0.025000
Training Epoch: 58 [15616/50000]	Loss: 0.4468	LR: 0.025000
Training Epoch: 58 [15744/50000]	Loss: 0.6453	LR: 0.025000
Training Epoch: 58 [15872/50000]	Loss: 0.5731	LR: 0.025000
Training Epoch: 58 [16000/50000]	Loss: 0.5426	LR: 0.025000
Training Epoch: 58 [16128/50000]	Loss: 0.5413	LR: 0.025000
Training Epoch: 58 [16256/50000]	Loss: 0.4156	LR: 0.025000
Training Epoch: 58 [16384/50000]	Loss: 0.5390	LR: 0.025000
Training Epoch: 58 [16512/50000]	Loss: 0.5254	LR: 0.025000
Training Epoch: 58 [16640/50000]	Loss: 0.4065	LR: 0.025000
Training Epoch: 58 [16768/50000]	Loss: 0.4809	LR: 0.025000
Training Epoch: 58 [16896/50000]	Loss: 0.5797	LR: 0.025000
Training Epoch: 58 [17024/50000]	Loss: 0.4447	LR: 0.025000
Training Epoch: 58 [17152/50000]	Loss: 0.5010	LR: 0.025000
Training Epoch: 58 [17280/50000]	Loss: 0.3330	LR: 0.025000
Training Epoch: 58 [17408/50000]	Loss: 0.4965	LR: 0.025000
Training Epoch: 58 [17536/50000]	Loss: 0.5340	LR: 0.025000
Training Epoch: 58 [17664/50000]	Loss: 0.4188	LR: 0.025000
Training Epoch: 58 [17792/50000]	Loss: 0.4546	LR: 0.025000
Training Epoch: 58 [17920/50000]	Loss: 0.5093	LR: 0.025000
Training Epoch: 58 [18048/50000]	Loss: 0.5042	LR: 0.025000
Training Epoch: 58 [18176/50000]	Loss: 0.4962	LR: 0.025000
Training Epoch: 58 [18304/50000]	Loss: 0.5495	LR: 0.025000
Training Epoch: 58 [18432/50000]	Loss: 0.5143	LR: 0.025000
Training Epoch: 58 [18560/50000]	Loss: 0.3736	LR: 0.025000
Training Epoch: 58 [18688/50000]	Loss: 0.5293	LR: 0.025000
Training Epoch: 58 [18816/50000]	Loss: 0.4300	LR: 0.025000
Training Epoch: 58 [18944/50000]	Loss: 0.5046	LR: 0.025000
Training Epoch: 58 [19072/50000]	Loss: 0.6890	LR: 0.025000
Training Epoch: 58 [19200/50000]	Loss: 0.5772	LR: 0.025000
Training Epoch: 58 [19328/50000]	Loss: 0.5901	LR: 0.025000
Training Epoch: 58 [19456/50000]	Loss: 0.6345	LR: 0.025000
Training Epoch: 58 [19584/50000]	Loss: 0.4855	LR: 0.025000
Training Epoch: 58 [19712/50000]	Loss: 0.4426	LR: 0.025000
Training Epoch: 58 [19840/50000]	Loss: 0.4628	LR: 0.025000
Training Epoch: 58 [19968/50000]	Loss: 0.6036	LR: 0.025000
Training Epoch: 58 [20096/50000]	Loss: 0.5159	LR: 0.025000
Training Epoch: 58 [20224/50000]	Loss: 0.7247	LR: 0.025000
Training Epoch: 58 [20352/50000]	Loss: 0.6999	LR: 0.025000
Training Epoch: 58 [20480/50000]	Loss: 0.4233	LR: 0.025000
Training Epoch: 58 [20608/50000]	Loss: 0.5696	LR: 0.025000
Training Epoch: 58 [20736/50000]	Loss: 0.4496	LR: 0.025000
Training Epoch: 58 [20864/50000]	Loss: 0.5022	LR: 0.025000
Training Epoch: 58 [20992/50000]	Loss: 0.6368	LR: 0.025000
Training Epoch: 58 [21120/50000]	Loss: 0.6613	LR: 0.025000
Training Epoch: 58 [21248/50000]	Loss: 0.4163	LR: 0.025000
Training Epoch: 58 [21376/50000]	Loss: 0.5055	LR: 0.025000
Training Epoch: 58 [21504/50000]	Loss: 0.4848	LR: 0.025000
Training Epoch: 58 [21632/50000]	Loss: 0.4458	LR: 0.025000
Training Epoch: 58 [21760/50000]	Loss: 0.5850	LR: 0.025000
Training Epoch: 58 [21888/50000]	Loss: 0.5508	LR: 0.025000
Training Epoch: 58 [22016/50000]	Loss: 0.5510	LR: 0.025000
Training Epoch: 58 [22144/50000]	Loss: 0.4138	LR: 0.025000
Training Epoch: 58 [22272/50000]	Loss: 0.4969	LR: 0.025000
Training Epoch: 58 [22400/50000]	Loss: 0.4919	LR: 0.025000
Training Epoch: 58 [22528/50000]	Loss: 0.4572	LR: 0.025000
Training Epoch: 58 [22656/50000]	Loss: 0.5901	LR: 0.025000
Training Epoch: 58 [22784/50000]	Loss: 0.5356	LR: 0.025000
Training Epoch: 58 [22912/50000]	Loss: 0.5105	LR: 0.025000
Training Epoch: 58 [23040/50000]	Loss: 0.4949	LR: 0.025000
Training Epoch: 58 [23168/50000]	Loss: 0.7320	LR: 0.025000
Training Epoch: 58 [23296/50000]	Loss: 0.4361	LR: 0.025000
Training Epoch: 58 [23424/50000]	Loss: 0.4415	LR: 0.025000
Training Epoch: 58 [23552/50000]	Loss: 0.5040	LR: 0.025000
Training Epoch: 58 [23680/50000]	Loss: 0.5348	LR: 0.025000
Training Epoch: 58 [23808/50000]	Loss: 0.5712	LR: 0.025000
Training Epoch: 58 [23936/50000]	Loss: 0.4886	LR: 0.025000
Training Epoch: 58 [24064/50000]	Loss: 0.4336	LR: 0.025000
Training Epoch: 58 [24192/50000]	Loss: 0.5402	LR: 0.025000
Training Epoch: 58 [24320/50000]	Loss: 0.4497	LR: 0.025000
Training Epoch: 58 [24448/50000]	Loss: 0.6229	LR: 0.025000
Training Epoch: 58 [24576/50000]	Loss: 0.5821	LR: 0.025000
Training Epoch: 58 [24704/50000]	Loss: 0.4523	LR: 0.025000
Training Epoch: 58 [24832/50000]	Loss: 0.4888	LR: 0.025000
Training Epoch: 58 [24960/50000]	Loss: 0.4254	LR: 0.025000
Training Epoch: 58 [25088/50000]	Loss: 0.6032	LR: 0.025000
Training Epoch: 58 [25216/50000]	Loss: 0.4901	LR: 0.025000
Training Epoch: 58 [25344/50000]	Loss: 0.5146	LR: 0.025000
Training Epoch: 58 [25472/50000]	Loss: 0.4858	LR: 0.025000
Training Epoch: 58 [25600/50000]	Loss: 0.5185	LR: 0.025000
Training Epoch: 58 [25728/50000]	Loss: 0.3981	LR: 0.025000
Training Epoch: 58 [25856/50000]	Loss: 0.6419	LR: 0.025000
Training Epoch: 58 [25984/50000]	Loss: 0.5091	LR: 0.025000
Training Epoch: 58 [26112/50000]	Loss: 0.5245	LR: 0.025000
Training Epoch: 58 [26240/50000]	Loss: 0.6493	LR: 0.025000
Training Epoch: 58 [26368/50000]	Loss: 0.5906	LR: 0.025000
Training Epoch: 58 [26496/50000]	Loss: 0.5313	LR: 0.025000
Training Epoch: 58 [26624/50000]	Loss: 0.6620	LR: 0.025000
Training Epoch: 58 [26752/50000]	Loss: 0.4041	LR: 0.025000
Training Epoch: 58 [26880/50000]	Loss: 0.5727	LR: 0.025000
Training Epoch: 58 [27008/50000]	Loss: 0.5076	LR: 0.025000
Training Epoch: 58 [27136/50000]	Loss: 0.4299	LR: 0.025000
Training Epoch: 58 [27264/50000]	Loss: 0.4810	LR: 0.025000
Training Epoch: 58 [27392/50000]	Loss: 0.4282	LR: 0.025000
Training Epoch: 58 [27520/50000]	Loss: 0.4776	LR: 0.025000
Training Epoch: 58 [27648/50000]	Loss: 0.7132	LR: 0.025000
Training Epoch: 58 [27776/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 58 [27904/50000]	Loss: 0.5727	LR: 0.025000
Training Epoch: 58 [28032/50000]	Loss: 0.5514	LR: 0.025000
Training Epoch: 58 [28160/50000]	Loss: 0.4039	LR: 0.025000
Training Epoch: 58 [28288/50000]	Loss: 0.5488	LR: 0.025000
Training Epoch: 58 [28416/50000]	Loss: 0.6597	LR: 0.025000
Training Epoch: 58 [28544/50000]	Loss: 0.6235	LR: 0.025000
Training Epoch: 58 [28672/50000]	Loss: 0.5328	LR: 0.025000
Training Epoch: 58 [28800/50000]	Loss: 0.5079	LR: 0.025000
Training Epoch: 58 [28928/50000]	Loss: 0.5139	LR: 0.025000
Training Epoch: 58 [29056/50000]	Loss: 0.5187	LR: 0.025000
Training Epoch: 58 [29184/50000]	Loss: 0.5186	LR: 0.025000
Training Epoch: 58 [29312/50000]	Loss: 0.5764	LR: 0.025000
Training Epoch: 58 [29440/50000]	Loss: 0.3911	LR: 0.025000
Training Epoch: 58 [29568/50000]	Loss: 0.5465	LR: 0.025000
Training Epoch: 58 [29696/50000]	Loss: 0.5639	LR: 0.025000
Training Epoch: 58 [29824/50000]	Loss: 0.6353	LR: 0.025000
Training Epoch: 58 [29952/50000]	Loss: 0.6839	LR: 0.025000
Training Epoch: 58 [30080/50000]	Loss: 0.6481	LR: 0.025000
Training Epoch: 58 [30208/50000]	Loss: 0.4862	LR: 0.025000
Training Epoch: 58 [30336/50000]	Loss: 0.5365	LR: 0.025000
Training Epoch: 58 [30464/50000]	Loss: 0.5533	LR: 0.025000
Training Epoch: 58 [30592/50000]	Loss: 0.5011	LR: 0.025000
Training Epoch: 58 [30720/50000]	Loss: 0.5670	LR: 0.025000
Training Epoch: 58 [30848/50000]	Loss: 0.3939	LR: 0.025000
Training Epoch: 58 [30976/50000]	Loss: 0.7366	LR: 0.025000
Training Epoch: 58 [31104/50000]	Loss: 0.5395	LR: 0.025000
Training Epoch: 58 [31232/50000]	Loss: 0.5650	LR: 0.025000
Training Epoch: 58 [31360/50000]	Loss: 0.3867	LR: 0.025000
Training Epoch: 58 [31488/50000]	Loss: 0.3692	LR: 0.025000
Training Epoch: 58 [31616/50000]	Loss: 0.4687	LR: 0.025000
Training Epoch: 58 [31744/50000]	Loss: 0.5141	LR: 0.025000
Training Epoch: 58 [31872/50000]	Loss: 0.5541	LR: 0.025000
Training Epoch: 58 [32000/50000]	Loss: 0.4519	LR: 0.025000
Training Epoch: 58 [32128/50000]	Loss: 0.6579	LR: 0.025000
Training Epoch: 58 [32256/50000]	Loss: 0.4938	LR: 0.025000
Training Epoch: 58 [32384/50000]	Loss: 0.5529	LR: 0.025000
Training Epoch: 58 [32512/50000]	Loss: 0.5383	LR: 0.025000
Training Epoch: 58 [32640/50000]	Loss: 0.7050	LR: 0.025000
Training Epoch: 58 [32768/50000]	Loss: 0.4535	LR: 0.025000
Training Epoch: 58 [32896/50000]	Loss: 0.4914	LR: 0.025000
Training Epoch: 58 [33024/50000]	Loss: 0.6837	LR: 0.025000
Training Epoch: 58 [33152/50000]	Loss: 0.5870	LR: 0.025000
Training Epoch: 58 [33280/50000]	Loss: 0.5275	LR: 0.025000
Training Epoch: 58 [33408/50000]	Loss: 0.5913	LR: 0.025000
Training Epoch: 58 [33536/50000]	Loss: 0.6403	LR: 0.025000
Training Epoch: 58 [33664/50000]	Loss: 0.5185	LR: 0.025000
Training Epoch: 58 [33792/50000]	Loss: 0.5503	LR: 0.025000
Training Epoch: 58 [33920/50000]	Loss: 0.6284	LR: 0.025000
Training Epoch: 58 [34048/50000]	Loss: 0.7051	LR: 0.025000
Training Epoch: 58 [34176/50000]	Loss: 0.5998	LR: 0.025000
Training Epoch: 58 [34304/50000]	Loss: 0.4312	LR: 0.025000
Training Epoch: 58 [34432/50000]	Loss: 0.5554	LR: 0.025000
Training Epoch: 58 [34560/50000]	Loss: 0.5331	LR: 0.025000
Training Epoch: 58 [34688/50000]	Loss: 0.5803	LR: 0.025000
Training Epoch: 58 [34816/50000]	Loss: 0.4391	LR: 0.025000
Training Epoch: 58 [34944/50000]	Loss: 0.5225	LR: 0.025000
Training Epoch: 58 [35072/50000]	Loss: 0.5428	LR: 0.025000
Training Epoch: 58 [35200/50000]	Loss: 0.3793	LR: 0.025000
Training Epoch: 58 [35328/50000]	Loss: 0.5339	LR: 0.025000
Training Epoch: 58 [35456/50000]	Loss: 0.5418	LR: 0.025000
Training Epoch: 58 [35584/50000]	Loss: 0.5270	LR: 0.025000
Training Epoch: 58 [35712/50000]	Loss: 0.5027	LR: 0.025000
Training Epoch: 58 [35840/50000]	Loss: 0.6286	LR: 0.025000
Training Epoch: 58 [35968/50000]	Loss: 0.5446	LR: 0.025000
Training Epoch: 58 [36096/50000]	Loss: 0.6516	LR: 0.025000
Training Epoch: 58 [36224/50000]	Loss: 0.6437	LR: 0.025000
Training Epoch: 58 [36352/50000]	Loss: 0.5915	LR: 0.025000
Training Epoch: 58 [36480/50000]	Loss: 0.4561	LR: 0.025000
Training Epoch: 58 [36608/50000]	Loss: 0.5566	LR: 0.025000
Training Epoch: 58 [36736/50000]	Loss: 0.5333	LR: 0.025000
Training Epoch: 58 [36864/50000]	Loss: 0.5024	LR: 0.025000
Training Epoch: 58 [36992/50000]	Loss: 0.6851	LR: 0.025000
Training Epoch: 58 [37120/50000]	Loss: 0.5836	LR: 0.025000
Training Epoch: 58 [37248/50000]	Loss: 0.6044	LR: 0.025000
Training Epoch: 58 [37376/50000]	Loss: 0.5003	LR: 0.025000
Training Epoch: 58 [37504/50000]	Loss: 0.4982	LR: 0.025000
Training Epoch: 58 [37632/50000]	Loss: 0.5517	LR: 0.025000
Training Epoch: 58 [37760/50000]	Loss: 0.4391	LR: 0.025000
Training Epoch: 58 [37888/50000]	Loss: 0.6231	LR: 0.025000
Training Epoch: 58 [38016/50000]	Loss: 0.6551	LR: 0.025000
Training Epoch: 58 [38144/50000]	Loss: 0.5890	LR: 0.025000
Training Epoch: 58 [38272/50000]	Loss: 0.4334	LR: 0.025000
Training Epoch: 58 [38400/50000]	Loss: 0.5501	LR: 0.025000
Training Epoch: 58 [38528/50000]	Loss: 0.5947	LR: 0.025000
Training Epoch: 58 [38656/50000]	Loss: 0.4928	LR: 0.025000
Training Epoch: 58 [38784/50000]	Loss: 0.5363	LR: 0.025000
Training Epoch: 58 [38912/50000]	Loss: 0.6449	LR: 0.025000
Training Epoch: 58 [39040/50000]	Loss: 0.5843	LR: 0.025000
Training Epoch: 58 [39168/50000]	Loss: 0.5854	LR: 0.025000
Training Epoch: 58 [39296/50000]	Loss: 0.5700	LR: 0.025000
Training Epoch: 58 [39424/50000]	Loss: 0.4710	LR: 0.025000
Training Epoch: 58 [39552/50000]	Loss: 0.6750	LR: 0.025000
Training Epoch: 58 [39680/50000]	Loss: 0.6419	LR: 0.025000
Training Epoch: 58 [39808/50000]	Loss: 0.6431	LR: 0.025000
Training Epoch: 58 [39936/50000]	Loss: 0.6383	LR: 0.025000
Training Epoch: 58 [40064/50000]	Loss: 0.4940	LR: 0.025000
Training Epoch: 58 [40192/50000]	Loss: 0.5125	LR: 0.025000
Training Epoch: 58 [40320/50000]	Loss: 0.5544	LR: 0.025000
Training Epoch: 58 [40448/50000]	Loss: 0.4998	LR: 0.025000
Training Epoch: 58 [40576/50000]	Loss: 0.4743	LR: 0.025000
Training Epoch: 58 [40704/50000]	Loss: 0.4292	LR: 0.025000
Training Epoch: 58 [40832/50000]	Loss: 0.5020	LR: 0.025000
Training Epoch: 58 [40960/50000]	Loss: 0.6239	LR: 0.025000
Training Epoch: 58 [41088/50000]	Loss: 0.6692	LR: 0.025000
Training Epoch: 58 [41216/50000]	Loss: 0.5947	LR: 0.025000
Training Epoch: 58 [41344/50000]	Loss: 0.5000	LR: 0.025000
Training Epoch: 58 [41472/50000]	Loss: 0.7119	LR: 0.025000
Training Epoch: 58 [41600/50000]	Loss: 0.8582	LR: 0.025000
Training Epoch: 58 [41728/50000]	Loss: 0.5267	LR: 0.025000
Training Epoch: 58 [41856/50000]	Loss: 0.5652	LR: 0.025000
Training Epoch: 58 [41984/50000]	Loss: 0.6102	LR: 0.025000
Training Epoch: 58 [42112/50000]	Loss: 0.5954	LR: 0.025000
Training Epoch: 58 [42240/50000]	Loss: 0.6174	LR: 0.025000
Training Epoch: 58 [42368/50000]	Loss: 0.7039	LR: 0.025000
Training Epoch: 58 [42496/50000]	Loss: 0.5517	LR: 0.025000
Training Epoch: 58 [42624/50000]	Loss: 0.7367	LR: 0.025000
Training Epoch: 58 [42752/50000]	Loss: 0.5379	LR: 0.025000
Training Epoch: 58 [42880/50000]	Loss: 0.3834	LR: 0.025000
Training Epoch: 58 [43008/50000]	Loss: 0.5043	LR: 0.025000
Training Epoch: 58 [43136/50000]	Loss: 0.5454	LR: 0.025000
Training Epoch: 58 [43264/50000]	Loss: 0.5469	LR: 0.025000
Training Epoch: 58 [43392/50000]	Loss: 0.6779	LR: 0.025000
Training Epoch: 58 [43520/50000]	Loss: 0.6542	LR: 0.025000
Training Epoch: 58 [43648/50000]	Loss: 0.6767	LR: 0.025000
Training Epoch: 58 [43776/50000]	Loss: 0.4974	LR: 0.025000
Training Epoch: 58 [43904/50000]	Loss: 0.4644	LR: 0.025000
Training Epoch: 58 [44032/50000]	Loss: 0.4366	LR: 0.025000
Training Epoch: 58 [44160/50000]	Loss: 0.7120	LR: 0.025000
Training Epoch: 58 [44288/50000]	Loss: 0.5632	LR: 0.025000
Training Epoch: 58 [44416/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 58 [44544/50000]	Loss: 0.7848	LR: 0.025000
Training Epoch: 58 [44672/50000]	Loss: 0.5336	LR: 0.025000
Training Epoch: 58 [44800/50000]	Loss: 0.5304	LR: 0.025000
Training Epoch: 58 [44928/50000]	Loss: 0.5357	LR: 0.025000
Training Epoch: 58 [45056/50000]	Loss: 0.4816	LR: 0.025000
Training Epoch: 58 [45184/50000]	Loss: 0.5296	LR: 0.025000
Training Epoch: 58 [45312/50000]	Loss: 0.5972	LR: 0.025000
Training Epoch: 58 [45440/50000]	Loss: 0.5769	LR: 0.025000
Training Epoch: 58 [45568/50000]	Loss: 0.5258	LR: 0.025000
Training Epoch: 58 [45696/50000]	Loss: 0.6989	LR: 0.025000
Training Epoch: 58 [45824/50000]	Loss: 0.4460	LR: 0.025000
Training Epoch: 58 [45952/50000]	Loss: 0.6343	LR: 0.025000
Training Epoch: 58 [46080/50000]	Loss: 0.7317	LR: 0.025000
Training Epoch: 58 [46208/50000]	Loss: 0.6435	LR: 0.025000
Training Epoch: 58 [46336/50000]	Loss: 0.5123	LR: 0.025000
Training Epoch: 58 [46464/50000]	Loss: 0.5000	LR: 0.025000
Training Epoch: 58 [46592/50000]	Loss: 0.7375	LR: 0.025000
Training Epoch: 58 [46720/50000]	Loss: 0.6730	LR: 0.025000
Training Epoch: 58 [46848/50000]	Loss: 0.5834	LR: 0.025000
Training Epoch: 58 [46976/50000]	Loss: 0.6677	LR: 0.025000
Training Epoch: 58 [47104/50000]	Loss: 0.5520	LR: 0.025000
Training Epoch: 58 [47232/50000]	Loss: 0.4928	LR: 0.025000
Training Epoch: 58 [47360/50000]	Loss: 0.5606	LR: 0.025000
Training Epoch: 58 [47488/50000]	Loss: 0.4645	LR: 0.025000
Training Epoch: 58 [47616/50000]	Loss: 0.4531	LR: 0.025000
Training Epoch: 58 [47744/50000]	Loss: 0.7812	LR: 0.025000
Training Epoch: 58 [47872/50000]	Loss: 0.5285	LR: 0.025000
Training Epoch: 58 [48000/50000]	Loss: 0.4043	LR: 0.025000
Training Epoch: 58 [48128/50000]	Loss: 0.7347	LR: 0.025000
Training Epoch: 58 [48256/50000]	Loss: 0.6409	LR: 0.025000
Training Epoch: 58 [48384/50000]	Loss: 0.5184	LR: 0.025000
Training Epoch: 58 [48512/50000]	Loss: 0.6306	LR: 0.025000
Training Epoch: 58 [48640/50000]	Loss: 0.4888	LR: 0.025000
Training Epoch: 58 [48768/50000]	Loss: 0.6641	LR: 0.025000
Training Epoch: 58 [48896/50000]	Loss: 0.6248	LR: 0.025000
Training Epoch: 58 [49024/50000]	Loss: 0.5372	LR: 0.025000
Training Epoch: 58 [49152/50000]	Loss: 0.7132	LR: 0.025000
Training Epoch: 58 [49280/50000]	Loss: 0.6978	LR: 0.025000
Training Epoch: 58 [49408/50000]	Loss: 0.6249	LR: 0.025000
Training Epoch: 58 [49536/50000]	Loss: 0.4946	LR: 0.025000
Training Epoch: 58 [49664/50000]	Loss: 0.6631	LR: 0.025000
Training Epoch: 58 [49792/50000]	Loss: 0.5440	LR: 0.025000
Training Epoch: 58 [49920/50000]	Loss: 0.5384	LR: 0.025000
Training Epoch: 58 [50000/50000]	Loss: 0.9499	LR: 0.025000
epoch 58 training time consumed: 53.86s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  212863 GB |  212863 GB |
|       from large pool |  123392 KB |    1034 MB |  212653 GB |  212653 GB |
|       from small pool |   10798 KB |      13 MB |     209 GB |     209 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  212863 GB |  212863 GB |
|       from large pool |  123392 KB |    1034 MB |  212653 GB |  212653 GB |
|       from small pool |   10798 KB |      13 MB |     209 GB |     209 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   93673 GB |   93673 GB |
|       from large pool |  155136 KB |  433088 KB |   93441 GB |   93441 GB |
|       from small pool |    1490 KB |    3494 KB |     231 GB |     231 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    8213 K  |    8213 K  |
|       from large pool |      24    |      65    |    4287 K  |    4287 K  |
|       from small pool |     231    |     274    |    3926 K  |    3926 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    8213 K  |    8213 K  |
|       from large pool |      24    |      65    |    4287 K  |    4287 K  |
|       from small pool |     231    |     274    |    3926 K  |    3926 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4059 K  |    4059 K  |
|       from large pool |       9    |      14    |    2075 K  |    2075 K  |
|       from small pool |      12    |      16    |    1984 K  |    1984 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 58, Average loss: 0.0114, Accuracy: 0.6492, Time consumed:3.46s

Training Epoch: 59 [128/50000]	Loss: 0.4944	LR: 0.025000
Training Epoch: 59 [256/50000]	Loss: 0.4416	LR: 0.025000
Training Epoch: 59 [384/50000]	Loss: 0.5053	LR: 0.025000
Training Epoch: 59 [512/50000]	Loss: 0.5425	LR: 0.025000
Training Epoch: 59 [640/50000]	Loss: 0.3235	LR: 0.025000
Training Epoch: 59 [768/50000]	Loss: 0.4619	LR: 0.025000
Training Epoch: 59 [896/50000]	Loss: 0.4198	LR: 0.025000
Training Epoch: 59 [1024/50000]	Loss: 0.5369	LR: 0.025000
Training Epoch: 59 [1152/50000]	Loss: 0.5104	LR: 0.025000
Training Epoch: 59 [1280/50000]	Loss: 0.5746	LR: 0.025000
Training Epoch: 59 [1408/50000]	Loss: 0.4602	LR: 0.025000
Training Epoch: 59 [1536/50000]	Loss: 0.5939	LR: 0.025000
Training Epoch: 59 [1664/50000]	Loss: 0.4839	LR: 0.025000
Training Epoch: 59 [1792/50000]	Loss: 0.4686	LR: 0.025000
Training Epoch: 59 [1920/50000]	Loss: 0.4486	LR: 0.025000
Training Epoch: 59 [2048/50000]	Loss: 0.4258	LR: 0.025000
Training Epoch: 59 [2176/50000]	Loss: 0.7263	LR: 0.025000
Training Epoch: 59 [2304/50000]	Loss: 0.4235	LR: 0.025000
Training Epoch: 59 [2432/50000]	Loss: 0.4015	LR: 0.025000
Training Epoch: 59 [2560/50000]	Loss: 0.3715	LR: 0.025000
Training Epoch: 59 [2688/50000]	Loss: 0.5354	LR: 0.025000
Training Epoch: 59 [2816/50000]	Loss: 0.7859	LR: 0.025000
Training Epoch: 59 [2944/50000]	Loss: 0.4891	LR: 0.025000
Training Epoch: 59 [3072/50000]	Loss: 0.4040	LR: 0.025000
Training Epoch: 59 [3200/50000]	Loss: 0.4276	LR: 0.025000
Training Epoch: 59 [3328/50000]	Loss: 0.4145	LR: 0.025000
Training Epoch: 59 [3456/50000]	Loss: 0.5936	LR: 0.025000
Training Epoch: 59 [3584/50000]	Loss: 0.4833	LR: 0.025000
Training Epoch: 59 [3712/50000]	Loss: 0.4436	LR: 0.025000
Training Epoch: 59 [3840/50000]	Loss: 0.4953	LR: 0.025000
Training Epoch: 59 [3968/50000]	Loss: 0.4535	LR: 0.025000
Training Epoch: 59 [4096/50000]	Loss: 0.4768	LR: 0.025000
Training Epoch: 59 [4224/50000]	Loss: 0.3768	LR: 0.025000
Training Epoch: 59 [4352/50000]	Loss: 0.4894	LR: 0.025000
Training Epoch: 59 [4480/50000]	Loss: 0.3688	LR: 0.025000
Training Epoch: 59 [4608/50000]	Loss: 0.5106	LR: 0.025000
Training Epoch: 59 [4736/50000]	Loss: 0.5243	LR: 0.025000
Training Epoch: 59 [4864/50000]	Loss: 0.4379	LR: 0.025000
Training Epoch: 59 [4992/50000]	Loss: 0.5236	LR: 0.025000
Training Epoch: 59 [5120/50000]	Loss: 0.4661	LR: 0.025000
Training Epoch: 59 [5248/50000]	Loss: 0.3626	LR: 0.025000
Training Epoch: 59 [5376/50000]	Loss: 0.3942	LR: 0.025000
Training Epoch: 59 [5504/50000]	Loss: 0.4470	LR: 0.025000
Training Epoch: 59 [5632/50000]	Loss: 0.5110	LR: 0.025000
Training Epoch: 59 [5760/50000]	Loss: 0.3985	LR: 0.025000
Training Epoch: 59 [5888/50000]	Loss: 0.5168	LR: 0.025000
Training Epoch: 59 [6016/50000]	Loss: 0.5756	LR: 0.025000
Training Epoch: 59 [6144/50000]	Loss: 0.4499	LR: 0.025000
Training Epoch: 59 [6272/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 59 [6400/50000]	Loss: 0.3972	LR: 0.025000
Training Epoch: 59 [6528/50000]	Loss: 0.4922	LR: 0.025000
Training Epoch: 59 [6656/50000]	Loss: 0.3568	LR: 0.025000
Training Epoch: 59 [6784/50000]	Loss: 0.4079	LR: 0.025000
Training Epoch: 59 [6912/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 59 [7040/50000]	Loss: 0.4730	LR: 0.025000
Training Epoch: 59 [7168/50000]	Loss: 0.4314	LR: 0.025000
Training Epoch: 59 [7296/50000]	Loss: 0.4123	LR: 0.025000
Training Epoch: 59 [7424/50000]	Loss: 0.4780	LR: 0.025000
Training Epoch: 59 [7552/50000]	Loss: 0.4002	LR: 0.025000
Training Epoch: 59 [7680/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 59 [7808/50000]	Loss: 0.4155	LR: 0.025000
Training Epoch: 59 [7936/50000]	Loss: 0.4864	LR: 0.025000
Training Epoch: 59 [8064/50000]	Loss: 0.3984	LR: 0.025000
Training Epoch: 59 [8192/50000]	Loss: 0.4382	LR: 0.025000
Training Epoch: 59 [8320/50000]	Loss: 0.4821	LR: 0.025000
Training Epoch: 59 [8448/50000]	Loss: 0.4273	LR: 0.025000
Training Epoch: 59 [8576/50000]	Loss: 0.4869	LR: 0.025000
Training Epoch: 59 [8704/50000]	Loss: 0.3206	LR: 0.025000
Training Epoch: 59 [8832/50000]	Loss: 0.5774	LR: 0.025000
Training Epoch: 59 [8960/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 59 [9088/50000]	Loss: 0.2616	LR: 0.025000
Training Epoch: 59 [9216/50000]	Loss: 0.4993	LR: 0.025000
Training Epoch: 59 [9344/50000]	Loss: 0.5386	LR: 0.025000
Training Epoch: 59 [9472/50000]	Loss: 0.5688	LR: 0.025000
Training Epoch: 59 [9600/50000]	Loss: 0.5017	LR: 0.025000
Training Epoch: 59 [9728/50000]	Loss: 0.4449	LR: 0.025000
Training Epoch: 59 [9856/50000]	Loss: 0.4631	LR: 0.025000
Training Epoch: 59 [9984/50000]	Loss: 0.5252	LR: 0.025000
Training Epoch: 59 [10112/50000]	Loss: 0.6640	LR: 0.025000
Training Epoch: 59 [10240/50000]	Loss: 0.3781	LR: 0.025000
Training Epoch: 59 [10368/50000]	Loss: 0.4014	LR: 0.025000
Training Epoch: 59 [10496/50000]	Loss: 0.4471	LR: 0.025000
Training Epoch: 59 [10624/50000]	Loss: 0.5432	LR: 0.025000
Training Epoch: 59 [10752/50000]	Loss: 0.3951	LR: 0.025000
Training Epoch: 59 [10880/50000]	Loss: 0.5742	LR: 0.025000
Training Epoch: 59 [11008/50000]	Loss: 0.5029	LR: 0.025000
Training Epoch: 59 [11136/50000]	Loss: 0.4200	LR: 0.025000
Training Epoch: 59 [11264/50000]	Loss: 0.3370	LR: 0.025000
Training Epoch: 59 [11392/50000]	Loss: 0.5545	LR: 0.025000
Training Epoch: 59 [11520/50000]	Loss: 0.5180	LR: 0.025000
Training Epoch: 59 [11648/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 59 [11776/50000]	Loss: 0.3972	LR: 0.025000
Training Epoch: 59 [11904/50000]	Loss: 0.5934	LR: 0.025000
Training Epoch: 59 [12032/50000]	Loss: 0.6180	LR: 0.025000
Training Epoch: 59 [12160/50000]	Loss: 0.4324	LR: 0.025000
Training Epoch: 59 [12288/50000]	Loss: 0.4838	LR: 0.025000
Training Epoch: 59 [12416/50000]	Loss: 0.5676	LR: 0.025000
Training Epoch: 59 [12544/50000]	Loss: 0.4424	LR: 0.025000
Training Epoch: 59 [12672/50000]	Loss: 0.4992	LR: 0.025000
Training Epoch: 59 [12800/50000]	Loss: 0.4198	LR: 0.025000
Training Epoch: 59 [12928/50000]	Loss: 0.4751	LR: 0.025000
Training Epoch: 59 [13056/50000]	Loss: 0.4915	LR: 0.025000
Training Epoch: 59 [13184/50000]	Loss: 0.4036	LR: 0.025000
Training Epoch: 59 [13312/50000]	Loss: 0.4254	LR: 0.025000
Training Epoch: 59 [13440/50000]	Loss: 0.4858	LR: 0.025000
Training Epoch: 59 [13568/50000]	Loss: 0.5137	LR: 0.025000
Training Epoch: 59 [13696/50000]	Loss: 0.4164	LR: 0.025000
Training Epoch: 59 [13824/50000]	Loss: 0.5275	LR: 0.025000
Training Epoch: 59 [13952/50000]	Loss: 0.5042	LR: 0.025000
Training Epoch: 59 [14080/50000]	Loss: 0.2955	LR: 0.025000
Training Epoch: 59 [14208/50000]	Loss: 0.6904	LR: 0.025000
Training Epoch: 59 [14336/50000]	Loss: 0.4427	LR: 0.025000
Training Epoch: 59 [14464/50000]	Loss: 0.4303	LR: 0.025000
Training Epoch: 59 [14592/50000]	Loss: 0.2640	LR: 0.025000
Training Epoch: 59 [14720/50000]	Loss: 0.5951	LR: 0.025000
Training Epoch: 59 [14848/50000]	Loss: 0.5132	LR: 0.025000
Training Epoch: 59 [14976/50000]	Loss: 0.4743	LR: 0.025000
Training Epoch: 59 [15104/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 59 [15232/50000]	Loss: 0.5557	LR: 0.025000
Training Epoch: 59 [15360/50000]	Loss: 0.3982	LR: 0.025000
Training Epoch: 59 [15488/50000]	Loss: 0.4899	LR: 0.025000
Training Epoch: 59 [15616/50000]	Loss: 0.4939	LR: 0.025000
Training Epoch: 59 [15744/50000]	Loss: 0.5852	LR: 0.025000
Training Epoch: 59 [15872/50000]	Loss: 0.4248	LR: 0.025000
Training Epoch: 59 [16000/50000]	Loss: 0.4509	LR: 0.025000
Training Epoch: 59 [16128/50000]	Loss: 0.4685	LR: 0.025000
Training Epoch: 59 [16256/50000]	Loss: 0.4960	LR: 0.025000
Training Epoch: 59 [16384/50000]	Loss: 0.4156	LR: 0.025000
Training Epoch: 59 [16512/50000]	Loss: 0.4797	LR: 0.025000
Training Epoch: 59 [16640/50000]	Loss: 0.5902	LR: 0.025000
Training Epoch: 59 [16768/50000]	Loss: 0.4207	LR: 0.025000
Training Epoch: 59 [16896/50000]	Loss: 0.6214	LR: 0.025000
Training Epoch: 59 [17024/50000]	Loss: 0.4258	LR: 0.025000
Training Epoch: 59 [17152/50000]	Loss: 0.3206	LR: 0.025000
Training Epoch: 59 [17280/50000]	Loss: 0.4030	LR: 0.025000
Training Epoch: 59 [17408/50000]	Loss: 0.4506	LR: 0.025000
Training Epoch: 59 [17536/50000]	Loss: 0.5004	LR: 0.025000
Training Epoch: 59 [17664/50000]	Loss: 0.5583	LR: 0.025000
Training Epoch: 59 [17792/50000]	Loss: 0.3901	LR: 0.025000
Training Epoch: 59 [17920/50000]	Loss: 0.4410	LR: 0.025000
Training Epoch: 59 [18048/50000]	Loss: 0.5110	LR: 0.025000
Training Epoch: 59 [18176/50000]	Loss: 0.4476	LR: 0.025000
Training Epoch: 59 [18304/50000]	Loss: 0.5374	LR: 0.025000
Training Epoch: 59 [18432/50000]	Loss: 0.3896	LR: 0.025000
Training Epoch: 59 [18560/50000]	Loss: 0.4973	LR: 0.025000
Training Epoch: 59 [18688/50000]	Loss: 0.5497	LR: 0.025000
Training Epoch: 59 [18816/50000]	Loss: 0.5612	LR: 0.025000
Training Epoch: 59 [18944/50000]	Loss: 0.5471	LR: 0.025000
Training Epoch: 59 [19072/50000]	Loss: 0.5110	LR: 0.025000
Training Epoch: 59 [19200/50000]	Loss: 0.5025	LR: 0.025000
Training Epoch: 59 [19328/50000]	Loss: 0.5809	LR: 0.025000
Training Epoch: 59 [19456/50000]	Loss: 0.4151	LR: 0.025000
Training Epoch: 59 [19584/50000]	Loss: 0.4446	LR: 0.025000
Training Epoch: 59 [19712/50000]	Loss: 0.4751	LR: 0.025000
Training Epoch: 59 [19840/50000]	Loss: 0.3809	LR: 0.025000
Training Epoch: 59 [19968/50000]	Loss: 0.4098	LR: 0.025000
Training Epoch: 59 [20096/50000]	Loss: 0.4297	LR: 0.025000
Training Epoch: 59 [20224/50000]	Loss: 0.3968	LR: 0.025000
Training Epoch: 59 [20352/50000]	Loss: 0.5352	LR: 0.025000
Training Epoch: 59 [20480/50000]	Loss: 0.4578	LR: 0.025000
Training Epoch: 59 [20608/50000]	Loss: 0.4956	LR: 0.025000
Training Epoch: 59 [20736/50000]	Loss: 0.4155	LR: 0.025000
Training Epoch: 59 [20864/50000]	Loss: 0.4572	LR: 0.025000
Training Epoch: 59 [20992/50000]	Loss: 0.5365	LR: 0.025000
Training Epoch: 59 [21120/50000]	Loss: 0.4594	LR: 0.025000
Training Epoch: 59 [21248/50000]	Loss: 0.5371	LR: 0.025000
Training Epoch: 59 [21376/50000]	Loss: 0.5836	LR: 0.025000
Training Epoch: 59 [21504/50000]	Loss: 0.6358	LR: 0.025000
Training Epoch: 59 [21632/50000]	Loss: 0.3822	LR: 0.025000
Training Epoch: 59 [21760/50000]	Loss: 0.6764	LR: 0.025000
Training Epoch: 59 [21888/50000]	Loss: 0.4828	LR: 0.025000
Training Epoch: 59 [22016/50000]	Loss: 0.4713	LR: 0.025000
Training Epoch: 59 [22144/50000]	Loss: 0.6696	LR: 0.025000
Training Epoch: 59 [22272/50000]	Loss: 0.4627	LR: 0.025000
Training Epoch: 59 [22400/50000]	Loss: 0.6328	LR: 0.025000
Training Epoch: 59 [22528/50000]	Loss: 0.5176	LR: 0.025000
Training Epoch: 59 [22656/50000]	Loss: 0.7257	LR: 0.025000
Training Epoch: 59 [22784/50000]	Loss: 0.5123	LR: 0.025000
Training Epoch: 59 [22912/50000]	Loss: 0.5369	LR: 0.025000
Training Epoch: 59 [23040/50000]	Loss: 0.4549	LR: 0.025000
Training Epoch: 59 [23168/50000]	Loss: 0.5836	LR: 0.025000
Training Epoch: 59 [23296/50000]	Loss: 0.4040	LR: 0.025000
Training Epoch: 59 [23424/50000]	Loss: 0.4423	LR: 0.025000
Training Epoch: 59 [23552/50000]	Loss: 0.5621	LR: 0.025000
Training Epoch: 59 [23680/50000]	Loss: 0.5838	LR: 0.025000
Training Epoch: 59 [23808/50000]	Loss: 0.6363	LR: 0.025000
Training Epoch: 59 [23936/50000]	Loss: 0.5486	LR: 0.025000
Training Epoch: 59 [24064/50000]	Loss: 0.5666	LR: 0.025000
Training Epoch: 59 [24192/50000]	Loss: 0.5765	LR: 0.025000
Training Epoch: 59 [24320/50000]	Loss: 0.5523	LR: 0.025000
Training Epoch: 59 [24448/50000]	Loss: 0.4756	LR: 0.025000
Training Epoch: 59 [24576/50000]	Loss: 0.6680	LR: 0.025000
Training Epoch: 59 [24704/50000]	Loss: 0.6329	LR: 0.025000
Training Epoch: 59 [24832/50000]	Loss: 0.5401	LR: 0.025000
Training Epoch: 59 [24960/50000]	Loss: 0.4855	LR: 0.025000
Training Epoch: 59 [25088/50000]	Loss: 0.5441	LR: 0.025000
Training Epoch: 59 [25216/50000]	Loss: 0.6704	LR: 0.025000
Training Epoch: 59 [25344/50000]	Loss: 0.6165	LR: 0.025000
Training Epoch: 59 [25472/50000]	Loss: 0.6537	LR: 0.025000
Training Epoch: 59 [25600/50000]	Loss: 0.5530	LR: 0.025000
Training Epoch: 59 [25728/50000]	Loss: 0.6025	LR: 0.025000
Training Epoch: 59 [25856/50000]	Loss: 0.5591	LR: 0.025000
Training Epoch: 59 [25984/50000]	Loss: 0.7103	LR: 0.025000
Training Epoch: 59 [26112/50000]	Loss: 0.5672	LR: 0.025000
Training Epoch: 59 [26240/50000]	Loss: 0.6579	LR: 0.025000
Training Epoch: 59 [26368/50000]	Loss: 0.5123	LR: 0.025000
Training Epoch: 59 [26496/50000]	Loss: 0.4451	LR: 0.025000
Training Epoch: 59 [26624/50000]	Loss: 0.4807	LR: 0.025000
Training Epoch: 59 [26752/50000]	Loss: 0.4362	LR: 0.025000
Training Epoch: 59 [26880/50000]	Loss: 0.3683	LR: 0.025000
Training Epoch: 59 [27008/50000]	Loss: 0.6787	LR: 0.025000
Training Epoch: 59 [27136/50000]	Loss: 0.4235	LR: 0.025000
Training Epoch: 59 [27264/50000]	Loss: 0.6091	LR: 0.025000
Training Epoch: 59 [27392/50000]	Loss: 0.4963	LR: 0.025000
Training Epoch: 59 [27520/50000]	Loss: 0.5433	LR: 0.025000
Training Epoch: 59 [27648/50000]	Loss: 0.6218	LR: 0.025000
Training Epoch: 59 [27776/50000]	Loss: 0.6440	LR: 0.025000
Training Epoch: 59 [27904/50000]	Loss: 0.5113	LR: 0.025000
Training Epoch: 59 [28032/50000]	Loss: 0.7163	LR: 0.025000
Training Epoch: 59 [28160/50000]	Loss: 0.5921	LR: 0.025000
Training Epoch: 59 [28288/50000]	Loss: 0.4349	LR: 0.025000
Training Epoch: 59 [28416/50000]	Loss: 0.6106	LR: 0.025000
Training Epoch: 59 [28544/50000]	Loss: 0.4354	LR: 0.025000
Training Epoch: 59 [28672/50000]	Loss: 0.4231	LR: 0.025000
Training Epoch: 59 [28800/50000]	Loss: 0.5734	LR: 0.025000
Training Epoch: 59 [28928/50000]	Loss: 0.6093	LR: 0.025000
Training Epoch: 59 [29056/50000]	Loss: 0.4728	LR: 0.025000
Training Epoch: 59 [29184/50000]	Loss: 0.6068	LR: 0.025000
Training Epoch: 59 [29312/50000]	Loss: 0.5620	LR: 0.025000
Training Epoch: 59 [29440/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 59 [29568/50000]	Loss: 0.5068	LR: 0.025000
Training Epoch: 59 [29696/50000]	Loss: 0.7152	LR: 0.025000
Training Epoch: 59 [29824/50000]	Loss: 0.4582	LR: 0.025000
Training Epoch: 59 [29952/50000]	Loss: 0.5776	LR: 0.025000
Training Epoch: 59 [30080/50000]	Loss: 0.5936	LR: 0.025000
Training Epoch: 59 [30208/50000]	Loss: 0.4897	LR: 0.025000
Training Epoch: 59 [30336/50000]	Loss: 0.6002	LR: 0.025000
Training Epoch: 59 [30464/50000]	Loss: 0.5944	LR: 0.025000
Training Epoch: 59 [30592/50000]	Loss: 0.6743	LR: 0.025000
Training Epoch: 59 [30720/50000]	Loss: 0.6523	LR: 0.025000
Training Epoch: 59 [30848/50000]	Loss: 0.5687	LR: 0.025000
Training Epoch: 59 [30976/50000]	Loss: 0.6528	LR: 0.025000
Training Epoch: 59 [31104/50000]	Loss: 0.5637	LR: 0.025000
Training Epoch: 59 [31232/50000]	Loss: 0.5762	LR: 0.025000
Training Epoch: 59 [31360/50000]	Loss: 0.4899	LR: 0.025000
Training Epoch: 59 [31488/50000]	Loss: 0.4842	LR: 0.025000
Training Epoch: 59 [31616/50000]	Loss: 0.7329	LR: 0.025000
Training Epoch: 59 [31744/50000]	Loss: 0.4778	LR: 0.025000
Training Epoch: 59 [31872/50000]	Loss: 0.6317	LR: 0.025000
Training Epoch: 59 [32000/50000]	Loss: 0.5215	LR: 0.025000
Training Epoch: 59 [32128/50000]	Loss: 0.5459	LR: 0.025000
Training Epoch: 59 [32256/50000]	Loss: 0.6129	LR: 0.025000
Training Epoch: 59 [32384/50000]	Loss: 0.6348	LR: 0.025000
Training Epoch: 59 [32512/50000]	Loss: 0.6319	LR: 0.025000
Training Epoch: 59 [32640/50000]	Loss: 0.5086	LR: 0.025000
Training Epoch: 59 [32768/50000]	Loss: 0.6637	LR: 0.025000
Training Epoch: 59 [32896/50000]	Loss: 0.5677	LR: 0.025000
Training Epoch: 59 [33024/50000]	Loss: 0.4035	LR: 0.025000
Training Epoch: 59 [33152/50000]	Loss: 0.4612	LR: 0.025000
Training Epoch: 59 [33280/50000]	Loss: 0.5469	LR: 0.025000
Training Epoch: 59 [33408/50000]	Loss: 0.6232	LR: 0.025000
Training Epoch: 59 [33536/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 59 [33664/50000]	Loss: 0.5508	LR: 0.025000
Training Epoch: 59 [33792/50000]	Loss: 0.5416	LR: 0.025000
Training Epoch: 59 [33920/50000]	Loss: 0.6503	LR: 0.025000
Training Epoch: 59 [34048/50000]	Loss: 0.5397	LR: 0.025000
Training Epoch: 59 [34176/50000]	Loss: 0.4508	LR: 0.025000
Training Epoch: 59 [34304/50000]	Loss: 0.5452	LR: 0.025000
Training Epoch: 59 [34432/50000]	Loss: 0.5282	LR: 0.025000
Training Epoch: 59 [34560/50000]	Loss: 0.6065	LR: 0.025000
Training Epoch: 59 [34688/50000]	Loss: 0.5167	LR: 0.025000
Training Epoch: 59 [34816/50000]	Loss: 0.5641	LR: 0.025000
Training Epoch: 59 [34944/50000]	Loss: 0.3717	LR: 0.025000
Training Epoch: 59 [35072/50000]	Loss: 0.7503	LR: 0.025000
Training Epoch: 59 [35200/50000]	Loss: 0.5586	LR: 0.025000
Training Epoch: 59 [35328/50000]	Loss: 0.5969	LR: 0.025000
Training Epoch: 59 [35456/50000]	Loss: 0.4597	LR: 0.025000
Training Epoch: 59 [35584/50000]	Loss: 0.3960	LR: 0.025000
Training Epoch: 59 [35712/50000]	Loss: 0.5250	LR: 0.025000
Training Epoch: 59 [35840/50000]	Loss: 0.6507	LR: 0.025000
Training Epoch: 59 [35968/50000]	Loss: 0.5179	LR: 0.025000
Training Epoch: 59 [36096/50000]	Loss: 0.4684	LR: 0.025000
Training Epoch: 59 [36224/50000]	Loss: 0.5742	LR: 0.025000
Training Epoch: 59 [36352/50000]	Loss: 0.4839	LR: 0.025000
Training Epoch: 59 [36480/50000]	Loss: 0.6072	LR: 0.025000
Training Epoch: 59 [36608/50000]	Loss: 0.5542	LR: 0.025000
Training Epoch: 59 [36736/50000]	Loss: 0.6321	LR: 0.025000
Training Epoch: 59 [36864/50000]	Loss: 0.6692	LR: 0.025000
Training Epoch: 59 [36992/50000]	Loss: 0.6208	LR: 0.025000
Training Epoch: 59 [37120/50000]	Loss: 0.5160	LR: 0.025000
Training Epoch: 59 [37248/50000]	Loss: 0.5616	LR: 0.025000
Training Epoch: 59 [37376/50000]	Loss: 0.5402	LR: 0.025000
Training Epoch: 59 [37504/50000]	Loss: 0.6853	LR: 0.025000
Training Epoch: 59 [37632/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 59 [37760/50000]	Loss: 0.4898	LR: 0.025000
Training Epoch: 59 [37888/50000]	Loss: 0.4739	LR: 0.025000
Training Epoch: 59 [38016/50000]	Loss: 0.5980	LR: 0.025000
Training Epoch: 59 [38144/50000]	Loss: 0.4701	LR: 0.025000
Training Epoch: 59 [38272/50000]	Loss: 0.5067	LR: 0.025000
Training Epoch: 59 [38400/50000]	Loss: 0.4143	LR: 0.025000
Training Epoch: 59 [38528/50000]	Loss: 0.6530	LR: 0.025000
Training Epoch: 59 [38656/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 59 [38784/50000]	Loss: 0.5250	LR: 0.025000
Training Epoch: 59 [38912/50000]	Loss: 0.6485	LR: 0.025000
Training Epoch: 59 [39040/50000]	Loss: 0.4798	LR: 0.025000
Training Epoch: 59 [39168/50000]	Loss: 0.5484	LR: 0.025000
Training Epoch: 59 [39296/50000]	Loss: 0.5534	LR: 0.025000
Training Epoch: 59 [39424/50000]	Loss: 0.5382	LR: 0.025000
Training Epoch: 59 [39552/50000]	Loss: 0.4763	LR: 0.025000
Training Epoch: 59 [39680/50000]	Loss: 0.4909	LR: 0.025000
Training Epoch: 59 [39808/50000]	Loss: 0.5754	LR: 0.025000
Training Epoch: 59 [39936/50000]	Loss: 0.5242	LR: 0.025000
Training Epoch: 59 [40064/50000]	Loss: 0.5555	LR: 0.025000
Training Epoch: 59 [40192/50000]	Loss: 0.5060	LR: 0.025000
Training Epoch: 59 [40320/50000]	Loss: 0.4816	LR: 0.025000
Training Epoch: 59 [40448/50000]	Loss: 0.5630	LR: 0.025000
Training Epoch: 59 [40576/50000]	Loss: 0.6013	LR: 0.025000
Training Epoch: 59 [40704/50000]	Loss: 0.6887	LR: 0.025000
Training Epoch: 59 [40832/50000]	Loss: 0.6763	LR: 0.025000
Training Epoch: 59 [40960/50000]	Loss: 0.4259	LR: 0.025000
Training Epoch: 59 [41088/50000]	Loss: 0.4570	LR: 0.025000
Training Epoch: 59 [41216/50000]	Loss: 0.5901	LR: 0.025000
Training Epoch: 59 [41344/50000]	Loss: 0.5935	LR: 0.025000
Training Epoch: 59 [41472/50000]	Loss: 0.5730	LR: 0.025000
Training Epoch: 59 [41600/50000]	Loss: 0.6040	LR: 0.025000
Training Epoch: 59 [41728/50000]	Loss: 0.6161	LR: 0.025000
Training Epoch: 59 [41856/50000]	Loss: 0.5783	LR: 0.025000
Training Epoch: 59 [41984/50000]	Loss: 0.5883	LR: 0.025000
Training Epoch: 59 [42112/50000]	Loss: 0.5576	LR: 0.025000
Training Epoch: 59 [42240/50000]	Loss: 0.5001	LR: 0.025000
Training Epoch: 59 [42368/50000]	Loss: 0.6903	LR: 0.025000
Training Epoch: 59 [42496/50000]	Loss: 0.6291	LR: 0.025000
Training Epoch: 59 [42624/50000]	Loss: 0.5513	LR: 0.025000
Training Epoch: 59 [42752/50000]	Loss: 0.6263	LR: 0.025000
Training Epoch: 59 [42880/50000]	Loss: 0.5243	LR: 0.025000
Training Epoch: 59 [43008/50000]	Loss: 0.6149	LR: 0.025000
Training Epoch: 59 [43136/50000]	Loss: 0.6616	LR: 0.025000
Training Epoch: 59 [43264/50000]	Loss: 0.5999	LR: 0.025000
Training Epoch: 59 [43392/50000]	Loss: 0.7836	LR: 0.025000
Training Epoch: 59 [43520/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 59 [43648/50000]	Loss: 0.5981	LR: 0.025000
Training Epoch: 59 [43776/50000]	Loss: 0.5946	LR: 0.025000
Training Epoch: 59 [43904/50000]	Loss: 0.7030	LR: 0.025000
Training Epoch: 59 [44032/50000]	Loss: 0.4692	LR: 0.025000
Training Epoch: 59 [44160/50000]	Loss: 0.6233	LR: 0.025000
Training Epoch: 59 [44288/50000]	Loss: 0.5427	LR: 0.025000
Training Epoch: 59 [44416/50000]	Loss: 0.5683	LR: 0.025000
Training Epoch: 59 [44544/50000]	Loss: 0.5451	LR: 0.025000
Training Epoch: 59 [44672/50000]	Loss: 0.5567	LR: 0.025000
Training Epoch: 59 [44800/50000]	Loss: 0.6190	LR: 0.025000
Training Epoch: 59 [44928/50000]	Loss: 0.6990	LR: 0.025000
Training Epoch: 59 [45056/50000]	Loss: 0.5927	LR: 0.025000
Training Epoch: 59 [45184/50000]	Loss: 0.6808	LR: 0.025000
Training Epoch: 59 [45312/50000]	Loss: 0.5745	LR: 0.025000
Training Epoch: 59 [45440/50000]	Loss: 0.3287	LR: 0.025000
Training Epoch: 59 [45568/50000]	Loss: 0.5397	LR: 0.025000
Training Epoch: 59 [45696/50000]	Loss: 0.5914	LR: 0.025000
Training Epoch: 59 [45824/50000]	Loss: 0.5744	LR: 0.025000
Training Epoch: 59 [45952/50000]	Loss: 0.5234	LR: 0.025000
Training Epoch: 59 [46080/50000]	Loss: 0.5338	LR: 0.025000
Training Epoch: 59 [46208/50000]	Loss: 0.6243	LR: 0.025000
Training Epoch: 59 [46336/50000]	Loss: 0.5604	LR: 0.025000
Training Epoch: 59 [46464/50000]	Loss: 0.6149	LR: 0.025000
Training Epoch: 59 [46592/50000]	Loss: 0.5139	LR: 0.025000
Training Epoch: 59 [46720/50000]	Loss: 0.5757	LR: 0.025000
Training Epoch: 59 [46848/50000]	Loss: 0.5751	LR: 0.025000
Training Epoch: 59 [46976/50000]	Loss: 0.6784	LR: 0.025000
Training Epoch: 59 [47104/50000]	Loss: 0.6245	LR: 0.025000
Training Epoch: 59 [47232/50000]	Loss: 0.6170	LR: 0.025000
Training Epoch: 59 [47360/50000]	Loss: 0.4596	LR: 0.025000
Training Epoch: 59 [47488/50000]	Loss: 0.6763	LR: 0.025000
Training Epoch: 59 [47616/50000]	Loss: 0.4777	LR: 0.025000
Training Epoch: 59 [47744/50000]	Loss: 0.5033	LR: 0.025000
Training Epoch: 59 [47872/50000]	Loss: 0.5892	LR: 0.025000
Training Epoch: 59 [48000/50000]	Loss: 0.5613	LR: 0.025000
Training Epoch: 59 [48128/50000]	Loss: 0.5748	LR: 0.025000
Training Epoch: 59 [48256/50000]	Loss: 0.6027	LR: 0.025000
Training Epoch: 59 [48384/50000]	Loss: 0.5050	LR: 0.025000
Training Epoch: 59 [48512/50000]	Loss: 0.5226	LR: 0.025000
Training Epoch: 59 [48640/50000]	Loss: 0.5299	LR: 0.025000
Training Epoch: 59 [48768/50000]	Loss: 0.4543	LR: 0.025000
Training Epoch: 59 [48896/50000]	Loss: 0.5940	LR: 0.025000
Training Epoch: 59 [49024/50000]	Loss: 0.6729	LR: 0.025000
Training Epoch: 59 [49152/50000]	Loss: 0.5606	LR: 0.025000
Training Epoch: 59 [49280/50000]	Loss: 0.6108	LR: 0.025000
Training Epoch: 59 [49408/50000]	Loss: 0.4460	LR: 0.025000
Training Epoch: 59 [49536/50000]	Loss: 0.4074	LR: 0.025000
Training Epoch: 59 [49664/50000]	Loss: 0.5092	LR: 0.025000
Training Epoch: 59 [49792/50000]	Loss: 0.7793	LR: 0.025000
Training Epoch: 59 [49920/50000]	Loss: 0.7215	LR: 0.025000
Training Epoch: 59 [50000/50000]	Loss: 0.6451	LR: 0.025000
epoch 59 training time consumed: 53.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  216533 GB |  216533 GB |
|       from large pool |  123392 KB |    1034 MB |  216320 GB |  216320 GB |
|       from small pool |   10798 KB |      13 MB |     213 GB |     213 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  216533 GB |  216533 GB |
|       from large pool |  123392 KB |    1034 MB |  216320 GB |  216320 GB |
|       from small pool |   10798 KB |      13 MB |     213 GB |     213 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   95288 GB |   95288 GB |
|       from large pool |  155136 KB |  433088 KB |   95052 GB |   95052 GB |
|       from small pool |    1490 KB |    3494 KB |     235 GB |     235 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    8355 K  |    8355 K  |
|       from large pool |      24    |      65    |    4361 K  |    4361 K  |
|       from small pool |     231    |     274    |    3994 K  |    3993 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    8355 K  |    8355 K  |
|       from large pool |      24    |      65    |    4361 K  |    4361 K  |
|       from small pool |     231    |     274    |    3994 K  |    3993 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4129 K  |    4129 K  |
|       from large pool |       9    |      14    |    2110 K  |    2110 K  |
|       from small pool |      12    |      16    |    2018 K  |    2018 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 59, Average loss: 0.0106, Accuracy: 0.6627, Time consumed:3.45s

Training Epoch: 60 [128/50000]	Loss: 0.3422	LR: 0.025000
Training Epoch: 60 [256/50000]	Loss: 0.3451	LR: 0.025000
Training Epoch: 60 [384/50000]	Loss: 0.4434	LR: 0.025000
Training Epoch: 60 [512/50000]	Loss: 0.4845	LR: 0.025000
Training Epoch: 60 [640/50000]	Loss: 0.5195	LR: 0.025000
Training Epoch: 60 [768/50000]	Loss: 0.4684	LR: 0.025000
Training Epoch: 60 [896/50000]	Loss: 0.3692	LR: 0.025000
Training Epoch: 60 [1024/50000]	Loss: 0.4094	LR: 0.025000
Training Epoch: 60 [1152/50000]	Loss: 0.4538	LR: 0.025000
Training Epoch: 60 [1280/50000]	Loss: 0.5076	LR: 0.025000
Training Epoch: 60 [1408/50000]	Loss: 0.4025	LR: 0.025000
Training Epoch: 60 [1536/50000]	Loss: 0.5253	LR: 0.025000
Training Epoch: 60 [1664/50000]	Loss: 0.5811	LR: 0.025000
Training Epoch: 60 [1792/50000]	Loss: 0.4788	LR: 0.025000
Training Epoch: 60 [1920/50000]	Loss: 0.4771	LR: 0.025000
Training Epoch: 60 [2048/50000]	Loss: 0.5035	LR: 0.025000
Training Epoch: 60 [2176/50000]	Loss: 0.5071	LR: 0.025000
Training Epoch: 60 [2304/50000]	Loss: 0.4746	LR: 0.025000
Training Epoch: 60 [2432/50000]	Loss: 0.3363	LR: 0.025000
Training Epoch: 60 [2560/50000]	Loss: 0.5455	LR: 0.025000
Training Epoch: 60 [2688/50000]	Loss: 0.4794	LR: 0.025000
Training Epoch: 60 [2816/50000]	Loss: 0.3872	LR: 0.025000
Training Epoch: 60 [2944/50000]	Loss: 0.4406	LR: 0.025000
Training Epoch: 60 [3072/50000]	Loss: 0.4573	LR: 0.025000
Training Epoch: 60 [3200/50000]	Loss: 0.4083	LR: 0.025000
Training Epoch: 60 [3328/50000]	Loss: 0.4100	LR: 0.025000
Training Epoch: 60 [3456/50000]	Loss: 0.4587	LR: 0.025000
Training Epoch: 60 [3584/50000]	Loss: 0.3486	LR: 0.025000
Training Epoch: 60 [3712/50000]	Loss: 0.5841	LR: 0.025000
Training Epoch: 60 [3840/50000]	Loss: 0.5328	LR: 0.025000
Training Epoch: 60 [3968/50000]	Loss: 0.4882	LR: 0.025000
Training Epoch: 60 [4096/50000]	Loss: 0.4294	LR: 0.025000
Training Epoch: 60 [4224/50000]	Loss: 0.4154	LR: 0.025000
Training Epoch: 60 [4352/50000]	Loss: 0.4325	LR: 0.025000
Training Epoch: 60 [4480/50000]	Loss: 0.4498	LR: 0.025000
Training Epoch: 60 [4608/50000]	Loss: 0.4064	LR: 0.025000
Training Epoch: 60 [4736/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 60 [4864/50000]	Loss: 0.5036	LR: 0.025000
Training Epoch: 60 [4992/50000]	Loss: 0.3638	LR: 0.025000
Training Epoch: 60 [5120/50000]	Loss: 0.5866	LR: 0.025000
Training Epoch: 60 [5248/50000]	Loss: 0.4583	LR: 0.025000
Training Epoch: 60 [5376/50000]	Loss: 0.3499	LR: 0.025000
Training Epoch: 60 [5504/50000]	Loss: 0.5005	LR: 0.025000
Training Epoch: 60 [5632/50000]	Loss: 0.4288	LR: 0.025000
Training Epoch: 60 [5760/50000]	Loss: 0.4412	LR: 0.025000
Training Epoch: 60 [5888/50000]	Loss: 0.4857	LR: 0.025000
Training Epoch: 60 [6016/50000]	Loss: 0.4465	LR: 0.025000
Training Epoch: 60 [6144/50000]	Loss: 0.4124	LR: 0.025000
Training Epoch: 60 [6272/50000]	Loss: 0.3814	LR: 0.025000
Training Epoch: 60 [6400/50000]	Loss: 0.4582	LR: 0.025000
Training Epoch: 60 [6528/50000]	Loss: 0.4759	LR: 0.025000
Training Epoch: 60 [6656/50000]	Loss: 0.4370	LR: 0.025000
Training Epoch: 60 [6784/50000]	Loss: 0.4479	LR: 0.025000
Training Epoch: 60 [6912/50000]	Loss: 0.4107	LR: 0.025000
Training Epoch: 60 [7040/50000]	Loss: 0.3547	LR: 0.025000
Training Epoch: 60 [7168/50000]	Loss: 0.3533	LR: 0.025000
Training Epoch: 60 [7296/50000]	Loss: 0.4741	LR: 0.025000
Training Epoch: 60 [7424/50000]	Loss: 0.5216	LR: 0.025000
Training Epoch: 60 [7552/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 60 [7680/50000]	Loss: 0.4628	LR: 0.025000
Training Epoch: 60 [7808/50000]	Loss: 0.4877	LR: 0.025000
Training Epoch: 60 [7936/50000]	Loss: 0.4979	LR: 0.025000
Training Epoch: 60 [8064/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 60 [8192/50000]	Loss: 0.4036	LR: 0.025000
Training Epoch: 60 [8320/50000]	Loss: 0.3525	LR: 0.025000
Training Epoch: 60 [8448/50000]	Loss: 0.5456	LR: 0.025000
Training Epoch: 60 [8576/50000]	Loss: 0.3822	LR: 0.025000
Training Epoch: 60 [8704/50000]	Loss: 0.5939	LR: 0.025000
Training Epoch: 60 [8832/50000]	Loss: 0.4186	LR: 0.025000
Training Epoch: 60 [8960/50000]	Loss: 0.4018	LR: 0.025000
Training Epoch: 60 [9088/50000]	Loss: 0.4912	LR: 0.025000
Training Epoch: 60 [9216/50000]	Loss: 0.3952	LR: 0.025000
Training Epoch: 60 [9344/50000]	Loss: 0.3943	LR: 0.025000
Training Epoch: 60 [9472/50000]	Loss: 0.5254	LR: 0.025000
Training Epoch: 60 [9600/50000]	Loss: 0.4125	LR: 0.025000
Training Epoch: 60 [9728/50000]	Loss: 0.3872	LR: 0.025000
Training Epoch: 60 [9856/50000]	Loss: 0.5855	LR: 0.025000
Training Epoch: 60 [9984/50000]	Loss: 0.4585	LR: 0.025000
Training Epoch: 60 [10112/50000]	Loss: 0.4342	LR: 0.025000
Training Epoch: 60 [10240/50000]	Loss: 0.4654	LR: 0.025000
Training Epoch: 60 [10368/50000]	Loss: 0.4290	LR: 0.025000
Training Epoch: 60 [10496/50000]	Loss: 0.5272	LR: 0.025000
Training Epoch: 60 [10624/50000]	Loss: 0.4641	LR: 0.025000
Training Epoch: 60 [10752/50000]	Loss: 0.6901	LR: 0.025000
Training Epoch: 60 [10880/50000]	Loss: 0.4573	LR: 0.025000
Training Epoch: 60 [11008/50000]	Loss: 0.5586	LR: 0.025000
Training Epoch: 60 [11136/50000]	Loss: 0.4065	LR: 0.025000
Training Epoch: 60 [11264/50000]	Loss: 0.5793	LR: 0.025000
Training Epoch: 60 [11392/50000]	Loss: 0.3727	LR: 0.025000
Training Epoch: 60 [11520/50000]	Loss: 0.3410	LR: 0.025000
Training Epoch: 60 [11648/50000]	Loss: 0.3862	LR: 0.025000
Training Epoch: 60 [11776/50000]	Loss: 0.4594	LR: 0.025000
Training Epoch: 60 [11904/50000]	Loss: 0.4900	LR: 0.025000
Training Epoch: 60 [12032/50000]	Loss: 0.3591	LR: 0.025000
Training Epoch: 60 [12160/50000]	Loss: 0.4448	LR: 0.025000
Training Epoch: 60 [12288/50000]	Loss: 0.3760	LR: 0.025000
Training Epoch: 60 [12416/50000]	Loss: 0.4673	LR: 0.025000
Training Epoch: 60 [12544/50000]	Loss: 0.4689	LR: 0.025000
Training Epoch: 60 [12672/50000]	Loss: 0.4920	LR: 0.025000
Training Epoch: 60 [12800/50000]	Loss: 0.4376	LR: 0.025000
Training Epoch: 60 [12928/50000]	Loss: 0.4684	LR: 0.025000
Training Epoch: 60 [13056/50000]	Loss: 0.4878	LR: 0.025000
Training Epoch: 60 [13184/50000]	Loss: 0.4332	LR: 0.025000
Training Epoch: 60 [13312/50000]	Loss: 0.3782	LR: 0.025000
Training Epoch: 60 [13440/50000]	Loss: 0.6450	LR: 0.025000
Training Epoch: 60 [13568/50000]	Loss: 0.3956	LR: 0.025000
Training Epoch: 60 [13696/50000]	Loss: 0.4151	LR: 0.025000
Training Epoch: 60 [13824/50000]	Loss: 0.6019	LR: 0.025000
Training Epoch: 60 [13952/50000]	Loss: 0.4585	LR: 0.025000
Training Epoch: 60 [14080/50000]	Loss: 0.4808	LR: 0.025000
Training Epoch: 60 [14208/50000]	Loss: 0.4001	LR: 0.025000
Training Epoch: 60 [14336/50000]	Loss: 0.5104	LR: 0.025000
Training Epoch: 60 [14464/50000]	Loss: 0.4341	LR: 0.025000
Training Epoch: 60 [14592/50000]	Loss: 0.4540	LR: 0.025000
Training Epoch: 60 [14720/50000]	Loss: 0.4572	LR: 0.025000
Training Epoch: 60 [14848/50000]	Loss: 0.5177	LR: 0.025000
Training Epoch: 60 [14976/50000]	Loss: 0.5673	LR: 0.025000
Training Epoch: 60 [15104/50000]	Loss: 0.6054	LR: 0.025000
Training Epoch: 60 [15232/50000]	Loss: 0.3042	LR: 0.025000
Training Epoch: 60 [15360/50000]	Loss: 0.5226	LR: 0.025000
Training Epoch: 60 [15488/50000]	Loss: 0.4080	LR: 0.025000
Training Epoch: 60 [15616/50000]	Loss: 0.4242	LR: 0.025000
Training Epoch: 60 [15744/50000]	Loss: 0.5140	LR: 0.025000
Training Epoch: 60 [15872/50000]	Loss: 0.4094	LR: 0.025000
Training Epoch: 60 [16000/50000]	Loss: 0.3990	LR: 0.025000
Training Epoch: 60 [16128/50000]	Loss: 0.4246	LR: 0.025000
Training Epoch: 60 [16256/50000]	Loss: 0.3405	LR: 0.025000
Training Epoch: 60 [16384/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 60 [16512/50000]	Loss: 0.4130	LR: 0.025000
Training Epoch: 60 [16640/50000]	Loss: 0.5541	LR: 0.025000
Training Epoch: 60 [16768/50000]	Loss: 0.3894	LR: 0.025000
Training Epoch: 60 [16896/50000]	Loss: 0.4078	LR: 0.025000
Training Epoch: 60 [17024/50000]	Loss: 0.4166	LR: 0.025000
Training Epoch: 60 [17152/50000]	Loss: 0.3846	LR: 0.025000
Training Epoch: 60 [17280/50000]	Loss: 0.5782	LR: 0.025000
Training Epoch: 60 [17408/50000]	Loss: 0.4451	LR: 0.025000
Training Epoch: 60 [17536/50000]	Loss: 0.4784	LR: 0.025000
Training Epoch: 60 [17664/50000]	Loss: 0.4911	LR: 0.025000
Training Epoch: 60 [17792/50000]	Loss: 0.3549	LR: 0.025000
Training Epoch: 60 [17920/50000]	Loss: 0.4388	LR: 0.025000
Training Epoch: 60 [18048/50000]	Loss: 0.5240	LR: 0.025000
Training Epoch: 60 [18176/50000]	Loss: 0.4365	LR: 0.025000
Training Epoch: 60 [18304/50000]	Loss: 0.4087	LR: 0.025000
Training Epoch: 60 [18432/50000]	Loss: 0.5345	LR: 0.025000
Training Epoch: 60 [18560/50000]	Loss: 0.5055	LR: 0.025000
Training Epoch: 60 [18688/50000]	Loss: 0.4747	LR: 0.025000
Training Epoch: 60 [18816/50000]	Loss: 0.4856	LR: 0.025000
Training Epoch: 60 [18944/50000]	Loss: 0.5471	LR: 0.025000
Training Epoch: 60 [19072/50000]	Loss: 0.4024	LR: 0.025000
Training Epoch: 60 [19200/50000]	Loss: 0.5132	LR: 0.025000
Training Epoch: 60 [19328/50000]	Loss: 0.4975	LR: 0.025000
Training Epoch: 60 [19456/50000]	Loss: 0.3878	LR: 0.025000
Training Epoch: 60 [19584/50000]	Loss: 0.4020	LR: 0.025000
Training Epoch: 60 [19712/50000]	Loss: 0.5300	LR: 0.025000
Training Epoch: 60 [19840/50000]	Loss: 0.5076	LR: 0.025000
Training Epoch: 60 [19968/50000]	Loss: 0.4610	LR: 0.025000
Training Epoch: 60 [20096/50000]	Loss: 0.5388	LR: 0.025000
Training Epoch: 60 [20224/50000]	Loss: 0.5200	LR: 0.025000
Training Epoch: 60 [20352/50000]	Loss: 0.3913	LR: 0.025000
Training Epoch: 60 [20480/50000]	Loss: 0.5818	LR: 0.025000
Training Epoch: 60 [20608/50000]	Loss: 0.5365	LR: 0.025000
Training Epoch: 60 [20736/50000]	Loss: 0.4903	LR: 0.025000
Training Epoch: 60 [20864/50000]	Loss: 0.4821	LR: 0.025000
Training Epoch: 60 [20992/50000]	Loss: 0.4997	LR: 0.025000
Training Epoch: 60 [21120/50000]	Loss: 0.3664	LR: 0.025000
Training Epoch: 60 [21248/50000]	Loss: 0.4437	LR: 0.025000
Training Epoch: 60 [21376/50000]	Loss: 0.4008	LR: 0.025000
Training Epoch: 60 [21504/50000]	Loss: 0.3848	LR: 0.025000
Training Epoch: 60 [21632/50000]	Loss: 0.5004	LR: 0.025000
Training Epoch: 60 [21760/50000]	Loss: 0.4267	LR: 0.025000
Training Epoch: 60 [21888/50000]	Loss: 0.6841	LR: 0.025000
Training Epoch: 60 [22016/50000]	Loss: 0.5424	LR: 0.025000
Training Epoch: 60 [22144/50000]	Loss: 0.3729	LR: 0.025000
Training Epoch: 60 [22272/50000]	Loss: 0.5331	LR: 0.025000
Training Epoch: 60 [22400/50000]	Loss: 0.6373	LR: 0.025000
Training Epoch: 60 [22528/50000]	Loss: 0.5046	LR: 0.025000
Training Epoch: 60 [22656/50000]	Loss: 0.4694	LR: 0.025000
Training Epoch: 60 [22784/50000]	Loss: 0.5549	LR: 0.025000
Training Epoch: 60 [22912/50000]	Loss: 0.5084	LR: 0.025000
Training Epoch: 60 [23040/50000]	Loss: 0.5735	LR: 0.025000
Training Epoch: 60 [23168/50000]	Loss: 0.5883	LR: 0.025000
Training Epoch: 60 [23296/50000]	Loss: 0.4258	LR: 0.025000
Training Epoch: 60 [23424/50000]	Loss: 0.5307	LR: 0.025000
Training Epoch: 60 [23552/50000]	Loss: 0.4147	LR: 0.025000
Training Epoch: 60 [23680/50000]	Loss: 0.4913	LR: 0.025000
Training Epoch: 60 [23808/50000]	Loss: 0.5369	LR: 0.025000
Training Epoch: 60 [23936/50000]	Loss: 0.4688	LR: 0.025000
Training Epoch: 60 [24064/50000]	Loss: 0.6076	LR: 0.025000
Training Epoch: 60 [24192/50000]	Loss: 0.3052	LR: 0.025000
Training Epoch: 60 [24320/50000]	Loss: 0.5215	LR: 0.025000
Training Epoch: 60 [24448/50000]	Loss: 0.4895	LR: 0.025000
Training Epoch: 60 [24576/50000]	Loss: 0.7307	LR: 0.025000
Training Epoch: 60 [24704/50000]	Loss: 0.3840	LR: 0.025000
Training Epoch: 60 [24832/50000]	Loss: 0.6739	LR: 0.025000
Training Epoch: 60 [24960/50000]	Loss: 0.5696	LR: 0.025000
Training Epoch: 60 [25088/50000]	Loss: 0.4381	LR: 0.025000
Training Epoch: 60 [25216/50000]	Loss: 0.4661	LR: 0.025000
Training Epoch: 60 [25344/50000]	Loss: 0.5785	LR: 0.025000
Training Epoch: 60 [25472/50000]	Loss: 0.5185	LR: 0.025000
Training Epoch: 60 [25600/50000]	Loss: 0.4722	LR: 0.025000
Training Epoch: 60 [25728/50000]	Loss: 0.5382	LR: 0.025000
Training Epoch: 60 [25856/50000]	Loss: 0.5248	LR: 0.025000
Training Epoch: 60 [25984/50000]	Loss: 0.5437	LR: 0.025000
Training Epoch: 60 [26112/50000]	Loss: 0.4712	LR: 0.025000
Training Epoch: 60 [26240/50000]	Loss: 0.3584	LR: 0.025000
Training Epoch: 60 [26368/50000]	Loss: 0.5940	LR: 0.025000
Training Epoch: 60 [26496/50000]	Loss: 0.3854	LR: 0.025000
Training Epoch: 60 [26624/50000]	Loss: 0.4310	LR: 0.025000
Training Epoch: 60 [26752/50000]	Loss: 0.5075	LR: 0.025000
Training Epoch: 60 [26880/50000]	Loss: 0.3974	LR: 0.025000
Training Epoch: 60 [27008/50000]	Loss: 0.4078	LR: 0.025000
Training Epoch: 60 [27136/50000]	Loss: 0.4364	LR: 0.025000
Training Epoch: 60 [27264/50000]	Loss: 0.4701	LR: 0.025000
Training Epoch: 60 [27392/50000]	Loss: 0.6212	LR: 0.025000
Training Epoch: 60 [27520/50000]	Loss: 0.4689	LR: 0.025000
Training Epoch: 60 [27648/50000]	Loss: 0.5580	LR: 0.025000
Training Epoch: 60 [27776/50000]	Loss: 0.4441	LR: 0.025000
Training Epoch: 60 [27904/50000]	Loss: 0.5733	LR: 0.025000
Training Epoch: 60 [28032/50000]	Loss: 0.5355	LR: 0.025000
Training Epoch: 60 [28160/50000]	Loss: 0.5768	LR: 0.025000
Training Epoch: 60 [28288/50000]	Loss: 0.5131	LR: 0.025000
Training Epoch: 60 [28416/50000]	Loss: 0.4392	LR: 0.025000
Training Epoch: 60 [28544/50000]	Loss: 0.5377	LR: 0.025000
Training Epoch: 60 [28672/50000]	Loss: 0.4677	LR: 0.025000
Training Epoch: 60 [28800/50000]	Loss: 0.5354	LR: 0.025000
Training Epoch: 60 [28928/50000]	Loss: 0.4820	LR: 0.025000
Training Epoch: 60 [29056/50000]	Loss: 0.5233	LR: 0.025000
Training Epoch: 60 [29184/50000]	Loss: 0.5175	LR: 0.025000
Training Epoch: 60 [29312/50000]	Loss: 0.5775	LR: 0.025000
Training Epoch: 60 [29440/50000]	Loss: 0.5923	LR: 0.025000
Training Epoch: 60 [29568/50000]	Loss: 0.4462	LR: 0.025000
Training Epoch: 60 [29696/50000]	Loss: 0.4731	LR: 0.025000
Training Epoch: 60 [29824/50000]	Loss: 0.5212	LR: 0.025000
Training Epoch: 60 [29952/50000]	Loss: 0.5205	LR: 0.025000
Training Epoch: 60 [30080/50000]	Loss: 0.6150	LR: 0.025000
Training Epoch: 60 [30208/50000]	Loss: 0.5800	LR: 0.025000
Training Epoch: 60 [30336/50000]	Loss: 0.5245	LR: 0.025000
Training Epoch: 60 [30464/50000]	Loss: 0.4808	LR: 0.025000
Training Epoch: 60 [30592/50000]	Loss: 0.5562	LR: 0.025000
Training Epoch: 60 [30720/50000]	Loss: 0.4892	LR: 0.025000
Training Epoch: 60 [30848/50000]	Loss: 0.5980	LR: 0.025000
Training Epoch: 60 [30976/50000]	Loss: 0.5359	LR: 0.025000
Training Epoch: 60 [31104/50000]	Loss: 0.4730	LR: 0.025000
Training Epoch: 60 [31232/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 60 [31360/50000]	Loss: 0.3263	LR: 0.025000
Training Epoch: 60 [31488/50000]	Loss: 0.5648	LR: 0.025000
Training Epoch: 60 [31616/50000]	Loss: 0.6437	LR: 0.025000
Training Epoch: 60 [31744/50000]	Loss: 0.4309	LR: 0.025000
Training Epoch: 60 [31872/50000]	Loss: 0.3602	LR: 0.025000
Training Epoch: 60 [32000/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 60 [32128/50000]	Loss: 0.5898	LR: 0.025000
Training Epoch: 60 [32256/50000]	Loss: 0.5418	LR: 0.025000
Training Epoch: 60 [32384/50000]	Loss: 0.4887	LR: 0.025000
Training Epoch: 60 [32512/50000]	Loss: 0.5919	LR: 0.025000
Training Epoch: 60 [32640/50000]	Loss: 0.5876	LR: 0.025000
Training Epoch: 60 [32768/50000]	Loss: 0.5047	LR: 0.025000
Training Epoch: 60 [32896/50000]	Loss: 0.5827	LR: 0.025000
Training Epoch: 60 [33024/50000]	Loss: 0.4031	LR: 0.025000
Training Epoch: 60 [33152/50000]	Loss: 0.6052	LR: 0.025000
Training Epoch: 60 [33280/50000]	Loss: 0.3885	LR: 0.025000
Training Epoch: 60 [33408/50000]	Loss: 0.4949	LR: 0.025000
Training Epoch: 60 [33536/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 60 [33664/50000]	Loss: 0.5115	LR: 0.025000
Training Epoch: 60 [33792/50000]	Loss: 0.5271	LR: 0.025000
Training Epoch: 60 [33920/50000]	Loss: 0.4168	LR: 0.025000
Training Epoch: 60 [34048/50000]	Loss: 0.5111	LR: 0.025000
Training Epoch: 60 [34176/50000]	Loss: 0.4949	LR: 0.025000
Training Epoch: 60 [34304/50000]	Loss: 0.4865	LR: 0.025000
Training Epoch: 60 [34432/50000]	Loss: 0.6132	LR: 0.025000
Training Epoch: 60 [34560/50000]	Loss: 0.5469	LR: 0.025000
Training Epoch: 60 [34688/50000]	Loss: 0.5461	LR: 0.025000
Training Epoch: 60 [34816/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 60 [34944/50000]	Loss: 0.4398	LR: 0.025000
Training Epoch: 60 [35072/50000]	Loss: 0.5618	LR: 0.025000
Training Epoch: 60 [35200/50000]	Loss: 0.5529	LR: 0.025000
Training Epoch: 60 [35328/50000]	Loss: 0.5131	LR: 0.025000
Training Epoch: 60 [35456/50000]	Loss: 0.6053	LR: 0.025000
Training Epoch: 60 [35584/50000]	Loss: 0.5949	LR: 0.025000
Training Epoch: 60 [35712/50000]	Loss: 0.6070	LR: 0.025000
Training Epoch: 60 [35840/50000]	Loss: 0.4279	LR: 0.025000
Training Epoch: 60 [35968/50000]	Loss: 0.5955	LR: 0.025000
Training Epoch: 60 [36096/50000]	Loss: 0.6157	LR: 0.025000
Training Epoch: 60 [36224/50000]	Loss: 0.5160	LR: 0.025000
Training Epoch: 60 [36352/50000]	Loss: 0.6002	LR: 0.025000
Training Epoch: 60 [36480/50000]	Loss: 0.6296	LR: 0.025000
Training Epoch: 60 [36608/50000]	Loss: 0.3809	LR: 0.025000
Training Epoch: 60 [36736/50000]	Loss: 0.5235	LR: 0.025000
Training Epoch: 60 [36864/50000]	Loss: 0.5589	LR: 0.025000
Training Epoch: 60 [36992/50000]	Loss: 0.3756	LR: 0.025000
Training Epoch: 60 [37120/50000]	Loss: 0.4243	LR: 0.025000
Training Epoch: 60 [37248/50000]	Loss: 0.4522	LR: 0.025000
Training Epoch: 60 [37376/50000]	Loss: 0.7321	LR: 0.025000
Training Epoch: 60 [37504/50000]	Loss: 0.5488	LR: 0.025000
Training Epoch: 60 [37632/50000]	Loss: 0.6206	LR: 0.025000
Training Epoch: 60 [37760/50000]	Loss: 0.4788	LR: 0.025000
Training Epoch: 60 [37888/50000]	Loss: 0.6348	LR: 0.025000
Training Epoch: 60 [38016/50000]	Loss: 0.5279	LR: 0.025000
Training Epoch: 60 [38144/50000]	Loss: 0.6102	LR: 0.025000
Training Epoch: 60 [38272/50000]	Loss: 0.5551	LR: 0.025000
Training Epoch: 60 [38400/50000]	Loss: 0.5765	LR: 0.025000
Training Epoch: 60 [38528/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 60 [38656/50000]	Loss: 0.4057	LR: 0.025000
Training Epoch: 60 [38784/50000]	Loss: 0.6069	LR: 0.025000
Training Epoch: 60 [38912/50000]	Loss: 0.5985	LR: 0.025000
Training Epoch: 60 [39040/50000]	Loss: 0.5540	LR: 0.025000
Training Epoch: 60 [39168/50000]	Loss: 0.7380	LR: 0.025000
Training Epoch: 60 [39296/50000]	Loss: 0.6938	LR: 0.025000
Training Epoch: 60 [39424/50000]	Loss: 0.5980	LR: 0.025000
Training Epoch: 60 [39552/50000]	Loss: 0.3880	LR: 0.025000
Training Epoch: 60 [39680/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 60 [39808/50000]	Loss: 0.4649	LR: 0.025000
Training Epoch: 60 [39936/50000]	Loss: 0.6194	LR: 0.025000
Training Epoch: 60 [40064/50000]	Loss: 0.4829	LR: 0.025000
Training Epoch: 60 [40192/50000]	Loss: 0.4287	LR: 0.025000
Training Epoch: 60 [40320/50000]	Loss: 0.6678	LR: 0.025000
Training Epoch: 60 [40448/50000]	Loss: 0.5976	LR: 0.025000
Training Epoch: 60 [40576/50000]	Loss: 0.4883	LR: 0.025000
Training Epoch: 60 [40704/50000]	Loss: 0.3910	LR: 0.025000
Training Epoch: 60 [40832/50000]	Loss: 0.4945	LR: 0.025000
Training Epoch: 60 [40960/50000]	Loss: 0.6507	LR: 0.025000
Training Epoch: 60 [41088/50000]	Loss: 0.5236	LR: 0.025000
Training Epoch: 60 [41216/50000]	Loss: 0.5379	LR: 0.025000
Training Epoch: 60 [41344/50000]	Loss: 0.5759	LR: 0.025000
Training Epoch: 60 [41472/50000]	Loss: 0.3909	LR: 0.025000
Training Epoch: 60 [41600/50000]	Loss: 0.5350	LR: 0.025000
Training Epoch: 60 [41728/50000]	Loss: 0.5390	LR: 0.025000
Training Epoch: 60 [41856/50000]	Loss: 0.5588	LR: 0.025000
Training Epoch: 60 [41984/50000]	Loss: 0.5457	LR: 0.025000
Training Epoch: 60 [42112/50000]	Loss: 0.4674	LR: 0.025000
Training Epoch: 60 [42240/50000]	Loss: 0.5446	LR: 0.025000
Training Epoch: 60 [42368/50000]	Loss: 0.5472	LR: 0.025000
Training Epoch: 60 [42496/50000]	Loss: 0.5469	LR: 0.025000
Training Epoch: 60 [42624/50000]	Loss: 0.5807	LR: 0.025000
Training Epoch: 60 [42752/50000]	Loss: 0.5556	LR: 0.025000
Training Epoch: 60 [42880/50000]	Loss: 0.4142	LR: 0.025000
Training Epoch: 60 [43008/50000]	Loss: 0.5539	LR: 0.025000
Training Epoch: 60 [43136/50000]	Loss: 0.6875	LR: 0.025000
Training Epoch: 60 [43264/50000]	Loss: 0.8173	LR: 0.025000
Training Epoch: 60 [43392/50000]	Loss: 0.7302	LR: 0.025000
Training Epoch: 60 [43520/50000]	Loss: 0.5580	LR: 0.025000
Training Epoch: 60 [43648/50000]	Loss: 0.6902	LR: 0.025000
Training Epoch: 60 [43776/50000]	Loss: 0.5198	LR: 0.025000
Training Epoch: 60 [43904/50000]	Loss: 0.5174	LR: 0.025000
Training Epoch: 60 [44032/50000]	Loss: 0.5210	LR: 0.025000
Training Epoch: 60 [44160/50000]	Loss: 0.3843	LR: 0.025000
Training Epoch: 60 [44288/50000]	Loss: 0.5216	LR: 0.025000
Training Epoch: 60 [44416/50000]	Loss: 0.6136	LR: 0.025000
Training Epoch: 60 [44544/50000]	Loss: 0.6491	LR: 0.025000
Training Epoch: 60 [44672/50000]	Loss: 0.6215	LR: 0.025000
Training Epoch: 60 [44800/50000]	Loss: 0.4754	LR: 0.025000
Training Epoch: 60 [44928/50000]	Loss: 0.5452	LR: 0.025000
Training Epoch: 60 [45056/50000]	Loss: 0.5956	LR: 0.025000
Training Epoch: 60 [45184/50000]	Loss: 0.5861	LR: 0.025000
Training Epoch: 60 [45312/50000]	Loss: 0.6636	LR: 0.025000
Training Epoch: 60 [45440/50000]	Loss: 0.5308	LR: 0.025000
Training Epoch: 60 [45568/50000]	Loss: 0.6471	LR: 0.025000
Training Epoch: 60 [45696/50000]	Loss: 0.4917	LR: 0.025000
Training Epoch: 60 [45824/50000]	Loss: 0.6081	LR: 0.025000
Training Epoch: 60 [45952/50000]	Loss: 0.5194	LR: 0.025000
Training Epoch: 60 [46080/50000]	Loss: 0.5725	LR: 0.025000
Training Epoch: 60 [46208/50000]	Loss: 0.6300	LR: 0.025000
Training Epoch: 60 [46336/50000]	Loss: 0.6672	LR: 0.025000
Training Epoch: 60 [46464/50000]	Loss: 0.7142	LR: 0.025000
Training Epoch: 60 [46592/50000]	Loss: 0.5103	LR: 0.025000
Training Epoch: 60 [46720/50000]	Loss: 0.7844	LR: 0.025000
Training Epoch: 60 [46848/50000]	Loss: 0.7055	LR: 0.025000
Training Epoch: 60 [46976/50000]	Loss: 0.4822	LR: 0.025000
Training Epoch: 60 [47104/50000]	Loss: 0.4983	LR: 0.025000
Training Epoch: 60 [47232/50000]	Loss: 0.4693	LR: 0.025000
Training Epoch: 60 [47360/50000]	Loss: 0.6546	LR: 0.025000
Training Epoch: 60 [47488/50000]	Loss: 0.5593	LR: 0.025000
Training Epoch: 60 [47616/50000]	Loss: 0.5255	LR: 0.025000
Training Epoch: 60 [47744/50000]	Loss: 0.8239	LR: 0.025000
Training Epoch: 60 [47872/50000]	Loss: 0.5565	LR: 0.025000
Training Epoch: 60 [48000/50000]	Loss: 0.5890	LR: 0.025000
Training Epoch: 60 [48128/50000]	Loss: 0.7339	LR: 0.025000
Training Epoch: 60 [48256/50000]	Loss: 0.6533	LR: 0.025000
Training Epoch: 60 [48384/50000]	Loss: 0.5903	LR: 0.025000
Training Epoch: 60 [48512/50000]	Loss: 0.5233	LR: 0.025000
Training Epoch: 60 [48640/50000]	Loss: 0.5736	LR: 0.025000
Training Epoch: 60 [48768/50000]	Loss: 0.5564	LR: 0.025000
Training Epoch: 60 [48896/50000]	Loss: 0.5728	LR: 0.025000
Training Epoch: 60 [49024/50000]	Loss: 0.5632	LR: 0.025000
Training Epoch: 60 [49152/50000]	Loss: 0.5589	LR: 0.025000
Training Epoch: 60 [49280/50000]	Loss: 0.6075	LR: 0.025000
Training Epoch: 60 [49408/50000]	Loss: 0.7394	LR: 0.025000
Training Epoch: 60 [49536/50000]	Loss: 0.4755	LR: 0.025000
Training Epoch: 60 [49664/50000]	Loss: 0.5314	LR: 0.025000
Training Epoch: 60 [49792/50000]	Loss: 0.4189	LR: 0.025000
Training Epoch: 60 [49920/50000]	Loss: 0.4761	LR: 0.025000
Training Epoch: 60 [50000/50000]	Loss: 0.5207	LR: 0.025000
epoch 60 training time consumed: 53.90s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  220203 GB |  220203 GB |
|       from large pool |  123392 KB |    1034 MB |  219986 GB |  219986 GB |
|       from small pool |   10798 KB |      13 MB |     216 GB |     216 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  220203 GB |  220203 GB |
|       from large pool |  123392 KB |    1034 MB |  219986 GB |  219986 GB |
|       from small pool |   10798 KB |      13 MB |     216 GB |     216 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   96903 GB |   96903 GB |
|       from large pool |  155136 KB |  433088 KB |   96663 GB |   96663 GB |
|       from small pool |    1490 KB |    3494 KB |     239 GB |     239 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    8497 K  |    8496 K  |
|       from large pool |      24    |      65    |    4435 K  |    4435 K  |
|       from small pool |     231    |     274    |    4061 K  |    4061 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    8497 K  |    8496 K  |
|       from large pool |      24    |      65    |    4435 K  |    4435 K  |
|       from small pool |     231    |     274    |    4061 K  |    4061 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4199 K  |    4199 K  |
|       from large pool |       9    |      14    |    2146 K  |    2146 K  |
|       from small pool |      12    |      16    |    2052 K  |    2052 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 60, Average loss: 0.0114, Accuracy: 0.6509, Time consumed:3.47s

Training Epoch: 61 [128/50000]	Loss: 0.3861	LR: 0.025000
Training Epoch: 61 [256/50000]	Loss: 0.3984	LR: 0.025000
Training Epoch: 61 [384/50000]	Loss: 0.3516	LR: 0.025000
Training Epoch: 61 [512/50000]	Loss: 0.5118	LR: 0.025000
Training Epoch: 61 [640/50000]	Loss: 0.3751	LR: 0.025000
Training Epoch: 61 [768/50000]	Loss: 0.5455	LR: 0.025000
Training Epoch: 61 [896/50000]	Loss: 0.4882	LR: 0.025000
Training Epoch: 61 [1024/50000]	Loss: 0.3530	LR: 0.025000
Training Epoch: 61 [1152/50000]	Loss: 0.3473	LR: 0.025000
Training Epoch: 61 [1280/50000]	Loss: 0.4865	LR: 0.025000
Training Epoch: 61 [1408/50000]	Loss: 0.5124	LR: 0.025000
Training Epoch: 61 [1536/50000]	Loss: 0.4735	LR: 0.025000
Training Epoch: 61 [1664/50000]	Loss: 0.4200	LR: 0.025000
Training Epoch: 61 [1792/50000]	Loss: 0.5205	LR: 0.025000
Training Epoch: 61 [1920/50000]	Loss: 0.4210	LR: 0.025000
Training Epoch: 61 [2048/50000]	Loss: 0.3482	LR: 0.025000
Training Epoch: 61 [2176/50000]	Loss: 0.4083	LR: 0.025000
Training Epoch: 61 [2304/50000]	Loss: 0.4691	LR: 0.025000
Training Epoch: 61 [2432/50000]	Loss: 0.5951	LR: 0.025000
Training Epoch: 61 [2560/50000]	Loss: 0.4647	LR: 0.025000
Training Epoch: 61 [2688/50000]	Loss: 0.3218	LR: 0.025000
Training Epoch: 61 [2816/50000]	Loss: 0.4033	LR: 0.025000
Training Epoch: 61 [2944/50000]	Loss: 0.2794	LR: 0.025000
Training Epoch: 61 [3072/50000]	Loss: 0.4766	LR: 0.025000
Training Epoch: 61 [3200/50000]	Loss: 0.5741	LR: 0.025000
Training Epoch: 61 [3328/50000]	Loss: 0.4387	LR: 0.025000
Training Epoch: 61 [3456/50000]	Loss: 0.4673	LR: 0.025000
Training Epoch: 61 [3584/50000]	Loss: 0.3338	LR: 0.025000
Training Epoch: 61 [3712/50000]	Loss: 0.4226	LR: 0.025000
Training Epoch: 61 [3840/50000]	Loss: 0.4144	LR: 0.025000
Training Epoch: 61 [3968/50000]	Loss: 0.3836	LR: 0.025000
Training Epoch: 61 [4096/50000]	Loss: 0.4602	LR: 0.025000
Training Epoch: 61 [4224/50000]	Loss: 0.4364	LR: 0.025000
Training Epoch: 61 [4352/50000]	Loss: 0.4408	LR: 0.025000
Training Epoch: 61 [4480/50000]	Loss: 0.4171	LR: 0.025000
Training Epoch: 61 [4608/50000]	Loss: 0.4397	LR: 0.025000
Training Epoch: 61 [4736/50000]	Loss: 0.4525	LR: 0.025000
Training Epoch: 61 [4864/50000]	Loss: 0.5112	LR: 0.025000
Training Epoch: 61 [4992/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 61 [5120/50000]	Loss: 0.5405	LR: 0.025000
Training Epoch: 61 [5248/50000]	Loss: 0.3560	LR: 0.025000
Training Epoch: 61 [5376/50000]	Loss: 0.4636	LR: 0.025000
Training Epoch: 61 [5504/50000]	Loss: 0.3799	LR: 0.025000
Training Epoch: 61 [5632/50000]	Loss: 0.6182	LR: 0.025000
Training Epoch: 61 [5760/50000]	Loss: 0.4467	LR: 0.025000
Training Epoch: 61 [5888/50000]	Loss: 0.3694	LR: 0.025000
Training Epoch: 61 [6016/50000]	Loss: 0.5237	LR: 0.025000
Training Epoch: 61 [6144/50000]	Loss: 0.4282	LR: 0.025000
Training Epoch: 61 [6272/50000]	Loss: 0.4220	LR: 0.025000
Training Epoch: 61 [6400/50000]	Loss: 0.4329	LR: 0.025000
Training Epoch: 61 [6528/50000]	Loss: 0.4425	LR: 0.025000
Training Epoch: 61 [6656/50000]	Loss: 0.5110	LR: 0.025000
Training Epoch: 61 [6784/50000]	Loss: 0.4185	LR: 0.025000
Training Epoch: 61 [6912/50000]	Loss: 0.4302	LR: 0.025000
Training Epoch: 61 [7040/50000]	Loss: 0.5620	LR: 0.025000
Training Epoch: 61 [7168/50000]	Loss: 0.4251	LR: 0.025000
Training Epoch: 61 [7296/50000]	Loss: 0.4547	LR: 0.025000
Training Epoch: 61 [7424/50000]	Loss: 0.3331	LR: 0.025000
Training Epoch: 61 [7552/50000]	Loss: 0.4340	LR: 0.025000
Training Epoch: 61 [7680/50000]	Loss: 0.3678	LR: 0.025000
Training Epoch: 61 [7808/50000]	Loss: 0.4843	LR: 0.025000
Training Epoch: 61 [7936/50000]	Loss: 0.4766	LR: 0.025000
Training Epoch: 61 [8064/50000]	Loss: 0.4442	LR: 0.025000
Training Epoch: 61 [8192/50000]	Loss: 0.4400	LR: 0.025000
Training Epoch: 61 [8320/50000]	Loss: 0.4073	LR: 0.025000
Training Epoch: 61 [8448/50000]	Loss: 0.4166	LR: 0.025000
Training Epoch: 61 [8576/50000]	Loss: 0.4535	LR: 0.025000
Training Epoch: 61 [8704/50000]	Loss: 0.3796	LR: 0.025000
Training Epoch: 61 [8832/50000]	Loss: 0.4236	LR: 0.025000
Training Epoch: 61 [8960/50000]	Loss: 0.4555	LR: 0.025000
Training Epoch: 61 [9088/50000]	Loss: 0.4917	LR: 0.025000
Training Epoch: 61 [9216/50000]	Loss: 0.4278	LR: 0.025000
Training Epoch: 61 [9344/50000]	Loss: 0.4338	LR: 0.025000
Training Epoch: 61 [9472/50000]	Loss: 0.4993	LR: 0.025000
Training Epoch: 61 [9600/50000]	Loss: 0.4887	LR: 0.025000
Training Epoch: 61 [9728/50000]	Loss: 0.4411	LR: 0.025000
Training Epoch: 61 [9856/50000]	Loss: 0.4301	LR: 0.025000
Training Epoch: 61 [9984/50000]	Loss: 0.3753	LR: 0.025000
Training Epoch: 61 [10112/50000]	Loss: 0.6661	LR: 0.025000
Training Epoch: 61 [10240/50000]	Loss: 0.4179	LR: 0.025000
Training Epoch: 61 [10368/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 61 [10496/50000]	Loss: 0.5816	LR: 0.025000
Training Epoch: 61 [10624/50000]	Loss: 0.4613	LR: 0.025000
Training Epoch: 61 [10752/50000]	Loss: 0.4002	LR: 0.025000
Training Epoch: 61 [10880/50000]	Loss: 0.3962	LR: 0.025000
Training Epoch: 61 [11008/50000]	Loss: 0.3938	LR: 0.025000
Training Epoch: 61 [11136/50000]	Loss: 0.4007	LR: 0.025000
Training Epoch: 61 [11264/50000]	Loss: 0.5574	LR: 0.025000
Training Epoch: 61 [11392/50000]	Loss: 0.5194	LR: 0.025000
Training Epoch: 61 [11520/50000]	Loss: 0.3793	LR: 0.025000
Training Epoch: 61 [11648/50000]	Loss: 0.4599	LR: 0.025000
Training Epoch: 61 [11776/50000]	Loss: 0.5556	LR: 0.025000
Training Epoch: 61 [11904/50000]	Loss: 0.4764	LR: 0.025000
Training Epoch: 61 [12032/50000]	Loss: 0.4400	LR: 0.025000
Training Epoch: 61 [12160/50000]	Loss: 0.5766	LR: 0.025000
Training Epoch: 61 [12288/50000]	Loss: 0.6686	LR: 0.025000
Training Epoch: 61 [12416/50000]	Loss: 0.5759	LR: 0.025000
Training Epoch: 61 [12544/50000]	Loss: 0.3531	LR: 0.025000
Training Epoch: 61 [12672/50000]	Loss: 0.3961	LR: 0.025000
Training Epoch: 61 [12800/50000]	Loss: 0.4132	LR: 0.025000
Training Epoch: 61 [12928/50000]	Loss: 0.4060	LR: 0.025000
Training Epoch: 61 [13056/50000]	Loss: 0.5365	LR: 0.025000
Training Epoch: 61 [13184/50000]	Loss: 0.4115	LR: 0.025000
Training Epoch: 61 [13312/50000]	Loss: 0.3765	LR: 0.025000
Training Epoch: 61 [13440/50000]	Loss: 0.3286	LR: 0.025000
Training Epoch: 61 [13568/50000]	Loss: 0.5134	LR: 0.025000
Training Epoch: 61 [13696/50000]	Loss: 0.4258	LR: 0.025000
Training Epoch: 61 [13824/50000]	Loss: 0.4738	LR: 0.025000
Training Epoch: 61 [13952/50000]	Loss: 0.5216	LR: 0.025000
Training Epoch: 61 [14080/50000]	Loss: 0.4898	LR: 0.025000
Training Epoch: 61 [14208/50000]	Loss: 0.3839	LR: 0.025000
Training Epoch: 61 [14336/50000]	Loss: 0.3648	LR: 0.025000
Training Epoch: 61 [14464/50000]	Loss: 0.3625	LR: 0.025000
Training Epoch: 61 [14592/50000]	Loss: 0.4916	LR: 0.025000
Training Epoch: 61 [14720/50000]	Loss: 0.6760	LR: 0.025000
Training Epoch: 61 [14848/50000]	Loss: 0.4969	LR: 0.025000
Training Epoch: 61 [14976/50000]	Loss: 0.4482	LR: 0.025000
Training Epoch: 61 [15104/50000]	Loss: 0.3532	LR: 0.025000
Training Epoch: 61 [15232/50000]	Loss: 0.5305	LR: 0.025000
Training Epoch: 61 [15360/50000]	Loss: 0.5655	LR: 0.025000
Training Epoch: 61 [15488/50000]	Loss: 0.4630	LR: 0.025000
Training Epoch: 61 [15616/50000]	Loss: 0.5366	LR: 0.025000
Training Epoch: 61 [15744/50000]	Loss: 0.5414	LR: 0.025000
Training Epoch: 61 [15872/50000]	Loss: 0.4810	LR: 0.025000
Training Epoch: 61 [16000/50000]	Loss: 0.3851	LR: 0.025000
Training Epoch: 61 [16128/50000]	Loss: 0.4096	LR: 0.025000
Training Epoch: 61 [16256/50000]	Loss: 0.3229	LR: 0.025000
Training Epoch: 61 [16384/50000]	Loss: 0.5235	LR: 0.025000
Training Epoch: 61 [16512/50000]	Loss: 0.4149	LR: 0.025000
Training Epoch: 61 [16640/50000]	Loss: 0.6052	LR: 0.025000
Training Epoch: 61 [16768/50000]	Loss: 0.4107	LR: 0.025000
Training Epoch: 61 [16896/50000]	Loss: 0.4865	LR: 0.025000
Training Epoch: 61 [17024/50000]	Loss: 0.6543	LR: 0.025000
Training Epoch: 61 [17152/50000]	Loss: 0.4086	LR: 0.025000
Training Epoch: 61 [17280/50000]	Loss: 0.5304	LR: 0.025000
Training Epoch: 61 [17408/50000]	Loss: 0.3882	LR: 0.025000
Training Epoch: 61 [17536/50000]	Loss: 0.4234	LR: 0.025000
Training Epoch: 61 [17664/50000]	Loss: 0.3402	LR: 0.025000
Training Epoch: 61 [17792/50000]	Loss: 0.6081	LR: 0.025000
Training Epoch: 61 [17920/50000]	Loss: 0.3426	LR: 0.025000
Training Epoch: 61 [18048/50000]	Loss: 0.4678	LR: 0.025000
Training Epoch: 61 [18176/50000]	Loss: 0.4273	LR: 0.025000
Training Epoch: 61 [18304/50000]	Loss: 0.5947	LR: 0.025000
Training Epoch: 61 [18432/50000]	Loss: 0.4416	LR: 0.025000
Training Epoch: 61 [18560/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 61 [18688/50000]	Loss: 0.4504	LR: 0.025000
Training Epoch: 61 [18816/50000]	Loss: 0.5030	LR: 0.025000
Training Epoch: 61 [18944/50000]	Loss: 0.5370	LR: 0.025000
Training Epoch: 61 [19072/50000]	Loss: 0.4756	LR: 0.025000
Training Epoch: 61 [19200/50000]	Loss: 0.4772	LR: 0.025000
Training Epoch: 61 [19328/50000]	Loss: 0.4906	LR: 0.025000
Training Epoch: 61 [19456/50000]	Loss: 0.4247	LR: 0.025000
Training Epoch: 61 [19584/50000]	Loss: 0.4900	LR: 0.025000
Training Epoch: 61 [19712/50000]	Loss: 0.5169	LR: 0.025000
Training Epoch: 61 [19840/50000]	Loss: 0.4565	LR: 0.025000
Training Epoch: 61 [19968/50000]	Loss: 0.5612	LR: 0.025000
Training Epoch: 61 [20096/50000]	Loss: 0.5389	LR: 0.025000
Training Epoch: 61 [20224/50000]	Loss: 0.4602	LR: 0.025000
Training Epoch: 61 [20352/50000]	Loss: 0.4196	LR: 0.025000
Training Epoch: 61 [20480/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 61 [20608/50000]	Loss: 0.4885	LR: 0.025000
Training Epoch: 61 [20736/50000]	Loss: 0.4730	LR: 0.025000
Training Epoch: 61 [20864/50000]	Loss: 0.4287	LR: 0.025000
Training Epoch: 61 [20992/50000]	Loss: 0.2683	LR: 0.025000
Training Epoch: 61 [21120/50000]	Loss: 0.4779	LR: 0.025000
Training Epoch: 61 [21248/50000]	Loss: 0.5172	LR: 0.025000
Training Epoch: 61 [21376/50000]	Loss: 0.5327	LR: 0.025000
Training Epoch: 61 [21504/50000]	Loss: 0.3907	LR: 0.025000
Training Epoch: 61 [21632/50000]	Loss: 0.3274	LR: 0.025000
Training Epoch: 61 [21760/50000]	Loss: 0.4117	LR: 0.025000
Training Epoch: 61 [21888/50000]	Loss: 0.5811	LR: 0.025000
Training Epoch: 61 [22016/50000]	Loss: 0.4323	LR: 0.025000
Training Epoch: 61 [22144/50000]	Loss: 0.4990	LR: 0.025000
Training Epoch: 61 [22272/50000]	Loss: 0.5432	LR: 0.025000
Training Epoch: 61 [22400/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 61 [22528/50000]	Loss: 0.3906	LR: 0.025000
Training Epoch: 61 [22656/50000]	Loss: 0.6243	LR: 0.025000
Training Epoch: 61 [22784/50000]	Loss: 0.4144	LR: 0.025000
Training Epoch: 61 [22912/50000]	Loss: 0.4487	LR: 0.025000
Training Epoch: 61 [23040/50000]	Loss: 0.3746	LR: 0.025000
Training Epoch: 61 [23168/50000]	Loss: 0.6007	LR: 0.025000
Training Epoch: 61 [23296/50000]	Loss: 0.5162	LR: 0.025000
Training Epoch: 61 [23424/50000]	Loss: 0.3514	LR: 0.025000
Training Epoch: 61 [23552/50000]	Loss: 0.3523	LR: 0.025000
Training Epoch: 61 [23680/50000]	Loss: 0.4930	LR: 0.025000
Training Epoch: 61 [23808/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 61 [23936/50000]	Loss: 0.4158	LR: 0.025000
Training Epoch: 61 [24064/50000]	Loss: 0.5139	LR: 0.025000
Training Epoch: 61 [24192/50000]	Loss: 0.5563	LR: 0.025000
Training Epoch: 61 [24320/50000]	Loss: 0.4926	LR: 0.025000
Training Epoch: 61 [24448/50000]	Loss: 0.6229	LR: 0.025000
Training Epoch: 61 [24576/50000]	Loss: 0.4927	LR: 0.025000
Training Epoch: 61 [24704/50000]	Loss: 0.5084	LR: 0.025000
Training Epoch: 61 [24832/50000]	Loss: 0.5349	LR: 0.025000
Training Epoch: 61 [24960/50000]	Loss: 0.3942	LR: 0.025000
Training Epoch: 61 [25088/50000]	Loss: 0.3609	LR: 0.025000
Training Epoch: 61 [25216/50000]	Loss: 0.5596	LR: 0.025000
Training Epoch: 61 [25344/50000]	Loss: 0.5214	LR: 0.025000
Training Epoch: 61 [25472/50000]	Loss: 0.4394	LR: 0.025000
Training Epoch: 61 [25600/50000]	Loss: 0.4689	LR: 0.025000
Training Epoch: 61 [25728/50000]	Loss: 0.4757	LR: 0.025000
Training Epoch: 61 [25856/50000]	Loss: 0.4310	LR: 0.025000
Training Epoch: 61 [25984/50000]	Loss: 0.3836	LR: 0.025000
Training Epoch: 61 [26112/50000]	Loss: 0.4191	LR: 0.025000
Training Epoch: 61 [26240/50000]	Loss: 0.4830	LR: 0.025000
Training Epoch: 61 [26368/50000]	Loss: 0.4856	LR: 0.025000
Training Epoch: 61 [26496/50000]	Loss: 0.6559	LR: 0.025000
Training Epoch: 61 [26624/50000]	Loss: 0.5570	LR: 0.025000
Training Epoch: 61 [26752/50000]	Loss: 0.5427	LR: 0.025000
Training Epoch: 61 [26880/50000]	Loss: 0.4520	LR: 0.025000
Training Epoch: 61 [27008/50000]	Loss: 0.5602	LR: 0.025000
Training Epoch: 61 [27136/50000]	Loss: 0.3770	LR: 0.025000
Training Epoch: 61 [27264/50000]	Loss: 0.5783	LR: 0.025000
Training Epoch: 61 [27392/50000]	Loss: 0.4492	LR: 0.025000
Training Epoch: 61 [27520/50000]	Loss: 0.3629	LR: 0.025000
Training Epoch: 61 [27648/50000]	Loss: 0.5286	LR: 0.025000
Training Epoch: 61 [27776/50000]	Loss: 0.3980	LR: 0.025000
Training Epoch: 61 [27904/50000]	Loss: 0.5936	LR: 0.025000
Training Epoch: 61 [28032/50000]	Loss: 0.5366	LR: 0.025000
Training Epoch: 61 [28160/50000]	Loss: 0.3640	LR: 0.025000
Training Epoch: 61 [28288/50000]	Loss: 0.4803	LR: 0.025000
Training Epoch: 61 [28416/50000]	Loss: 0.5203	LR: 0.025000
Training Epoch: 61 [28544/50000]	Loss: 0.4473	LR: 0.025000
Training Epoch: 61 [28672/50000]	Loss: 0.5675	LR: 0.025000
Training Epoch: 61 [28800/50000]	Loss: 0.5210	LR: 0.025000
Training Epoch: 61 [28928/50000]	Loss: 0.4817	LR: 0.025000
Training Epoch: 61 [29056/50000]	Loss: 0.5970	LR: 0.025000
Training Epoch: 61 [29184/50000]	Loss: 0.3103	LR: 0.025000
Training Epoch: 61 [29312/50000]	Loss: 0.4784	LR: 0.025000
Training Epoch: 61 [29440/50000]	Loss: 0.5926	LR: 0.025000
Training Epoch: 61 [29568/50000]	Loss: 0.4908	LR: 0.025000
Training Epoch: 61 [29696/50000]	Loss: 0.5793	LR: 0.025000
Training Epoch: 61 [29824/50000]	Loss: 0.4928	LR: 0.025000
Training Epoch: 61 [29952/50000]	Loss: 0.4605	LR: 0.025000
Training Epoch: 61 [30080/50000]	Loss: 0.5122	LR: 0.025000
Training Epoch: 61 [30208/50000]	Loss: 0.4144	LR: 0.025000
Training Epoch: 61 [30336/50000]	Loss: 0.3853	LR: 0.025000
Training Epoch: 61 [30464/50000]	Loss: 0.4887	LR: 0.025000
Training Epoch: 61 [30592/50000]	Loss: 0.6064	LR: 0.025000
Training Epoch: 61 [30720/50000]	Loss: 0.4428	LR: 0.025000
Training Epoch: 61 [30848/50000]	Loss: 0.4704	LR: 0.025000
Training Epoch: 61 [30976/50000]	Loss: 0.5287	LR: 0.025000
Training Epoch: 61 [31104/50000]	Loss: 0.5628	LR: 0.025000
Training Epoch: 61 [31232/50000]	Loss: 0.5606	LR: 0.025000
Training Epoch: 61 [31360/50000]	Loss: 0.5852	LR: 0.025000
Training Epoch: 61 [31488/50000]	Loss: 0.5606	LR: 0.025000
Training Epoch: 61 [31616/50000]	Loss: 0.5072	LR: 0.025000
Training Epoch: 61 [31744/50000]	Loss: 0.4460	LR: 0.025000
Training Epoch: 61 [31872/50000]	Loss: 0.4315	LR: 0.025000
Training Epoch: 61 [32000/50000]	Loss: 0.5649	LR: 0.025000
Training Epoch: 61 [32128/50000]	Loss: 0.5018	LR: 0.025000
Training Epoch: 61 [32256/50000]	Loss: 0.4741	LR: 0.025000
Training Epoch: 61 [32384/50000]	Loss: 0.5025	LR: 0.025000
Training Epoch: 61 [32512/50000]	Loss: 0.6380	LR: 0.025000
Training Epoch: 61 [32640/50000]	Loss: 0.5038	LR: 0.025000
Training Epoch: 61 [32768/50000]	Loss: 0.5092	LR: 0.025000
Training Epoch: 61 [32896/50000]	Loss: 0.3723	LR: 0.025000
Training Epoch: 61 [33024/50000]	Loss: 0.5286	LR: 0.025000
Training Epoch: 61 [33152/50000]	Loss: 0.5727	LR: 0.025000
Training Epoch: 61 [33280/50000]	Loss: 0.6244	LR: 0.025000
Training Epoch: 61 [33408/50000]	Loss: 0.4747	LR: 0.025000
Training Epoch: 61 [33536/50000]	Loss: 0.6339	LR: 0.025000
Training Epoch: 61 [33664/50000]	Loss: 0.4924	LR: 0.025000
Training Epoch: 61 [33792/50000]	Loss: 0.7174	LR: 0.025000
Training Epoch: 61 [33920/50000]	Loss: 0.5155	LR: 0.025000
Training Epoch: 61 [34048/50000]	Loss: 0.5828	LR: 0.025000
Training Epoch: 61 [34176/50000]	Loss: 0.6678	LR: 0.025000
Training Epoch: 61 [34304/50000]	Loss: 0.6153	LR: 0.025000
Training Epoch: 61 [34432/50000]	Loss: 0.5322	LR: 0.025000
Training Epoch: 61 [34560/50000]	Loss: 0.5504	LR: 0.025000
Training Epoch: 61 [34688/50000]	Loss: 0.6756	LR: 0.025000
Training Epoch: 61 [34816/50000]	Loss: 0.5171	LR: 0.025000
Training Epoch: 61 [34944/50000]	Loss: 0.6952	LR: 0.025000
Training Epoch: 61 [35072/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 61 [35200/50000]	Loss: 0.4849	LR: 0.025000
Training Epoch: 61 [35328/50000]	Loss: 0.5382	LR: 0.025000
Training Epoch: 61 [35456/50000]	Loss: 0.5568	LR: 0.025000
Training Epoch: 61 [35584/50000]	Loss: 0.5855	LR: 0.025000
Training Epoch: 61 [35712/50000]	Loss: 0.5074	LR: 0.025000
Training Epoch: 61 [35840/50000]	Loss: 0.4416	LR: 0.025000
Training Epoch: 61 [35968/50000]	Loss: 0.6085	LR: 0.025000
Training Epoch: 61 [36096/50000]	Loss: 0.5804	LR: 0.025000
Training Epoch: 61 [36224/50000]	Loss: 0.5508	LR: 0.025000
Training Epoch: 61 [36352/50000]	Loss: 0.4113	LR: 0.025000
Training Epoch: 61 [36480/50000]	Loss: 0.3665	LR: 0.025000
Training Epoch: 61 [36608/50000]	Loss: 0.5748	LR: 0.025000
Training Epoch: 61 [36736/50000]	Loss: 0.6104	LR: 0.025000
Training Epoch: 61 [36864/50000]	Loss: 0.4829	LR: 0.025000
Training Epoch: 61 [36992/50000]	Loss: 0.4865	LR: 0.025000
Training Epoch: 61 [37120/50000]	Loss: 0.5925	LR: 0.025000
Training Epoch: 61 [37248/50000]	Loss: 0.5611	LR: 0.025000
Training Epoch: 61 [37376/50000]	Loss: 0.5140	LR: 0.025000
Training Epoch: 61 [37504/50000]	Loss: 0.5009	LR: 0.025000
Training Epoch: 61 [37632/50000]	Loss: 0.4733	LR: 0.025000
Training Epoch: 61 [37760/50000]	Loss: 0.6119	LR: 0.025000
Training Epoch: 61 [37888/50000]	Loss: 0.4485	LR: 0.025000
Training Epoch: 61 [38016/50000]	Loss: 0.6562	LR: 0.025000
Training Epoch: 61 [38144/50000]	Loss: 0.4800	LR: 0.025000
Training Epoch: 61 [38272/50000]	Loss: 0.7060	LR: 0.025000
Training Epoch: 61 [38400/50000]	Loss: 0.5047	LR: 0.025000
Training Epoch: 61 [38528/50000]	Loss: 0.6125	LR: 0.025000
Training Epoch: 61 [38656/50000]	Loss: 0.4930	LR: 0.025000
Training Epoch: 61 [38784/50000]	Loss: 0.6990	LR: 0.025000
Training Epoch: 61 [38912/50000]	Loss: 0.6311	LR: 0.025000
Training Epoch: 61 [39040/50000]	Loss: 0.4640	LR: 0.025000
Training Epoch: 61 [39168/50000]	Loss: 0.4617	LR: 0.025000
Training Epoch: 61 [39296/50000]	Loss: 0.5617	LR: 0.025000
Training Epoch: 61 [39424/50000]	Loss: 0.3297	LR: 0.025000
Training Epoch: 61 [39552/50000]	Loss: 0.6730	LR: 0.025000
Training Epoch: 61 [39680/50000]	Loss: 0.4681	LR: 0.025000
Training Epoch: 61 [39808/50000]	Loss: 0.4959	LR: 0.025000
Training Epoch: 61 [39936/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 61 [40064/50000]	Loss: 0.3980	LR: 0.025000
Training Epoch: 61 [40192/50000]	Loss: 0.3997	LR: 0.025000
Training Epoch: 61 [40320/50000]	Loss: 0.4749	LR: 0.025000
Training Epoch: 61 [40448/50000]	Loss: 0.4230	LR: 0.025000
Training Epoch: 61 [40576/50000]	Loss: 0.5793	LR: 0.025000
Training Epoch: 61 [40704/50000]	Loss: 0.5996	LR: 0.025000
Training Epoch: 61 [40832/50000]	Loss: 0.4946	LR: 0.025000
Training Epoch: 61 [40960/50000]	Loss: 0.5187	LR: 0.025000
Training Epoch: 61 [41088/50000]	Loss: 0.5175	LR: 0.025000
Training Epoch: 61 [41216/50000]	Loss: 0.3682	LR: 0.025000
Training Epoch: 61 [41344/50000]	Loss: 0.5397	LR: 0.025000
Training Epoch: 61 [41472/50000]	Loss: 0.4723	LR: 0.025000
Training Epoch: 61 [41600/50000]	Loss: 0.6151	LR: 0.025000
Training Epoch: 61 [41728/50000]	Loss: 0.4596	LR: 0.025000
Training Epoch: 61 [41856/50000]	Loss: 0.5788	LR: 0.025000
Training Epoch: 61 [41984/50000]	Loss: 0.5138	LR: 0.025000
Training Epoch: 61 [42112/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 61 [42240/50000]	Loss: 0.5545	LR: 0.025000
Training Epoch: 61 [42368/50000]	Loss: 0.4884	LR: 0.025000
Training Epoch: 61 [42496/50000]	Loss: 0.6435	LR: 0.025000
Training Epoch: 61 [42624/50000]	Loss: 0.6314	LR: 0.025000
Training Epoch: 61 [42752/50000]	Loss: 0.5062	LR: 0.025000
Training Epoch: 61 [42880/50000]	Loss: 0.6681	LR: 0.025000
Training Epoch: 61 [43008/50000]	Loss: 0.7734	LR: 0.025000
Training Epoch: 61 [43136/50000]	Loss: 0.4726	LR: 0.025000
Training Epoch: 61 [43264/50000]	Loss: 0.6589	LR: 0.025000
Training Epoch: 61 [43392/50000]	Loss: 0.5070	LR: 0.025000
Training Epoch: 61 [43520/50000]	Loss: 0.4609	LR: 0.025000
Training Epoch: 61 [43648/50000]	Loss: 0.6297	LR: 0.025000
Training Epoch: 61 [43776/50000]	Loss: 0.4979	LR: 0.025000
Training Epoch: 61 [43904/50000]	Loss: 0.4893	LR: 0.025000
Training Epoch: 61 [44032/50000]	Loss: 0.4511	LR: 0.025000
Training Epoch: 61 [44160/50000]	Loss: 0.6200	LR: 0.025000
Training Epoch: 61 [44288/50000]	Loss: 0.4152	LR: 0.025000
Training Epoch: 61 [44416/50000]	Loss: 0.6021	LR: 0.025000
Training Epoch: 61 [44544/50000]	Loss: 0.3970	LR: 0.025000
Training Epoch: 61 [44672/50000]	Loss: 0.5695	LR: 0.025000
Training Epoch: 61 [44800/50000]	Loss: 0.7166	LR: 0.025000
Training Epoch: 61 [44928/50000]	Loss: 0.5310	LR: 0.025000
Training Epoch: 61 [45056/50000]	Loss: 0.6144	LR: 0.025000
Training Epoch: 61 [45184/50000]	Loss: 0.4818	LR: 0.025000
Training Epoch: 61 [45312/50000]	Loss: 0.5986	LR: 0.025000
Training Epoch: 61 [45440/50000]	Loss: 0.6547	LR: 0.025000
Training Epoch: 61 [45568/50000]	Loss: 0.5880	LR: 0.025000
Training Epoch: 61 [45696/50000]	Loss: 0.6326	LR: 0.025000
Training Epoch: 61 [45824/50000]	Loss: 0.5645	LR: 0.025000
Training Epoch: 61 [45952/50000]	Loss: 0.4599	LR: 0.025000
Training Epoch: 61 [46080/50000]	Loss: 0.5988	LR: 0.025000
Training Epoch: 61 [46208/50000]	Loss: 0.4768	LR: 0.025000
Training Epoch: 61 [46336/50000]	Loss: 0.5235	LR: 0.025000
Training Epoch: 61 [46464/50000]	Loss: 0.5043	LR: 0.025000
Training Epoch: 61 [46592/50000]	Loss: 0.5828	LR: 0.025000
Training Epoch: 61 [46720/50000]	Loss: 0.4637	LR: 0.025000
Training Epoch: 61 [46848/50000]	Loss: 0.6514	LR: 0.025000
Training Epoch: 61 [46976/50000]	Loss: 0.4776	LR: 0.025000
Training Epoch: 61 [47104/50000]	Loss: 0.6357	LR: 0.025000
Training Epoch: 61 [47232/50000]	Loss: 0.4714	LR: 0.025000
Training Epoch: 61 [47360/50000]	Loss: 0.5111	LR: 0.025000
Training Epoch: 61 [47488/50000]	Loss: 0.4730	LR: 0.025000
Training Epoch: 61 [47616/50000]	Loss: 0.5496	LR: 0.025000
Training Epoch: 61 [47744/50000]	Loss: 0.6328	LR: 0.025000
Training Epoch: 61 [47872/50000]	Loss: 0.3934	LR: 0.025000
Training Epoch: 61 [48000/50000]	Loss: 0.5525	LR: 0.025000
Training Epoch: 61 [48128/50000]	Loss: 0.6315	LR: 0.025000
Training Epoch: 61 [48256/50000]	Loss: 0.4883	LR: 0.025000
Training Epoch: 61 [48384/50000]	Loss: 0.6083	LR: 0.025000
Training Epoch: 61 [48512/50000]	Loss: 0.4068	LR: 0.025000
Training Epoch: 61 [48640/50000]	Loss: 0.5233	LR: 0.025000
Training Epoch: 61 [48768/50000]	Loss: 0.5845	LR: 0.025000
Training Epoch: 61 [48896/50000]	Loss: 0.5659	LR: 0.025000
Training Epoch: 61 [49024/50000]	Loss: 0.5716	LR: 0.025000
Training Epoch: 61 [49152/50000]	Loss: 0.6412	LR: 0.025000
Training Epoch: 61 [49280/50000]	Loss: 0.4537	LR: 0.025000
Training Epoch: 61 [49408/50000]	Loss: 0.6189	LR: 0.025000
Training Epoch: 61 [49536/50000]	Loss: 0.4354	LR: 0.025000
Training Epoch: 61 [49664/50000]	Loss: 0.6173	LR: 0.025000
Training Epoch: 61 [49792/50000]	Loss: 0.4075	LR: 0.025000
Training Epoch: 61 [49920/50000]	Loss: 0.5514	LR: 0.025000
Training Epoch: 61 [50000/50000]	Loss: 0.6161	LR: 0.025000
epoch 61 training time consumed: 53.82s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  223873 GB |  223873 GB |
|       from large pool |  123392 KB |    1034 MB |  223653 GB |  223653 GB |
|       from small pool |   10798 KB |      13 MB |     220 GB |     220 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  223873 GB |  223873 GB |
|       from large pool |  123392 KB |    1034 MB |  223653 GB |  223653 GB |
|       from small pool |   10798 KB |      13 MB |     220 GB |     220 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |   98518 GB |   98518 GB |
|       from large pool |  155136 KB |  433088 KB |   98274 GB |   98274 GB |
|       from small pool |    1490 KB |    3494 KB |     243 GB |     243 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    8638 K  |    8638 K  |
|       from large pool |      24    |      65    |    4509 K  |    4509 K  |
|       from small pool |     231    |     274    |    4129 K  |    4129 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    8638 K  |    8638 K  |
|       from large pool |      24    |      65    |    4509 K  |    4509 K  |
|       from small pool |     231    |     274    |    4129 K  |    4129 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4269 K  |    4268 K  |
|       from large pool |       9    |      14    |    2182 K  |    2182 K  |
|       from small pool |      12    |      16    |    2086 K  |    2086 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 61, Average loss: 0.0110, Accuracy: 0.6580, Time consumed:3.47s

Training Epoch: 62 [128/50000]	Loss: 0.4063	LR: 0.025000
Training Epoch: 62 [256/50000]	Loss: 0.4632	LR: 0.025000
Training Epoch: 62 [384/50000]	Loss: 0.3910	LR: 0.025000
Training Epoch: 62 [512/50000]	Loss: 0.5118	LR: 0.025000
Training Epoch: 62 [640/50000]	Loss: 0.5833	LR: 0.025000
Training Epoch: 62 [768/50000]	Loss: 0.4455	LR: 0.025000
Training Epoch: 62 [896/50000]	Loss: 0.3913	LR: 0.025000
Training Epoch: 62 [1024/50000]	Loss: 0.4383	LR: 0.025000
Training Epoch: 62 [1152/50000]	Loss: 0.4975	LR: 0.025000
Training Epoch: 62 [1280/50000]	Loss: 0.5291	LR: 0.025000
Training Epoch: 62 [1408/50000]	Loss: 0.5194	LR: 0.025000
Training Epoch: 62 [1536/50000]	Loss: 0.5224	LR: 0.025000
Training Epoch: 62 [1664/50000]	Loss: 0.3312	LR: 0.025000
Training Epoch: 62 [1792/50000]	Loss: 0.4418	LR: 0.025000
Training Epoch: 62 [1920/50000]	Loss: 0.4977	LR: 0.025000
Training Epoch: 62 [2048/50000]	Loss: 0.3811	LR: 0.025000
Training Epoch: 62 [2176/50000]	Loss: 0.5351	LR: 0.025000
Training Epoch: 62 [2304/50000]	Loss: 0.3878	LR: 0.025000
Training Epoch: 62 [2432/50000]	Loss: 0.4365	LR: 0.025000
Training Epoch: 62 [2560/50000]	Loss: 0.5088	LR: 0.025000
Training Epoch: 62 [2688/50000]	Loss: 0.4471	LR: 0.025000
Training Epoch: 62 [2816/50000]	Loss: 0.4226	LR: 0.025000
Training Epoch: 62 [2944/50000]	Loss: 0.4071	LR: 0.025000
Training Epoch: 62 [3072/50000]	Loss: 0.5750	LR: 0.025000
Training Epoch: 62 [3200/50000]	Loss: 0.4201	LR: 0.025000
Training Epoch: 62 [3328/50000]	Loss: 0.5009	LR: 0.025000
Training Epoch: 62 [3456/50000]	Loss: 0.5460	LR: 0.025000
Training Epoch: 62 [3584/50000]	Loss: 0.5212	LR: 0.025000
Training Epoch: 62 [3712/50000]	Loss: 0.4704	LR: 0.025000
Training Epoch: 62 [3840/50000]	Loss: 0.4686	LR: 0.025000
Training Epoch: 62 [3968/50000]	Loss: 0.4023	LR: 0.025000
Training Epoch: 62 [4096/50000]	Loss: 0.4401	LR: 0.025000
Training Epoch: 62 [4224/50000]	Loss: 0.4315	LR: 0.025000
Training Epoch: 62 [4352/50000]	Loss: 0.4680	LR: 0.025000
Training Epoch: 62 [4480/50000]	Loss: 0.3793	LR: 0.025000
Training Epoch: 62 [4608/50000]	Loss: 0.5332	LR: 0.025000
Training Epoch: 62 [4736/50000]	Loss: 0.4067	LR: 0.025000
Training Epoch: 62 [4864/50000]	Loss: 0.5466	LR: 0.025000
Training Epoch: 62 [4992/50000]	Loss: 0.4454	LR: 0.025000
Training Epoch: 62 [5120/50000]	Loss: 0.5611	LR: 0.025000
Training Epoch: 62 [5248/50000]	Loss: 0.4830	LR: 0.025000
Training Epoch: 62 [5376/50000]	Loss: 0.4381	LR: 0.025000
Training Epoch: 62 [5504/50000]	Loss: 0.3476	LR: 0.025000
Training Epoch: 62 [5632/50000]	Loss: 0.3460	LR: 0.025000
Training Epoch: 62 [5760/50000]	Loss: 0.4326	LR: 0.025000
Training Epoch: 62 [5888/50000]	Loss: 0.4151	LR: 0.025000
Training Epoch: 62 [6016/50000]	Loss: 0.4669	LR: 0.025000
Training Epoch: 62 [6144/50000]	Loss: 0.4265	LR: 0.025000
Training Epoch: 62 [6272/50000]	Loss: 0.3524	LR: 0.025000
Training Epoch: 62 [6400/50000]	Loss: 0.4901	LR: 0.025000
Training Epoch: 62 [6528/50000]	Loss: 0.4049	LR: 0.025000
Training Epoch: 62 [6656/50000]	Loss: 0.4316	LR: 0.025000
Training Epoch: 62 [6784/50000]	Loss: 0.3692	LR: 0.025000
Training Epoch: 62 [6912/50000]	Loss: 0.3708	LR: 0.025000
Training Epoch: 62 [7040/50000]	Loss: 0.6040	LR: 0.025000
Training Epoch: 62 [7168/50000]	Loss: 0.4317	LR: 0.025000
Training Epoch: 62 [7296/50000]	Loss: 0.3555	LR: 0.025000
Training Epoch: 62 [7424/50000]	Loss: 0.3717	LR: 0.025000
Training Epoch: 62 [7552/50000]	Loss: 0.4740	LR: 0.025000
Training Epoch: 62 [7680/50000]	Loss: 0.5309	LR: 0.025000
Training Epoch: 62 [7808/50000]	Loss: 0.4613	LR: 0.025000
Training Epoch: 62 [7936/50000]	Loss: 0.5203	LR: 0.025000
Training Epoch: 62 [8064/50000]	Loss: 0.4654	LR: 0.025000
Training Epoch: 62 [8192/50000]	Loss: 0.3941	LR: 0.025000
Training Epoch: 62 [8320/50000]	Loss: 0.5011	LR: 0.025000
Training Epoch: 62 [8448/50000]	Loss: 0.3791	LR: 0.025000
Training Epoch: 62 [8576/50000]	Loss: 0.3307	LR: 0.025000
Training Epoch: 62 [8704/50000]	Loss: 0.2657	LR: 0.025000
Training Epoch: 62 [8832/50000]	Loss: 0.3458	LR: 0.025000
Training Epoch: 62 [8960/50000]	Loss: 0.4292	LR: 0.025000
Training Epoch: 62 [9088/50000]	Loss: 0.3459	LR: 0.025000
Training Epoch: 62 [9216/50000]	Loss: 0.4423	LR: 0.025000
Training Epoch: 62 [9344/50000]	Loss: 0.4745	LR: 0.025000
Training Epoch: 62 [9472/50000]	Loss: 0.4741	LR: 0.025000
Training Epoch: 62 [9600/50000]	Loss: 0.4278	LR: 0.025000
Training Epoch: 62 [9728/50000]	Loss: 0.5604	LR: 0.025000
Training Epoch: 62 [9856/50000]	Loss: 0.4706	LR: 0.025000
Training Epoch: 62 [9984/50000]	Loss: 0.4721	LR: 0.025000
Training Epoch: 62 [10112/50000]	Loss: 0.4053	LR: 0.025000
Training Epoch: 62 [10240/50000]	Loss: 0.3882	LR: 0.025000
Training Epoch: 62 [10368/50000]	Loss: 0.5282	LR: 0.025000
Training Epoch: 62 [10496/50000]	Loss: 0.3201	LR: 0.025000
Training Epoch: 62 [10624/50000]	Loss: 0.3738	LR: 0.025000
Training Epoch: 62 [10752/50000]	Loss: 0.6257	LR: 0.025000
Training Epoch: 62 [10880/50000]	Loss: 0.4015	LR: 0.025000
Training Epoch: 62 [11008/50000]	Loss: 0.4039	LR: 0.025000
Training Epoch: 62 [11136/50000]	Loss: 0.7831	LR: 0.025000
Training Epoch: 62 [11264/50000]	Loss: 0.3957	LR: 0.025000
Training Epoch: 62 [11392/50000]	Loss: 0.5624	LR: 0.025000
Training Epoch: 62 [11520/50000]	Loss: 0.4066	LR: 0.025000
Training Epoch: 62 [11648/50000]	Loss: 0.4890	LR: 0.025000
Training Epoch: 62 [11776/50000]	Loss: 0.5280	LR: 0.025000
Training Epoch: 62 [11904/50000]	Loss: 0.5540	LR: 0.025000
Training Epoch: 62 [12032/50000]	Loss: 0.5359	LR: 0.025000
Training Epoch: 62 [12160/50000]	Loss: 0.3662	LR: 0.025000
Training Epoch: 62 [12288/50000]	Loss: 0.5366	LR: 0.025000
Training Epoch: 62 [12416/50000]	Loss: 0.3967	LR: 0.025000
Training Epoch: 62 [12544/50000]	Loss: 0.5274	LR: 0.025000
Training Epoch: 62 [12672/50000]	Loss: 0.3407	LR: 0.025000
Training Epoch: 62 [12800/50000]	Loss: 0.4133	LR: 0.025000
Training Epoch: 62 [12928/50000]	Loss: 0.3864	LR: 0.025000
Training Epoch: 62 [13056/50000]	Loss: 0.4950	LR: 0.025000
Training Epoch: 62 [13184/50000]	Loss: 0.5306	LR: 0.025000
Training Epoch: 62 [13312/50000]	Loss: 0.4432	LR: 0.025000
Training Epoch: 62 [13440/50000]	Loss: 0.6215	LR: 0.025000
Training Epoch: 62 [13568/50000]	Loss: 0.5412	LR: 0.025000
Training Epoch: 62 [13696/50000]	Loss: 0.3090	LR: 0.025000
Training Epoch: 62 [13824/50000]	Loss: 0.3640	LR: 0.025000
Training Epoch: 62 [13952/50000]	Loss: 0.3731	LR: 0.025000
Training Epoch: 62 [14080/50000]	Loss: 0.3839	LR: 0.025000
Training Epoch: 62 [14208/50000]	Loss: 0.4438	LR: 0.025000
Training Epoch: 62 [14336/50000]	Loss: 0.4145	LR: 0.025000
Training Epoch: 62 [14464/50000]	Loss: 0.5980	LR: 0.025000
Training Epoch: 62 [14592/50000]	Loss: 0.3755	LR: 0.025000
Training Epoch: 62 [14720/50000]	Loss: 0.4248	LR: 0.025000
Training Epoch: 62 [14848/50000]	Loss: 0.6444	LR: 0.025000
Training Epoch: 62 [14976/50000]	Loss: 0.4996	LR: 0.025000
Training Epoch: 62 [15104/50000]	Loss: 0.5230	LR: 0.025000
Training Epoch: 62 [15232/50000]	Loss: 0.4367	LR: 0.025000
Training Epoch: 62 [15360/50000]	Loss: 0.3936	LR: 0.025000
Training Epoch: 62 [15488/50000]	Loss: 0.4115	LR: 0.025000
Training Epoch: 62 [15616/50000]	Loss: 0.4609	LR: 0.025000
Training Epoch: 62 [15744/50000]	Loss: 0.3606	LR: 0.025000
Training Epoch: 62 [15872/50000]	Loss: 0.4341	LR: 0.025000
Training Epoch: 62 [16000/50000]	Loss: 0.5096	LR: 0.025000
Training Epoch: 62 [16128/50000]	Loss: 0.4435	LR: 0.025000
Training Epoch: 62 [16256/50000]	Loss: 0.4785	LR: 0.025000
Training Epoch: 62 [16384/50000]	Loss: 0.3393	LR: 0.025000
Training Epoch: 62 [16512/50000]	Loss: 0.4517	LR: 0.025000
Training Epoch: 62 [16640/50000]	Loss: 0.5369	LR: 0.025000
Training Epoch: 62 [16768/50000]	Loss: 0.3900	LR: 0.025000
Training Epoch: 62 [16896/50000]	Loss: 0.5234	LR: 0.025000
Training Epoch: 62 [17024/50000]	Loss: 0.4899	LR: 0.025000
Training Epoch: 62 [17152/50000]	Loss: 0.3899	LR: 0.025000
Training Epoch: 62 [17280/50000]	Loss: 0.4594	LR: 0.025000
Training Epoch: 62 [17408/50000]	Loss: 0.3724	LR: 0.025000
Training Epoch: 62 [17536/50000]	Loss: 0.4204	LR: 0.025000
Training Epoch: 62 [17664/50000]	Loss: 0.4053	LR: 0.025000
Training Epoch: 62 [17792/50000]	Loss: 0.3703	LR: 0.025000
Training Epoch: 62 [17920/50000]	Loss: 0.4490	LR: 0.025000
Training Epoch: 62 [18048/50000]	Loss: 0.5067	LR: 0.025000
Training Epoch: 62 [18176/50000]	Loss: 0.3824	LR: 0.025000
Training Epoch: 62 [18304/50000]	Loss: 0.5333	LR: 0.025000
Training Epoch: 62 [18432/50000]	Loss: 0.4639	LR: 0.025000
Training Epoch: 62 [18560/50000]	Loss: 0.4423	LR: 0.025000
Training Epoch: 62 [18688/50000]	Loss: 0.5030	LR: 0.025000
Training Epoch: 62 [18816/50000]	Loss: 0.4640	LR: 0.025000
Training Epoch: 62 [18944/50000]	Loss: 0.4266	LR: 0.025000
Training Epoch: 62 [19072/50000]	Loss: 0.5696	LR: 0.025000
Training Epoch: 62 [19200/50000]	Loss: 0.5216	LR: 0.025000
Training Epoch: 62 [19328/50000]	Loss: 0.7353	LR: 0.025000
Training Epoch: 62 [19456/50000]	Loss: 0.3717	LR: 0.025000
Training Epoch: 62 [19584/50000]	Loss: 0.4084	LR: 0.025000
Training Epoch: 62 [19712/50000]	Loss: 0.5772	LR: 0.025000
Training Epoch: 62 [19840/50000]	Loss: 0.5105	LR: 0.025000
Training Epoch: 62 [19968/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 62 [20096/50000]	Loss: 0.5081	LR: 0.025000
Training Epoch: 62 [20224/50000]	Loss: 0.3962	LR: 0.025000
Training Epoch: 62 [20352/50000]	Loss: 0.5442	LR: 0.025000
Training Epoch: 62 [20480/50000]	Loss: 0.5583	LR: 0.025000
Training Epoch: 62 [20608/50000]	Loss: 0.4195	LR: 0.025000
Training Epoch: 62 [20736/50000]	Loss: 0.4807	LR: 0.025000
Training Epoch: 62 [20864/50000]	Loss: 0.5560	LR: 0.025000
Training Epoch: 62 [20992/50000]	Loss: 0.4155	LR: 0.025000
Training Epoch: 62 [21120/50000]	Loss: 0.4750	LR: 0.025000
Training Epoch: 62 [21248/50000]	Loss: 0.3751	LR: 0.025000
Training Epoch: 62 [21376/50000]	Loss: 0.4706	LR: 0.025000
Training Epoch: 62 [21504/50000]	Loss: 0.4213	LR: 0.025000
Training Epoch: 62 [21632/50000]	Loss: 0.3836	LR: 0.025000
Training Epoch: 62 [21760/50000]	Loss: 0.4508	LR: 0.025000
Training Epoch: 62 [21888/50000]	Loss: 0.4597	LR: 0.025000
Training Epoch: 62 [22016/50000]	Loss: 0.4975	LR: 0.025000
Training Epoch: 62 [22144/50000]	Loss: 0.4254	LR: 0.025000
Training Epoch: 62 [22272/50000]	Loss: 0.4746	LR: 0.025000
Training Epoch: 62 [22400/50000]	Loss: 0.4441	LR: 0.025000
Training Epoch: 62 [22528/50000]	Loss: 0.4149	LR: 0.025000
Training Epoch: 62 [22656/50000]	Loss: 0.4336	LR: 0.025000
Training Epoch: 62 [22784/50000]	Loss: 0.4716	LR: 0.025000
Training Epoch: 62 [22912/50000]	Loss: 0.4921	LR: 0.025000
Training Epoch: 62 [23040/50000]	Loss: 0.3788	LR: 0.025000
Training Epoch: 62 [23168/50000]	Loss: 0.4861	LR: 0.025000
Training Epoch: 62 [23296/50000]	Loss: 0.5633	LR: 0.025000
Training Epoch: 62 [23424/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 62 [23552/50000]	Loss: 0.3256	LR: 0.025000
Training Epoch: 62 [23680/50000]	Loss: 0.5529	LR: 0.025000
Training Epoch: 62 [23808/50000]	Loss: 0.5125	LR: 0.025000
Training Epoch: 62 [23936/50000]	Loss: 0.3924	LR: 0.025000
Training Epoch: 62 [24064/50000]	Loss: 0.4907	LR: 0.025000
Training Epoch: 62 [24192/50000]	Loss: 0.5065	LR: 0.025000
Training Epoch: 62 [24320/50000]	Loss: 0.5225	LR: 0.025000
Training Epoch: 62 [24448/50000]	Loss: 0.3498	LR: 0.025000
Training Epoch: 62 [24576/50000]	Loss: 0.3497	LR: 0.025000
Training Epoch: 62 [24704/50000]	Loss: 0.5399	LR: 0.025000
Training Epoch: 62 [24832/50000]	Loss: 0.4388	LR: 0.025000
Training Epoch: 62 [24960/50000]	Loss: 0.5215	LR: 0.025000
Training Epoch: 62 [25088/50000]	Loss: 0.4465	LR: 0.025000
Training Epoch: 62 [25216/50000]	Loss: 0.5402	LR: 0.025000
Training Epoch: 62 [25344/50000]	Loss: 0.7170	LR: 0.025000
Training Epoch: 62 [25472/50000]	Loss: 0.4827	LR: 0.025000
Training Epoch: 62 [25600/50000]	Loss: 0.2923	LR: 0.025000
Training Epoch: 62 [25728/50000]	Loss: 0.3704	LR: 0.025000
Training Epoch: 62 [25856/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 62 [25984/50000]	Loss: 0.6826	LR: 0.025000
Training Epoch: 62 [26112/50000]	Loss: 0.4293	LR: 0.025000
Training Epoch: 62 [26240/50000]	Loss: 0.4231	LR: 0.025000
Training Epoch: 62 [26368/50000]	Loss: 0.5865	LR: 0.025000
Training Epoch: 62 [26496/50000]	Loss: 0.4183	LR: 0.025000
Training Epoch: 62 [26624/50000]	Loss: 0.4227	LR: 0.025000
Training Epoch: 62 [26752/50000]	Loss: 0.5482	LR: 0.025000
Training Epoch: 62 [26880/50000]	Loss: 0.4083	LR: 0.025000
Training Epoch: 62 [27008/50000]	Loss: 0.4436	LR: 0.025000
Training Epoch: 62 [27136/50000]	Loss: 0.4868	LR: 0.025000
Training Epoch: 62 [27264/50000]	Loss: 0.4695	LR: 0.025000
Training Epoch: 62 [27392/50000]	Loss: 0.4778	LR: 0.025000
Training Epoch: 62 [27520/50000]	Loss: 0.3777	LR: 0.025000
Training Epoch: 62 [27648/50000]	Loss: 0.5187	LR: 0.025000
Training Epoch: 62 [27776/50000]	Loss: 0.5098	LR: 0.025000
Training Epoch: 62 [27904/50000]	Loss: 0.5000	LR: 0.025000
Training Epoch: 62 [28032/50000]	Loss: 0.5035	LR: 0.025000
Training Epoch: 62 [28160/50000]	Loss: 0.5350	LR: 0.025000
Training Epoch: 62 [28288/50000]	Loss: 0.4237	LR: 0.025000
Training Epoch: 62 [28416/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 62 [28544/50000]	Loss: 0.5771	LR: 0.025000
Training Epoch: 62 [28672/50000]	Loss: 0.4605	LR: 0.025000
Training Epoch: 62 [28800/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 62 [28928/50000]	Loss: 0.4475	LR: 0.025000
Training Epoch: 62 [29056/50000]	Loss: 0.5876	LR: 0.025000
Training Epoch: 62 [29184/50000]	Loss: 0.6604	LR: 0.025000
Training Epoch: 62 [29312/50000]	Loss: 0.5741	LR: 0.025000
Training Epoch: 62 [29440/50000]	Loss: 0.5179	LR: 0.025000
Training Epoch: 62 [29568/50000]	Loss: 0.4036	LR: 0.025000
Training Epoch: 62 [29696/50000]	Loss: 0.4608	LR: 0.025000
Training Epoch: 62 [29824/50000]	Loss: 0.3967	LR: 0.025000
Training Epoch: 62 [29952/50000]	Loss: 0.4195	LR: 0.025000
Training Epoch: 62 [30080/50000]	Loss: 0.4167	LR: 0.025000
Training Epoch: 62 [30208/50000]	Loss: 0.4296	LR: 0.025000
Training Epoch: 62 [30336/50000]	Loss: 0.3973	LR: 0.025000
Training Epoch: 62 [30464/50000]	Loss: 0.4239	LR: 0.025000
Training Epoch: 62 [30592/50000]	Loss: 0.4529	LR: 0.025000
Training Epoch: 62 [30720/50000]	Loss: 0.3681	LR: 0.025000
Training Epoch: 62 [30848/50000]	Loss: 0.6046	LR: 0.025000
Training Epoch: 62 [30976/50000]	Loss: 0.5276	LR: 0.025000
Training Epoch: 62 [31104/50000]	Loss: 0.3585	LR: 0.025000
Training Epoch: 62 [31232/50000]	Loss: 0.3940	LR: 0.025000
Training Epoch: 62 [31360/50000]	Loss: 0.4172	LR: 0.025000
Training Epoch: 62 [31488/50000]	Loss: 0.4906	LR: 0.025000
Training Epoch: 62 [31616/50000]	Loss: 0.4540	LR: 0.025000
Training Epoch: 62 [31744/50000]	Loss: 0.5444	LR: 0.025000
Training Epoch: 62 [31872/50000]	Loss: 0.7122	LR: 0.025000
Training Epoch: 62 [32000/50000]	Loss: 0.5302	LR: 0.025000
Training Epoch: 62 [32128/50000]	Loss: 0.4835	LR: 0.025000
Training Epoch: 62 [32256/50000]	Loss: 0.4899	LR: 0.025000
Training Epoch: 62 [32384/50000]	Loss: 0.5706	LR: 0.025000
Training Epoch: 62 [32512/50000]	Loss: 0.5202	LR: 0.025000
Training Epoch: 62 [32640/50000]	Loss: 0.5453	LR: 0.025000
Training Epoch: 62 [32768/50000]	Loss: 0.4634	LR: 0.025000
Training Epoch: 62 [32896/50000]	Loss: 0.5738	LR: 0.025000
Training Epoch: 62 [33024/50000]	Loss: 0.5980	LR: 0.025000
Training Epoch: 62 [33152/50000]	Loss: 0.6088	LR: 0.025000
Training Epoch: 62 [33280/50000]	Loss: 0.5175	LR: 0.025000
Training Epoch: 62 [33408/50000]	Loss: 0.5509	LR: 0.025000
Training Epoch: 62 [33536/50000]	Loss: 0.6201	LR: 0.025000
Training Epoch: 62 [33664/50000]	Loss: 0.4794	LR: 0.025000
Training Epoch: 62 [33792/50000]	Loss: 0.4189	LR: 0.025000
Training Epoch: 62 [33920/50000]	Loss: 0.3810	LR: 0.025000
Training Epoch: 62 [34048/50000]	Loss: 0.4777	LR: 0.025000
Training Epoch: 62 [34176/50000]	Loss: 0.5150	LR: 0.025000
Training Epoch: 62 [34304/50000]	Loss: 0.5410	LR: 0.025000
Training Epoch: 62 [34432/50000]	Loss: 0.4570	LR: 0.025000
Training Epoch: 62 [34560/50000]	Loss: 0.4834	LR: 0.025000
Training Epoch: 62 [34688/50000]	Loss: 0.5313	LR: 0.025000
Training Epoch: 62 [34816/50000]	Loss: 0.5992	LR: 0.025000
Training Epoch: 62 [34944/50000]	Loss: 0.6071	LR: 0.025000
Training Epoch: 62 [35072/50000]	Loss: 0.4345	LR: 0.025000
Training Epoch: 62 [35200/50000]	Loss: 0.5592	LR: 0.025000
Training Epoch: 62 [35328/50000]	Loss: 0.5560	LR: 0.025000
Training Epoch: 62 [35456/50000]	Loss: 0.7322	LR: 0.025000
Training Epoch: 62 [35584/50000]	Loss: 0.7349	LR: 0.025000
Training Epoch: 62 [35712/50000]	Loss: 0.4423	LR: 0.025000
Training Epoch: 62 [35840/50000]	Loss: 0.6217	LR: 0.025000
Training Epoch: 62 [35968/50000]	Loss: 0.4896	LR: 0.025000
Training Epoch: 62 [36096/50000]	Loss: 0.5692	LR: 0.025000
Training Epoch: 62 [36224/50000]	Loss: 0.5668	LR: 0.025000
Training Epoch: 62 [36352/50000]	Loss: 0.4534	LR: 0.025000
Training Epoch: 62 [36480/50000]	Loss: 0.4823	LR: 0.025000
Training Epoch: 62 [36608/50000]	Loss: 0.3197	LR: 0.025000
Training Epoch: 62 [36736/50000]	Loss: 0.3461	LR: 0.025000
Training Epoch: 62 [36864/50000]	Loss: 0.5992	LR: 0.025000
Training Epoch: 62 [36992/50000]	Loss: 0.5063	LR: 0.025000
Training Epoch: 62 [37120/50000]	Loss: 0.6066	LR: 0.025000
Training Epoch: 62 [37248/50000]	Loss: 0.5929	LR: 0.025000
Training Epoch: 62 [37376/50000]	Loss: 0.5029	LR: 0.025000
Training Epoch: 62 [37504/50000]	Loss: 0.4568	LR: 0.025000
Training Epoch: 62 [37632/50000]	Loss: 0.4826	LR: 0.025000
Training Epoch: 62 [37760/50000]	Loss: 0.5282	LR: 0.025000
Training Epoch: 62 [37888/50000]	Loss: 0.4403	LR: 0.025000
Training Epoch: 62 [38016/50000]	Loss: 0.5389	LR: 0.025000
Training Epoch: 62 [38144/50000]	Loss: 0.5641	LR: 0.025000
Training Epoch: 62 [38272/50000]	Loss: 0.5573	LR: 0.025000
Training Epoch: 62 [38400/50000]	Loss: 0.5411	LR: 0.025000
Training Epoch: 62 [38528/50000]	Loss: 0.4817	LR: 0.025000
Training Epoch: 62 [38656/50000]	Loss: 0.6087	LR: 0.025000
Training Epoch: 62 [38784/50000]	Loss: 0.5729	LR: 0.025000
Training Epoch: 62 [38912/50000]	Loss: 0.5036	LR: 0.025000
Training Epoch: 62 [39040/50000]	Loss: 0.4739	LR: 0.025000
Training Epoch: 62 [39168/50000]	Loss: 0.5028	LR: 0.025000
Training Epoch: 62 [39296/50000]	Loss: 0.5707	LR: 0.025000
Training Epoch: 62 [39424/50000]	Loss: 0.6545	LR: 0.025000
Training Epoch: 62 [39552/50000]	Loss: 0.4282	LR: 0.025000
Training Epoch: 62 [39680/50000]	Loss: 0.6699	LR: 0.025000
Training Epoch: 62 [39808/50000]	Loss: 0.6044	LR: 0.025000
Training Epoch: 62 [39936/50000]	Loss: 0.4563	LR: 0.025000
Training Epoch: 62 [40064/50000]	Loss: 0.6020	LR: 0.025000
Training Epoch: 62 [40192/50000]	Loss: 0.4809	LR: 0.025000
Training Epoch: 62 [40320/50000]	Loss: 0.6217	LR: 0.025000
Training Epoch: 62 [40448/50000]	Loss: 0.5435	LR: 0.025000
Training Epoch: 62 [40576/50000]	Loss: 0.4447	LR: 0.025000
Training Epoch: 62 [40704/50000]	Loss: 0.5888	LR: 0.025000
Training Epoch: 62 [40832/50000]	Loss: 0.5753	LR: 0.025000
Training Epoch: 62 [40960/50000]	Loss: 0.4396	LR: 0.025000
Training Epoch: 62 [41088/50000]	Loss: 0.4523	LR: 0.025000
Training Epoch: 62 [41216/50000]	Loss: 0.5205	LR: 0.025000
Training Epoch: 62 [41344/50000]	Loss: 0.4840	LR: 0.025000
Training Epoch: 62 [41472/50000]	Loss: 0.5654	LR: 0.025000
Training Epoch: 62 [41600/50000]	Loss: 0.4418	LR: 0.025000
Training Epoch: 62 [41728/50000]	Loss: 0.5512	LR: 0.025000
Training Epoch: 62 [41856/50000]	Loss: 0.4767	LR: 0.025000
Training Epoch: 62 [41984/50000]	Loss: 0.4295	LR: 0.025000
Training Epoch: 62 [42112/50000]	Loss: 0.5764	LR: 0.025000
Training Epoch: 62 [42240/50000]	Loss: 0.4118	LR: 0.025000
Training Epoch: 62 [42368/50000]	Loss: 0.6192	LR: 0.025000
Training Epoch: 62 [42496/50000]	Loss: 0.5290	LR: 0.025000
Training Epoch: 62 [42624/50000]	Loss: 0.7226	LR: 0.025000
Training Epoch: 62 [42752/50000]	Loss: 0.5672	LR: 0.025000
Training Epoch: 62 [42880/50000]	Loss: 0.5472	LR: 0.025000
Training Epoch: 62 [43008/50000]	Loss: 0.5457	LR: 0.025000
Training Epoch: 62 [43136/50000]	Loss: 0.6006	LR: 0.025000
Training Epoch: 62 [43264/50000]	Loss: 0.5893	LR: 0.025000
Training Epoch: 62 [43392/50000]	Loss: 0.5145	LR: 0.025000
Training Epoch: 62 [43520/50000]	Loss: 0.6460	LR: 0.025000
Training Epoch: 62 [43648/50000]	Loss: 0.5684	LR: 0.025000
Training Epoch: 62 [43776/50000]	Loss: 0.5784	LR: 0.025000
Training Epoch: 62 [43904/50000]	Loss: 0.5987	LR: 0.025000
Training Epoch: 62 [44032/50000]	Loss: 0.4804	LR: 0.025000
Training Epoch: 62 [44160/50000]	Loss: 0.5475	LR: 0.025000
Training Epoch: 62 [44288/50000]	Loss: 0.5207	LR: 0.025000
Training Epoch: 62 [44416/50000]	Loss: 0.5302	LR: 0.025000
Training Epoch: 62 [44544/50000]	Loss: 0.6549	LR: 0.025000
Training Epoch: 62 [44672/50000]	Loss: 0.3576	LR: 0.025000
Training Epoch: 62 [44800/50000]	Loss: 0.5860	LR: 0.025000
Training Epoch: 62 [44928/50000]	Loss: 0.5647	LR: 0.025000
Training Epoch: 62 [45056/50000]	Loss: 0.4384	LR: 0.025000
Training Epoch: 62 [45184/50000]	Loss: 0.6900	LR: 0.025000
Training Epoch: 62 [45312/50000]	Loss: 0.5937	LR: 0.025000
Training Epoch: 62 [45440/50000]	Loss: 0.4293	LR: 0.025000
Training Epoch: 62 [45568/50000]	Loss: 0.5261	LR: 0.025000
Training Epoch: 62 [45696/50000]	Loss: 0.6620	LR: 0.025000
Training Epoch: 62 [45824/50000]	Loss: 0.6386	LR: 0.025000
Training Epoch: 62 [45952/50000]	Loss: 0.5951	LR: 0.025000
Training Epoch: 62 [46080/50000]	Loss: 0.4473	LR: 0.025000
Training Epoch: 62 [46208/50000]	Loss: 0.4563	LR: 0.025000
Training Epoch: 62 [46336/50000]	Loss: 0.4798	LR: 0.025000
Training Epoch: 62 [46464/50000]	Loss: 0.5364	LR: 0.025000
Training Epoch: 62 [46592/50000]	Loss: 0.5230	LR: 0.025000
Training Epoch: 62 [46720/50000]	Loss: 0.6744	LR: 0.025000
Training Epoch: 62 [46848/50000]	Loss: 0.4781	LR: 0.025000
Training Epoch: 62 [46976/50000]	Loss: 0.4757	LR: 0.025000
Training Epoch: 62 [47104/50000]	Loss: 0.5438	LR: 0.025000
Training Epoch: 62 [47232/50000]	Loss: 0.5900	LR: 0.025000
Training Epoch: 62 [47360/50000]	Loss: 0.4196	LR: 0.025000
Training Epoch: 62 [47488/50000]	Loss: 0.6424	LR: 0.025000
Training Epoch: 62 [47616/50000]	Loss: 0.3499	LR: 0.025000
Training Epoch: 62 [47744/50000]	Loss: 0.4704	LR: 0.025000
Training Epoch: 62 [47872/50000]	Loss: 0.6320	LR: 0.025000
Training Epoch: 62 [48000/50000]	Loss: 0.4833	LR: 0.025000
Training Epoch: 62 [48128/50000]	Loss: 0.5734	LR: 0.025000
Training Epoch: 62 [48256/50000]	Loss: 0.4777	LR: 0.025000
Training Epoch: 62 [48384/50000]	Loss: 0.5777	LR: 0.025000
Training Epoch: 62 [48512/50000]	Loss: 0.4661	LR: 0.025000
Training Epoch: 62 [48640/50000]	Loss: 0.4959	LR: 0.025000
Training Epoch: 62 [48768/50000]	Loss: 0.4676	LR: 0.025000
Training Epoch: 62 [48896/50000]	Loss: 0.6981	LR: 0.025000
Training Epoch: 62 [49024/50000]	Loss: 0.6131	LR: 0.025000
Training Epoch: 62 [49152/50000]	Loss: 0.6070	LR: 0.025000
Training Epoch: 62 [49280/50000]	Loss: 0.6682	LR: 0.025000
Training Epoch: 62 [49408/50000]	Loss: 0.5347	LR: 0.025000
Training Epoch: 62 [49536/50000]	Loss: 0.5169	LR: 0.025000
Training Epoch: 62 [49664/50000]	Loss: 0.7126	LR: 0.025000
Training Epoch: 62 [49792/50000]	Loss: 0.5967	LR: 0.025000
Training Epoch: 62 [49920/50000]	Loss: 0.5102	LR: 0.025000
Training Epoch: 62 [50000/50000]	Loss: 0.6659	LR: 0.025000
epoch 62 training time consumed: 54.09s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  227543 GB |  227543 GB |
|       from large pool |  123392 KB |    1034 MB |  227319 GB |  227319 GB |
|       from small pool |   10798 KB |      13 MB |     224 GB |     224 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  227543 GB |  227543 GB |
|       from large pool |  123392 KB |    1034 MB |  227319 GB |  227319 GB |
|       from small pool |   10798 KB |      13 MB |     224 GB |     224 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  100133 GB |  100133 GB |
|       from large pool |  155136 KB |  433088 KB |   99885 GB |   99885 GB |
|       from small pool |    1490 KB |    3494 KB |     247 GB |     247 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    8780 K  |    8780 K  |
|       from large pool |      24    |      65    |    4583 K  |    4583 K  |
|       from small pool |     231    |     274    |    4197 K  |    4196 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    8780 K  |    8780 K  |
|       from large pool |      24    |      65    |    4583 K  |    4583 K  |
|       from small pool |     231    |     274    |    4197 K  |    4196 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4338 K  |    4338 K  |
|       from large pool |       9    |      14    |    2218 K  |    2218 K  |
|       from small pool |      12    |      16    |    2120 K  |    2120 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 62, Average loss: 0.0140, Accuracy: 0.6012, Time consumed:3.49s

Training Epoch: 63 [128/50000]	Loss: 0.5207	LR: 0.025000
Training Epoch: 63 [256/50000]	Loss: 0.5146	LR: 0.025000
Training Epoch: 63 [384/50000]	Loss: 0.4581	LR: 0.025000
Training Epoch: 63 [512/50000]	Loss: 0.5571	LR: 0.025000
Training Epoch: 63 [640/50000]	Loss: 0.5065	LR: 0.025000
Training Epoch: 63 [768/50000]	Loss: 0.4205	LR: 0.025000
Training Epoch: 63 [896/50000]	Loss: 0.4629	LR: 0.025000
Training Epoch: 63 [1024/50000]	Loss: 0.4857	LR: 0.025000
Training Epoch: 63 [1152/50000]	Loss: 0.3876	LR: 0.025000
Training Epoch: 63 [1280/50000]	Loss: 0.3773	LR: 0.025000
Training Epoch: 63 [1408/50000]	Loss: 0.6309	LR: 0.025000
Training Epoch: 63 [1536/50000]	Loss: 0.4643	LR: 0.025000
Training Epoch: 63 [1664/50000]	Loss: 0.4650	LR: 0.025000
Training Epoch: 63 [1792/50000]	Loss: 0.5029	LR: 0.025000
Training Epoch: 63 [1920/50000]	Loss: 0.5413	LR: 0.025000
Training Epoch: 63 [2048/50000]	Loss: 0.4177	LR: 0.025000
Training Epoch: 63 [2176/50000]	Loss: 0.4317	LR: 0.025000
Training Epoch: 63 [2304/50000]	Loss: 0.4216	LR: 0.025000
Training Epoch: 63 [2432/50000]	Loss: 0.2978	LR: 0.025000
Training Epoch: 63 [2560/50000]	Loss: 0.6304	LR: 0.025000
Training Epoch: 63 [2688/50000]	Loss: 0.3907	LR: 0.025000
Training Epoch: 63 [2816/50000]	Loss: 0.4742	LR: 0.025000
Training Epoch: 63 [2944/50000]	Loss: 0.5039	LR: 0.025000
Training Epoch: 63 [3072/50000]	Loss: 0.4598	LR: 0.025000
Training Epoch: 63 [3200/50000]	Loss: 0.5790	LR: 0.025000
Training Epoch: 63 [3328/50000]	Loss: 0.4808	LR: 0.025000
Training Epoch: 63 [3456/50000]	Loss: 0.4709	LR: 0.025000
Training Epoch: 63 [3584/50000]	Loss: 0.3050	LR: 0.025000
Training Epoch: 63 [3712/50000]	Loss: 0.3446	LR: 0.025000
Training Epoch: 63 [3840/50000]	Loss: 0.5333	LR: 0.025000
Training Epoch: 63 [3968/50000]	Loss: 0.6757	LR: 0.025000
Training Epoch: 63 [4096/50000]	Loss: 0.4893	LR: 0.025000
Training Epoch: 63 [4224/50000]	Loss: 0.4337	LR: 0.025000
Training Epoch: 63 [4352/50000]	Loss: 0.4320	LR: 0.025000
Training Epoch: 63 [4480/50000]	Loss: 0.3430	LR: 0.025000
Training Epoch: 63 [4608/50000]	Loss: 0.5703	LR: 0.025000
Training Epoch: 63 [4736/50000]	Loss: 0.3745	LR: 0.025000
Training Epoch: 63 [4864/50000]	Loss: 0.4013	LR: 0.025000
Training Epoch: 63 [4992/50000]	Loss: 0.5768	LR: 0.025000
Training Epoch: 63 [5120/50000]	Loss: 0.5514	LR: 0.025000
Training Epoch: 63 [5248/50000]	Loss: 0.4504	LR: 0.025000
Training Epoch: 63 [5376/50000]	Loss: 0.4528	LR: 0.025000
Training Epoch: 63 [5504/50000]	Loss: 0.3419	LR: 0.025000
Training Epoch: 63 [5632/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 63 [5760/50000]	Loss: 0.5012	LR: 0.025000
Training Epoch: 63 [5888/50000]	Loss: 0.4552	LR: 0.025000
Training Epoch: 63 [6016/50000]	Loss: 0.5866	LR: 0.025000
Training Epoch: 63 [6144/50000]	Loss: 0.3708	LR: 0.025000
Training Epoch: 63 [6272/50000]	Loss: 0.4335	LR: 0.025000
Training Epoch: 63 [6400/50000]	Loss: 0.5022	LR: 0.025000
Training Epoch: 63 [6528/50000]	Loss: 0.5100	LR: 0.025000
Training Epoch: 63 [6656/50000]	Loss: 0.3916	LR: 0.025000
Training Epoch: 63 [6784/50000]	Loss: 0.3519	LR: 0.025000
Training Epoch: 63 [6912/50000]	Loss: 0.5222	LR: 0.025000
Training Epoch: 63 [7040/50000]	Loss: 0.5414	LR: 0.025000
Training Epoch: 63 [7168/50000]	Loss: 0.4959	LR: 0.025000
Training Epoch: 63 [7296/50000]	Loss: 0.4379	LR: 0.025000
Training Epoch: 63 [7424/50000]	Loss: 0.5362	LR: 0.025000
Training Epoch: 63 [7552/50000]	Loss: 0.4174	LR: 0.025000
Training Epoch: 63 [7680/50000]	Loss: 0.4230	LR: 0.025000
Training Epoch: 63 [7808/50000]	Loss: 0.3982	LR: 0.025000
Training Epoch: 63 [7936/50000]	Loss: 0.6496	LR: 0.025000
Training Epoch: 63 [8064/50000]	Loss: 0.4616	LR: 0.025000
Training Epoch: 63 [8192/50000]	Loss: 0.4077	LR: 0.025000
Training Epoch: 63 [8320/50000]	Loss: 0.4291	LR: 0.025000
Training Epoch: 63 [8448/50000]	Loss: 0.4789	LR: 0.025000
Training Epoch: 63 [8576/50000]	Loss: 0.5385	LR: 0.025000
Training Epoch: 63 [8704/50000]	Loss: 0.4834	LR: 0.025000
Training Epoch: 63 [8832/50000]	Loss: 0.4751	LR: 0.025000
Training Epoch: 63 [8960/50000]	Loss: 0.5585	LR: 0.025000
Training Epoch: 63 [9088/50000]	Loss: 0.6511	LR: 0.025000
Training Epoch: 63 [9216/50000]	Loss: 0.4469	LR: 0.025000
Training Epoch: 63 [9344/50000]	Loss: 0.5049	LR: 0.025000
Training Epoch: 63 [9472/50000]	Loss: 0.4965	LR: 0.025000
Training Epoch: 63 [9600/50000]	Loss: 0.4920	LR: 0.025000
Training Epoch: 63 [9728/50000]	Loss: 0.3874	LR: 0.025000
Training Epoch: 63 [9856/50000]	Loss: 0.4126	LR: 0.025000
Training Epoch: 63 [9984/50000]	Loss: 0.3925	LR: 0.025000
Training Epoch: 63 [10112/50000]	Loss: 0.3514	LR: 0.025000
Training Epoch: 63 [10240/50000]	Loss: 0.4650	LR: 0.025000
Training Epoch: 63 [10368/50000]	Loss: 0.4683	LR: 0.025000
Training Epoch: 63 [10496/50000]	Loss: 0.3487	LR: 0.025000
Training Epoch: 63 [10624/50000]	Loss: 0.4038	LR: 0.025000
Training Epoch: 63 [10752/50000]	Loss: 0.3970	LR: 0.025000
Training Epoch: 63 [10880/50000]	Loss: 0.5232	LR: 0.025000
Training Epoch: 63 [11008/50000]	Loss: 0.4687	LR: 0.025000
Training Epoch: 63 [11136/50000]	Loss: 0.3934	LR: 0.025000
Training Epoch: 63 [11264/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 63 [11392/50000]	Loss: 0.3612	LR: 0.025000
Training Epoch: 63 [11520/50000]	Loss: 0.3277	LR: 0.025000
Training Epoch: 63 [11648/50000]	Loss: 0.5236	LR: 0.025000
Training Epoch: 63 [11776/50000]	Loss: 0.5530	LR: 0.025000
Training Epoch: 63 [11904/50000]	Loss: 0.3552	LR: 0.025000
Training Epoch: 63 [12032/50000]	Loss: 0.5031	LR: 0.025000
Training Epoch: 63 [12160/50000]	Loss: 0.4534	LR: 0.025000
Training Epoch: 63 [12288/50000]	Loss: 0.5388	LR: 0.025000
Training Epoch: 63 [12416/50000]	Loss: 0.3772	LR: 0.025000
Training Epoch: 63 [12544/50000]	Loss: 0.4255	LR: 0.025000
Training Epoch: 63 [12672/50000]	Loss: 0.5633	LR: 0.025000
Training Epoch: 63 [12800/50000]	Loss: 0.3488	LR: 0.025000
Training Epoch: 63 [12928/50000]	Loss: 0.7211	LR: 0.025000
Training Epoch: 63 [13056/50000]	Loss: 0.3469	LR: 0.025000
Training Epoch: 63 [13184/50000]	Loss: 0.4542	LR: 0.025000
Training Epoch: 63 [13312/50000]	Loss: 0.5706	LR: 0.025000
Training Epoch: 63 [13440/50000]	Loss: 0.6746	LR: 0.025000
Training Epoch: 63 [13568/50000]	Loss: 0.4193	LR: 0.025000
Training Epoch: 63 [13696/50000]	Loss: 0.6010	LR: 0.025000
Training Epoch: 63 [13824/50000]	Loss: 0.4322	LR: 0.025000
Training Epoch: 63 [13952/50000]	Loss: 0.5859	LR: 0.025000
Training Epoch: 63 [14080/50000]	Loss: 0.3671	LR: 0.025000
Training Epoch: 63 [14208/50000]	Loss: 0.4642	LR: 0.025000
Training Epoch: 63 [14336/50000]	Loss: 0.2480	LR: 0.025000
Training Epoch: 63 [14464/50000]	Loss: 0.4275	LR: 0.025000
Training Epoch: 63 [14592/50000]	Loss: 0.7692	LR: 0.025000
Training Epoch: 63 [14720/50000]	Loss: 0.5796	LR: 0.025000
Training Epoch: 63 [14848/50000]	Loss: 0.4278	LR: 0.025000
Training Epoch: 63 [14976/50000]	Loss: 0.3559	LR: 0.025000
Training Epoch: 63 [15104/50000]	Loss: 0.5038	LR: 0.025000
Training Epoch: 63 [15232/50000]	Loss: 0.4147	LR: 0.025000
Training Epoch: 63 [15360/50000]	Loss: 0.4325	LR: 0.025000
Training Epoch: 63 [15488/50000]	Loss: 0.3495	LR: 0.025000
Training Epoch: 63 [15616/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 63 [15744/50000]	Loss: 0.5336	LR: 0.025000
Training Epoch: 63 [15872/50000]	Loss: 0.5606	LR: 0.025000
Training Epoch: 63 [16000/50000]	Loss: 0.4119	LR: 0.025000
Training Epoch: 63 [16128/50000]	Loss: 0.4886	LR: 0.025000
Training Epoch: 63 [16256/50000]	Loss: 0.4639	LR: 0.025000
Training Epoch: 63 [16384/50000]	Loss: 0.4624	LR: 0.025000
Training Epoch: 63 [16512/50000]	Loss: 0.5851	LR: 0.025000
Training Epoch: 63 [16640/50000]	Loss: 0.5039	LR: 0.025000
Training Epoch: 63 [16768/50000]	Loss: 0.5069	LR: 0.025000
Training Epoch: 63 [16896/50000]	Loss: 0.4827	LR: 0.025000
Training Epoch: 63 [17024/50000]	Loss: 0.4473	LR: 0.025000
Training Epoch: 63 [17152/50000]	Loss: 0.4993	LR: 0.025000
Training Epoch: 63 [17280/50000]	Loss: 0.5614	LR: 0.025000
Training Epoch: 63 [17408/50000]	Loss: 0.4970	LR: 0.025000
Training Epoch: 63 [17536/50000]	Loss: 0.3990	LR: 0.025000
Training Epoch: 63 [17664/50000]	Loss: 0.5043	LR: 0.025000
Training Epoch: 63 [17792/50000]	Loss: 0.4093	LR: 0.025000
Training Epoch: 63 [17920/50000]	Loss: 0.4431	LR: 0.025000
Training Epoch: 63 [18048/50000]	Loss: 0.3451	LR: 0.025000
Training Epoch: 63 [18176/50000]	Loss: 0.5440	LR: 0.025000
Training Epoch: 63 [18304/50000]	Loss: 0.4474	LR: 0.025000
Training Epoch: 63 [18432/50000]	Loss: 0.4000	LR: 0.025000
Training Epoch: 63 [18560/50000]	Loss: 0.4964	LR: 0.025000
Training Epoch: 63 [18688/50000]	Loss: 0.5213	LR: 0.025000
Training Epoch: 63 [18816/50000]	Loss: 0.5283	LR: 0.025000
Training Epoch: 63 [18944/50000]	Loss: 0.4569	LR: 0.025000
Training Epoch: 63 [19072/50000]	Loss: 0.3191	LR: 0.025000
Training Epoch: 63 [19200/50000]	Loss: 0.3896	LR: 0.025000
Training Epoch: 63 [19328/50000]	Loss: 0.5094	LR: 0.025000
Training Epoch: 63 [19456/50000]	Loss: 0.5685	LR: 0.025000
Training Epoch: 63 [19584/50000]	Loss: 0.4342	LR: 0.025000
Training Epoch: 63 [19712/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 63 [19840/50000]	Loss: 0.5997	LR: 0.025000
Training Epoch: 63 [19968/50000]	Loss: 0.6892	LR: 0.025000
Training Epoch: 63 [20096/50000]	Loss: 0.4380	LR: 0.025000
Training Epoch: 63 [20224/50000]	Loss: 0.5954	LR: 0.025000
Training Epoch: 63 [20352/50000]	Loss: 0.4682	LR: 0.025000
Training Epoch: 63 [20480/50000]	Loss: 0.5012	LR: 0.025000
Training Epoch: 63 [20608/50000]	Loss: 0.3392	LR: 0.025000
Training Epoch: 63 [20736/50000]	Loss: 0.4569	LR: 0.025000
Training Epoch: 63 [20864/50000]	Loss: 0.4736	LR: 0.025000
Training Epoch: 63 [20992/50000]	Loss: 0.4677	LR: 0.025000
Training Epoch: 63 [21120/50000]	Loss: 0.6876	LR: 0.025000
Training Epoch: 63 [21248/50000]	Loss: 0.4151	LR: 0.025000
Training Epoch: 63 [21376/50000]	Loss: 0.4195	LR: 0.025000
Training Epoch: 63 [21504/50000]	Loss: 0.4368	LR: 0.025000
Training Epoch: 63 [21632/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 63 [21760/50000]	Loss: 0.5525	LR: 0.025000
Training Epoch: 63 [21888/50000]	Loss: 0.5185	LR: 0.025000
Training Epoch: 63 [22016/50000]	Loss: 0.3554	LR: 0.025000
Training Epoch: 63 [22144/50000]	Loss: 0.4631	LR: 0.025000
Training Epoch: 63 [22272/50000]	Loss: 0.4769	LR: 0.025000
Training Epoch: 63 [22400/50000]	Loss: 0.6642	LR: 0.025000
Training Epoch: 63 [22528/50000]	Loss: 0.5605	LR: 0.025000
Training Epoch: 63 [22656/50000]	Loss: 0.3631	LR: 0.025000
Training Epoch: 63 [22784/50000]	Loss: 0.5096	LR: 0.025000
Training Epoch: 63 [22912/50000]	Loss: 0.4404	LR: 0.025000
Training Epoch: 63 [23040/50000]	Loss: 0.5419	LR: 0.025000
Training Epoch: 63 [23168/50000]	Loss: 0.4728	LR: 0.025000
Training Epoch: 63 [23296/50000]	Loss: 0.6180	LR: 0.025000
Training Epoch: 63 [23424/50000]	Loss: 0.3694	LR: 0.025000
Training Epoch: 63 [23552/50000]	Loss: 0.3892	LR: 0.025000
Training Epoch: 63 [23680/50000]	Loss: 0.4052	LR: 0.025000
Training Epoch: 63 [23808/50000]	Loss: 0.4205	LR: 0.025000
Training Epoch: 63 [23936/50000]	Loss: 0.5617	LR: 0.025000
Training Epoch: 63 [24064/50000]	Loss: 0.4183	LR: 0.025000
Training Epoch: 63 [24192/50000]	Loss: 0.4061	LR: 0.025000
Training Epoch: 63 [24320/50000]	Loss: 0.3678	LR: 0.025000
Training Epoch: 63 [24448/50000]	Loss: 0.4808	LR: 0.025000
Training Epoch: 63 [24576/50000]	Loss: 0.6708	LR: 0.025000
Training Epoch: 63 [24704/50000]	Loss: 0.4599	LR: 0.025000
Training Epoch: 63 [24832/50000]	Loss: 0.5539	LR: 0.025000
Training Epoch: 63 [24960/50000]	Loss: 0.4454	LR: 0.025000
Training Epoch: 63 [25088/50000]	Loss: 0.4670	LR: 0.025000
Training Epoch: 63 [25216/50000]	Loss: 0.4827	LR: 0.025000
Training Epoch: 63 [25344/50000]	Loss: 0.3328	LR: 0.025000
Training Epoch: 63 [25472/50000]	Loss: 0.3765	LR: 0.025000
Training Epoch: 63 [25600/50000]	Loss: 0.4826	LR: 0.025000
Training Epoch: 63 [25728/50000]	Loss: 0.6354	LR: 0.025000
Training Epoch: 63 [25856/50000]	Loss: 0.5418	LR: 0.025000
Training Epoch: 63 [25984/50000]	Loss: 0.4884	LR: 0.025000
Training Epoch: 63 [26112/50000]	Loss: 0.4761	LR: 0.025000
Training Epoch: 63 [26240/50000]	Loss: 0.5351	LR: 0.025000
Training Epoch: 63 [26368/50000]	Loss: 0.5457	LR: 0.025000
Training Epoch: 63 [26496/50000]	Loss: 0.4737	LR: 0.025000
Training Epoch: 63 [26624/50000]	Loss: 0.5204	LR: 0.025000
Training Epoch: 63 [26752/50000]	Loss: 0.4373	LR: 0.025000
Training Epoch: 63 [26880/50000]	Loss: 0.5612	LR: 0.025000
Training Epoch: 63 [27008/50000]	Loss: 0.6628	LR: 0.025000
Training Epoch: 63 [27136/50000]	Loss: 0.5819	LR: 0.025000
Training Epoch: 63 [27264/50000]	Loss: 0.4418	LR: 0.025000
Training Epoch: 63 [27392/50000]	Loss: 0.5076	LR: 0.025000
Training Epoch: 63 [27520/50000]	Loss: 0.4839	LR: 0.025000
Training Epoch: 63 [27648/50000]	Loss: 0.3562	LR: 0.025000
Training Epoch: 63 [27776/50000]	Loss: 0.4872	LR: 0.025000
Training Epoch: 63 [27904/50000]	Loss: 0.4667	LR: 0.025000
Training Epoch: 63 [28032/50000]	Loss: 0.3841	LR: 0.025000
Training Epoch: 63 [28160/50000]	Loss: 0.5016	LR: 0.025000
Training Epoch: 63 [28288/50000]	Loss: 0.5817	LR: 0.025000
Training Epoch: 63 [28416/50000]	Loss: 0.4454	LR: 0.025000
Training Epoch: 63 [28544/50000]	Loss: 0.4313	LR: 0.025000
Training Epoch: 63 [28672/50000]	Loss: 0.4850	LR: 0.025000
Training Epoch: 63 [28800/50000]	Loss: 0.4519	LR: 0.025000
Training Epoch: 63 [28928/50000]	Loss: 0.5033	LR: 0.025000
Training Epoch: 63 [29056/50000]	Loss: 0.4122	LR: 0.025000
Training Epoch: 63 [29184/50000]	Loss: 0.4433	LR: 0.025000
Training Epoch: 63 [29312/50000]	Loss: 0.5316	LR: 0.025000
Training Epoch: 63 [29440/50000]	Loss: 0.4675	LR: 0.025000
Training Epoch: 63 [29568/50000]	Loss: 0.4795	LR: 0.025000
Training Epoch: 63 [29696/50000]	Loss: 0.4379	LR: 0.025000
Training Epoch: 63 [29824/50000]	Loss: 0.5180	LR: 0.025000
Training Epoch: 63 [29952/50000]	Loss: 0.5364	LR: 0.025000
Training Epoch: 63 [30080/50000]	Loss: 0.4352	LR: 0.025000
Training Epoch: 63 [30208/50000]	Loss: 0.5075	LR: 0.025000
Training Epoch: 63 [30336/50000]	Loss: 0.5827	LR: 0.025000
Training Epoch: 63 [30464/50000]	Loss: 0.5249	LR: 0.025000
Training Epoch: 63 [30592/50000]	Loss: 0.3931	LR: 0.025000
Training Epoch: 63 [30720/50000]	Loss: 0.5568	LR: 0.025000
Training Epoch: 63 [30848/50000]	Loss: 0.5034	LR: 0.025000
Training Epoch: 63 [30976/50000]	Loss: 0.4206	LR: 0.025000
Training Epoch: 63 [31104/50000]	Loss: 0.5083	LR: 0.025000
Training Epoch: 63 [31232/50000]	Loss: 0.5109	LR: 0.025000
Training Epoch: 63 [31360/50000]	Loss: 0.2929	LR: 0.025000
Training Epoch: 63 [31488/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 63 [31616/50000]	Loss: 0.5325	LR: 0.025000
Training Epoch: 63 [31744/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 63 [31872/50000]	Loss: 0.4728	LR: 0.025000
Training Epoch: 63 [32000/50000]	Loss: 0.5794	LR: 0.025000
Training Epoch: 63 [32128/50000]	Loss: 0.4166	LR: 0.025000
Training Epoch: 63 [32256/50000]	Loss: 0.4361	LR: 0.025000
Training Epoch: 63 [32384/50000]	Loss: 0.6153	LR: 0.025000
Training Epoch: 63 [32512/50000]	Loss: 0.5375	LR: 0.025000
Training Epoch: 63 [32640/50000]	Loss: 0.5851	LR: 0.025000
Training Epoch: 63 [32768/50000]	Loss: 0.6496	LR: 0.025000
Training Epoch: 63 [32896/50000]	Loss: 0.5381	LR: 0.025000
Training Epoch: 63 [33024/50000]	Loss: 0.4903	LR: 0.025000
Training Epoch: 63 [33152/50000]	Loss: 0.5347	LR: 0.025000
Training Epoch: 63 [33280/50000]	Loss: 0.4113	LR: 0.025000
Training Epoch: 63 [33408/50000]	Loss: 0.5082	LR: 0.025000
Training Epoch: 63 [33536/50000]	Loss: 0.3588	LR: 0.025000
Training Epoch: 63 [33664/50000]	Loss: 0.5141	LR: 0.025000
Training Epoch: 63 [33792/50000]	Loss: 0.4909	LR: 0.025000
Training Epoch: 63 [33920/50000]	Loss: 0.4881	LR: 0.025000
Training Epoch: 63 [34048/50000]	Loss: 0.4784	LR: 0.025000
Training Epoch: 63 [34176/50000]	Loss: 0.4053	LR: 0.025000
Training Epoch: 63 [34304/50000]	Loss: 0.4901	LR: 0.025000
Training Epoch: 63 [34432/50000]	Loss: 0.5853	LR: 0.025000
Training Epoch: 63 [34560/50000]	Loss: 0.4578	LR: 0.025000
Training Epoch: 63 [34688/50000]	Loss: 0.4514	LR: 0.025000
Training Epoch: 63 [34816/50000]	Loss: 0.4173	LR: 0.025000
Training Epoch: 63 [34944/50000]	Loss: 0.4087	LR: 0.025000
Training Epoch: 63 [35072/50000]	Loss: 0.5410	LR: 0.025000
Training Epoch: 63 [35200/50000]	Loss: 0.4624	LR: 0.025000
Training Epoch: 63 [35328/50000]	Loss: 0.4432	LR: 0.025000
Training Epoch: 63 [35456/50000]	Loss: 0.5268	LR: 0.025000
Training Epoch: 63 [35584/50000]	Loss: 0.4826	LR: 0.025000
Training Epoch: 63 [35712/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 63 [35840/50000]	Loss: 0.6207	LR: 0.025000
Training Epoch: 63 [35968/50000]	Loss: 0.4015	LR: 0.025000
Training Epoch: 63 [36096/50000]	Loss: 0.6746	LR: 0.025000
Training Epoch: 63 [36224/50000]	Loss: 0.4794	LR: 0.025000
Training Epoch: 63 [36352/50000]	Loss: 0.5753	LR: 0.025000
Training Epoch: 63 [36480/50000]	Loss: 0.5254	LR: 0.025000
Training Epoch: 63 [36608/50000]	Loss: 0.3765	LR: 0.025000
Training Epoch: 63 [36736/50000]	Loss: 0.6134	LR: 0.025000
Training Epoch: 63 [36864/50000]	Loss: 0.5075	LR: 0.025000
Training Epoch: 63 [36992/50000]	Loss: 0.5012	LR: 0.025000
Training Epoch: 63 [37120/50000]	Loss: 0.4620	LR: 0.025000
Training Epoch: 63 [37248/50000]	Loss: 0.6589	LR: 0.025000
Training Epoch: 63 [37376/50000]	Loss: 0.5334	LR: 0.025000
Training Epoch: 63 [37504/50000]	Loss: 0.5198	LR: 0.025000
Training Epoch: 63 [37632/50000]	Loss: 0.5230	LR: 0.025000
Training Epoch: 63 [37760/50000]	Loss: 0.5271	LR: 0.025000
Training Epoch: 63 [37888/50000]	Loss: 0.3798	LR: 0.025000
Training Epoch: 63 [38016/50000]	Loss: 0.5120	LR: 0.025000
Training Epoch: 63 [38144/50000]	Loss: 0.7013	LR: 0.025000
Training Epoch: 63 [38272/50000]	Loss: 0.5070	LR: 0.025000
Training Epoch: 63 [38400/50000]	Loss: 0.4623	LR: 0.025000
Training Epoch: 63 [38528/50000]	Loss: 0.5884	LR: 0.025000
Training Epoch: 63 [38656/50000]	Loss: 0.4915	LR: 0.025000
Training Epoch: 63 [38784/50000]	Loss: 0.4062	LR: 0.025000
Training Epoch: 63 [38912/50000]	Loss: 0.6247	LR: 0.025000
Training Epoch: 63 [39040/50000]	Loss: 0.4475	LR: 0.025000
Training Epoch: 63 [39168/50000]	Loss: 0.4533	LR: 0.025000
Training Epoch: 63 [39296/50000]	Loss: 0.5548	LR: 0.025000
Training Epoch: 63 [39424/50000]	Loss: 0.5634	LR: 0.025000
Training Epoch: 63 [39552/50000]	Loss: 0.6331	LR: 0.025000
Training Epoch: 63 [39680/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 63 [39808/50000]	Loss: 0.5320	LR: 0.025000
Training Epoch: 63 [39936/50000]	Loss: 0.4546	LR: 0.025000
Training Epoch: 63 [40064/50000]	Loss: 0.7840	LR: 0.025000
Training Epoch: 63 [40192/50000]	Loss: 0.5682	LR: 0.025000
Training Epoch: 63 [40320/50000]	Loss: 0.5328	LR: 0.025000
Training Epoch: 63 [40448/50000]	Loss: 0.3858	LR: 0.025000
Training Epoch: 63 [40576/50000]	Loss: 0.6057	LR: 0.025000
Training Epoch: 63 [40704/50000]	Loss: 0.5379	LR: 0.025000
Training Epoch: 63 [40832/50000]	Loss: 0.5526	LR: 0.025000
Training Epoch: 63 [40960/50000]	Loss: 0.5476	LR: 0.025000
Training Epoch: 63 [41088/50000]	Loss: 0.5698	LR: 0.025000
Training Epoch: 63 [41216/50000]	Loss: 0.3751	LR: 0.025000
Training Epoch: 63 [41344/50000]	Loss: 0.5763	LR: 0.025000
Training Epoch: 63 [41472/50000]	Loss: 0.5240	LR: 0.025000
Training Epoch: 63 [41600/50000]	Loss: 0.8238	LR: 0.025000
Training Epoch: 63 [41728/50000]	Loss: 0.8083	LR: 0.025000
Training Epoch: 63 [41856/50000]	Loss: 0.4590	LR: 0.025000
Training Epoch: 63 [41984/50000]	Loss: 0.5270	LR: 0.025000
Training Epoch: 63 [42112/50000]	Loss: 0.6548	LR: 0.025000
Training Epoch: 63 [42240/50000]	Loss: 0.5044	LR: 0.025000
Training Epoch: 63 [42368/50000]	Loss: 0.5118	LR: 0.025000
Training Epoch: 63 [42496/50000]	Loss: 0.6967	LR: 0.025000
Training Epoch: 63 [42624/50000]	Loss: 0.6017	LR: 0.025000
Training Epoch: 63 [42752/50000]	Loss: 0.5744	LR: 0.025000
Training Epoch: 63 [42880/50000]	Loss: 0.5718	LR: 0.025000
Training Epoch: 63 [43008/50000]	Loss: 0.5594	LR: 0.025000
Training Epoch: 63 [43136/50000]	Loss: 0.5787	LR: 0.025000
Training Epoch: 63 [43264/50000]	Loss: 0.4472	LR: 0.025000
Training Epoch: 63 [43392/50000]	Loss: 0.5519	LR: 0.025000
Training Epoch: 63 [43520/50000]	Loss: 0.4361	LR: 0.025000
Training Epoch: 63 [43648/50000]	Loss: 0.4818	LR: 0.025000
Training Epoch: 63 [43776/50000]	Loss: 0.5364	LR: 0.025000
Training Epoch: 63 [43904/50000]	Loss: 0.3826	LR: 0.025000
Training Epoch: 63 [44032/50000]	Loss: 0.5999	LR: 0.025000
Training Epoch: 63 [44160/50000]	Loss: 0.4749	LR: 0.025000
Training Epoch: 63 [44288/50000]	Loss: 0.5755	LR: 0.025000
Training Epoch: 63 [44416/50000]	Loss: 0.5034	LR: 0.025000
Training Epoch: 63 [44544/50000]	Loss: 0.6391	LR: 0.025000
Training Epoch: 63 [44672/50000]	Loss: 0.4816	LR: 0.025000
Training Epoch: 63 [44800/50000]	Loss: 0.4691	LR: 0.025000
Training Epoch: 63 [44928/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 63 [45056/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 63 [45184/50000]	Loss: 0.4936	LR: 0.025000
Training Epoch: 63 [45312/50000]	Loss: 0.4961	LR: 0.025000
Training Epoch: 63 [45440/50000]	Loss: 0.6151	LR: 0.025000
Training Epoch: 63 [45568/50000]	Loss: 0.5816	LR: 0.025000
Training Epoch: 63 [45696/50000]	Loss: 0.4564	LR: 0.025000
Training Epoch: 63 [45824/50000]	Loss: 0.4902	LR: 0.025000
Training Epoch: 63 [45952/50000]	Loss: 0.4109	LR: 0.025000
Training Epoch: 63 [46080/50000]	Loss: 0.4828	LR: 0.025000
Training Epoch: 63 [46208/50000]	Loss: 0.4784	LR: 0.025000
Training Epoch: 63 [46336/50000]	Loss: 0.3662	LR: 0.025000
Training Epoch: 63 [46464/50000]	Loss: 0.3828	LR: 0.025000
Training Epoch: 63 [46592/50000]	Loss: 0.5651	LR: 0.025000
Training Epoch: 63 [46720/50000]	Loss: 0.5231	LR: 0.025000
Training Epoch: 63 [46848/50000]	Loss: 0.5666	LR: 0.025000
Training Epoch: 63 [46976/50000]	Loss: 0.4274	LR: 0.025000
Training Epoch: 63 [47104/50000]	Loss: 0.4196	LR: 0.025000
Training Epoch: 63 [47232/50000]	Loss: 0.7357	LR: 0.025000
Training Epoch: 63 [47360/50000]	Loss: 0.5172	LR: 0.025000
Training Epoch: 63 [47488/50000]	Loss: 0.4039	LR: 0.025000
Training Epoch: 63 [47616/50000]	Loss: 0.6306	LR: 0.025000
Training Epoch: 63 [47744/50000]	Loss: 0.6464	LR: 0.025000
Training Epoch: 63 [47872/50000]	Loss: 0.5966	LR: 0.025000
Training Epoch: 63 [48000/50000]	Loss: 0.4306	LR: 0.025000
Training Epoch: 63 [48128/50000]	Loss: 0.5079	LR: 0.025000
Training Epoch: 63 [48256/50000]	Loss: 0.5120	LR: 0.025000
Training Epoch: 63 [48384/50000]	Loss: 0.6087	LR: 0.025000
Training Epoch: 63 [48512/50000]	Loss: 0.5706	LR: 0.025000
Training Epoch: 63 [48640/50000]	Loss: 0.3399	LR: 0.025000
Training Epoch: 63 [48768/50000]	Loss: 0.4460	LR: 0.025000
Training Epoch: 63 [48896/50000]	Loss: 0.6729	LR: 0.025000
Training Epoch: 63 [49024/50000]	Loss: 0.5279	LR: 0.025000
Training Epoch: 63 [49152/50000]	Loss: 0.4939	LR: 0.025000
Training Epoch: 63 [49280/50000]	Loss: 0.4938	LR: 0.025000
Training Epoch: 63 [49408/50000]	Loss: 0.4688	LR: 0.025000
Training Epoch: 63 [49536/50000]	Loss: 0.5845	LR: 0.025000
Training Epoch: 63 [49664/50000]	Loss: 0.5741	LR: 0.025000
Training Epoch: 63 [49792/50000]	Loss: 0.5085	LR: 0.025000
Training Epoch: 63 [49920/50000]	Loss: 0.4405	LR: 0.025000
Training Epoch: 63 [50000/50000]	Loss: 0.6064	LR: 0.025000
epoch 63 training time consumed: 53.95s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  231214 GB |  231213 GB |
|       from large pool |  123392 KB |    1034 MB |  230986 GB |  230986 GB |
|       from small pool |   10798 KB |      13 MB |     227 GB |     227 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  231214 GB |  231213 GB |
|       from large pool |  123392 KB |    1034 MB |  230986 GB |  230986 GB |
|       from small pool |   10798 KB |      13 MB |     227 GB |     227 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  101748 GB |  101748 GB |
|       from large pool |  155136 KB |  433088 KB |  101496 GB |  101496 GB |
|       from small pool |    1490 KB |    3494 KB |     251 GB |     251 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    8921 K  |    8921 K  |
|       from large pool |      24    |      65    |    4657 K  |    4657 K  |
|       from small pool |     231    |     274    |    4264 K  |    4264 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    8921 K  |    8921 K  |
|       from large pool |      24    |      65    |    4657 K  |    4657 K  |
|       from small pool |     231    |     274    |    4264 K  |    4264 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4408 K  |    4408 K  |
|       from large pool |       9    |      14    |    2254 K  |    2254 K  |
|       from small pool |      12    |      16    |    2154 K  |    2154 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 63, Average loss: 0.0109, Accuracy: 0.6663, Time consumed:3.46s

Training Epoch: 64 [128/50000]	Loss: 0.4251	LR: 0.025000
Training Epoch: 64 [256/50000]	Loss: 0.4854	LR: 0.025000
Training Epoch: 64 [384/50000]	Loss: 0.4464	LR: 0.025000
Training Epoch: 64 [512/50000]	Loss: 0.3928	LR: 0.025000
Training Epoch: 64 [640/50000]	Loss: 0.5139	LR: 0.025000
Training Epoch: 64 [768/50000]	Loss: 0.3890	LR: 0.025000
Training Epoch: 64 [896/50000]	Loss: 0.3898	LR: 0.025000
Training Epoch: 64 [1024/50000]	Loss: 0.3508	LR: 0.025000
Training Epoch: 64 [1152/50000]	Loss: 0.4516	LR: 0.025000
Training Epoch: 64 [1280/50000]	Loss: 0.5128	LR: 0.025000
Training Epoch: 64 [1408/50000]	Loss: 0.3831	LR: 0.025000
Training Epoch: 64 [1536/50000]	Loss: 0.3688	LR: 0.025000
Training Epoch: 64 [1664/50000]	Loss: 0.3152	LR: 0.025000
Training Epoch: 64 [1792/50000]	Loss: 0.4531	LR: 0.025000
Training Epoch: 64 [1920/50000]	Loss: 0.5080	LR: 0.025000
Training Epoch: 64 [2048/50000]	Loss: 0.3966	LR: 0.025000
Training Epoch: 64 [2176/50000]	Loss: 0.5214	LR: 0.025000
Training Epoch: 64 [2304/50000]	Loss: 0.3647	LR: 0.025000
Training Epoch: 64 [2432/50000]	Loss: 0.3950	LR: 0.025000
Training Epoch: 64 [2560/50000]	Loss: 0.5696	LR: 0.025000
Training Epoch: 64 [2688/50000]	Loss: 0.3671	LR: 0.025000
Training Epoch: 64 [2816/50000]	Loss: 0.4389	LR: 0.025000
Training Epoch: 64 [2944/50000]	Loss: 0.5144	LR: 0.025000
Training Epoch: 64 [3072/50000]	Loss: 0.3611	LR: 0.025000
Training Epoch: 64 [3200/50000]	Loss: 0.4864	LR: 0.025000
Training Epoch: 64 [3328/50000]	Loss: 0.4228	LR: 0.025000
Training Epoch: 64 [3456/50000]	Loss: 0.5858	LR: 0.025000
Training Epoch: 64 [3584/50000]	Loss: 0.4506	LR: 0.025000
Training Epoch: 64 [3712/50000]	Loss: 0.6040	LR: 0.025000
Training Epoch: 64 [3840/50000]	Loss: 0.3907	LR: 0.025000
Training Epoch: 64 [3968/50000]	Loss: 0.3018	LR: 0.025000
Training Epoch: 64 [4096/50000]	Loss: 0.4368	LR: 0.025000
Training Epoch: 64 [4224/50000]	Loss: 0.4289	LR: 0.025000
Training Epoch: 64 [4352/50000]	Loss: 0.3668	LR: 0.025000
Training Epoch: 64 [4480/50000]	Loss: 0.5711	LR: 0.025000
Training Epoch: 64 [4608/50000]	Loss: 0.4876	LR: 0.025000
Training Epoch: 64 [4736/50000]	Loss: 0.3532	LR: 0.025000
Training Epoch: 64 [4864/50000]	Loss: 0.4745	LR: 0.025000
Training Epoch: 64 [4992/50000]	Loss: 0.3994	LR: 0.025000
Training Epoch: 64 [5120/50000]	Loss: 0.4541	LR: 0.025000
Training Epoch: 64 [5248/50000]	Loss: 0.3526	LR: 0.025000
Training Epoch: 64 [5376/50000]	Loss: 0.5031	LR: 0.025000
Training Epoch: 64 [5504/50000]	Loss: 0.3774	LR: 0.025000
Training Epoch: 64 [5632/50000]	Loss: 0.3451	LR: 0.025000
Training Epoch: 64 [5760/50000]	Loss: 0.5754	LR: 0.025000
Training Epoch: 64 [5888/50000]	Loss: 0.3935	LR: 0.025000
Training Epoch: 64 [6016/50000]	Loss: 0.5239	LR: 0.025000
Training Epoch: 64 [6144/50000]	Loss: 0.3436	LR: 0.025000
Training Epoch: 64 [6272/50000]	Loss: 0.4067	LR: 0.025000
Training Epoch: 64 [6400/50000]	Loss: 0.3842	LR: 0.025000
Training Epoch: 64 [6528/50000]	Loss: 0.3555	LR: 0.025000
Training Epoch: 64 [6656/50000]	Loss: 0.3332	LR: 0.025000
Training Epoch: 64 [6784/50000]	Loss: 0.3399	LR: 0.025000
Training Epoch: 64 [6912/50000]	Loss: 0.4170	LR: 0.025000
Training Epoch: 64 [7040/50000]	Loss: 0.3822	LR: 0.025000
Training Epoch: 64 [7168/50000]	Loss: 0.4161	LR: 0.025000
Training Epoch: 64 [7296/50000]	Loss: 0.4174	LR: 0.025000
Training Epoch: 64 [7424/50000]	Loss: 0.4740	LR: 0.025000
Training Epoch: 64 [7552/50000]	Loss: 0.3988	LR: 0.025000
Training Epoch: 64 [7680/50000]	Loss: 0.5124	LR: 0.025000
Training Epoch: 64 [7808/50000]	Loss: 0.4117	LR: 0.025000
Training Epoch: 64 [7936/50000]	Loss: 0.3232	LR: 0.025000
Training Epoch: 64 [8064/50000]	Loss: 0.3605	LR: 0.025000
Training Epoch: 64 [8192/50000]	Loss: 0.4254	LR: 0.025000
Training Epoch: 64 [8320/50000]	Loss: 0.3908	LR: 0.025000
Training Epoch: 64 [8448/50000]	Loss: 0.5757	LR: 0.025000
Training Epoch: 64 [8576/50000]	Loss: 0.4410	LR: 0.025000
Training Epoch: 64 [8704/50000]	Loss: 0.5072	LR: 0.025000
Training Epoch: 64 [8832/50000]	Loss: 0.3958	LR: 0.025000
Training Epoch: 64 [8960/50000]	Loss: 0.4172	LR: 0.025000
Training Epoch: 64 [9088/50000]	Loss: 0.6080	LR: 0.025000
Training Epoch: 64 [9216/50000]	Loss: 0.4625	LR: 0.025000
Training Epoch: 64 [9344/50000]	Loss: 0.4264	LR: 0.025000
Training Epoch: 64 [9472/50000]	Loss: 0.3727	LR: 0.025000
Training Epoch: 64 [9600/50000]	Loss: 0.5118	LR: 0.025000
Training Epoch: 64 [9728/50000]	Loss: 0.4706	LR: 0.025000
Training Epoch: 64 [9856/50000]	Loss: 0.6587	LR: 0.025000
Training Epoch: 64 [9984/50000]	Loss: 0.4829	LR: 0.025000
Training Epoch: 64 [10112/50000]	Loss: 0.3593	LR: 0.025000
Training Epoch: 64 [10240/50000]	Loss: 0.4858	LR: 0.025000
Training Epoch: 64 [10368/50000]	Loss: 0.4009	LR: 0.025000
Training Epoch: 64 [10496/50000]	Loss: 0.3573	LR: 0.025000
Training Epoch: 64 [10624/50000]	Loss: 0.4685	LR: 0.025000
Training Epoch: 64 [10752/50000]	Loss: 0.3966	LR: 0.025000
Training Epoch: 64 [10880/50000]	Loss: 0.3596	LR: 0.025000
Training Epoch: 64 [11008/50000]	Loss: 0.4065	LR: 0.025000
Training Epoch: 64 [11136/50000]	Loss: 0.4416	LR: 0.025000
Training Epoch: 64 [11264/50000]	Loss: 0.5853	LR: 0.025000
Training Epoch: 64 [11392/50000]	Loss: 0.3937	LR: 0.025000
Training Epoch: 64 [11520/50000]	Loss: 0.3857	LR: 0.025000
Training Epoch: 64 [11648/50000]	Loss: 0.3832	LR: 0.025000
Training Epoch: 64 [11776/50000]	Loss: 0.4574	LR: 0.025000
Training Epoch: 64 [11904/50000]	Loss: 0.4902	LR: 0.025000
Training Epoch: 64 [12032/50000]	Loss: 0.4892	LR: 0.025000
Training Epoch: 64 [12160/50000]	Loss: 0.4177	LR: 0.025000
Training Epoch: 64 [12288/50000]	Loss: 0.4068	LR: 0.025000
Training Epoch: 64 [12416/50000]	Loss: 0.3222	LR: 0.025000
Training Epoch: 64 [12544/50000]	Loss: 0.3511	LR: 0.025000
Training Epoch: 64 [12672/50000]	Loss: 0.3262	LR: 0.025000
Training Epoch: 64 [12800/50000]	Loss: 0.5995	LR: 0.025000
Training Epoch: 64 [12928/50000]	Loss: 0.5520	LR: 0.025000
Training Epoch: 64 [13056/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 64 [13184/50000]	Loss: 0.3870	LR: 0.025000
Training Epoch: 64 [13312/50000]	Loss: 0.5462	LR: 0.025000
Training Epoch: 64 [13440/50000]	Loss: 0.3949	LR: 0.025000
Training Epoch: 64 [13568/50000]	Loss: 0.6037	LR: 0.025000
Training Epoch: 64 [13696/50000]	Loss: 0.5150	LR: 0.025000
Training Epoch: 64 [13824/50000]	Loss: 0.5851	LR: 0.025000
Training Epoch: 64 [13952/50000]	Loss: 0.4991	LR: 0.025000
Training Epoch: 64 [14080/50000]	Loss: 0.5268	LR: 0.025000
Training Epoch: 64 [14208/50000]	Loss: 0.3463	LR: 0.025000
Training Epoch: 64 [14336/50000]	Loss: 0.4474	LR: 0.025000
Training Epoch: 64 [14464/50000]	Loss: 0.3564	LR: 0.025000
Training Epoch: 64 [14592/50000]	Loss: 0.4601	LR: 0.025000
Training Epoch: 64 [14720/50000]	Loss: 0.4915	LR: 0.025000
Training Epoch: 64 [14848/50000]	Loss: 0.7178	LR: 0.025000
Training Epoch: 64 [14976/50000]	Loss: 0.5452	LR: 0.025000
Training Epoch: 64 [15104/50000]	Loss: 0.4805	LR: 0.025000
Training Epoch: 64 [15232/50000]	Loss: 0.5838	LR: 0.025000
Training Epoch: 64 [15360/50000]	Loss: 0.4247	LR: 0.025000
Training Epoch: 64 [15488/50000]	Loss: 0.5609	LR: 0.025000
Training Epoch: 64 [15616/50000]	Loss: 0.4401	LR: 0.025000
Training Epoch: 64 [15744/50000]	Loss: 0.4837	LR: 0.025000
Training Epoch: 64 [15872/50000]	Loss: 0.5395	LR: 0.025000
Training Epoch: 64 [16000/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 64 [16128/50000]	Loss: 0.5081	LR: 0.025000
Training Epoch: 64 [16256/50000]	Loss: 0.4549	LR: 0.025000
Training Epoch: 64 [16384/50000]	Loss: 0.5374	LR: 0.025000
Training Epoch: 64 [16512/50000]	Loss: 0.3876	LR: 0.025000
Training Epoch: 64 [16640/50000]	Loss: 0.3860	LR: 0.025000
Training Epoch: 64 [16768/50000]	Loss: 0.4506	LR: 0.025000
Training Epoch: 64 [16896/50000]	Loss: 0.5077	LR: 0.025000
Training Epoch: 64 [17024/50000]	Loss: 0.3607	LR: 0.025000
Training Epoch: 64 [17152/50000]	Loss: 0.4770	LR: 0.025000
Training Epoch: 64 [17280/50000]	Loss: 0.5218	LR: 0.025000
Training Epoch: 64 [17408/50000]	Loss: 0.6600	LR: 0.025000
Training Epoch: 64 [17536/50000]	Loss: 0.6320	LR: 0.025000
Training Epoch: 64 [17664/50000]	Loss: 0.4891	LR: 0.025000
Training Epoch: 64 [17792/50000]	Loss: 0.5858	LR: 0.025000
Training Epoch: 64 [17920/50000]	Loss: 0.5415	LR: 0.025000
Training Epoch: 64 [18048/50000]	Loss: 0.4164	LR: 0.025000
Training Epoch: 64 [18176/50000]	Loss: 0.3351	LR: 0.025000
Training Epoch: 64 [18304/50000]	Loss: 0.3424	LR: 0.025000
Training Epoch: 64 [18432/50000]	Loss: 0.5128	LR: 0.025000
Training Epoch: 64 [18560/50000]	Loss: 0.5033	LR: 0.025000
Training Epoch: 64 [18688/50000]	Loss: 0.5507	LR: 0.025000
Training Epoch: 64 [18816/50000]	Loss: 0.5421	LR: 0.025000
Training Epoch: 64 [18944/50000]	Loss: 0.4173	LR: 0.025000
Training Epoch: 64 [19072/50000]	Loss: 0.4638	LR: 0.025000
Training Epoch: 64 [19200/50000]	Loss: 0.4404	LR: 0.025000
Training Epoch: 64 [19328/50000]	Loss: 0.4155	LR: 0.025000
Training Epoch: 64 [19456/50000]	Loss: 0.4622	LR: 0.025000
Training Epoch: 64 [19584/50000]	Loss: 0.4905	LR: 0.025000
Training Epoch: 64 [19712/50000]	Loss: 0.4684	LR: 0.025000
Training Epoch: 64 [19840/50000]	Loss: 0.3577	LR: 0.025000
Training Epoch: 64 [19968/50000]	Loss: 0.5264	LR: 0.025000
Training Epoch: 64 [20096/50000]	Loss: 0.3905	LR: 0.025000
Training Epoch: 64 [20224/50000]	Loss: 0.5918	LR: 0.025000
Training Epoch: 64 [20352/50000]	Loss: 0.4833	LR: 0.025000
Training Epoch: 64 [20480/50000]	Loss: 0.3711	LR: 0.025000
Training Epoch: 64 [20608/50000]	Loss: 0.4297	LR: 0.025000
Training Epoch: 64 [20736/50000]	Loss: 0.4404	LR: 0.025000
Training Epoch: 64 [20864/50000]	Loss: 0.4434	LR: 0.025000
Training Epoch: 64 [20992/50000]	Loss: 0.4839	LR: 0.025000
Training Epoch: 64 [21120/50000]	Loss: 0.4102	LR: 0.025000
Training Epoch: 64 [21248/50000]	Loss: 0.3591	LR: 0.025000
Training Epoch: 64 [21376/50000]	Loss: 0.4292	LR: 0.025000
Training Epoch: 64 [21504/50000]	Loss: 0.5799	LR: 0.025000
Training Epoch: 64 [21632/50000]	Loss: 0.4406	LR: 0.025000
Training Epoch: 64 [21760/50000]	Loss: 0.6023	LR: 0.025000
Training Epoch: 64 [21888/50000]	Loss: 0.4382	LR: 0.025000
Training Epoch: 64 [22016/50000]	Loss: 0.5566	LR: 0.025000
Training Epoch: 64 [22144/50000]	Loss: 0.5319	LR: 0.025000
Training Epoch: 64 [22272/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 64 [22400/50000]	Loss: 0.4829	LR: 0.025000
Training Epoch: 64 [22528/50000]	Loss: 0.4898	LR: 0.025000
Training Epoch: 64 [22656/50000]	Loss: 0.5822	LR: 0.025000
Training Epoch: 64 [22784/50000]	Loss: 0.4710	LR: 0.025000
Training Epoch: 64 [22912/50000]	Loss: 0.5249	LR: 0.025000
Training Epoch: 64 [23040/50000]	Loss: 0.5073	LR: 0.025000
Training Epoch: 64 [23168/50000]	Loss: 0.4937	LR: 0.025000
Training Epoch: 64 [23296/50000]	Loss: 0.3121	LR: 0.025000
Training Epoch: 64 [23424/50000]	Loss: 0.5727	LR: 0.025000
Training Epoch: 64 [23552/50000]	Loss: 0.3736	LR: 0.025000
Training Epoch: 64 [23680/50000]	Loss: 0.4428	LR: 0.025000
Training Epoch: 64 [23808/50000]	Loss: 0.4696	LR: 0.025000
Training Epoch: 64 [23936/50000]	Loss: 0.4309	LR: 0.025000
Training Epoch: 64 [24064/50000]	Loss: 0.6013	LR: 0.025000
Training Epoch: 64 [24192/50000]	Loss: 0.3937	LR: 0.025000
Training Epoch: 64 [24320/50000]	Loss: 0.6734	LR: 0.025000
Training Epoch: 64 [24448/50000]	Loss: 0.4819	LR: 0.025000
Training Epoch: 64 [24576/50000]	Loss: 0.4347	LR: 0.025000
Training Epoch: 64 [24704/50000]	Loss: 0.4217	LR: 0.025000
Training Epoch: 64 [24832/50000]	Loss: 0.5495	LR: 0.025000
Training Epoch: 64 [24960/50000]	Loss: 0.3817	LR: 0.025000
Training Epoch: 64 [25088/50000]	Loss: 0.4330	LR: 0.025000
Training Epoch: 64 [25216/50000]	Loss: 0.4847	LR: 0.025000
Training Epoch: 64 [25344/50000]	Loss: 0.6165	LR: 0.025000
Training Epoch: 64 [25472/50000]	Loss: 0.3127	LR: 0.025000
Training Epoch: 64 [25600/50000]	Loss: 0.4730	LR: 0.025000
Training Epoch: 64 [25728/50000]	Loss: 0.4633	LR: 0.025000
Training Epoch: 64 [25856/50000]	Loss: 0.5366	LR: 0.025000
Training Epoch: 64 [25984/50000]	Loss: 0.5900	LR: 0.025000
Training Epoch: 64 [26112/50000]	Loss: 0.5196	LR: 0.025000
Training Epoch: 64 [26240/50000]	Loss: 0.4136	LR: 0.025000
Training Epoch: 64 [26368/50000]	Loss: 0.4495	LR: 0.025000
Training Epoch: 64 [26496/50000]	Loss: 0.5416	LR: 0.025000
Training Epoch: 64 [26624/50000]	Loss: 0.5045	LR: 0.025000
Training Epoch: 64 [26752/50000]	Loss: 0.5181	LR: 0.025000
Training Epoch: 64 [26880/50000]	Loss: 0.4192	LR: 0.025000
Training Epoch: 64 [27008/50000]	Loss: 0.5164	LR: 0.025000
Training Epoch: 64 [27136/50000]	Loss: 0.3564	LR: 0.025000
Training Epoch: 64 [27264/50000]	Loss: 0.5672	LR: 0.025000
Training Epoch: 64 [27392/50000]	Loss: 0.4285	LR: 0.025000
Training Epoch: 64 [27520/50000]	Loss: 0.4922	LR: 0.025000
Training Epoch: 64 [27648/50000]	Loss: 0.3413	LR: 0.025000
Training Epoch: 64 [27776/50000]	Loss: 0.4126	LR: 0.025000
Training Epoch: 64 [27904/50000]	Loss: 0.5525	LR: 0.025000
Training Epoch: 64 [28032/50000]	Loss: 0.5598	LR: 0.025000
Training Epoch: 64 [28160/50000]	Loss: 0.6404	LR: 0.025000
Training Epoch: 64 [28288/50000]	Loss: 0.5449	LR: 0.025000
Training Epoch: 64 [28416/50000]	Loss: 0.5080	LR: 0.025000
Training Epoch: 64 [28544/50000]	Loss: 0.5341	LR: 0.025000
Training Epoch: 64 [28672/50000]	Loss: 0.5577	LR: 0.025000
Training Epoch: 64 [28800/50000]	Loss: 0.3827	LR: 0.025000
Training Epoch: 64 [28928/50000]	Loss: 0.4558	LR: 0.025000
Training Epoch: 64 [29056/50000]	Loss: 0.5281	LR: 0.025000
Training Epoch: 64 [29184/50000]	Loss: 0.3752	LR: 0.025000
Training Epoch: 64 [29312/50000]	Loss: 0.5349	LR: 0.025000
Training Epoch: 64 [29440/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 64 [29568/50000]	Loss: 0.4937	LR: 0.025000
Training Epoch: 64 [29696/50000]	Loss: 0.4327	LR: 0.025000
Training Epoch: 64 [29824/50000]	Loss: 0.4911	LR: 0.025000
Training Epoch: 64 [29952/50000]	Loss: 0.4806	LR: 0.025000
Training Epoch: 64 [30080/50000]	Loss: 0.5498	LR: 0.025000
Training Epoch: 64 [30208/50000]	Loss: 0.4702	LR: 0.025000
Training Epoch: 64 [30336/50000]	Loss: 0.4226	LR: 0.025000
Training Epoch: 64 [30464/50000]	Loss: 0.6954	LR: 0.025000
Training Epoch: 64 [30592/50000]	Loss: 0.5050	LR: 0.025000
Training Epoch: 64 [30720/50000]	Loss: 0.3869	LR: 0.025000
Training Epoch: 64 [30848/50000]	Loss: 0.4942	LR: 0.025000
Training Epoch: 64 [30976/50000]	Loss: 0.6255	LR: 0.025000
Training Epoch: 64 [31104/50000]	Loss: 0.5239	LR: 0.025000
Training Epoch: 64 [31232/50000]	Loss: 0.5069	LR: 0.025000
Training Epoch: 64 [31360/50000]	Loss: 0.5137	LR: 0.025000
Training Epoch: 64 [31488/50000]	Loss: 0.2734	LR: 0.025000
Training Epoch: 64 [31616/50000]	Loss: 0.4987	LR: 0.025000
Training Epoch: 64 [31744/50000]	Loss: 0.5337	LR: 0.025000
Training Epoch: 64 [31872/50000]	Loss: 0.4880	LR: 0.025000
Training Epoch: 64 [32000/50000]	Loss: 0.4175	LR: 0.025000
Training Epoch: 64 [32128/50000]	Loss: 0.5184	LR: 0.025000
Training Epoch: 64 [32256/50000]	Loss: 0.5058	LR: 0.025000
Training Epoch: 64 [32384/50000]	Loss: 0.4094	LR: 0.025000
Training Epoch: 64 [32512/50000]	Loss: 0.5166	LR: 0.025000
Training Epoch: 64 [32640/50000]	Loss: 0.3743	LR: 0.025000
Training Epoch: 64 [32768/50000]	Loss: 0.4468	LR: 0.025000
Training Epoch: 64 [32896/50000]	Loss: 0.4466	LR: 0.025000
Training Epoch: 64 [33024/50000]	Loss: 0.4876	LR: 0.025000
Training Epoch: 64 [33152/50000]	Loss: 0.5028	LR: 0.025000
Training Epoch: 64 [33280/50000]	Loss: 0.5776	LR: 0.025000
Training Epoch: 64 [33408/50000]	Loss: 0.5975	LR: 0.025000
Training Epoch: 64 [33536/50000]	Loss: 0.5493	LR: 0.025000
Training Epoch: 64 [33664/50000]	Loss: 0.4065	LR: 0.025000
Training Epoch: 64 [33792/50000]	Loss: 0.5774	LR: 0.025000
Training Epoch: 64 [33920/50000]	Loss: 0.4682	LR: 0.025000
Training Epoch: 64 [34048/50000]	Loss: 0.5234	LR: 0.025000
Training Epoch: 64 [34176/50000]	Loss: 0.4941	LR: 0.025000
Training Epoch: 64 [34304/50000]	Loss: 0.4338	LR: 0.025000
Training Epoch: 64 [34432/50000]	Loss: 0.4331	LR: 0.025000
Training Epoch: 64 [34560/50000]	Loss: 0.4755	LR: 0.025000
Training Epoch: 64 [34688/50000]	Loss: 0.5522	LR: 0.025000
Training Epoch: 64 [34816/50000]	Loss: 0.5417	LR: 0.025000
Training Epoch: 64 [34944/50000]	Loss: 0.5083	LR: 0.025000
Training Epoch: 64 [35072/50000]	Loss: 0.4948	LR: 0.025000
Training Epoch: 64 [35200/50000]	Loss: 0.5836	LR: 0.025000
Training Epoch: 64 [35328/50000]	Loss: 0.5804	LR: 0.025000
Training Epoch: 64 [35456/50000]	Loss: 0.5044	LR: 0.025000
Training Epoch: 64 [35584/50000]	Loss: 0.5811	LR: 0.025000
Training Epoch: 64 [35712/50000]	Loss: 0.6697	LR: 0.025000
Training Epoch: 64 [35840/50000]	Loss: 0.5360	LR: 0.025000
Training Epoch: 64 [35968/50000]	Loss: 0.5504	LR: 0.025000
Training Epoch: 64 [36096/50000]	Loss: 0.4288	LR: 0.025000
Training Epoch: 64 [36224/50000]	Loss: 0.3956	LR: 0.025000
Training Epoch: 64 [36352/50000]	Loss: 0.7372	LR: 0.025000
Training Epoch: 64 [36480/50000]	Loss: 0.5639	LR: 0.025000
Training Epoch: 64 [36608/50000]	Loss: 0.2712	LR: 0.025000
Training Epoch: 64 [36736/50000]	Loss: 0.4358	LR: 0.025000
Training Epoch: 64 [36864/50000]	Loss: 0.4339	LR: 0.025000
Training Epoch: 64 [36992/50000]	Loss: 0.5551	LR: 0.025000
Training Epoch: 64 [37120/50000]	Loss: 0.6000	LR: 0.025000
Training Epoch: 64 [37248/50000]	Loss: 0.4988	LR: 0.025000
Training Epoch: 64 [37376/50000]	Loss: 0.6739	LR: 0.025000
Training Epoch: 64 [37504/50000]	Loss: 0.5819	LR: 0.025000
Training Epoch: 64 [37632/50000]	Loss: 0.4108	LR: 0.025000
Training Epoch: 64 [37760/50000]	Loss: 0.3376	LR: 0.025000
Training Epoch: 64 [37888/50000]	Loss: 0.5016	LR: 0.025000
Training Epoch: 64 [38016/50000]	Loss: 0.5856	LR: 0.025000
Training Epoch: 64 [38144/50000]	Loss: 0.5041	LR: 0.025000
Training Epoch: 64 [38272/50000]	Loss: 0.5813	LR: 0.025000
Training Epoch: 64 [38400/50000]	Loss: 0.4170	LR: 0.025000
Training Epoch: 64 [38528/50000]	Loss: 0.5048	LR: 0.025000
Training Epoch: 64 [38656/50000]	Loss: 0.7136	LR: 0.025000
Training Epoch: 64 [38784/50000]	Loss: 0.7680	LR: 0.025000
Training Epoch: 64 [38912/50000]	Loss: 0.5571	LR: 0.025000
Training Epoch: 64 [39040/50000]	Loss: 0.6914	LR: 0.025000
Training Epoch: 64 [39168/50000]	Loss: 0.6307	LR: 0.025000
Training Epoch: 64 [39296/50000]	Loss: 0.3915	LR: 0.025000
Training Epoch: 64 [39424/50000]	Loss: 0.3379	LR: 0.025000
Training Epoch: 64 [39552/50000]	Loss: 0.4656	LR: 0.025000
Training Epoch: 64 [39680/50000]	Loss: 0.6732	LR: 0.025000
Training Epoch: 64 [39808/50000]	Loss: 0.4643	LR: 0.025000
Training Epoch: 64 [39936/50000]	Loss: 0.5936	LR: 0.025000
Training Epoch: 64 [40064/50000]	Loss: 0.5895	LR: 0.025000
Training Epoch: 64 [40192/50000]	Loss: 0.6162	LR: 0.025000
Training Epoch: 64 [40320/50000]	Loss: 0.3760	LR: 0.025000
Training Epoch: 64 [40448/50000]	Loss: 0.5846	LR: 0.025000
Training Epoch: 64 [40576/50000]	Loss: 0.5264	LR: 0.025000
Training Epoch: 64 [40704/50000]	Loss: 0.6374	LR: 0.025000
Training Epoch: 64 [40832/50000]	Loss: 0.3736	LR: 0.025000
Training Epoch: 64 [40960/50000]	Loss: 0.5823	LR: 0.025000
Training Epoch: 64 [41088/50000]	Loss: 0.4219	LR: 0.025000
Training Epoch: 64 [41216/50000]	Loss: 0.6852	LR: 0.025000
Training Epoch: 64 [41344/50000]	Loss: 0.5389	LR: 0.025000
Training Epoch: 64 [41472/50000]	Loss: 0.3862	LR: 0.025000
Training Epoch: 64 [41600/50000]	Loss: 0.3801	LR: 0.025000
Training Epoch: 64 [41728/50000]	Loss: 0.3311	LR: 0.025000
Training Epoch: 64 [41856/50000]	Loss: 0.4978	LR: 0.025000
Training Epoch: 64 [41984/50000]	Loss: 0.5605	LR: 0.025000
Training Epoch: 64 [42112/50000]	Loss: 0.4152	LR: 0.025000
Training Epoch: 64 [42240/50000]	Loss: 0.5247	LR: 0.025000
Training Epoch: 64 [42368/50000]	Loss: 0.5470	LR: 0.025000
Training Epoch: 64 [42496/50000]	Loss: 0.6642	LR: 0.025000
Training Epoch: 64 [42624/50000]	Loss: 0.4800	LR: 0.025000
Training Epoch: 64 [42752/50000]	Loss: 0.5227	LR: 0.025000
Training Epoch: 64 [42880/50000]	Loss: 0.5233	LR: 0.025000
Training Epoch: 64 [43008/50000]	Loss: 0.4281	LR: 0.025000
Training Epoch: 64 [43136/50000]	Loss: 0.6305	LR: 0.025000
Training Epoch: 64 [43264/50000]	Loss: 0.7387	LR: 0.025000
Training Epoch: 64 [43392/50000]	Loss: 0.5021	LR: 0.025000
Training Epoch: 64 [43520/50000]	Loss: 0.5610	LR: 0.025000
Training Epoch: 64 [43648/50000]	Loss: 0.4499	LR: 0.025000
Training Epoch: 64 [43776/50000]	Loss: 0.5768	LR: 0.025000
Training Epoch: 64 [43904/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 64 [44032/50000]	Loss: 0.7739	LR: 0.025000
Training Epoch: 64 [44160/50000]	Loss: 0.6045	LR: 0.025000
Training Epoch: 64 [44288/50000]	Loss: 0.6646	LR: 0.025000
Training Epoch: 64 [44416/50000]	Loss: 0.6058	LR: 0.025000
Training Epoch: 64 [44544/50000]	Loss: 0.5475	LR: 0.025000
Training Epoch: 64 [44672/50000]	Loss: 0.6940	LR: 0.025000
Training Epoch: 64 [44800/50000]	Loss: 0.5742	LR: 0.025000
Training Epoch: 64 [44928/50000]	Loss: 0.4329	LR: 0.025000
Training Epoch: 64 [45056/50000]	Loss: 0.4726	LR: 0.025000
Training Epoch: 64 [45184/50000]	Loss: 0.6827	LR: 0.025000
Training Epoch: 64 [45312/50000]	Loss: 0.4845	LR: 0.025000
Training Epoch: 64 [45440/50000]	Loss: 0.4683	LR: 0.025000
Training Epoch: 64 [45568/50000]	Loss: 0.4575	LR: 0.025000
Training Epoch: 64 [45696/50000]	Loss: 0.6153	LR: 0.025000
Training Epoch: 64 [45824/50000]	Loss: 0.4514	LR: 0.025000
Training Epoch: 64 [45952/50000]	Loss: 0.6170	LR: 0.025000
Training Epoch: 64 [46080/50000]	Loss: 0.6042	LR: 0.025000
Training Epoch: 64 [46208/50000]	Loss: 0.5279	LR: 0.025000
Training Epoch: 64 [46336/50000]	Loss: 0.5121	LR: 0.025000
Training Epoch: 64 [46464/50000]	Loss: 0.5486	LR: 0.025000
Training Epoch: 64 [46592/50000]	Loss: 0.3849	LR: 0.025000
Training Epoch: 64 [46720/50000]	Loss: 0.6323	LR: 0.025000
Training Epoch: 64 [46848/50000]	Loss: 0.5937	LR: 0.025000
Training Epoch: 64 [46976/50000]	Loss: 0.6715	LR: 0.025000
Training Epoch: 64 [47104/50000]	Loss: 0.5667	LR: 0.025000
Training Epoch: 64 [47232/50000]	Loss: 0.4864	LR: 0.025000
Training Epoch: 64 [47360/50000]	Loss: 0.5701	LR: 0.025000
Training Epoch: 64 [47488/50000]	Loss: 0.6151	LR: 0.025000
Training Epoch: 64 [47616/50000]	Loss: 0.5092	LR: 0.025000
Training Epoch: 64 [47744/50000]	Loss: 0.6410	LR: 0.025000
Training Epoch: 64 [47872/50000]	Loss: 0.6144	LR: 0.025000
Training Epoch: 64 [48000/50000]	Loss: 0.6355	LR: 0.025000
Training Epoch: 64 [48128/50000]	Loss: 0.4400	LR: 0.025000
Training Epoch: 64 [48256/50000]	Loss: 0.6633	LR: 0.025000
Training Epoch: 64 [48384/50000]	Loss: 0.6318	LR: 0.025000
Training Epoch: 64 [48512/50000]	Loss: 0.4332	LR: 0.025000
Training Epoch: 64 [48640/50000]	Loss: 0.6158	LR: 0.025000
Training Epoch: 64 [48768/50000]	Loss: 0.5140	LR: 0.025000
Training Epoch: 64 [48896/50000]	Loss: 0.6288	LR: 0.025000
Training Epoch: 64 [49024/50000]	Loss: 0.5349	LR: 0.025000
Training Epoch: 64 [49152/50000]	Loss: 0.4905	LR: 0.025000
Training Epoch: 64 [49280/50000]	Loss: 0.6188	LR: 0.025000
Training Epoch: 64 [49408/50000]	Loss: 0.6762	LR: 0.025000
Training Epoch: 64 [49536/50000]	Loss: 0.5308	LR: 0.025000
Training Epoch: 64 [49664/50000]	Loss: 0.6131	LR: 0.025000
Training Epoch: 64 [49792/50000]	Loss: 0.6063	LR: 0.025000
Training Epoch: 64 [49920/50000]	Loss: 0.5272	LR: 0.025000
Training Epoch: 64 [50000/50000]	Loss: 0.4084	LR: 0.025000
epoch 64 training time consumed: 53.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  234884 GB |  234883 GB |
|       from large pool |  123392 KB |    1034 MB |  234652 GB |  234652 GB |
|       from small pool |   10798 KB |      13 MB |     231 GB |     231 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  234884 GB |  234883 GB |
|       from large pool |  123392 KB |    1034 MB |  234652 GB |  234652 GB |
|       from small pool |   10798 KB |      13 MB |     231 GB |     231 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  103363 GB |  103363 GB |
|       from large pool |  155136 KB |  433088 KB |  103107 GB |  103107 GB |
|       from small pool |    1490 KB |    3494 KB |     255 GB |     255 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    9063 K  |    9063 K  |
|       from large pool |      24    |      65    |    4730 K  |    4730 K  |
|       from small pool |     231    |     274    |    4332 K  |    4332 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    9063 K  |    9063 K  |
|       from large pool |      24    |      65    |    4730 K  |    4730 K  |
|       from small pool |     231    |     274    |    4332 K  |    4332 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4478 K  |    4478 K  |
|       from large pool |       9    |      14    |    2289 K  |    2289 K  |
|       from small pool |      12    |      16    |    2188 K  |    2188 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 64, Average loss: 0.0109, Accuracy: 0.6590, Time consumed:3.44s

Training Epoch: 65 [128/50000]	Loss: 0.4813	LR: 0.025000
Training Epoch: 65 [256/50000]	Loss: 0.3744	LR: 0.025000
Training Epoch: 65 [384/50000]	Loss: 0.4696	LR: 0.025000
Training Epoch: 65 [512/50000]	Loss: 0.3897	LR: 0.025000
Training Epoch: 65 [640/50000]	Loss: 0.4286	LR: 0.025000
Training Epoch: 65 [768/50000]	Loss: 0.3817	LR: 0.025000
Training Epoch: 65 [896/50000]	Loss: 0.5422	LR: 0.025000
Training Epoch: 65 [1024/50000]	Loss: 0.3782	LR: 0.025000
Training Epoch: 65 [1152/50000]	Loss: 0.4681	LR: 0.025000
Training Epoch: 65 [1280/50000]	Loss: 0.5332	LR: 0.025000
Training Epoch: 65 [1408/50000]	Loss: 0.4498	LR: 0.025000
Training Epoch: 65 [1536/50000]	Loss: 0.3354	LR: 0.025000
Training Epoch: 65 [1664/50000]	Loss: 0.4898	LR: 0.025000
Training Epoch: 65 [1792/50000]	Loss: 0.4015	LR: 0.025000
Training Epoch: 65 [1920/50000]	Loss: 0.4225	LR: 0.025000
Training Epoch: 65 [2048/50000]	Loss: 0.3993	LR: 0.025000
Training Epoch: 65 [2176/50000]	Loss: 0.4917	LR: 0.025000
Training Epoch: 65 [2304/50000]	Loss: 0.4540	LR: 0.025000
Training Epoch: 65 [2432/50000]	Loss: 0.3909	LR: 0.025000
Training Epoch: 65 [2560/50000]	Loss: 0.4543	LR: 0.025000
Training Epoch: 65 [2688/50000]	Loss: 0.5037	LR: 0.025000
Training Epoch: 65 [2816/50000]	Loss: 0.3649	LR: 0.025000
Training Epoch: 65 [2944/50000]	Loss: 0.5579	LR: 0.025000
Training Epoch: 65 [3072/50000]	Loss: 0.4532	LR: 0.025000
Training Epoch: 65 [3200/50000]	Loss: 0.4340	LR: 0.025000
Training Epoch: 65 [3328/50000]	Loss: 0.5063	LR: 0.025000
Training Epoch: 65 [3456/50000]	Loss: 0.4262	LR: 0.025000
Training Epoch: 65 [3584/50000]	Loss: 0.6493	LR: 0.025000
Training Epoch: 65 [3712/50000]	Loss: 0.4409	LR: 0.025000
Training Epoch: 65 [3840/50000]	Loss: 0.4213	LR: 0.025000
Training Epoch: 65 [3968/50000]	Loss: 0.3780	LR: 0.025000
Training Epoch: 65 [4096/50000]	Loss: 0.4785	LR: 0.025000
Training Epoch: 65 [4224/50000]	Loss: 0.4081	LR: 0.025000
Training Epoch: 65 [4352/50000]	Loss: 0.4980	LR: 0.025000
Training Epoch: 65 [4480/50000]	Loss: 0.5508	LR: 0.025000
Training Epoch: 65 [4608/50000]	Loss: 0.4033	LR: 0.025000
Training Epoch: 65 [4736/50000]	Loss: 0.4606	LR: 0.025000
Training Epoch: 65 [4864/50000]	Loss: 0.4446	LR: 0.025000
Training Epoch: 65 [4992/50000]	Loss: 0.5012	LR: 0.025000
Training Epoch: 65 [5120/50000]	Loss: 0.4102	LR: 0.025000
Training Epoch: 65 [5248/50000]	Loss: 0.3681	LR: 0.025000
Training Epoch: 65 [5376/50000]	Loss: 0.3739	LR: 0.025000
Training Epoch: 65 [5504/50000]	Loss: 0.3896	LR: 0.025000
Training Epoch: 65 [5632/50000]	Loss: 0.4204	LR: 0.025000
Training Epoch: 65 [5760/50000]	Loss: 0.3917	LR: 0.025000
Training Epoch: 65 [5888/50000]	Loss: 0.4043	LR: 0.025000
Training Epoch: 65 [6016/50000]	Loss: 0.4780	LR: 0.025000
Training Epoch: 65 [6144/50000]	Loss: 0.2911	LR: 0.025000
Training Epoch: 65 [6272/50000]	Loss: 0.5138	LR: 0.025000
Training Epoch: 65 [6400/50000]	Loss: 0.4880	LR: 0.025000
Training Epoch: 65 [6528/50000]	Loss: 0.3000	LR: 0.025000
Training Epoch: 65 [6656/50000]	Loss: 0.4999	LR: 0.025000
Training Epoch: 65 [6784/50000]	Loss: 0.3566	LR: 0.025000
Training Epoch: 65 [6912/50000]	Loss: 0.4806	LR: 0.025000
Training Epoch: 65 [7040/50000]	Loss: 0.3447	LR: 0.025000
Training Epoch: 65 [7168/50000]	Loss: 0.5144	LR: 0.025000
Training Epoch: 65 [7296/50000]	Loss: 0.4397	LR: 0.025000
Training Epoch: 65 [7424/50000]	Loss: 0.3794	LR: 0.025000
Training Epoch: 65 [7552/50000]	Loss: 0.4219	LR: 0.025000
Training Epoch: 65 [7680/50000]	Loss: 0.6646	LR: 0.025000
Training Epoch: 65 [7808/50000]	Loss: 0.4266	LR: 0.025000
Training Epoch: 65 [7936/50000]	Loss: 0.4777	LR: 0.025000
Training Epoch: 65 [8064/50000]	Loss: 0.4312	LR: 0.025000
Training Epoch: 65 [8192/50000]	Loss: 0.3492	LR: 0.025000
Training Epoch: 65 [8320/50000]	Loss: 0.4212	LR: 0.025000
Training Epoch: 65 [8448/50000]	Loss: 0.5860	LR: 0.025000
Training Epoch: 65 [8576/50000]	Loss: 0.4029	LR: 0.025000
Training Epoch: 65 [8704/50000]	Loss: 0.3999	LR: 0.025000
Training Epoch: 65 [8832/50000]	Loss: 0.3499	LR: 0.025000
Training Epoch: 65 [8960/50000]	Loss: 0.3599	LR: 0.025000
Training Epoch: 65 [9088/50000]	Loss: 0.4463	LR: 0.025000
Training Epoch: 65 [9216/50000]	Loss: 0.4023	LR: 0.025000
Training Epoch: 65 [9344/50000]	Loss: 0.4485	LR: 0.025000
Training Epoch: 65 [9472/50000]	Loss: 0.4730	LR: 0.025000
Training Epoch: 65 [9600/50000]	Loss: 0.3780	LR: 0.025000
Training Epoch: 65 [9728/50000]	Loss: 0.5071	LR: 0.025000
Training Epoch: 65 [9856/50000]	Loss: 0.5210	LR: 0.025000
Training Epoch: 65 [9984/50000]	Loss: 0.4720	LR: 0.025000
Training Epoch: 65 [10112/50000]	Loss: 0.4484	LR: 0.025000
Training Epoch: 65 [10240/50000]	Loss: 0.4775	LR: 0.025000
Training Epoch: 65 [10368/50000]	Loss: 0.5418	LR: 0.025000
Training Epoch: 65 [10496/50000]	Loss: 0.5055	LR: 0.025000
Training Epoch: 65 [10624/50000]	Loss: 0.5300	LR: 0.025000
Training Epoch: 65 [10752/50000]	Loss: 0.4741	LR: 0.025000
Training Epoch: 65 [10880/50000]	Loss: 0.4101	LR: 0.025000
Training Epoch: 65 [11008/50000]	Loss: 0.4081	LR: 0.025000
Training Epoch: 65 [11136/50000]	Loss: 0.5680	LR: 0.025000
Training Epoch: 65 [11264/50000]	Loss: 0.5109	LR: 0.025000
Training Epoch: 65 [11392/50000]	Loss: 0.5537	LR: 0.025000
Training Epoch: 65 [11520/50000]	Loss: 0.3456	LR: 0.025000
Training Epoch: 65 [11648/50000]	Loss: 0.4611	LR: 0.025000
Training Epoch: 65 [11776/50000]	Loss: 0.3644	LR: 0.025000
Training Epoch: 65 [11904/50000]	Loss: 0.4349	LR: 0.025000
Training Epoch: 65 [12032/50000]	Loss: 0.4438	LR: 0.025000
Training Epoch: 65 [12160/50000]	Loss: 0.4280	LR: 0.025000
Training Epoch: 65 [12288/50000]	Loss: 0.5983	LR: 0.025000
Training Epoch: 65 [12416/50000]	Loss: 0.4224	LR: 0.025000
Training Epoch: 65 [12544/50000]	Loss: 0.5597	LR: 0.025000
Training Epoch: 65 [12672/50000]	Loss: 0.3759	LR: 0.025000
Training Epoch: 65 [12800/50000]	Loss: 0.4682	LR: 0.025000
Training Epoch: 65 [12928/50000]	Loss: 0.3867	LR: 0.025000
Training Epoch: 65 [13056/50000]	Loss: 0.4947	LR: 0.025000
Training Epoch: 65 [13184/50000]	Loss: 0.4341	LR: 0.025000
Training Epoch: 65 [13312/50000]	Loss: 0.4690	LR: 0.025000
Training Epoch: 65 [13440/50000]	Loss: 0.4823	LR: 0.025000
Training Epoch: 65 [13568/50000]	Loss: 0.3924	LR: 0.025000
Training Epoch: 65 [13696/50000]	Loss: 0.3816	LR: 0.025000
Training Epoch: 65 [13824/50000]	Loss: 0.3651	LR: 0.025000
Training Epoch: 65 [13952/50000]	Loss: 0.4003	LR: 0.025000
Training Epoch: 65 [14080/50000]	Loss: 0.4826	LR: 0.025000
Training Epoch: 65 [14208/50000]	Loss: 0.3711	LR: 0.025000
Training Epoch: 65 [14336/50000]	Loss: 0.6280	LR: 0.025000
Training Epoch: 65 [14464/50000]	Loss: 0.3460	LR: 0.025000
Training Epoch: 65 [14592/50000]	Loss: 0.5717	LR: 0.025000
Training Epoch: 65 [14720/50000]	Loss: 0.5361	LR: 0.025000
Training Epoch: 65 [14848/50000]	Loss: 0.5242	LR: 0.025000
Training Epoch: 65 [14976/50000]	Loss: 0.5487	LR: 0.025000
Training Epoch: 65 [15104/50000]	Loss: 0.5098	LR: 0.025000
Training Epoch: 65 [15232/50000]	Loss: 0.3264	LR: 0.025000
Training Epoch: 65 [15360/50000]	Loss: 0.4012	LR: 0.025000
Training Epoch: 65 [15488/50000]	Loss: 0.3866	LR: 0.025000
Training Epoch: 65 [15616/50000]	Loss: 0.5703	LR: 0.025000
Training Epoch: 65 [15744/50000]	Loss: 0.4446	LR: 0.025000
Training Epoch: 65 [15872/50000]	Loss: 0.5290	LR: 0.025000
Training Epoch: 65 [16000/50000]	Loss: 0.3992	LR: 0.025000
Training Epoch: 65 [16128/50000]	Loss: 0.4603	LR: 0.025000
Training Epoch: 65 [16256/50000]	Loss: 0.4776	LR: 0.025000
Training Epoch: 65 [16384/50000]	Loss: 0.6202	LR: 0.025000
Training Epoch: 65 [16512/50000]	Loss: 0.4624	LR: 0.025000
Training Epoch: 65 [16640/50000]	Loss: 0.5102	LR: 0.025000
Training Epoch: 65 [16768/50000]	Loss: 0.2987	LR: 0.025000
Training Epoch: 65 [16896/50000]	Loss: 0.4215	LR: 0.025000
Training Epoch: 65 [17024/50000]	Loss: 0.5287	LR: 0.025000
Training Epoch: 65 [17152/50000]	Loss: 0.5088	LR: 0.025000
Training Epoch: 65 [17280/50000]	Loss: 0.3971	LR: 0.025000
Training Epoch: 65 [17408/50000]	Loss: 0.3598	LR: 0.025000
Training Epoch: 65 [17536/50000]	Loss: 0.3802	LR: 0.025000
Training Epoch: 65 [17664/50000]	Loss: 0.3543	LR: 0.025000
Training Epoch: 65 [17792/50000]	Loss: 0.3298	LR: 0.025000
Training Epoch: 65 [17920/50000]	Loss: 0.4679	LR: 0.025000
Training Epoch: 65 [18048/50000]	Loss: 0.5152	LR: 0.025000
Training Epoch: 65 [18176/50000]	Loss: 0.4448	LR: 0.025000
Training Epoch: 65 [18304/50000]	Loss: 0.5989	LR: 0.025000
Training Epoch: 65 [18432/50000]	Loss: 0.4683	LR: 0.025000
Training Epoch: 65 [18560/50000]	Loss: 0.3533	LR: 0.025000
Training Epoch: 65 [18688/50000]	Loss: 0.3674	LR: 0.025000
Training Epoch: 65 [18816/50000]	Loss: 0.5226	LR: 0.025000
Training Epoch: 65 [18944/50000]	Loss: 0.4258	LR: 0.025000
Training Epoch: 65 [19072/50000]	Loss: 0.4117	LR: 0.025000
Training Epoch: 65 [19200/50000]	Loss: 0.4318	LR: 0.025000
Training Epoch: 65 [19328/50000]	Loss: 0.5815	LR: 0.025000
Training Epoch: 65 [19456/50000]	Loss: 0.6264	LR: 0.025000
Training Epoch: 65 [19584/50000]	Loss: 0.3299	LR: 0.025000
Training Epoch: 65 [19712/50000]	Loss: 0.3879	LR: 0.025000
Training Epoch: 65 [19840/50000]	Loss: 0.4665	LR: 0.025000
Training Epoch: 65 [19968/50000]	Loss: 0.4788	LR: 0.025000
Training Epoch: 65 [20096/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 65 [20224/50000]	Loss: 0.5428	LR: 0.025000
Training Epoch: 65 [20352/50000]	Loss: 0.3797	LR: 0.025000
Training Epoch: 65 [20480/50000]	Loss: 0.6388	LR: 0.025000
Training Epoch: 65 [20608/50000]	Loss: 0.5171	LR: 0.025000
Training Epoch: 65 [20736/50000]	Loss: 0.4897	LR: 0.025000
Training Epoch: 65 [20864/50000]	Loss: 0.2807	LR: 0.025000
Training Epoch: 65 [20992/50000]	Loss: 0.4923	LR: 0.025000
Training Epoch: 65 [21120/50000]	Loss: 0.3895	LR: 0.025000
Training Epoch: 65 [21248/50000]	Loss: 0.5055	LR: 0.025000
Training Epoch: 65 [21376/50000]	Loss: 0.5788	LR: 0.025000
Training Epoch: 65 [21504/50000]	Loss: 0.6056	LR: 0.025000
Training Epoch: 65 [21632/50000]	Loss: 0.4940	LR: 0.025000
Training Epoch: 65 [21760/50000]	Loss: 0.4720	LR: 0.025000
Training Epoch: 65 [21888/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 65 [22016/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 65 [22144/50000]	Loss: 0.3548	LR: 0.025000
Training Epoch: 65 [22272/50000]	Loss: 0.3257	LR: 0.025000
Training Epoch: 65 [22400/50000]	Loss: 0.4795	LR: 0.025000
Training Epoch: 65 [22528/50000]	Loss: 0.3889	LR: 0.025000
Training Epoch: 65 [22656/50000]	Loss: 0.3891	LR: 0.025000
Training Epoch: 65 [22784/50000]	Loss: 0.4999	LR: 0.025000
Training Epoch: 65 [22912/50000]	Loss: 0.4849	LR: 0.025000
Training Epoch: 65 [23040/50000]	Loss: 0.5257	LR: 0.025000
Training Epoch: 65 [23168/50000]	Loss: 0.4802	LR: 0.025000
Training Epoch: 65 [23296/50000]	Loss: 0.4870	LR: 0.025000
Training Epoch: 65 [23424/50000]	Loss: 0.5154	LR: 0.025000
Training Epoch: 65 [23552/50000]	Loss: 0.4369	LR: 0.025000
Training Epoch: 65 [23680/50000]	Loss: 0.4642	LR: 0.025000
Training Epoch: 65 [23808/50000]	Loss: 0.4346	LR: 0.025000
Training Epoch: 65 [23936/50000]	Loss: 0.5292	LR: 0.025000
Training Epoch: 65 [24064/50000]	Loss: 0.5610	LR: 0.025000
Training Epoch: 65 [24192/50000]	Loss: 0.4208	LR: 0.025000
Training Epoch: 65 [24320/50000]	Loss: 0.6086	LR: 0.025000
Training Epoch: 65 [24448/50000]	Loss: 0.4740	LR: 0.025000
Training Epoch: 65 [24576/50000]	Loss: 0.3763	LR: 0.025000
Training Epoch: 65 [24704/50000]	Loss: 0.4515	LR: 0.025000
Training Epoch: 65 [24832/50000]	Loss: 0.4963	LR: 0.025000
Training Epoch: 65 [24960/50000]	Loss: 0.5652	LR: 0.025000
Training Epoch: 65 [25088/50000]	Loss: 0.3855	LR: 0.025000
Training Epoch: 65 [25216/50000]	Loss: 0.6753	LR: 0.025000
Training Epoch: 65 [25344/50000]	Loss: 0.3835	LR: 0.025000
Training Epoch: 65 [25472/50000]	Loss: 0.3295	LR: 0.025000
Training Epoch: 65 [25600/50000]	Loss: 0.4815	LR: 0.025000
Training Epoch: 65 [25728/50000]	Loss: 0.4854	LR: 0.025000
Training Epoch: 65 [25856/50000]	Loss: 0.5616	LR: 0.025000
Training Epoch: 65 [25984/50000]	Loss: 0.5940	LR: 0.025000
Training Epoch: 65 [26112/50000]	Loss: 0.4011	LR: 0.025000
Training Epoch: 65 [26240/50000]	Loss: 0.3999	LR: 0.025000
Training Epoch: 65 [26368/50000]	Loss: 0.5119	LR: 0.025000
Training Epoch: 65 [26496/50000]	Loss: 0.4929	LR: 0.025000
Training Epoch: 65 [26624/50000]	Loss: 0.5517	LR: 0.025000
Training Epoch: 65 [26752/50000]	Loss: 0.4508	LR: 0.025000
Training Epoch: 65 [26880/50000]	Loss: 0.3361	LR: 0.025000
Training Epoch: 65 [27008/50000]	Loss: 0.3363	LR: 0.025000
Training Epoch: 65 [27136/50000]	Loss: 0.3543	LR: 0.025000
Training Epoch: 65 [27264/50000]	Loss: 0.4923	LR: 0.025000
Training Epoch: 65 [27392/50000]	Loss: 0.4967	LR: 0.025000
Training Epoch: 65 [27520/50000]	Loss: 0.4009	LR: 0.025000
Training Epoch: 65 [27648/50000]	Loss: 0.4892	LR: 0.025000
Training Epoch: 65 [27776/50000]	Loss: 0.5462	LR: 0.025000
Training Epoch: 65 [27904/50000]	Loss: 0.6112	LR: 0.025000
Training Epoch: 65 [28032/50000]	Loss: 0.5657	LR: 0.025000
Training Epoch: 65 [28160/50000]	Loss: 0.5386	LR: 0.025000
Training Epoch: 65 [28288/50000]	Loss: 0.5663	LR: 0.025000
Training Epoch: 65 [28416/50000]	Loss: 0.5334	LR: 0.025000
Training Epoch: 65 [28544/50000]	Loss: 0.5324	LR: 0.025000
Training Epoch: 65 [28672/50000]	Loss: 0.5866	LR: 0.025000
Training Epoch: 65 [28800/50000]	Loss: 0.6179	LR: 0.025000
Training Epoch: 65 [28928/50000]	Loss: 0.4867	LR: 0.025000
Training Epoch: 65 [29056/50000]	Loss: 0.4767	LR: 0.025000
Training Epoch: 65 [29184/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 65 [29312/50000]	Loss: 0.5123	LR: 0.025000
Training Epoch: 65 [29440/50000]	Loss: 0.3820	LR: 0.025000
Training Epoch: 65 [29568/50000]	Loss: 0.5797	LR: 0.025000
Training Epoch: 65 [29696/50000]	Loss: 0.4652	LR: 0.025000
Training Epoch: 65 [29824/50000]	Loss: 0.6624	LR: 0.025000
Training Epoch: 65 [29952/50000]	Loss: 0.6141	LR: 0.025000
Training Epoch: 65 [30080/50000]	Loss: 0.4537	LR: 0.025000
Training Epoch: 65 [30208/50000]	Loss: 0.5143	LR: 0.025000
Training Epoch: 65 [30336/50000]	Loss: 0.6275	LR: 0.025000
Training Epoch: 65 [30464/50000]	Loss: 0.6471	LR: 0.025000
Training Epoch: 65 [30592/50000]	Loss: 0.4953	LR: 0.025000
Training Epoch: 65 [30720/50000]	Loss: 0.5274	LR: 0.025000
Training Epoch: 65 [30848/50000]	Loss: 0.4561	LR: 0.025000
Training Epoch: 65 [30976/50000]	Loss: 0.3208	LR: 0.025000
Training Epoch: 65 [31104/50000]	Loss: 0.4629	LR: 0.025000
Training Epoch: 65 [31232/50000]	Loss: 0.4594	LR: 0.025000
Training Epoch: 65 [31360/50000]	Loss: 0.6500	LR: 0.025000
Training Epoch: 65 [31488/50000]	Loss: 0.4907	LR: 0.025000
Training Epoch: 65 [31616/50000]	Loss: 0.5341	LR: 0.025000
Training Epoch: 65 [31744/50000]	Loss: 0.3950	LR: 0.025000
Training Epoch: 65 [31872/50000]	Loss: 0.6610	LR: 0.025000
Training Epoch: 65 [32000/50000]	Loss: 0.5431	LR: 0.025000
Training Epoch: 65 [32128/50000]	Loss: 0.4865	LR: 0.025000
Training Epoch: 65 [32256/50000]	Loss: 0.4950	LR: 0.025000
Training Epoch: 65 [32384/50000]	Loss: 0.4821	LR: 0.025000
Training Epoch: 65 [32512/50000]	Loss: 0.3929	LR: 0.025000
Training Epoch: 65 [32640/50000]	Loss: 0.6523	LR: 0.025000
Training Epoch: 65 [32768/50000]	Loss: 0.4748	LR: 0.025000
Training Epoch: 65 [32896/50000]	Loss: 0.4784	LR: 0.025000
Training Epoch: 65 [33024/50000]	Loss: 0.4780	LR: 0.025000
Training Epoch: 65 [33152/50000]	Loss: 0.4596	LR: 0.025000
Training Epoch: 65 [33280/50000]	Loss: 0.6900	LR: 0.025000
Training Epoch: 65 [33408/50000]	Loss: 0.6943	LR: 0.025000
Training Epoch: 65 [33536/50000]	Loss: 0.5012	LR: 0.025000
Training Epoch: 65 [33664/50000]	Loss: 0.5975	LR: 0.025000
Training Epoch: 65 [33792/50000]	Loss: 0.5081	LR: 0.025000
Training Epoch: 65 [33920/50000]	Loss: 0.4816	LR: 0.025000
Training Epoch: 65 [34048/50000]	Loss: 0.5282	LR: 0.025000
Training Epoch: 65 [34176/50000]	Loss: 0.5775	LR: 0.025000
Training Epoch: 65 [34304/50000]	Loss: 0.4170	LR: 0.025000
Training Epoch: 65 [34432/50000]	Loss: 0.6488	LR: 0.025000
Training Epoch: 65 [34560/50000]	Loss: 0.4246	LR: 0.025000
Training Epoch: 65 [34688/50000]	Loss: 0.5712	LR: 0.025000
Training Epoch: 65 [34816/50000]	Loss: 0.5085	LR: 0.025000
Training Epoch: 65 [34944/50000]	Loss: 0.4642	LR: 0.025000
Training Epoch: 65 [35072/50000]	Loss: 0.4812	LR: 0.025000
Training Epoch: 65 [35200/50000]	Loss: 0.4158	LR: 0.025000
Training Epoch: 65 [35328/50000]	Loss: 0.7025	LR: 0.025000
Training Epoch: 65 [35456/50000]	Loss: 0.4457	LR: 0.025000
Training Epoch: 65 [35584/50000]	Loss: 0.6166	LR: 0.025000
Training Epoch: 65 [35712/50000]	Loss: 0.6775	LR: 0.025000
Training Epoch: 65 [35840/50000]	Loss: 0.5626	LR: 0.025000
Training Epoch: 65 [35968/50000]	Loss: 0.5487	LR: 0.025000
Training Epoch: 65 [36096/50000]	Loss: 0.5458	LR: 0.025000
Training Epoch: 65 [36224/50000]	Loss: 0.6070	LR: 0.025000
Training Epoch: 65 [36352/50000]	Loss: 0.5322	LR: 0.025000
Training Epoch: 65 [36480/50000]	Loss: 0.4000	LR: 0.025000
Training Epoch: 65 [36608/50000]	Loss: 0.3603	LR: 0.025000
Training Epoch: 65 [36736/50000]	Loss: 0.5002	LR: 0.025000
Training Epoch: 65 [36864/50000]	Loss: 0.5786	LR: 0.025000
Training Epoch: 65 [36992/50000]	Loss: 0.6006	LR: 0.025000
Training Epoch: 65 [37120/50000]	Loss: 0.5358	LR: 0.025000
Training Epoch: 65 [37248/50000]	Loss: 0.5654	LR: 0.025000
Training Epoch: 65 [37376/50000]	Loss: 0.5775	LR: 0.025000
Training Epoch: 65 [37504/50000]	Loss: 0.4907	LR: 0.025000
Training Epoch: 65 [37632/50000]	Loss: 0.6582	LR: 0.025000
Training Epoch: 65 [37760/50000]	Loss: 0.5473	LR: 0.025000
Training Epoch: 65 [37888/50000]	Loss: 0.5130	LR: 0.025000
Training Epoch: 65 [38016/50000]	Loss: 0.3743	LR: 0.025000
Training Epoch: 65 [38144/50000]	Loss: 0.4915	LR: 0.025000
Training Epoch: 65 [38272/50000]	Loss: 0.4540	LR: 0.025000
Training Epoch: 65 [38400/50000]	Loss: 0.6183	LR: 0.025000
Training Epoch: 65 [38528/50000]	Loss: 0.4948	LR: 0.025000
Training Epoch: 65 [38656/50000]	Loss: 0.6518	LR: 0.025000
Training Epoch: 65 [38784/50000]	Loss: 0.6994	LR: 0.025000
Training Epoch: 65 [38912/50000]	Loss: 0.5909	LR: 0.025000
Training Epoch: 65 [39040/50000]	Loss: 0.5067	LR: 0.025000
Training Epoch: 65 [39168/50000]	Loss: 0.3915	LR: 0.025000
Training Epoch: 65 [39296/50000]	Loss: 0.5692	LR: 0.025000
Training Epoch: 65 [39424/50000]	Loss: 0.5478	LR: 0.025000
Training Epoch: 65 [39552/50000]	Loss: 0.4922	LR: 0.025000
Training Epoch: 65 [39680/50000]	Loss: 0.5957	LR: 0.025000
Training Epoch: 65 [39808/50000]	Loss: 0.5135	LR: 0.025000
Training Epoch: 65 [39936/50000]	Loss: 0.4974	LR: 0.025000
Training Epoch: 65 [40064/50000]	Loss: 0.6171	LR: 0.025000
Training Epoch: 65 [40192/50000]	Loss: 0.6439	LR: 0.025000
Training Epoch: 65 [40320/50000]	Loss: 0.6535	LR: 0.025000
Training Epoch: 65 [40448/50000]	Loss: 0.4979	LR: 0.025000
Training Epoch: 65 [40576/50000]	Loss: 0.6162	LR: 0.025000
Training Epoch: 65 [40704/50000]	Loss: 0.5499	LR: 0.025000
Training Epoch: 65 [40832/50000]	Loss: 0.3718	LR: 0.025000
Training Epoch: 65 [40960/50000]	Loss: 0.6216	LR: 0.025000
Training Epoch: 65 [41088/50000]	Loss: 0.6750	LR: 0.025000
Training Epoch: 65 [41216/50000]	Loss: 0.6249	LR: 0.025000
Training Epoch: 65 [41344/50000]	Loss: 0.6215	LR: 0.025000
Training Epoch: 65 [41472/50000]	Loss: 0.5336	LR: 0.025000
Training Epoch: 65 [41600/50000]	Loss: 0.5433	LR: 0.025000
Training Epoch: 65 [41728/50000]	Loss: 0.5313	LR: 0.025000
Training Epoch: 65 [41856/50000]	Loss: 0.4854	LR: 0.025000
Training Epoch: 65 [41984/50000]	Loss: 0.6774	LR: 0.025000
Training Epoch: 65 [42112/50000]	Loss: 0.5841	LR: 0.025000
Training Epoch: 65 [42240/50000]	Loss: 0.6306	LR: 0.025000
Training Epoch: 65 [42368/50000]	Loss: 0.6498	LR: 0.025000
Training Epoch: 65 [42496/50000]	Loss: 0.6236	LR: 0.025000
Training Epoch: 65 [42624/50000]	Loss: 0.5927	LR: 0.025000
Training Epoch: 65 [42752/50000]	Loss: 0.5767	LR: 0.025000
Training Epoch: 65 [42880/50000]	Loss: 0.5689	LR: 0.025000
Training Epoch: 65 [43008/50000]	Loss: 0.5634	LR: 0.025000
Training Epoch: 65 [43136/50000]	Loss: 0.7984	LR: 0.025000
Training Epoch: 65 [43264/50000]	Loss: 0.5434	LR: 0.025000
Training Epoch: 65 [43392/50000]	Loss: 0.7914	LR: 0.025000
Training Epoch: 65 [43520/50000]	Loss: 0.3901	LR: 0.025000
Training Epoch: 65 [43648/50000]	Loss: 0.3569	LR: 0.025000
Training Epoch: 65 [43776/50000]	Loss: 0.5299	LR: 0.025000
Training Epoch: 65 [43904/50000]	Loss: 0.4066	LR: 0.025000
Training Epoch: 65 [44032/50000]	Loss: 0.6541	LR: 0.025000
Training Epoch: 65 [44160/50000]	Loss: 0.4308	LR: 0.025000
Training Epoch: 65 [44288/50000]	Loss: 0.5848	LR: 0.025000
Training Epoch: 65 [44416/50000]	Loss: 0.5273	LR: 0.025000
Training Epoch: 65 [44544/50000]	Loss: 0.5176	LR: 0.025000
Training Epoch: 65 [44672/50000]	Loss: 0.4911	LR: 0.025000
Training Epoch: 65 [44800/50000]	Loss: 0.4018	LR: 0.025000
Training Epoch: 65 [44928/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 65 [45056/50000]	Loss: 0.5341	LR: 0.025000
Training Epoch: 65 [45184/50000]	Loss: 0.6314	LR: 0.025000
Training Epoch: 65 [45312/50000]	Loss: 0.4697	LR: 0.025000
Training Epoch: 65 [45440/50000]	Loss: 0.5399	LR: 0.025000
Training Epoch: 65 [45568/50000]	Loss: 0.5504	LR: 0.025000
Training Epoch: 65 [45696/50000]	Loss: 0.6996	LR: 0.025000
Training Epoch: 65 [45824/50000]	Loss: 0.6418	LR: 0.025000
Training Epoch: 65 [45952/50000]	Loss: 0.6892	LR: 0.025000
Training Epoch: 65 [46080/50000]	Loss: 0.4606	LR: 0.025000
Training Epoch: 65 [46208/50000]	Loss: 0.4881	LR: 0.025000
Training Epoch: 65 [46336/50000]	Loss: 0.4718	LR: 0.025000
Training Epoch: 65 [46464/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 65 [46592/50000]	Loss: 0.6195	LR: 0.025000
Training Epoch: 65 [46720/50000]	Loss: 0.4016	LR: 0.025000
Training Epoch: 65 [46848/50000]	Loss: 0.5262	LR: 0.025000
Training Epoch: 65 [46976/50000]	Loss: 0.3808	LR: 0.025000
Training Epoch: 65 [47104/50000]	Loss: 0.4450	LR: 0.025000
Training Epoch: 65 [47232/50000]	Loss: 0.6222	LR: 0.025000
Training Epoch: 65 [47360/50000]	Loss: 0.6699	LR: 0.025000
Training Epoch: 65 [47488/50000]	Loss: 0.7093	LR: 0.025000
Training Epoch: 65 [47616/50000]	Loss: 0.5534	LR: 0.025000
Training Epoch: 65 [47744/50000]	Loss: 0.6002	LR: 0.025000
Training Epoch: 65 [47872/50000]	Loss: 0.4195	LR: 0.025000
Training Epoch: 65 [48000/50000]	Loss: 0.5026	LR: 0.025000
Training Epoch: 65 [48128/50000]	Loss: 0.5897	LR: 0.025000
Training Epoch: 65 [48256/50000]	Loss: 0.4870	LR: 0.025000
Training Epoch: 65 [48384/50000]	Loss: 0.6258	LR: 0.025000
Training Epoch: 65 [48512/50000]	Loss: 0.4783	LR: 0.025000
Training Epoch: 65 [48640/50000]	Loss: 0.6175	LR: 0.025000
Training Epoch: 65 [48768/50000]	Loss: 0.5330	LR: 0.025000
Training Epoch: 65 [48896/50000]	Loss: 0.4338	LR: 0.025000
Training Epoch: 65 [49024/50000]	Loss: 0.6737	LR: 0.025000
Training Epoch: 65 [49152/50000]	Loss: 0.6681	LR: 0.025000
Training Epoch: 65 [49280/50000]	Loss: 0.5049	LR: 0.025000
Training Epoch: 65 [49408/50000]	Loss: 0.5245	LR: 0.025000
Training Epoch: 65 [49536/50000]	Loss: 0.5982	LR: 0.025000
Training Epoch: 65 [49664/50000]	Loss: 0.5080	LR: 0.025000
Training Epoch: 65 [49792/50000]	Loss: 0.5170	LR: 0.025000
Training Epoch: 65 [49920/50000]	Loss: 0.5971	LR: 0.025000
Training Epoch: 65 [50000/50000]	Loss: 0.5668	LR: 0.025000
epoch 65 training time consumed: 53.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  238554 GB |  238553 GB |
|       from large pool |  123392 KB |    1034 MB |  238319 GB |  238318 GB |
|       from small pool |   10798 KB |      13 MB |     235 GB |     235 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  238554 GB |  238553 GB |
|       from large pool |  123392 KB |    1034 MB |  238319 GB |  238318 GB |
|       from small pool |   10798 KB |      13 MB |     235 GB |     235 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  104978 GB |  104978 GB |
|       from large pool |  155136 KB |  433088 KB |  104718 GB |  104718 GB |
|       from small pool |    1490 KB |    3494 KB |     259 GB |     259 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    9205 K  |    9204 K  |
|       from large pool |      24    |      65    |    4804 K  |    4804 K  |
|       from small pool |     231    |     274    |    4400 K  |    4400 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    9205 K  |    9204 K  |
|       from large pool |      24    |      65    |    4804 K  |    4804 K  |
|       from small pool |     231    |     274    |    4400 K  |    4400 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4548 K  |    4548 K  |
|       from large pool |       9    |      14    |    2325 K  |    2325 K  |
|       from small pool |      12    |      16    |    2223 K  |    2223 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 65, Average loss: 0.0112, Accuracy: 0.6621, Time consumed:3.46s

Training Epoch: 66 [128/50000]	Loss: 0.5772	LR: 0.025000
Training Epoch: 66 [256/50000]	Loss: 0.4271	LR: 0.025000
Training Epoch: 66 [384/50000]	Loss: 0.5646	LR: 0.025000
Training Epoch: 66 [512/50000]	Loss: 0.4911	LR: 0.025000
Training Epoch: 66 [640/50000]	Loss: 0.5412	LR: 0.025000
Training Epoch: 66 [768/50000]	Loss: 0.5163	LR: 0.025000
Training Epoch: 66 [896/50000]	Loss: 0.3339	LR: 0.025000
Training Epoch: 66 [1024/50000]	Loss: 0.2920	LR: 0.025000
Training Epoch: 66 [1152/50000]	Loss: 0.5238	LR: 0.025000
Training Epoch: 66 [1280/50000]	Loss: 0.5133	LR: 0.025000
Training Epoch: 66 [1408/50000]	Loss: 0.5478	LR: 0.025000
Training Epoch: 66 [1536/50000]	Loss: 0.3342	LR: 0.025000
Training Epoch: 66 [1664/50000]	Loss: 0.5580	LR: 0.025000
Training Epoch: 66 [1792/50000]	Loss: 0.4976	LR: 0.025000
Training Epoch: 66 [1920/50000]	Loss: 0.3567	LR: 0.025000
Training Epoch: 66 [2048/50000]	Loss: 0.4346	LR: 0.025000
Training Epoch: 66 [2176/50000]	Loss: 0.4794	LR: 0.025000
Training Epoch: 66 [2304/50000]	Loss: 0.3335	LR: 0.025000
Training Epoch: 66 [2432/50000]	Loss: 0.4158	LR: 0.025000
Training Epoch: 66 [2560/50000]	Loss: 0.4666	LR: 0.025000
Training Epoch: 66 [2688/50000]	Loss: 0.3419	LR: 0.025000
Training Epoch: 66 [2816/50000]	Loss: 0.4441	LR: 0.025000
Training Epoch: 66 [2944/50000]	Loss: 0.5759	LR: 0.025000
Training Epoch: 66 [3072/50000]	Loss: 0.3506	LR: 0.025000
Training Epoch: 66 [3200/50000]	Loss: 0.5750	LR: 0.025000
Training Epoch: 66 [3328/50000]	Loss: 0.4674	LR: 0.025000
Training Epoch: 66 [3456/50000]	Loss: 0.4427	LR: 0.025000
Training Epoch: 66 [3584/50000]	Loss: 0.3061	LR: 0.025000
Training Epoch: 66 [3712/50000]	Loss: 0.5233	LR: 0.025000
Training Epoch: 66 [3840/50000]	Loss: 0.4662	LR: 0.025000
Training Epoch: 66 [3968/50000]	Loss: 0.4016	LR: 0.025000
Training Epoch: 66 [4096/50000]	Loss: 0.4864	LR: 0.025000
Training Epoch: 66 [4224/50000]	Loss: 0.4615	LR: 0.025000
Training Epoch: 66 [4352/50000]	Loss: 0.4927	LR: 0.025000
Training Epoch: 66 [4480/50000]	Loss: 0.4488	LR: 0.025000
Training Epoch: 66 [4608/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 66 [4736/50000]	Loss: 0.5131	LR: 0.025000
Training Epoch: 66 [4864/50000]	Loss: 0.5507	LR: 0.025000
Training Epoch: 66 [4992/50000]	Loss: 0.3154	LR: 0.025000
Training Epoch: 66 [5120/50000]	Loss: 0.5305	LR: 0.025000
Training Epoch: 66 [5248/50000]	Loss: 0.4966	LR: 0.025000
Training Epoch: 66 [5376/50000]	Loss: 0.3964	LR: 0.025000
Training Epoch: 66 [5504/50000]	Loss: 0.3897	LR: 0.025000
Training Epoch: 66 [5632/50000]	Loss: 0.4236	LR: 0.025000
Training Epoch: 66 [5760/50000]	Loss: 0.3765	LR: 0.025000
Training Epoch: 66 [5888/50000]	Loss: 0.3686	LR: 0.025000
Training Epoch: 66 [6016/50000]	Loss: 0.3641	LR: 0.025000
Training Epoch: 66 [6144/50000]	Loss: 0.3930	LR: 0.025000
Training Epoch: 66 [6272/50000]	Loss: 0.4543	LR: 0.025000
Training Epoch: 66 [6400/50000]	Loss: 0.4277	LR: 0.025000
Training Epoch: 66 [6528/50000]	Loss: 0.4262	LR: 0.025000
Training Epoch: 66 [6656/50000]	Loss: 0.5607	LR: 0.025000
Training Epoch: 66 [6784/50000]	Loss: 0.4916	LR: 0.025000
Training Epoch: 66 [6912/50000]	Loss: 0.5235	LR: 0.025000
Training Epoch: 66 [7040/50000]	Loss: 0.4195	LR: 0.025000
Training Epoch: 66 [7168/50000]	Loss: 0.5216	LR: 0.025000
Training Epoch: 66 [7296/50000]	Loss: 0.3800	LR: 0.025000
Training Epoch: 66 [7424/50000]	Loss: 0.4727	LR: 0.025000
Training Epoch: 66 [7552/50000]	Loss: 0.4822	LR: 0.025000
Training Epoch: 66 [7680/50000]	Loss: 0.3976	LR: 0.025000
Training Epoch: 66 [7808/50000]	Loss: 0.4184	LR: 0.025000
Training Epoch: 66 [7936/50000]	Loss: 0.4808	LR: 0.025000
Training Epoch: 66 [8064/50000]	Loss: 0.4536	LR: 0.025000
Training Epoch: 66 [8192/50000]	Loss: 0.2789	LR: 0.025000
Training Epoch: 66 [8320/50000]	Loss: 0.3489	LR: 0.025000
Training Epoch: 66 [8448/50000]	Loss: 0.4061	LR: 0.025000
Training Epoch: 66 [8576/50000]	Loss: 0.6519	LR: 0.025000
Training Epoch: 66 [8704/50000]	Loss: 0.3553	LR: 0.025000
Training Epoch: 66 [8832/50000]	Loss: 0.3907	LR: 0.025000
Training Epoch: 66 [8960/50000]	Loss: 0.4859	LR: 0.025000
Training Epoch: 66 [9088/50000]	Loss: 0.3694	LR: 0.025000
Training Epoch: 66 [9216/50000]	Loss: 0.5247	LR: 0.025000
Training Epoch: 66 [9344/50000]	Loss: 0.3563	LR: 0.025000
Training Epoch: 66 [9472/50000]	Loss: 0.3740	LR: 0.025000
Training Epoch: 66 [9600/50000]	Loss: 0.3727	LR: 0.025000
Training Epoch: 66 [9728/50000]	Loss: 0.4684	LR: 0.025000
Training Epoch: 66 [9856/50000]	Loss: 0.3719	LR: 0.025000
Training Epoch: 66 [9984/50000]	Loss: 0.4807	LR: 0.025000
Training Epoch: 66 [10112/50000]	Loss: 0.4628	LR: 0.025000
Training Epoch: 66 [10240/50000]	Loss: 0.3419	LR: 0.025000
Training Epoch: 66 [10368/50000]	Loss: 0.3172	LR: 0.025000
Training Epoch: 66 [10496/50000]	Loss: 0.4330	LR: 0.025000
Training Epoch: 66 [10624/50000]	Loss: 0.6491	LR: 0.025000
Training Epoch: 66 [10752/50000]	Loss: 0.4343	LR: 0.025000
Training Epoch: 66 [10880/50000]	Loss: 0.4054	LR: 0.025000
Training Epoch: 66 [11008/50000]	Loss: 0.3773	LR: 0.025000
Training Epoch: 66 [11136/50000]	Loss: 0.3833	LR: 0.025000
Training Epoch: 66 [11264/50000]	Loss: 0.3863	LR: 0.025000
Training Epoch: 66 [11392/50000]	Loss: 0.5709	LR: 0.025000
Training Epoch: 66 [11520/50000]	Loss: 0.5314	LR: 0.025000
Training Epoch: 66 [11648/50000]	Loss: 0.5591	LR: 0.025000
Training Epoch: 66 [11776/50000]	Loss: 0.3341	LR: 0.025000
Training Epoch: 66 [11904/50000]	Loss: 0.3716	LR: 0.025000
Training Epoch: 66 [12032/50000]	Loss: 0.3915	LR: 0.025000
Training Epoch: 66 [12160/50000]	Loss: 0.3398	LR: 0.025000
Training Epoch: 66 [12288/50000]	Loss: 0.4101	LR: 0.025000
Training Epoch: 66 [12416/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 66 [12544/50000]	Loss: 0.5796	LR: 0.025000
Training Epoch: 66 [12672/50000]	Loss: 0.4970	LR: 0.025000
Training Epoch: 66 [12800/50000]	Loss: 0.4972	LR: 0.025000
Training Epoch: 66 [12928/50000]	Loss: 0.4236	LR: 0.025000
Training Epoch: 66 [13056/50000]	Loss: 0.4370	LR: 0.025000
Training Epoch: 66 [13184/50000]	Loss: 0.3757	LR: 0.025000
Training Epoch: 66 [13312/50000]	Loss: 0.3558	LR: 0.025000
Training Epoch: 66 [13440/50000]	Loss: 0.5488	LR: 0.025000
Training Epoch: 66 [13568/50000]	Loss: 0.3751	LR: 0.025000
Training Epoch: 66 [13696/50000]	Loss: 0.4108	LR: 0.025000
Training Epoch: 66 [13824/50000]	Loss: 0.4811	LR: 0.025000
Training Epoch: 66 [13952/50000]	Loss: 0.4616	LR: 0.025000
Training Epoch: 66 [14080/50000]	Loss: 0.4256	LR: 0.025000
Training Epoch: 66 [14208/50000]	Loss: 0.5073	LR: 0.025000
Training Epoch: 66 [14336/50000]	Loss: 0.5190	LR: 0.025000
Training Epoch: 66 [14464/50000]	Loss: 0.4837	LR: 0.025000
Training Epoch: 66 [14592/50000]	Loss: 0.4531	LR: 0.025000
Training Epoch: 66 [14720/50000]	Loss: 0.4690	LR: 0.025000
Training Epoch: 66 [14848/50000]	Loss: 0.4132	LR: 0.025000
Training Epoch: 66 [14976/50000]	Loss: 0.3544	LR: 0.025000
Training Epoch: 66 [15104/50000]	Loss: 0.6376	LR: 0.025000
Training Epoch: 66 [15232/50000]	Loss: 0.4076	LR: 0.025000
Training Epoch: 66 [15360/50000]	Loss: 0.4431	LR: 0.025000
Training Epoch: 66 [15488/50000]	Loss: 0.4538	LR: 0.025000
Training Epoch: 66 [15616/50000]	Loss: 0.2960	LR: 0.025000
Training Epoch: 66 [15744/50000]	Loss: 0.3943	LR: 0.025000
Training Epoch: 66 [15872/50000]	Loss: 0.5924	LR: 0.025000
Training Epoch: 66 [16000/50000]	Loss: 0.4118	LR: 0.025000
Training Epoch: 66 [16128/50000]	Loss: 0.3138	LR: 0.025000
Training Epoch: 66 [16256/50000]	Loss: 0.4518	LR: 0.025000
Training Epoch: 66 [16384/50000]	Loss: 0.4265	LR: 0.025000
Training Epoch: 66 [16512/50000]	Loss: 0.3945	LR: 0.025000
Training Epoch: 66 [16640/50000]	Loss: 0.3478	LR: 0.025000
Training Epoch: 66 [16768/50000]	Loss: 0.5786	LR: 0.025000
Training Epoch: 66 [16896/50000]	Loss: 0.4980	LR: 0.025000
Training Epoch: 66 [17024/50000]	Loss: 0.4293	LR: 0.025000
Training Epoch: 66 [17152/50000]	Loss: 0.4926	LR: 0.025000
Training Epoch: 66 [17280/50000]	Loss: 0.4576	LR: 0.025000
Training Epoch: 66 [17408/50000]	Loss: 0.4172	LR: 0.025000
Training Epoch: 66 [17536/50000]	Loss: 0.3684	LR: 0.025000
Training Epoch: 66 [17664/50000]	Loss: 0.5398	LR: 0.025000
Training Epoch: 66 [17792/50000]	Loss: 0.4980	LR: 0.025000
Training Epoch: 66 [17920/50000]	Loss: 0.4602	LR: 0.025000
Training Epoch: 66 [18048/50000]	Loss: 0.4176	LR: 0.025000
Training Epoch: 66 [18176/50000]	Loss: 0.5689	LR: 0.025000
Training Epoch: 66 [18304/50000]	Loss: 0.3837	LR: 0.025000
Training Epoch: 66 [18432/50000]	Loss: 0.3790	LR: 0.025000
Training Epoch: 66 [18560/50000]	Loss: 0.4322	LR: 0.025000
Training Epoch: 66 [18688/50000]	Loss: 0.3788	LR: 0.025000
Training Epoch: 66 [18816/50000]	Loss: 0.3627	LR: 0.025000
Training Epoch: 66 [18944/50000]	Loss: 0.4938	LR: 0.025000
Training Epoch: 66 [19072/50000]	Loss: 0.3767	LR: 0.025000
Training Epoch: 66 [19200/50000]	Loss: 0.3791	LR: 0.025000
Training Epoch: 66 [19328/50000]	Loss: 0.4262	LR: 0.025000
Training Epoch: 66 [19456/50000]	Loss: 0.4206	LR: 0.025000
Training Epoch: 66 [19584/50000]	Loss: 0.5569	LR: 0.025000
Training Epoch: 66 [19712/50000]	Loss: 0.5644	LR: 0.025000
Training Epoch: 66 [19840/50000]	Loss: 0.5705	LR: 0.025000
Training Epoch: 66 [19968/50000]	Loss: 0.4538	LR: 0.025000
Training Epoch: 66 [20096/50000]	Loss: 0.4538	LR: 0.025000
Training Epoch: 66 [20224/50000]	Loss: 0.5098	LR: 0.025000
Training Epoch: 66 [20352/50000]	Loss: 0.4446	LR: 0.025000
Training Epoch: 66 [20480/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 66 [20608/50000]	Loss: 0.3724	LR: 0.025000
Training Epoch: 66 [20736/50000]	Loss: 0.3342	LR: 0.025000
Training Epoch: 66 [20864/50000]	Loss: 0.2922	LR: 0.025000
Training Epoch: 66 [20992/50000]	Loss: 0.6549	LR: 0.025000
Training Epoch: 66 [21120/50000]	Loss: 0.5588	LR: 0.025000
Training Epoch: 66 [21248/50000]	Loss: 0.5465	LR: 0.025000
Training Epoch: 66 [21376/50000]	Loss: 0.4179	LR: 0.025000
Training Epoch: 66 [21504/50000]	Loss: 0.4475	LR: 0.025000
Training Epoch: 66 [21632/50000]	Loss: 0.5859	LR: 0.025000
Training Epoch: 66 [21760/50000]	Loss: 0.4339	LR: 0.025000
Training Epoch: 66 [21888/50000]	Loss: 0.5009	LR: 0.025000
Training Epoch: 66 [22016/50000]	Loss: 0.5411	LR: 0.025000
Training Epoch: 66 [22144/50000]	Loss: 0.5094	LR: 0.025000
Training Epoch: 66 [22272/50000]	Loss: 0.3779	LR: 0.025000
Training Epoch: 66 [22400/50000]	Loss: 0.4538	LR: 0.025000
Training Epoch: 66 [22528/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 66 [22656/50000]	Loss: 0.5153	LR: 0.025000
Training Epoch: 66 [22784/50000]	Loss: 0.3978	LR: 0.025000
Training Epoch: 66 [22912/50000]	Loss: 0.3629	LR: 0.025000
Training Epoch: 66 [23040/50000]	Loss: 0.4932	LR: 0.025000
Training Epoch: 66 [23168/50000]	Loss: 0.5355	LR: 0.025000
Training Epoch: 66 [23296/50000]	Loss: 0.4461	LR: 0.025000
Training Epoch: 66 [23424/50000]	Loss: 0.3601	LR: 0.025000
Training Epoch: 66 [23552/50000]	Loss: 0.3949	LR: 0.025000
Training Epoch: 66 [23680/50000]	Loss: 0.4943	LR: 0.025000
Training Epoch: 66 [23808/50000]	Loss: 0.4692	LR: 0.025000
Training Epoch: 66 [23936/50000]	Loss: 0.6419	LR: 0.025000
Training Epoch: 66 [24064/50000]	Loss: 0.5262	LR: 0.025000
Training Epoch: 66 [24192/50000]	Loss: 0.4770	LR: 0.025000
Training Epoch: 66 [24320/50000]	Loss: 0.4534	LR: 0.025000
Training Epoch: 66 [24448/50000]	Loss: 0.4788	LR: 0.025000
Training Epoch: 66 [24576/50000]	Loss: 0.4864	LR: 0.025000
Training Epoch: 66 [24704/50000]	Loss: 0.4057	LR: 0.025000
Training Epoch: 66 [24832/50000]	Loss: 0.4532	LR: 0.025000
Training Epoch: 66 [24960/50000]	Loss: 0.5257	LR: 0.025000
Training Epoch: 66 [25088/50000]	Loss: 0.4435	LR: 0.025000
Training Epoch: 66 [25216/50000]	Loss: 0.5470	LR: 0.025000
Training Epoch: 66 [25344/50000]	Loss: 0.3585	LR: 0.025000
Training Epoch: 66 [25472/50000]	Loss: 0.3964	LR: 0.025000
Training Epoch: 66 [25600/50000]	Loss: 0.4360	LR: 0.025000
Training Epoch: 66 [25728/50000]	Loss: 0.4220	LR: 0.025000
Training Epoch: 66 [25856/50000]	Loss: 0.4554	LR: 0.025000
Training Epoch: 66 [25984/50000]	Loss: 0.4728	LR: 0.025000
Training Epoch: 66 [26112/50000]	Loss: 0.4101	LR: 0.025000
Training Epoch: 66 [26240/50000]	Loss: 0.4082	LR: 0.025000
Training Epoch: 66 [26368/50000]	Loss: 0.4543	LR: 0.025000
Training Epoch: 66 [26496/50000]	Loss: 0.4931	LR: 0.025000
Training Epoch: 66 [26624/50000]	Loss: 0.5371	LR: 0.025000
Training Epoch: 66 [26752/50000]	Loss: 0.5762	LR: 0.025000
Training Epoch: 66 [26880/50000]	Loss: 0.4332	LR: 0.025000
Training Epoch: 66 [27008/50000]	Loss: 0.4522	LR: 0.025000
Training Epoch: 66 [27136/50000]	Loss: 0.5137	LR: 0.025000
Training Epoch: 66 [27264/50000]	Loss: 0.4851	LR: 0.025000
Training Epoch: 66 [27392/50000]	Loss: 0.4809	LR: 0.025000
Training Epoch: 66 [27520/50000]	Loss: 0.4508	LR: 0.025000
Training Epoch: 66 [27648/50000]	Loss: 0.4757	LR: 0.025000
Training Epoch: 66 [27776/50000]	Loss: 0.5520	LR: 0.025000
Training Epoch: 66 [27904/50000]	Loss: 0.4425	LR: 0.025000
Training Epoch: 66 [28032/50000]	Loss: 0.5580	LR: 0.025000
Training Epoch: 66 [28160/50000]	Loss: 0.4815	LR: 0.025000
Training Epoch: 66 [28288/50000]	Loss: 0.4080	LR: 0.025000
Training Epoch: 66 [28416/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 66 [28544/50000]	Loss: 0.4327	LR: 0.025000
Training Epoch: 66 [28672/50000]	Loss: 0.4568	LR: 0.025000
Training Epoch: 66 [28800/50000]	Loss: 0.3736	LR: 0.025000
Training Epoch: 66 [28928/50000]	Loss: 0.4122	LR: 0.025000
Training Epoch: 66 [29056/50000]	Loss: 0.3958	LR: 0.025000
Training Epoch: 66 [29184/50000]	Loss: 0.3767	LR: 0.025000
Training Epoch: 66 [29312/50000]	Loss: 0.4296	LR: 0.025000
Training Epoch: 66 [29440/50000]	Loss: 0.3898	LR: 0.025000
Training Epoch: 66 [29568/50000]	Loss: 0.3965	LR: 0.025000
Training Epoch: 66 [29696/50000]	Loss: 0.3503	LR: 0.025000
Training Epoch: 66 [29824/50000]	Loss: 0.4917	LR: 0.025000
Training Epoch: 66 [29952/50000]	Loss: 0.6294	LR: 0.025000
Training Epoch: 66 [30080/50000]	Loss: 0.4458	LR: 0.025000
Training Epoch: 66 [30208/50000]	Loss: 0.4914	LR: 0.025000
Training Epoch: 66 [30336/50000]	Loss: 0.4972	LR: 0.025000
Training Epoch: 66 [30464/50000]	Loss: 0.4386	LR: 0.025000
Training Epoch: 66 [30592/50000]	Loss: 0.3212	LR: 0.025000
Training Epoch: 66 [30720/50000]	Loss: 0.6640	LR: 0.025000
Training Epoch: 66 [30848/50000]	Loss: 0.4554	LR: 0.025000
Training Epoch: 66 [30976/50000]	Loss: 0.5509	LR: 0.025000
Training Epoch: 66 [31104/50000]	Loss: 0.5390	LR: 0.025000
Training Epoch: 66 [31232/50000]	Loss: 0.3931	LR: 0.025000
Training Epoch: 66 [31360/50000]	Loss: 0.5379	LR: 0.025000
Training Epoch: 66 [31488/50000]	Loss: 0.5022	LR: 0.025000
Training Epoch: 66 [31616/50000]	Loss: 0.6262	LR: 0.025000
Training Epoch: 66 [31744/50000]	Loss: 0.4977	LR: 0.025000
Training Epoch: 66 [31872/50000]	Loss: 0.5869	LR: 0.025000
Training Epoch: 66 [32000/50000]	Loss: 0.4138	LR: 0.025000
Training Epoch: 66 [32128/50000]	Loss: 0.4246	LR: 0.025000
Training Epoch: 66 [32256/50000]	Loss: 0.4854	LR: 0.025000
Training Epoch: 66 [32384/50000]	Loss: 0.3604	LR: 0.025000
Training Epoch: 66 [32512/50000]	Loss: 0.5120	LR: 0.025000
Training Epoch: 66 [32640/50000]	Loss: 0.4902	LR: 0.025000
Training Epoch: 66 [32768/50000]	Loss: 0.4923	LR: 0.025000
Training Epoch: 66 [32896/50000]	Loss: 0.5357	LR: 0.025000
Training Epoch: 66 [33024/50000]	Loss: 0.5168	LR: 0.025000
Training Epoch: 66 [33152/50000]	Loss: 0.4641	LR: 0.025000
Training Epoch: 66 [33280/50000]	Loss: 0.6429	LR: 0.025000
Training Epoch: 66 [33408/50000]	Loss: 0.5661	LR: 0.025000
Training Epoch: 66 [33536/50000]	Loss: 0.5959	LR: 0.025000
Training Epoch: 66 [33664/50000]	Loss: 0.5957	LR: 0.025000
Training Epoch: 66 [33792/50000]	Loss: 0.5678	LR: 0.025000
Training Epoch: 66 [33920/50000]	Loss: 0.4801	LR: 0.025000
Training Epoch: 66 [34048/50000]	Loss: 0.5595	LR: 0.025000
Training Epoch: 66 [34176/50000]	Loss: 0.6096	LR: 0.025000
Training Epoch: 66 [34304/50000]	Loss: 0.4374	LR: 0.025000
Training Epoch: 66 [34432/50000]	Loss: 0.4159	LR: 0.025000
Training Epoch: 66 [34560/50000]	Loss: 0.4626	LR: 0.025000
Training Epoch: 66 [34688/50000]	Loss: 0.5115	LR: 0.025000
Training Epoch: 66 [34816/50000]	Loss: 0.4644	LR: 0.025000
Training Epoch: 66 [34944/50000]	Loss: 0.5449	LR: 0.025000
Training Epoch: 66 [35072/50000]	Loss: 0.4638	LR: 0.025000
Training Epoch: 66 [35200/50000]	Loss: 0.5866	LR: 0.025000
Training Epoch: 66 [35328/50000]	Loss: 0.4720	LR: 0.025000
Training Epoch: 66 [35456/50000]	Loss: 0.5093	LR: 0.025000
Training Epoch: 66 [35584/50000]	Loss: 0.5390	LR: 0.025000
Training Epoch: 66 [35712/50000]	Loss: 0.5471	LR: 0.025000
Training Epoch: 66 [35840/50000]	Loss: 0.5487	LR: 0.025000
Training Epoch: 66 [35968/50000]	Loss: 0.5139	LR: 0.025000
Training Epoch: 66 [36096/50000]	Loss: 0.5173	LR: 0.025000
Training Epoch: 66 [36224/50000]	Loss: 0.4587	LR: 0.025000
Training Epoch: 66 [36352/50000]	Loss: 0.4900	LR: 0.025000
Training Epoch: 66 [36480/50000]	Loss: 0.6121	LR: 0.025000
Training Epoch: 66 [36608/50000]	Loss: 0.5222	LR: 0.025000
Training Epoch: 66 [36736/50000]	Loss: 0.4971	LR: 0.025000
Training Epoch: 66 [36864/50000]	Loss: 0.4571	LR: 0.025000
Training Epoch: 66 [36992/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 66 [37120/50000]	Loss: 0.5188	LR: 0.025000
Training Epoch: 66 [37248/50000]	Loss: 0.5097	LR: 0.025000
Training Epoch: 66 [37376/50000]	Loss: 0.4495	LR: 0.025000
Training Epoch: 66 [37504/50000]	Loss: 0.4994	LR: 0.025000
Training Epoch: 66 [37632/50000]	Loss: 0.6528	LR: 0.025000
Training Epoch: 66 [37760/50000]	Loss: 0.5363	LR: 0.025000
Training Epoch: 66 [37888/50000]	Loss: 0.4038	LR: 0.025000
Training Epoch: 66 [38016/50000]	Loss: 0.4149	LR: 0.025000
Training Epoch: 66 [38144/50000]	Loss: 0.6911	LR: 0.025000
Training Epoch: 66 [38272/50000]	Loss: 0.4131	LR: 0.025000
Training Epoch: 66 [38400/50000]	Loss: 0.4239	LR: 0.025000
Training Epoch: 66 [38528/50000]	Loss: 0.5002	LR: 0.025000
Training Epoch: 66 [38656/50000]	Loss: 0.4750	LR: 0.025000
Training Epoch: 66 [38784/50000]	Loss: 0.4958	LR: 0.025000
Training Epoch: 66 [38912/50000]	Loss: 0.5703	LR: 0.025000
Training Epoch: 66 [39040/50000]	Loss: 0.5286	LR: 0.025000
Training Epoch: 66 [39168/50000]	Loss: 0.5498	LR: 0.025000
Training Epoch: 66 [39296/50000]	Loss: 0.3990	LR: 0.025000
Training Epoch: 66 [39424/50000]	Loss: 0.4562	LR: 0.025000
Training Epoch: 66 [39552/50000]	Loss: 0.5302	LR: 0.025000
Training Epoch: 66 [39680/50000]	Loss: 0.4788	LR: 0.025000
Training Epoch: 66 [39808/50000]	Loss: 0.6924	LR: 0.025000
Training Epoch: 66 [39936/50000]	Loss: 0.4590	LR: 0.025000
Training Epoch: 66 [40064/50000]	Loss: 0.5254	LR: 0.025000
Training Epoch: 66 [40192/50000]	Loss: 0.5679	LR: 0.025000
Training Epoch: 66 [40320/50000]	Loss: 0.4219	LR: 0.025000
Training Epoch: 66 [40448/50000]	Loss: 0.4519	LR: 0.025000
Training Epoch: 66 [40576/50000]	Loss: 0.4417	LR: 0.025000
Training Epoch: 66 [40704/50000]	Loss: 0.5906	LR: 0.025000
Training Epoch: 66 [40832/50000]	Loss: 0.6660	LR: 0.025000
Training Epoch: 66 [40960/50000]	Loss: 0.7498	LR: 0.025000
Training Epoch: 66 [41088/50000]	Loss: 0.6657	LR: 0.025000
Training Epoch: 66 [41216/50000]	Loss: 0.6131	LR: 0.025000
Training Epoch: 66 [41344/50000]	Loss: 0.4799	LR: 0.025000
Training Epoch: 66 [41472/50000]	Loss: 0.5839	LR: 0.025000
Training Epoch: 66 [41600/50000]	Loss: 0.5296	LR: 0.025000
Training Epoch: 66 [41728/50000]	Loss: 0.7035	LR: 0.025000
Training Epoch: 66 [41856/50000]	Loss: 0.5483	LR: 0.025000
Training Epoch: 66 [41984/50000]	Loss: 0.5601	LR: 0.025000
Training Epoch: 66 [42112/50000]	Loss: 0.5589	LR: 0.025000
Training Epoch: 66 [42240/50000]	Loss: 0.4491	LR: 0.025000
Training Epoch: 66 [42368/50000]	Loss: 0.4371	LR: 0.025000
Training Epoch: 66 [42496/50000]	Loss: 0.4574	LR: 0.025000
Training Epoch: 66 [42624/50000]	Loss: 0.4072	LR: 0.025000
Training Epoch: 66 [42752/50000]	Loss: 0.6860	LR: 0.025000
Training Epoch: 66 [42880/50000]	Loss: 0.7411	LR: 0.025000
Training Epoch: 66 [43008/50000]	Loss: 0.6394	LR: 0.025000
Training Epoch: 66 [43136/50000]	Loss: 0.6621	LR: 0.025000
Training Epoch: 66 [43264/50000]	Loss: 0.5598	LR: 0.025000
Training Epoch: 66 [43392/50000]	Loss: 0.8067	LR: 0.025000
Training Epoch: 66 [43520/50000]	Loss: 0.7406	LR: 0.025000
Training Epoch: 66 [43648/50000]	Loss: 0.5538	LR: 0.025000
Training Epoch: 66 [43776/50000]	Loss: 0.5521	LR: 0.025000
Training Epoch: 66 [43904/50000]	Loss: 0.5851	LR: 0.025000
Training Epoch: 66 [44032/50000]	Loss: 0.4932	LR: 0.025000
Training Epoch: 66 [44160/50000]	Loss: 0.4181	LR: 0.025000
Training Epoch: 66 [44288/50000]	Loss: 0.6608	LR: 0.025000
Training Epoch: 66 [44416/50000]	Loss: 0.5394	LR: 0.025000
Training Epoch: 66 [44544/50000]	Loss: 0.4529	LR: 0.025000
Training Epoch: 66 [44672/50000]	Loss: 0.4552	LR: 0.025000
Training Epoch: 66 [44800/50000]	Loss: 0.6823	LR: 0.025000
Training Epoch: 66 [44928/50000]	Loss: 0.6638	LR: 0.025000
Training Epoch: 66 [45056/50000]	Loss: 0.5499	LR: 0.025000
Training Epoch: 66 [45184/50000]	Loss: 0.4106	LR: 0.025000
Training Epoch: 66 [45312/50000]	Loss: 0.6442	LR: 0.025000
Training Epoch: 66 [45440/50000]	Loss: 0.5792	LR: 0.025000
Training Epoch: 66 [45568/50000]	Loss: 0.5725	LR: 0.025000
Training Epoch: 66 [45696/50000]	Loss: 0.5290	LR: 0.025000
Training Epoch: 66 [45824/50000]	Loss: 0.4620	LR: 0.025000
Training Epoch: 66 [45952/50000]	Loss: 0.4449	LR: 0.025000
Training Epoch: 66 [46080/50000]	Loss: 0.5457	LR: 0.025000
Training Epoch: 66 [46208/50000]	Loss: 0.5040	LR: 0.025000
Training Epoch: 66 [46336/50000]	Loss: 0.6055	LR: 0.025000
Training Epoch: 66 [46464/50000]	Loss: 0.6254	LR: 0.025000
Training Epoch: 66 [46592/50000]	Loss: 0.5205	LR: 0.025000
Training Epoch: 66 [46720/50000]	Loss: 0.6785	LR: 0.025000
Training Epoch: 66 [46848/50000]	Loss: 0.5504	LR: 0.025000
Training Epoch: 66 [46976/50000]	Loss: 0.5063	LR: 0.025000
Training Epoch: 66 [47104/50000]	Loss: 0.6907	LR: 0.025000
Training Epoch: 66 [47232/50000]	Loss: 0.6739	LR: 0.025000
Training Epoch: 66 [47360/50000]	Loss: 0.6106	LR: 0.025000
Training Epoch: 66 [47488/50000]	Loss: 0.3841	LR: 0.025000
Training Epoch: 66 [47616/50000]	Loss: 0.5236	LR: 0.025000
Training Epoch: 66 [47744/50000]	Loss: 0.5490	LR: 0.025000
Training Epoch: 66 [47872/50000]	Loss: 0.6743	LR: 0.025000
Training Epoch: 66 [48000/50000]	Loss: 0.5617	LR: 0.025000
Training Epoch: 66 [48128/50000]	Loss: 0.5320	LR: 0.025000
Training Epoch: 66 [48256/50000]	Loss: 0.5359	LR: 0.025000
Training Epoch: 66 [48384/50000]	Loss: 0.5057	LR: 0.025000
Training Epoch: 66 [48512/50000]	Loss: 0.4636	LR: 0.025000
Training Epoch: 66 [48640/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 66 [48768/50000]	Loss: 0.4607	LR: 0.025000
Training Epoch: 66 [48896/50000]	Loss: 0.6736	LR: 0.025000
Training Epoch: 66 [49024/50000]	Loss: 0.6055	LR: 0.025000
Training Epoch: 66 [49152/50000]	Loss: 0.5985	LR: 0.025000
Training Epoch: 66 [49280/50000]	Loss: 0.4705	LR: 0.025000
Training Epoch: 66 [49408/50000]	Loss: 0.5675	LR: 0.025000
Training Epoch: 66 [49536/50000]	Loss: 0.4893	LR: 0.025000
Training Epoch: 66 [49664/50000]	Loss: 0.5003	LR: 0.025000
Training Epoch: 66 [49792/50000]	Loss: 0.4778	LR: 0.025000
Training Epoch: 66 [49920/50000]	Loss: 0.5267	LR: 0.025000
Training Epoch: 66 [50000/50000]	Loss: 0.5087	LR: 0.025000
epoch 66 training time consumed: 53.97s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  242224 GB |  242224 GB |
|       from large pool |  123392 KB |    1034 MB |  241985 GB |  241985 GB |
|       from small pool |   10798 KB |      13 MB |     238 GB |     238 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  242224 GB |  242224 GB |
|       from large pool |  123392 KB |    1034 MB |  241985 GB |  241985 GB |
|       from small pool |   10798 KB |      13 MB |     238 GB |     238 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  106593 GB |  106593 GB |
|       from large pool |  155136 KB |  433088 KB |  106329 GB |  106329 GB |
|       from small pool |    1490 KB |    3494 KB |     263 GB |     263 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    9346 K  |    9346 K  |
|       from large pool |      24    |      65    |    4878 K  |    4878 K  |
|       from small pool |     231    |     274    |    4467 K  |    4467 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    9346 K  |    9346 K  |
|       from large pool |      24    |      65    |    4878 K  |    4878 K  |
|       from small pool |     231    |     274    |    4467 K  |    4467 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4618 K  |    4618 K  |
|       from large pool |       9    |      14    |    2361 K  |    2361 K  |
|       from small pool |      12    |      16    |    2257 K  |    2257 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 66, Average loss: 0.0110, Accuracy: 0.6575, Time consumed:3.46s

Training Epoch: 67 [128/50000]	Loss: 0.4219	LR: 0.025000
Training Epoch: 67 [256/50000]	Loss: 0.5949	LR: 0.025000
Training Epoch: 67 [384/50000]	Loss: 0.5330	LR: 0.025000
Training Epoch: 67 [512/50000]	Loss: 0.5052	LR: 0.025000
Training Epoch: 67 [640/50000]	Loss: 0.4618	LR: 0.025000
Training Epoch: 67 [768/50000]	Loss: 0.4689	LR: 0.025000
Training Epoch: 67 [896/50000]	Loss: 0.5067	LR: 0.025000
Training Epoch: 67 [1024/50000]	Loss: 0.3817	LR: 0.025000
Training Epoch: 67 [1152/50000]	Loss: 0.4748	LR: 0.025000
Training Epoch: 67 [1280/50000]	Loss: 0.3883	LR: 0.025000
Training Epoch: 67 [1408/50000]	Loss: 0.3615	LR: 0.025000
Training Epoch: 67 [1536/50000]	Loss: 0.4428	LR: 0.025000
Training Epoch: 67 [1664/50000]	Loss: 0.4152	LR: 0.025000
Training Epoch: 67 [1792/50000]	Loss: 0.5250	LR: 0.025000
Training Epoch: 67 [1920/50000]	Loss: 0.4418	LR: 0.025000
Training Epoch: 67 [2048/50000]	Loss: 0.5087	LR: 0.025000
Training Epoch: 67 [2176/50000]	Loss: 0.5300	LR: 0.025000
Training Epoch: 67 [2304/50000]	Loss: 0.4142	LR: 0.025000
Training Epoch: 67 [2432/50000]	Loss: 0.4400	LR: 0.025000
Training Epoch: 67 [2560/50000]	Loss: 0.4459	LR: 0.025000
Training Epoch: 67 [2688/50000]	Loss: 0.4172	LR: 0.025000
Training Epoch: 67 [2816/50000]	Loss: 0.4022	LR: 0.025000
Training Epoch: 67 [2944/50000]	Loss: 0.3986	LR: 0.025000
Training Epoch: 67 [3072/50000]	Loss: 0.3368	LR: 0.025000
Training Epoch: 67 [3200/50000]	Loss: 0.5616	LR: 0.025000
Training Epoch: 67 [3328/50000]	Loss: 0.3894	LR: 0.025000
Training Epoch: 67 [3456/50000]	Loss: 0.3679	LR: 0.025000
Training Epoch: 67 [3584/50000]	Loss: 0.4684	LR: 0.025000
Training Epoch: 67 [3712/50000]	Loss: 0.4081	LR: 0.025000
Training Epoch: 67 [3840/50000]	Loss: 0.4961	LR: 0.025000
Training Epoch: 67 [3968/50000]	Loss: 0.4958	LR: 0.025000
Training Epoch: 67 [4096/50000]	Loss: 0.5831	LR: 0.025000
Training Epoch: 67 [4224/50000]	Loss: 0.3599	LR: 0.025000
Training Epoch: 67 [4352/50000]	Loss: 0.4263	LR: 0.025000
Training Epoch: 67 [4480/50000]	Loss: 0.4322	LR: 0.025000
Training Epoch: 67 [4608/50000]	Loss: 0.5942	LR: 0.025000
Training Epoch: 67 [4736/50000]	Loss: 0.4435	LR: 0.025000
Training Epoch: 67 [4864/50000]	Loss: 0.4555	LR: 0.025000
Training Epoch: 67 [4992/50000]	Loss: 0.5072	LR: 0.025000
Training Epoch: 67 [5120/50000]	Loss: 0.4669	LR: 0.025000
Training Epoch: 67 [5248/50000]	Loss: 0.4547	LR: 0.025000
Training Epoch: 67 [5376/50000]	Loss: 0.4053	LR: 0.025000
Training Epoch: 67 [5504/50000]	Loss: 0.4353	LR: 0.025000
Training Epoch: 67 [5632/50000]	Loss: 0.3920	LR: 0.025000
Training Epoch: 67 [5760/50000]	Loss: 0.3777	LR: 0.025000
Training Epoch: 67 [5888/50000]	Loss: 0.4793	LR: 0.025000
Training Epoch: 67 [6016/50000]	Loss: 0.3566	LR: 0.025000
Training Epoch: 67 [6144/50000]	Loss: 0.4353	LR: 0.025000
Training Epoch: 67 [6272/50000]	Loss: 0.4550	LR: 0.025000
Training Epoch: 67 [6400/50000]	Loss: 0.4766	LR: 0.025000
Training Epoch: 67 [6528/50000]	Loss: 0.3562	LR: 0.025000
Training Epoch: 67 [6656/50000]	Loss: 0.5698	LR: 0.025000
Training Epoch: 67 [6784/50000]	Loss: 0.2617	LR: 0.025000
Training Epoch: 67 [6912/50000]	Loss: 0.4476	LR: 0.025000
Training Epoch: 67 [7040/50000]	Loss: 0.4241	LR: 0.025000
Training Epoch: 67 [7168/50000]	Loss: 0.4028	LR: 0.025000
Training Epoch: 67 [7296/50000]	Loss: 0.3678	LR: 0.025000
Training Epoch: 67 [7424/50000]	Loss: 0.4621	LR: 0.025000
Training Epoch: 67 [7552/50000]	Loss: 0.5729	LR: 0.025000
Training Epoch: 67 [7680/50000]	Loss: 0.4295	LR: 0.025000
Training Epoch: 67 [7808/50000]	Loss: 0.4706	LR: 0.025000
Training Epoch: 67 [7936/50000]	Loss: 0.5170	LR: 0.025000
Training Epoch: 67 [8064/50000]	Loss: 0.3783	LR: 0.025000
Training Epoch: 67 [8192/50000]	Loss: 0.5263	LR: 0.025000
Training Epoch: 67 [8320/50000]	Loss: 0.3155	LR: 0.025000
Training Epoch: 67 [8448/50000]	Loss: 0.3826	LR: 0.025000
Training Epoch: 67 [8576/50000]	Loss: 0.3902	LR: 0.025000
Training Epoch: 67 [8704/50000]	Loss: 0.4293	LR: 0.025000
Training Epoch: 67 [8832/50000]	Loss: 0.3510	LR: 0.025000
Training Epoch: 67 [8960/50000]	Loss: 0.3972	LR: 0.025000
Training Epoch: 67 [9088/50000]	Loss: 0.4860	LR: 0.025000
Training Epoch: 67 [9216/50000]	Loss: 0.5182	LR: 0.025000
Training Epoch: 67 [9344/50000]	Loss: 0.5077	LR: 0.025000
Training Epoch: 67 [9472/50000]	Loss: 0.3677	LR: 0.025000
Training Epoch: 67 [9600/50000]	Loss: 0.4029	LR: 0.025000
Training Epoch: 67 [9728/50000]	Loss: 0.3704	LR: 0.025000
Training Epoch: 67 [9856/50000]	Loss: 0.4708	LR: 0.025000
Training Epoch: 67 [9984/50000]	Loss: 0.5534	LR: 0.025000
Training Epoch: 67 [10112/50000]	Loss: 0.4711	LR: 0.025000
Training Epoch: 67 [10240/50000]	Loss: 0.4095	LR: 0.025000
Training Epoch: 67 [10368/50000]	Loss: 0.4206	LR: 0.025000
Training Epoch: 67 [10496/50000]	Loss: 0.3432	LR: 0.025000
Training Epoch: 67 [10624/50000]	Loss: 0.5068	LR: 0.025000
Training Epoch: 67 [10752/50000]	Loss: 0.4030	LR: 0.025000
Training Epoch: 67 [10880/50000]	Loss: 0.5815	LR: 0.025000
Training Epoch: 67 [11008/50000]	Loss: 0.3945	LR: 0.025000
Training Epoch: 67 [11136/50000]	Loss: 0.4283	LR: 0.025000
Training Epoch: 67 [11264/50000]	Loss: 0.4039	LR: 0.025000
Training Epoch: 67 [11392/50000]	Loss: 0.3738	LR: 0.025000
Training Epoch: 67 [11520/50000]	Loss: 0.4276	LR: 0.025000
Training Epoch: 67 [11648/50000]	Loss: 0.5388	LR: 0.025000
Training Epoch: 67 [11776/50000]	Loss: 0.3425	LR: 0.025000
Training Epoch: 67 [11904/50000]	Loss: 0.5317	LR: 0.025000
Training Epoch: 67 [12032/50000]	Loss: 0.3827	LR: 0.025000
Training Epoch: 67 [12160/50000]	Loss: 0.3636	LR: 0.025000
Training Epoch: 67 [12288/50000]	Loss: 0.4899	LR: 0.025000
Training Epoch: 67 [12416/50000]	Loss: 0.4560	LR: 0.025000
Training Epoch: 67 [12544/50000]	Loss: 0.4071	LR: 0.025000
Training Epoch: 67 [12672/50000]	Loss: 0.4046	LR: 0.025000
Training Epoch: 67 [12800/50000]	Loss: 0.2985	LR: 0.025000
Training Epoch: 67 [12928/50000]	Loss: 0.6017	LR: 0.025000
Training Epoch: 67 [13056/50000]	Loss: 0.5571	LR: 0.025000
Training Epoch: 67 [13184/50000]	Loss: 0.4008	LR: 0.025000
Training Epoch: 67 [13312/50000]	Loss: 0.4537	LR: 0.025000
Training Epoch: 67 [13440/50000]	Loss: 0.4097	LR: 0.025000
Training Epoch: 67 [13568/50000]	Loss: 0.4436	LR: 0.025000
Training Epoch: 67 [13696/50000]	Loss: 0.4269	LR: 0.025000
Training Epoch: 67 [13824/50000]	Loss: 0.4026	LR: 0.025000
Training Epoch: 67 [13952/50000]	Loss: 0.4428	LR: 0.025000
Training Epoch: 67 [14080/50000]	Loss: 0.4467	LR: 0.025000
Training Epoch: 67 [14208/50000]	Loss: 0.4656	LR: 0.025000
Training Epoch: 67 [14336/50000]	Loss: 0.4419	LR: 0.025000
Training Epoch: 67 [14464/50000]	Loss: 0.6053	LR: 0.025000
Training Epoch: 67 [14592/50000]	Loss: 0.3365	LR: 0.025000
Training Epoch: 67 [14720/50000]	Loss: 0.4437	LR: 0.025000
Training Epoch: 67 [14848/50000]	Loss: 0.5074	LR: 0.025000
Training Epoch: 67 [14976/50000]	Loss: 0.3940	LR: 0.025000
Training Epoch: 67 [15104/50000]	Loss: 0.3522	LR: 0.025000
Training Epoch: 67 [15232/50000]	Loss: 0.4408	LR: 0.025000
Training Epoch: 67 [15360/50000]	Loss: 0.4472	LR: 0.025000
Training Epoch: 67 [15488/50000]	Loss: 0.3679	LR: 0.025000
Training Epoch: 67 [15616/50000]	Loss: 0.4461	LR: 0.025000
Training Epoch: 67 [15744/50000]	Loss: 0.4347	LR: 0.025000
Training Epoch: 67 [15872/50000]	Loss: 0.4710	LR: 0.025000
Training Epoch: 67 [16000/50000]	Loss: 0.3682	LR: 0.025000
Training Epoch: 67 [16128/50000]	Loss: 0.3688	LR: 0.025000
Training Epoch: 67 [16256/50000]	Loss: 0.5279	LR: 0.025000
Training Epoch: 67 [16384/50000]	Loss: 0.5884	LR: 0.025000
Training Epoch: 67 [16512/50000]	Loss: 0.5362	LR: 0.025000
Training Epoch: 67 [16640/50000]	Loss: 0.6989	LR: 0.025000
Training Epoch: 67 [16768/50000]	Loss: 0.4460	LR: 0.025000
Training Epoch: 67 [16896/50000]	Loss: 0.3992	LR: 0.025000
Training Epoch: 67 [17024/50000]	Loss: 0.6707	LR: 0.025000
Training Epoch: 67 [17152/50000]	Loss: 0.4190	LR: 0.025000
Training Epoch: 67 [17280/50000]	Loss: 0.3606	LR: 0.025000
Training Epoch: 67 [17408/50000]	Loss: 0.3669	LR: 0.025000
Training Epoch: 67 [17536/50000]	Loss: 0.5533	LR: 0.025000
Training Epoch: 67 [17664/50000]	Loss: 0.4821	LR: 0.025000
Training Epoch: 67 [17792/50000]	Loss: 0.3360	LR: 0.025000
Training Epoch: 67 [17920/50000]	Loss: 0.3753	LR: 0.025000
Training Epoch: 67 [18048/50000]	Loss: 0.3830	LR: 0.025000
Training Epoch: 67 [18176/50000]	Loss: 0.3733	LR: 0.025000
Training Epoch: 67 [18304/50000]	Loss: 0.5090	LR: 0.025000
Training Epoch: 67 [18432/50000]	Loss: 0.4396	LR: 0.025000
Training Epoch: 67 [18560/50000]	Loss: 0.4316	LR: 0.025000
Training Epoch: 67 [18688/50000]	Loss: 0.5773	LR: 0.025000
Training Epoch: 67 [18816/50000]	Loss: 0.3843	LR: 0.025000
Training Epoch: 67 [18944/50000]	Loss: 0.4118	LR: 0.025000
Training Epoch: 67 [19072/50000]	Loss: 0.5779	LR: 0.025000
Training Epoch: 67 [19200/50000]	Loss: 0.3339	LR: 0.025000
Training Epoch: 67 [19328/50000]	Loss: 0.3201	LR: 0.025000
Training Epoch: 67 [19456/50000]	Loss: 0.4883	LR: 0.025000
Training Epoch: 67 [19584/50000]	Loss: 0.5164	LR: 0.025000
Training Epoch: 67 [19712/50000]	Loss: 0.4163	LR: 0.025000
Training Epoch: 67 [19840/50000]	Loss: 0.3468	LR: 0.025000
Training Epoch: 67 [19968/50000]	Loss: 0.4960	LR: 0.025000
Training Epoch: 67 [20096/50000]	Loss: 0.3114	LR: 0.025000
Training Epoch: 67 [20224/50000]	Loss: 0.3477	LR: 0.025000
Training Epoch: 67 [20352/50000]	Loss: 0.5817	LR: 0.025000
Training Epoch: 67 [20480/50000]	Loss: 0.4842	LR: 0.025000
Training Epoch: 67 [20608/50000]	Loss: 0.4054	LR: 0.025000
Training Epoch: 67 [20736/50000]	Loss: 0.4222	LR: 0.025000
Training Epoch: 67 [20864/50000]	Loss: 0.4859	LR: 0.025000
Training Epoch: 67 [20992/50000]	Loss: 0.3965	LR: 0.025000
Training Epoch: 67 [21120/50000]	Loss: 0.2980	LR: 0.025000
Training Epoch: 67 [21248/50000]	Loss: 0.2664	LR: 0.025000
Training Epoch: 67 [21376/50000]	Loss: 0.4577	LR: 0.025000
Training Epoch: 67 [21504/50000]	Loss: 0.5341	LR: 0.025000
Training Epoch: 67 [21632/50000]	Loss: 0.5915	LR: 0.025000
Training Epoch: 67 [21760/50000]	Loss: 0.4056	LR: 0.025000
Training Epoch: 67 [21888/50000]	Loss: 0.4558	LR: 0.025000
Training Epoch: 67 [22016/50000]	Loss: 0.5390	LR: 0.025000
Training Epoch: 67 [22144/50000]	Loss: 0.4198	LR: 0.025000
Training Epoch: 67 [22272/50000]	Loss: 0.3772	LR: 0.025000
Training Epoch: 67 [22400/50000]	Loss: 0.4999	LR: 0.025000
Training Epoch: 67 [22528/50000]	Loss: 0.5001	LR: 0.025000
Training Epoch: 67 [22656/50000]	Loss: 0.4348	LR: 0.025000
Training Epoch: 67 [22784/50000]	Loss: 0.4334	LR: 0.025000
Training Epoch: 67 [22912/50000]	Loss: 0.4181	LR: 0.025000
Training Epoch: 67 [23040/50000]	Loss: 0.4983	LR: 0.025000
Training Epoch: 67 [23168/50000]	Loss: 0.5478	LR: 0.025000
Training Epoch: 67 [23296/50000]	Loss: 0.4311	LR: 0.025000
Training Epoch: 67 [23424/50000]	Loss: 0.5147	LR: 0.025000
Training Epoch: 67 [23552/50000]	Loss: 0.4150	LR: 0.025000
Training Epoch: 67 [23680/50000]	Loss: 0.3837	LR: 0.025000
Training Epoch: 67 [23808/50000]	Loss: 0.5759	LR: 0.025000
Training Epoch: 67 [23936/50000]	Loss: 0.3618	LR: 0.025000
Training Epoch: 67 [24064/50000]	Loss: 0.4441	LR: 0.025000
Training Epoch: 67 [24192/50000]	Loss: 0.5411	LR: 0.025000
Training Epoch: 67 [24320/50000]	Loss: 0.5075	LR: 0.025000
Training Epoch: 67 [24448/50000]	Loss: 0.4663	LR: 0.025000
Training Epoch: 67 [24576/50000]	Loss: 0.4758	LR: 0.025000
Training Epoch: 67 [24704/50000]	Loss: 0.6529	LR: 0.025000
Training Epoch: 67 [24832/50000]	Loss: 0.4506	LR: 0.025000
Training Epoch: 67 [24960/50000]	Loss: 0.3281	LR: 0.025000
Training Epoch: 67 [25088/50000]	Loss: 0.3765	LR: 0.025000
Training Epoch: 67 [25216/50000]	Loss: 0.4870	LR: 0.025000
Training Epoch: 67 [25344/50000]	Loss: 0.3699	LR: 0.025000
Training Epoch: 67 [25472/50000]	Loss: 0.4101	LR: 0.025000
Training Epoch: 67 [25600/50000]	Loss: 0.3100	LR: 0.025000
Training Epoch: 67 [25728/50000]	Loss: 0.4311	LR: 0.025000
Training Epoch: 67 [25856/50000]	Loss: 0.5260	LR: 0.025000
Training Epoch: 67 [25984/50000]	Loss: 0.5218	LR: 0.025000
Training Epoch: 67 [26112/50000]	Loss: 0.5121	LR: 0.025000
Training Epoch: 67 [26240/50000]	Loss: 0.5339	LR: 0.025000
Training Epoch: 67 [26368/50000]	Loss: 0.4855	LR: 0.025000
Training Epoch: 67 [26496/50000]	Loss: 0.5010	LR: 0.025000
Training Epoch: 67 [26624/50000]	Loss: 0.4433	LR: 0.025000
Training Epoch: 67 [26752/50000]	Loss: 0.4151	LR: 0.025000
Training Epoch: 67 [26880/50000]	Loss: 0.5564	LR: 0.025000
Training Epoch: 67 [27008/50000]	Loss: 0.5347	LR: 0.025000
Training Epoch: 67 [27136/50000]	Loss: 0.4491	LR: 0.025000
Training Epoch: 67 [27264/50000]	Loss: 0.4134	LR: 0.025000
Training Epoch: 67 [27392/50000]	Loss: 0.4421	LR: 0.025000
Training Epoch: 67 [27520/50000]	Loss: 0.4561	LR: 0.025000
Training Epoch: 67 [27648/50000]	Loss: 0.4638	LR: 0.025000
Training Epoch: 67 [27776/50000]	Loss: 0.3694	LR: 0.025000
Training Epoch: 67 [27904/50000]	Loss: 0.4565	LR: 0.025000
Training Epoch: 67 [28032/50000]	Loss: 0.5800	LR: 0.025000
Training Epoch: 67 [28160/50000]	Loss: 0.4178	LR: 0.025000
Training Epoch: 67 [28288/50000]	Loss: 0.3216	LR: 0.025000
Training Epoch: 67 [28416/50000]	Loss: 0.5752	LR: 0.025000
Training Epoch: 67 [28544/50000]	Loss: 0.4422	LR: 0.025000
Training Epoch: 67 [28672/50000]	Loss: 0.5107	LR: 0.025000
Training Epoch: 67 [28800/50000]	Loss: 0.4160	LR: 0.025000
Training Epoch: 67 [28928/50000]	Loss: 0.4412	LR: 0.025000
Training Epoch: 67 [29056/50000]	Loss: 0.5276	LR: 0.025000
Training Epoch: 67 [29184/50000]	Loss: 0.4862	LR: 0.025000
Training Epoch: 67 [29312/50000]	Loss: 0.5362	LR: 0.025000
Training Epoch: 67 [29440/50000]	Loss: 0.6810	LR: 0.025000
Training Epoch: 67 [29568/50000]	Loss: 0.5749	LR: 0.025000
Training Epoch: 67 [29696/50000]	Loss: 0.4671	LR: 0.025000
Training Epoch: 67 [29824/50000]	Loss: 0.5988	LR: 0.025000
Training Epoch: 67 [29952/50000]	Loss: 0.4152	LR: 0.025000
Training Epoch: 67 [30080/50000]	Loss: 0.5786	LR: 0.025000
Training Epoch: 67 [30208/50000]	Loss: 0.6179	LR: 0.025000
Training Epoch: 67 [30336/50000]	Loss: 0.5830	LR: 0.025000
Training Epoch: 67 [30464/50000]	Loss: 0.4434	LR: 0.025000
Training Epoch: 67 [30592/50000]	Loss: 0.4175	LR: 0.025000
Training Epoch: 67 [30720/50000]	Loss: 0.4586	LR: 0.025000
Training Epoch: 67 [30848/50000]	Loss: 0.5376	LR: 0.025000
Training Epoch: 67 [30976/50000]	Loss: 0.6478	LR: 0.025000
Training Epoch: 67 [31104/50000]	Loss: 0.4520	LR: 0.025000
Training Epoch: 67 [31232/50000]	Loss: 0.5189	LR: 0.025000
Training Epoch: 67 [31360/50000]	Loss: 0.6236	LR: 0.025000
Training Epoch: 67 [31488/50000]	Loss: 0.5491	LR: 0.025000
Training Epoch: 67 [31616/50000]	Loss: 0.5315	LR: 0.025000
Training Epoch: 67 [31744/50000]	Loss: 0.5149	LR: 0.025000
Training Epoch: 67 [31872/50000]	Loss: 0.5120	LR: 0.025000
Training Epoch: 67 [32000/50000]	Loss: 0.4387	LR: 0.025000
Training Epoch: 67 [32128/50000]	Loss: 0.4927	LR: 0.025000
Training Epoch: 67 [32256/50000]	Loss: 0.6916	LR: 0.025000
Training Epoch: 67 [32384/50000]	Loss: 0.5268	LR: 0.025000
Training Epoch: 67 [32512/50000]	Loss: 0.5897	LR: 0.025000
Training Epoch: 67 [32640/50000]	Loss: 0.4365	LR: 0.025000
Training Epoch: 67 [32768/50000]	Loss: 0.5540	LR: 0.025000
Training Epoch: 67 [32896/50000]	Loss: 0.5231	LR: 0.025000
Training Epoch: 67 [33024/50000]	Loss: 0.4210	LR: 0.025000
Training Epoch: 67 [33152/50000]	Loss: 0.5151	LR: 0.025000
Training Epoch: 67 [33280/50000]	Loss: 0.5076	LR: 0.025000
Training Epoch: 67 [33408/50000]	Loss: 0.6158	LR: 0.025000
Training Epoch: 67 [33536/50000]	Loss: 0.5386	LR: 0.025000
Training Epoch: 67 [33664/50000]	Loss: 0.4661	LR: 0.025000
Training Epoch: 67 [33792/50000]	Loss: 0.4620	LR: 0.025000
Training Epoch: 67 [33920/50000]	Loss: 0.6982	LR: 0.025000
Training Epoch: 67 [34048/50000]	Loss: 0.5098	LR: 0.025000
Training Epoch: 67 [34176/50000]	Loss: 0.4986	LR: 0.025000
Training Epoch: 67 [34304/50000]	Loss: 0.4561	LR: 0.025000
Training Epoch: 67 [34432/50000]	Loss: 0.4685	LR: 0.025000
Training Epoch: 67 [34560/50000]	Loss: 0.3615	LR: 0.025000
Training Epoch: 67 [34688/50000]	Loss: 0.5706	LR: 0.025000
Training Epoch: 67 [34816/50000]	Loss: 0.6931	LR: 0.025000
Training Epoch: 67 [34944/50000]	Loss: 0.6501	LR: 0.025000
Training Epoch: 67 [35072/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 67 [35200/50000]	Loss: 0.5261	LR: 0.025000
Training Epoch: 67 [35328/50000]	Loss: 0.3326	LR: 0.025000
Training Epoch: 67 [35456/50000]	Loss: 0.5613	LR: 0.025000
Training Epoch: 67 [35584/50000]	Loss: 0.5343	LR: 0.025000
Training Epoch: 67 [35712/50000]	Loss: 0.5912	LR: 0.025000
Training Epoch: 67 [35840/50000]	Loss: 0.5996	LR: 0.025000
Training Epoch: 67 [35968/50000]	Loss: 0.4133	LR: 0.025000
Training Epoch: 67 [36096/50000]	Loss: 0.3905	LR: 0.025000
Training Epoch: 67 [36224/50000]	Loss: 0.5223	LR: 0.025000
Training Epoch: 67 [36352/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 67 [36480/50000]	Loss: 0.5512	LR: 0.025000
Training Epoch: 67 [36608/50000]	Loss: 0.5011	LR: 0.025000
Training Epoch: 67 [36736/50000]	Loss: 0.5633	LR: 0.025000
Training Epoch: 67 [36864/50000]	Loss: 0.5243	LR: 0.025000
Training Epoch: 67 [36992/50000]	Loss: 0.3857	LR: 0.025000
Training Epoch: 67 [37120/50000]	Loss: 0.4405	LR: 0.025000
Training Epoch: 67 [37248/50000]	Loss: 0.5243	LR: 0.025000
Training Epoch: 67 [37376/50000]	Loss: 0.3897	LR: 0.025000
Training Epoch: 67 [37504/50000]	Loss: 0.6209	LR: 0.025000
Training Epoch: 67 [37632/50000]	Loss: 0.5167	LR: 0.025000
Training Epoch: 67 [37760/50000]	Loss: 0.4673	LR: 0.025000
Training Epoch: 67 [37888/50000]	Loss: 0.7398	LR: 0.025000
Training Epoch: 67 [38016/50000]	Loss: 0.4209	LR: 0.025000
Training Epoch: 67 [38144/50000]	Loss: 0.4931	LR: 0.025000
Training Epoch: 67 [38272/50000]	Loss: 0.4979	LR: 0.025000
Training Epoch: 67 [38400/50000]	Loss: 0.4838	LR: 0.025000
Training Epoch: 67 [38528/50000]	Loss: 0.4036	LR: 0.025000
Training Epoch: 67 [38656/50000]	Loss: 0.4861	LR: 0.025000
Training Epoch: 67 [38784/50000]	Loss: 0.4141	LR: 0.025000
Training Epoch: 67 [38912/50000]	Loss: 0.6486	LR: 0.025000
Training Epoch: 67 [39040/50000]	Loss: 0.4795	LR: 0.025000
Training Epoch: 67 [39168/50000]	Loss: 0.6367	LR: 0.025000
Training Epoch: 67 [39296/50000]	Loss: 0.5307	LR: 0.025000
Training Epoch: 67 [39424/50000]	Loss: 0.4501	LR: 0.025000
Training Epoch: 67 [39552/50000]	Loss: 0.5802	LR: 0.025000
Training Epoch: 67 [39680/50000]	Loss: 0.4435	LR: 0.025000
Training Epoch: 67 [39808/50000]	Loss: 0.4744	LR: 0.025000
Training Epoch: 67 [39936/50000]	Loss: 0.7120	LR: 0.025000
Training Epoch: 67 [40064/50000]	Loss: 0.5404	LR: 0.025000
Training Epoch: 67 [40192/50000]	Loss: 0.5006	LR: 0.025000
Training Epoch: 67 [40320/50000]	Loss: 0.5822	LR: 0.025000
Training Epoch: 67 [40448/50000]	Loss: 0.4965	LR: 0.025000
Training Epoch: 67 [40576/50000]	Loss: 0.4497	LR: 0.025000
Training Epoch: 67 [40704/50000]	Loss: 0.3539	LR: 0.025000
Training Epoch: 67 [40832/50000]	Loss: 0.3466	LR: 0.025000
Training Epoch: 67 [40960/50000]	Loss: 0.5843	LR: 0.025000
Training Epoch: 67 [41088/50000]	Loss: 0.6392	LR: 0.025000
Training Epoch: 67 [41216/50000]	Loss: 0.4854	LR: 0.025000
Training Epoch: 67 [41344/50000]	Loss: 0.6014	LR: 0.025000
Training Epoch: 67 [41472/50000]	Loss: 0.6101	LR: 0.025000
Training Epoch: 67 [41600/50000]	Loss: 0.5125	LR: 0.025000
Training Epoch: 67 [41728/50000]	Loss: 0.4908	LR: 0.025000
Training Epoch: 67 [41856/50000]	Loss: 0.4121	LR: 0.025000
Training Epoch: 67 [41984/50000]	Loss: 0.5073	LR: 0.025000
Training Epoch: 67 [42112/50000]	Loss: 0.6576	LR: 0.025000
Training Epoch: 67 [42240/50000]	Loss: 0.5932	LR: 0.025000
Training Epoch: 67 [42368/50000]	Loss: 0.3943	LR: 0.025000
Training Epoch: 67 [42496/50000]	Loss: 0.5081	LR: 0.025000
Training Epoch: 67 [42624/50000]	Loss: 0.4897	LR: 0.025000
Training Epoch: 67 [42752/50000]	Loss: 0.4991	LR: 0.025000
Training Epoch: 67 [42880/50000]	Loss: 0.4413	LR: 0.025000
Training Epoch: 67 [43008/50000]	Loss: 0.5434	LR: 0.025000
Training Epoch: 67 [43136/50000]	Loss: 0.4134	LR: 0.025000
Training Epoch: 67 [43264/50000]	Loss: 0.5044	LR: 0.025000
Training Epoch: 67 [43392/50000]	Loss: 0.3545	LR: 0.025000
Training Epoch: 67 [43520/50000]	Loss: 0.4628	LR: 0.025000
Training Epoch: 67 [43648/50000]	Loss: 0.4885	LR: 0.025000
Training Epoch: 67 [43776/50000]	Loss: 0.6610	LR: 0.025000
Training Epoch: 67 [43904/50000]	Loss: 0.5074	LR: 0.025000
Training Epoch: 67 [44032/50000]	Loss: 0.5444	LR: 0.025000
Training Epoch: 67 [44160/50000]	Loss: 0.4860	LR: 0.025000
Training Epoch: 67 [44288/50000]	Loss: 0.5533	LR: 0.025000
Training Epoch: 67 [44416/50000]	Loss: 0.6939	LR: 0.025000
Training Epoch: 67 [44544/50000]	Loss: 0.3632	LR: 0.025000
Training Epoch: 67 [44672/50000]	Loss: 0.4702	LR: 0.025000
Training Epoch: 67 [44800/50000]	Loss: 0.5074	LR: 0.025000
Training Epoch: 67 [44928/50000]	Loss: 0.4529	LR: 0.025000
Training Epoch: 67 [45056/50000]	Loss: 0.5716	LR: 0.025000
Training Epoch: 67 [45184/50000]	Loss: 0.4596	LR: 0.025000
Training Epoch: 67 [45312/50000]	Loss: 0.4816	LR: 0.025000
Training Epoch: 67 [45440/50000]	Loss: 0.5723	LR: 0.025000
Training Epoch: 67 [45568/50000]	Loss: 0.5253	LR: 0.025000
Training Epoch: 67 [45696/50000]	Loss: 0.5115	LR: 0.025000
Training Epoch: 67 [45824/50000]	Loss: 0.5783	LR: 0.025000
Training Epoch: 67 [45952/50000]	Loss: 0.3876	LR: 0.025000
Training Epoch: 67 [46080/50000]	Loss: 0.5655	LR: 0.025000
Training Epoch: 67 [46208/50000]	Loss: 0.5733	LR: 0.025000
Training Epoch: 67 [46336/50000]	Loss: 0.5861	LR: 0.025000
Training Epoch: 67 [46464/50000]	Loss: 0.5192	LR: 0.025000
Training Epoch: 67 [46592/50000]	Loss: 0.3986	LR: 0.025000
Training Epoch: 67 [46720/50000]	Loss: 0.5391	LR: 0.025000
Training Epoch: 67 [46848/50000]	Loss: 0.4740	LR: 0.025000
Training Epoch: 67 [46976/50000]	Loss: 0.5774	LR: 0.025000
Training Epoch: 67 [47104/50000]	Loss: 0.4564	LR: 0.025000
Training Epoch: 67 [47232/50000]	Loss: 0.6048	LR: 0.025000
Training Epoch: 67 [47360/50000]	Loss: 0.7164	LR: 0.025000
Training Epoch: 67 [47488/50000]	Loss: 0.5407	LR: 0.025000
Training Epoch: 67 [47616/50000]	Loss: 0.5914	LR: 0.025000
Training Epoch: 67 [47744/50000]	Loss: 0.5487	LR: 0.025000
Training Epoch: 67 [47872/50000]	Loss: 0.6133	LR: 0.025000
Training Epoch: 67 [48000/50000]	Loss: 0.5404	LR: 0.025000
Training Epoch: 67 [48128/50000]	Loss: 0.4646	LR: 0.025000
Training Epoch: 67 [48256/50000]	Loss: 0.6034	LR: 0.025000
Training Epoch: 67 [48384/50000]	Loss: 0.4359	LR: 0.025000
Training Epoch: 67 [48512/50000]	Loss: 0.5063	LR: 0.025000
Training Epoch: 67 [48640/50000]	Loss: 0.6773	LR: 0.025000
Training Epoch: 67 [48768/50000]	Loss: 0.4491	LR: 0.025000
Training Epoch: 67 [48896/50000]	Loss: 0.5962	LR: 0.025000
Training Epoch: 67 [49024/50000]	Loss: 0.4333	LR: 0.025000
Training Epoch: 67 [49152/50000]	Loss: 0.6102	LR: 0.025000
Training Epoch: 67 [49280/50000]	Loss: 0.5726	LR: 0.025000
Training Epoch: 67 [49408/50000]	Loss: 0.6053	LR: 0.025000
Training Epoch: 67 [49536/50000]	Loss: 0.4083	LR: 0.025000
Training Epoch: 67 [49664/50000]	Loss: 0.4791	LR: 0.025000
Training Epoch: 67 [49792/50000]	Loss: 0.5332	LR: 0.025000
Training Epoch: 67 [49920/50000]	Loss: 0.4086	LR: 0.025000
Training Epoch: 67 [50000/50000]	Loss: 0.5322	LR: 0.025000
epoch 67 training time consumed: 53.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  245894 GB |  245894 GB |
|       from large pool |  123392 KB |    1034 MB |  245651 GB |  245651 GB |
|       from small pool |   10798 KB |      13 MB |     242 GB |     242 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  245894 GB |  245894 GB |
|       from large pool |  123392 KB |    1034 MB |  245651 GB |  245651 GB |
|       from small pool |   10798 KB |      13 MB |     242 GB |     242 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  108208 GB |  108208 GB |
|       from large pool |  155136 KB |  433088 KB |  107940 GB |  107940 GB |
|       from small pool |    1490 KB |    3494 KB |     267 GB |     267 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    9488 K  |    9488 K  |
|       from large pool |      24    |      65    |    4952 K  |    4952 K  |
|       from small pool |     231    |     274    |    4535 K  |    4535 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    9488 K  |    9488 K  |
|       from large pool |      24    |      65    |    4952 K  |    4952 K  |
|       from small pool |     231    |     274    |    4535 K  |    4535 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4688 K  |    4688 K  |
|       from large pool |       9    |      14    |    2397 K  |    2397 K  |
|       from small pool |      12    |      16    |    2291 K  |    2291 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 67, Average loss: 0.0110, Accuracy: 0.6621, Time consumed:3.47s

Training Epoch: 68 [128/50000]	Loss: 0.5337	LR: 0.025000
Training Epoch: 68 [256/50000]	Loss: 0.5524	LR: 0.025000
Training Epoch: 68 [384/50000]	Loss: 0.3566	LR: 0.025000
Training Epoch: 68 [512/50000]	Loss: 0.4274	LR: 0.025000
Training Epoch: 68 [640/50000]	Loss: 0.5490	LR: 0.025000
Training Epoch: 68 [768/50000]	Loss: 0.4600	LR: 0.025000
Training Epoch: 68 [896/50000]	Loss: 0.4127	LR: 0.025000
Training Epoch: 68 [1024/50000]	Loss: 0.4541	LR: 0.025000
Training Epoch: 68 [1152/50000]	Loss: 0.3713	LR: 0.025000
Training Epoch: 68 [1280/50000]	Loss: 0.4111	LR: 0.025000
Training Epoch: 68 [1408/50000]	Loss: 0.5577	LR: 0.025000
Training Epoch: 68 [1536/50000]	Loss: 0.4495	LR: 0.025000
Training Epoch: 68 [1664/50000]	Loss: 0.4436	LR: 0.025000
Training Epoch: 68 [1792/50000]	Loss: 0.4391	LR: 0.025000
Training Epoch: 68 [1920/50000]	Loss: 0.4755	LR: 0.025000
Training Epoch: 68 [2048/50000]	Loss: 0.3005	LR: 0.025000
Training Epoch: 68 [2176/50000]	Loss: 0.4467	LR: 0.025000
Training Epoch: 68 [2304/50000]	Loss: 0.5172	LR: 0.025000
Training Epoch: 68 [2432/50000]	Loss: 0.3663	LR: 0.025000
Training Epoch: 68 [2560/50000]	Loss: 0.5059	LR: 0.025000
Training Epoch: 68 [2688/50000]	Loss: 0.3464	LR: 0.025000
Training Epoch: 68 [2816/50000]	Loss: 0.5169	LR: 0.025000
Training Epoch: 68 [2944/50000]	Loss: 0.4630	LR: 0.025000
Training Epoch: 68 [3072/50000]	Loss: 0.4380	LR: 0.025000
Training Epoch: 68 [3200/50000]	Loss: 0.3654	LR: 0.025000
Training Epoch: 68 [3328/50000]	Loss: 0.4617	LR: 0.025000
Training Epoch: 68 [3456/50000]	Loss: 0.5806	LR: 0.025000
Training Epoch: 68 [3584/50000]	Loss: 0.5095	LR: 0.025000
Training Epoch: 68 [3712/50000]	Loss: 0.4089	LR: 0.025000
Training Epoch: 68 [3840/50000]	Loss: 0.3530	LR: 0.025000
Training Epoch: 68 [3968/50000]	Loss: 0.4003	LR: 0.025000
Training Epoch: 68 [4096/50000]	Loss: 0.3845	LR: 0.025000
Training Epoch: 68 [4224/50000]	Loss: 0.4404	LR: 0.025000
Training Epoch: 68 [4352/50000]	Loss: 0.3584	LR: 0.025000
Training Epoch: 68 [4480/50000]	Loss: 0.2838	LR: 0.025000
Training Epoch: 68 [4608/50000]	Loss: 0.3313	LR: 0.025000
Training Epoch: 68 [4736/50000]	Loss: 0.3485	LR: 0.025000
Training Epoch: 68 [4864/50000]	Loss: 0.3278	LR: 0.025000
Training Epoch: 68 [4992/50000]	Loss: 0.3787	LR: 0.025000
Training Epoch: 68 [5120/50000]	Loss: 0.3976	LR: 0.025000
Training Epoch: 68 [5248/50000]	Loss: 0.4252	LR: 0.025000
Training Epoch: 68 [5376/50000]	Loss: 0.4485	LR: 0.025000
Training Epoch: 68 [5504/50000]	Loss: 0.4629	LR: 0.025000
Training Epoch: 68 [5632/50000]	Loss: 0.3020	LR: 0.025000
Training Epoch: 68 [5760/50000]	Loss: 0.4764	LR: 0.025000
Training Epoch: 68 [5888/50000]	Loss: 0.3057	LR: 0.025000
Training Epoch: 68 [6016/50000]	Loss: 0.4175	LR: 0.025000
Training Epoch: 68 [6144/50000]	Loss: 0.3294	LR: 0.025000
Training Epoch: 68 [6272/50000]	Loss: 0.3285	LR: 0.025000
Training Epoch: 68 [6400/50000]	Loss: 0.3653	LR: 0.025000
Training Epoch: 68 [6528/50000]	Loss: 0.4877	LR: 0.025000
Training Epoch: 68 [6656/50000]	Loss: 0.4440	LR: 0.025000
Training Epoch: 68 [6784/50000]	Loss: 0.3842	LR: 0.025000
Training Epoch: 68 [6912/50000]	Loss: 0.4336	LR: 0.025000
Training Epoch: 68 [7040/50000]	Loss: 0.2507	LR: 0.025000
Training Epoch: 68 [7168/50000]	Loss: 0.5002	LR: 0.025000
Training Epoch: 68 [7296/50000]	Loss: 0.3851	LR: 0.025000
Training Epoch: 68 [7424/50000]	Loss: 0.3872	LR: 0.025000
Training Epoch: 68 [7552/50000]	Loss: 0.5280	LR: 0.025000
Training Epoch: 68 [7680/50000]	Loss: 0.5430	LR: 0.025000
Training Epoch: 68 [7808/50000]	Loss: 0.5678	LR: 0.025000
Training Epoch: 68 [7936/50000]	Loss: 0.4199	LR: 0.025000
Training Epoch: 68 [8064/50000]	Loss: 0.4436	LR: 0.025000
Training Epoch: 68 [8192/50000]	Loss: 0.4749	LR: 0.025000
Training Epoch: 68 [8320/50000]	Loss: 0.3326	LR: 0.025000
Training Epoch: 68 [8448/50000]	Loss: 0.4216	LR: 0.025000
Training Epoch: 68 [8576/50000]	Loss: 0.4426	LR: 0.025000
Training Epoch: 68 [8704/50000]	Loss: 0.3977	LR: 0.025000
Training Epoch: 68 [8832/50000]	Loss: 0.4519	LR: 0.025000
Training Epoch: 68 [8960/50000]	Loss: 0.3249	LR: 0.025000
Training Epoch: 68 [9088/50000]	Loss: 0.4033	LR: 0.025000
Training Epoch: 68 [9216/50000]	Loss: 0.4658	LR: 0.025000
Training Epoch: 68 [9344/50000]	Loss: 0.2741	LR: 0.025000
Training Epoch: 68 [9472/50000]	Loss: 0.4801	LR: 0.025000
Training Epoch: 68 [9600/50000]	Loss: 0.4490	LR: 0.025000
Training Epoch: 68 [9728/50000]	Loss: 0.5829	LR: 0.025000
Training Epoch: 68 [9856/50000]	Loss: 0.3096	LR: 0.025000
Training Epoch: 68 [9984/50000]	Loss: 0.4355	LR: 0.025000
Training Epoch: 68 [10112/50000]	Loss: 0.5336	LR: 0.025000
Training Epoch: 68 [10240/50000]	Loss: 0.4692	LR: 0.025000
Training Epoch: 68 [10368/50000]	Loss: 0.4575	LR: 0.025000
Training Epoch: 68 [10496/50000]	Loss: 0.3943	LR: 0.025000
Training Epoch: 68 [10624/50000]	Loss: 0.2701	LR: 0.025000
Training Epoch: 68 [10752/50000]	Loss: 0.5810	LR: 0.025000
Training Epoch: 68 [10880/50000]	Loss: 0.3519	LR: 0.025000
Training Epoch: 68 [11008/50000]	Loss: 0.3717	LR: 0.025000
Training Epoch: 68 [11136/50000]	Loss: 0.4640	LR: 0.025000
Training Epoch: 68 [11264/50000]	Loss: 0.5900	LR: 0.025000
Training Epoch: 68 [11392/50000]	Loss: 0.4781	LR: 0.025000
Training Epoch: 68 [11520/50000]	Loss: 0.4211	LR: 0.025000
Training Epoch: 68 [11648/50000]	Loss: 0.4643	LR: 0.025000
Training Epoch: 68 [11776/50000]	Loss: 0.5331	LR: 0.025000
Training Epoch: 68 [11904/50000]	Loss: 0.4707	LR: 0.025000
Training Epoch: 68 [12032/50000]	Loss: 0.5742	LR: 0.025000
Training Epoch: 68 [12160/50000]	Loss: 0.3964	LR: 0.025000
Training Epoch: 68 [12288/50000]	Loss: 0.4593	LR: 0.025000
Training Epoch: 68 [12416/50000]	Loss: 0.4003	LR: 0.025000
Training Epoch: 68 [12544/50000]	Loss: 0.5542	LR: 0.025000
Training Epoch: 68 [12672/50000]	Loss: 0.2993	LR: 0.025000
Training Epoch: 68 [12800/50000]	Loss: 0.4136	LR: 0.025000
Training Epoch: 68 [12928/50000]	Loss: 0.4132	LR: 0.025000
Training Epoch: 68 [13056/50000]	Loss: 0.4084	LR: 0.025000
Training Epoch: 68 [13184/50000]	Loss: 0.4333	LR: 0.025000
Training Epoch: 68 [13312/50000]	Loss: 0.4178	LR: 0.025000
Training Epoch: 68 [13440/50000]	Loss: 0.4979	LR: 0.025000
Training Epoch: 68 [13568/50000]	Loss: 0.4349	LR: 0.025000
Training Epoch: 68 [13696/50000]	Loss: 0.4385	LR: 0.025000
Training Epoch: 68 [13824/50000]	Loss: 0.3379	LR: 0.025000
Training Epoch: 68 [13952/50000]	Loss: 0.3945	LR: 0.025000
Training Epoch: 68 [14080/50000]	Loss: 0.3578	LR: 0.025000
Training Epoch: 68 [14208/50000]	Loss: 0.5669	LR: 0.025000
Training Epoch: 68 [14336/50000]	Loss: 0.4224	LR: 0.025000
Training Epoch: 68 [14464/50000]	Loss: 0.3225	LR: 0.025000
Training Epoch: 68 [14592/50000]	Loss: 0.3591	LR: 0.025000
Training Epoch: 68 [14720/50000]	Loss: 0.4743	LR: 0.025000
Training Epoch: 68 [14848/50000]	Loss: 0.5607	LR: 0.025000
Training Epoch: 68 [14976/50000]	Loss: 0.3542	LR: 0.025000
Training Epoch: 68 [15104/50000]	Loss: 0.2706	LR: 0.025000
Training Epoch: 68 [15232/50000]	Loss: 0.4432	LR: 0.025000
Training Epoch: 68 [15360/50000]	Loss: 0.3274	LR: 0.025000
Training Epoch: 68 [15488/50000]	Loss: 0.5003	LR: 0.025000
Training Epoch: 68 [15616/50000]	Loss: 0.4130	LR: 0.025000
Training Epoch: 68 [15744/50000]	Loss: 0.3345	LR: 0.025000
Training Epoch: 68 [15872/50000]	Loss: 0.3964	LR: 0.025000
Training Epoch: 68 [16000/50000]	Loss: 0.4995	LR: 0.025000
Training Epoch: 68 [16128/50000]	Loss: 0.3761	LR: 0.025000
Training Epoch: 68 [16256/50000]	Loss: 0.4635	LR: 0.025000
Training Epoch: 68 [16384/50000]	Loss: 0.4200	LR: 0.025000
Training Epoch: 68 [16512/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 68 [16640/50000]	Loss: 0.4578	LR: 0.025000
Training Epoch: 68 [16768/50000]	Loss: 0.5764	LR: 0.025000
Training Epoch: 68 [16896/50000]	Loss: 0.4712	LR: 0.025000
Training Epoch: 68 [17024/50000]	Loss: 0.3556	LR: 0.025000
Training Epoch: 68 [17152/50000]	Loss: 0.3224	LR: 0.025000
Training Epoch: 68 [17280/50000]	Loss: 0.4201	LR: 0.025000
Training Epoch: 68 [17408/50000]	Loss: 0.3682	LR: 0.025000
Training Epoch: 68 [17536/50000]	Loss: 0.4495	LR: 0.025000
Training Epoch: 68 [17664/50000]	Loss: 0.3234	LR: 0.025000
Training Epoch: 68 [17792/50000]	Loss: 0.4286	LR: 0.025000
Training Epoch: 68 [17920/50000]	Loss: 0.3821	LR: 0.025000
Training Epoch: 68 [18048/50000]	Loss: 0.4598	LR: 0.025000
Training Epoch: 68 [18176/50000]	Loss: 0.6177	LR: 0.025000
Training Epoch: 68 [18304/50000]	Loss: 0.4602	LR: 0.025000
Training Epoch: 68 [18432/50000]	Loss: 0.3626	LR: 0.025000
Training Epoch: 68 [18560/50000]	Loss: 0.4185	LR: 0.025000
Training Epoch: 68 [18688/50000]	Loss: 0.4382	LR: 0.025000
Training Epoch: 68 [18816/50000]	Loss: 0.4034	LR: 0.025000
Training Epoch: 68 [18944/50000]	Loss: 0.3359	LR: 0.025000
Training Epoch: 68 [19072/50000]	Loss: 0.3770	LR: 0.025000
Training Epoch: 68 [19200/50000]	Loss: 0.5166	LR: 0.025000
Training Epoch: 68 [19328/50000]	Loss: 0.4628	LR: 0.025000
Training Epoch: 68 [19456/50000]	Loss: 0.4636	LR: 0.025000
Training Epoch: 68 [19584/50000]	Loss: 0.6778	LR: 0.025000
Training Epoch: 68 [19712/50000]	Loss: 0.3153	LR: 0.025000
Training Epoch: 68 [19840/50000]	Loss: 0.3865	LR: 0.025000
Training Epoch: 68 [19968/50000]	Loss: 0.4071	LR: 0.025000
Training Epoch: 68 [20096/50000]	Loss: 0.3139	LR: 0.025000
Training Epoch: 68 [20224/50000]	Loss: 0.4413	LR: 0.025000
Training Epoch: 68 [20352/50000]	Loss: 0.4882	LR: 0.025000
Training Epoch: 68 [20480/50000]	Loss: 0.3838	LR: 0.025000
Training Epoch: 68 [20608/50000]	Loss: 0.3021	LR: 0.025000
Training Epoch: 68 [20736/50000]	Loss: 0.4645	LR: 0.025000
Training Epoch: 68 [20864/50000]	Loss: 0.3260	LR: 0.025000
Training Epoch: 68 [20992/50000]	Loss: 0.3519	LR: 0.025000
Training Epoch: 68 [21120/50000]	Loss: 0.5429	LR: 0.025000
Training Epoch: 68 [21248/50000]	Loss: 0.4768	LR: 0.025000
Training Epoch: 68 [21376/50000]	Loss: 0.5080	LR: 0.025000
Training Epoch: 68 [21504/50000]	Loss: 0.4726	LR: 0.025000
Training Epoch: 68 [21632/50000]	Loss: 0.5716	LR: 0.025000
Training Epoch: 68 [21760/50000]	Loss: 0.3965	LR: 0.025000
Training Epoch: 68 [21888/50000]	Loss: 0.3541	LR: 0.025000
Training Epoch: 68 [22016/50000]	Loss: 0.4786	LR: 0.025000
Training Epoch: 68 [22144/50000]	Loss: 0.4198	LR: 0.025000
Training Epoch: 68 [22272/50000]	Loss: 0.4243	LR: 0.025000
Training Epoch: 68 [22400/50000]	Loss: 0.4937	LR: 0.025000
Training Epoch: 68 [22528/50000]	Loss: 0.3387	LR: 0.025000
Training Epoch: 68 [22656/50000]	Loss: 0.3198	LR: 0.025000
Training Epoch: 68 [22784/50000]	Loss: 0.3720	LR: 0.025000
Training Epoch: 68 [22912/50000]	Loss: 0.4806	LR: 0.025000
Training Epoch: 68 [23040/50000]	Loss: 0.5632	LR: 0.025000
Training Epoch: 68 [23168/50000]	Loss: 0.3822	LR: 0.025000
Training Epoch: 68 [23296/50000]	Loss: 0.5027	LR: 0.025000
Training Epoch: 68 [23424/50000]	Loss: 0.5697	LR: 0.025000
Training Epoch: 68 [23552/50000]	Loss: 0.4086	LR: 0.025000
Training Epoch: 68 [23680/50000]	Loss: 0.3548	LR: 0.025000
Training Epoch: 68 [23808/50000]	Loss: 0.4011	LR: 0.025000
Training Epoch: 68 [23936/50000]	Loss: 0.4443	LR: 0.025000
Training Epoch: 68 [24064/50000]	Loss: 0.6106	LR: 0.025000
Training Epoch: 68 [24192/50000]	Loss: 0.4948	LR: 0.025000
Training Epoch: 68 [24320/50000]	Loss: 0.3905	LR: 0.025000
Training Epoch: 68 [24448/50000]	Loss: 0.4079	LR: 0.025000
Training Epoch: 68 [24576/50000]	Loss: 0.4386	LR: 0.025000
Training Epoch: 68 [24704/50000]	Loss: 0.3909	LR: 0.025000
Training Epoch: 68 [24832/50000]	Loss: 0.5777	LR: 0.025000
Training Epoch: 68 [24960/50000]	Loss: 0.5213	LR: 0.025000
Training Epoch: 68 [25088/50000]	Loss: 0.6795	LR: 0.025000
Training Epoch: 68 [25216/50000]	Loss: 0.5274	LR: 0.025000
Training Epoch: 68 [25344/50000]	Loss: 0.4058	LR: 0.025000
Training Epoch: 68 [25472/50000]	Loss: 0.4242	LR: 0.025000
Training Epoch: 68 [25600/50000]	Loss: 0.5040	LR: 0.025000
Training Epoch: 68 [25728/50000]	Loss: 0.4013	LR: 0.025000
Training Epoch: 68 [25856/50000]	Loss: 0.3703	LR: 0.025000
Training Epoch: 68 [25984/50000]	Loss: 0.3327	LR: 0.025000
Training Epoch: 68 [26112/50000]	Loss: 0.5343	LR: 0.025000
Training Epoch: 68 [26240/50000]	Loss: 0.4842	LR: 0.025000
Training Epoch: 68 [26368/50000]	Loss: 0.3606	LR: 0.025000
Training Epoch: 68 [26496/50000]	Loss: 0.3883	LR: 0.025000
Training Epoch: 68 [26624/50000]	Loss: 0.4597	LR: 0.025000
Training Epoch: 68 [26752/50000]	Loss: 0.3600	LR: 0.025000
Training Epoch: 68 [26880/50000]	Loss: 0.5172	LR: 0.025000
Training Epoch: 68 [27008/50000]	Loss: 0.5281	LR: 0.025000
Training Epoch: 68 [27136/50000]	Loss: 0.6036	LR: 0.025000
Training Epoch: 68 [27264/50000]	Loss: 0.4205	LR: 0.025000
Training Epoch: 68 [27392/50000]	Loss: 0.4772	LR: 0.025000
Training Epoch: 68 [27520/50000]	Loss: 0.4597	LR: 0.025000
Training Epoch: 68 [27648/50000]	Loss: 0.5642	LR: 0.025000
Training Epoch: 68 [27776/50000]	Loss: 0.3416	LR: 0.025000
Training Epoch: 68 [27904/50000]	Loss: 0.6624	LR: 0.025000
Training Epoch: 68 [28032/50000]	Loss: 0.5073	LR: 0.025000
Training Epoch: 68 [28160/50000]	Loss: 0.4669	LR: 0.025000
Training Epoch: 68 [28288/50000]	Loss: 0.5363	LR: 0.025000
Training Epoch: 68 [28416/50000]	Loss: 0.5128	LR: 0.025000
Training Epoch: 68 [28544/50000]	Loss: 0.5308	LR: 0.025000
Training Epoch: 68 [28672/50000]	Loss: 0.4278	LR: 0.025000
Training Epoch: 68 [28800/50000]	Loss: 0.3968	LR: 0.025000
Training Epoch: 68 [28928/50000]	Loss: 0.3673	LR: 0.025000
Training Epoch: 68 [29056/50000]	Loss: 0.3815	LR: 0.025000
Training Epoch: 68 [29184/50000]	Loss: 0.4410	LR: 0.025000
Training Epoch: 68 [29312/50000]	Loss: 0.5141	LR: 0.025000
Training Epoch: 68 [29440/50000]	Loss: 0.4744	LR: 0.025000
Training Epoch: 68 [29568/50000]	Loss: 0.5161	LR: 0.025000
Training Epoch: 68 [29696/50000]	Loss: 0.3707	LR: 0.025000
Training Epoch: 68 [29824/50000]	Loss: 0.4512	LR: 0.025000
Training Epoch: 68 [29952/50000]	Loss: 0.4576	LR: 0.025000
Training Epoch: 68 [30080/50000]	Loss: 0.4069	LR: 0.025000
Training Epoch: 68 [30208/50000]	Loss: 0.7146	LR: 0.025000
Training Epoch: 68 [30336/50000]	Loss: 0.4017	LR: 0.025000
Training Epoch: 68 [30464/50000]	Loss: 0.4695	LR: 0.025000
Training Epoch: 68 [30592/50000]	Loss: 0.5616	LR: 0.025000
Training Epoch: 68 [30720/50000]	Loss: 0.3983	LR: 0.025000
Training Epoch: 68 [30848/50000]	Loss: 0.4449	LR: 0.025000
Training Epoch: 68 [30976/50000]	Loss: 0.5560	LR: 0.025000
Training Epoch: 68 [31104/50000]	Loss: 0.4437	LR: 0.025000
Training Epoch: 68 [31232/50000]	Loss: 0.6174	LR: 0.025000
Training Epoch: 68 [31360/50000]	Loss: 0.4984	LR: 0.025000
Training Epoch: 68 [31488/50000]	Loss: 0.4485	LR: 0.025000
Training Epoch: 68 [31616/50000]	Loss: 0.5717	LR: 0.025000
Training Epoch: 68 [31744/50000]	Loss: 0.2875	LR: 0.025000
Training Epoch: 68 [31872/50000]	Loss: 0.5492	LR: 0.025000
Training Epoch: 68 [32000/50000]	Loss: 0.6172	LR: 0.025000
Training Epoch: 68 [32128/50000]	Loss: 0.5513	LR: 0.025000
Training Epoch: 68 [32256/50000]	Loss: 0.5368	LR: 0.025000
Training Epoch: 68 [32384/50000]	Loss: 0.5208	LR: 0.025000
Training Epoch: 68 [32512/50000]	Loss: 0.5387	LR: 0.025000
Training Epoch: 68 [32640/50000]	Loss: 0.5150	LR: 0.025000
Training Epoch: 68 [32768/50000]	Loss: 0.6318	LR: 0.025000
Training Epoch: 68 [32896/50000]	Loss: 0.5445	LR: 0.025000
Training Epoch: 68 [33024/50000]	Loss: 0.5182	LR: 0.025000
Training Epoch: 68 [33152/50000]	Loss: 0.4270	LR: 0.025000
Training Epoch: 68 [33280/50000]	Loss: 0.4420	LR: 0.025000
Training Epoch: 68 [33408/50000]	Loss: 0.4969	LR: 0.025000
Training Epoch: 68 [33536/50000]	Loss: 0.5622	LR: 0.025000
Training Epoch: 68 [33664/50000]	Loss: 0.5963	LR: 0.025000
Training Epoch: 68 [33792/50000]	Loss: 0.4893	LR: 0.025000
Training Epoch: 68 [33920/50000]	Loss: 0.5149	LR: 0.025000
Training Epoch: 68 [34048/50000]	Loss: 0.5157	LR: 0.025000
Training Epoch: 68 [34176/50000]	Loss: 0.5205	LR: 0.025000
Training Epoch: 68 [34304/50000]	Loss: 0.3583	LR: 0.025000
Training Epoch: 68 [34432/50000]	Loss: 0.5612	LR: 0.025000
Training Epoch: 68 [34560/50000]	Loss: 0.5659	LR: 0.025000
Training Epoch: 68 [34688/50000]	Loss: 0.3965	LR: 0.025000
Training Epoch: 68 [34816/50000]	Loss: 0.3136	LR: 0.025000
Training Epoch: 68 [34944/50000]	Loss: 0.5755	LR: 0.025000
Training Epoch: 68 [35072/50000]	Loss: 0.5022	LR: 0.025000
Training Epoch: 68 [35200/50000]	Loss: 0.4833	LR: 0.025000
Training Epoch: 68 [35328/50000]	Loss: 0.4365	LR: 0.025000
Training Epoch: 68 [35456/50000]	Loss: 0.6264	LR: 0.025000
Training Epoch: 68 [35584/50000]	Loss: 0.5113	LR: 0.025000
Training Epoch: 68 [35712/50000]	Loss: 0.6502	LR: 0.025000
Training Epoch: 68 [35840/50000]	Loss: 0.4753	LR: 0.025000
Training Epoch: 68 [35968/50000]	Loss: 0.4077	LR: 0.025000
Training Epoch: 68 [36096/50000]	Loss: 0.4103	LR: 0.025000
Training Epoch: 68 [36224/50000]	Loss: 0.4664	LR: 0.025000
Training Epoch: 68 [36352/50000]	Loss: 0.4898	LR: 0.025000
Training Epoch: 68 [36480/50000]	Loss: 0.7227	LR: 0.025000
Training Epoch: 68 [36608/50000]	Loss: 0.6178	LR: 0.025000
Training Epoch: 68 [36736/50000]	Loss: 0.5726	LR: 0.025000
Training Epoch: 68 [36864/50000]	Loss: 0.4966	LR: 0.025000
Training Epoch: 68 [36992/50000]	Loss: 0.5102	LR: 0.025000
Training Epoch: 68 [37120/50000]	Loss: 0.5790	LR: 0.025000
Training Epoch: 68 [37248/50000]	Loss: 0.6244	LR: 0.025000
Training Epoch: 68 [37376/50000]	Loss: 0.6270	LR: 0.025000
Training Epoch: 68 [37504/50000]	Loss: 0.4712	LR: 0.025000
Training Epoch: 68 [37632/50000]	Loss: 0.5466	LR: 0.025000
Training Epoch: 68 [37760/50000]	Loss: 0.4258	LR: 0.025000
Training Epoch: 68 [37888/50000]	Loss: 0.5216	LR: 0.025000
Training Epoch: 68 [38016/50000]	Loss: 0.5147	LR: 0.025000
Training Epoch: 68 [38144/50000]	Loss: 0.6059	LR: 0.025000
Training Epoch: 68 [38272/50000]	Loss: 0.5041	LR: 0.025000
Training Epoch: 68 [38400/50000]	Loss: 0.4784	LR: 0.025000
Training Epoch: 68 [38528/50000]	Loss: 0.4596	LR: 0.025000
Training Epoch: 68 [38656/50000]	Loss: 0.5212	LR: 0.025000
Training Epoch: 68 [38784/50000]	Loss: 0.3956	LR: 0.025000
Training Epoch: 68 [38912/50000]	Loss: 0.4783	LR: 0.025000
Training Epoch: 68 [39040/50000]	Loss: 0.5052	LR: 0.025000
Training Epoch: 68 [39168/50000]	Loss: 0.5590	LR: 0.025000
Training Epoch: 68 [39296/50000]	Loss: 0.5451	LR: 0.025000
Training Epoch: 68 [39424/50000]	Loss: 0.6115	LR: 0.025000
Training Epoch: 68 [39552/50000]	Loss: 0.6999	LR: 0.025000
Training Epoch: 68 [39680/50000]	Loss: 0.5570	LR: 0.025000
Training Epoch: 68 [39808/50000]	Loss: 0.5520	LR: 0.025000
Training Epoch: 68 [39936/50000]	Loss: 0.6018	LR: 0.025000
Training Epoch: 68 [40064/50000]	Loss: 0.6300	LR: 0.025000
Training Epoch: 68 [40192/50000]	Loss: 0.5027	LR: 0.025000
Training Epoch: 68 [40320/50000]	Loss: 0.4228	LR: 0.025000
Training Epoch: 68 [40448/50000]	Loss: 0.5985	LR: 0.025000
Training Epoch: 68 [40576/50000]	Loss: 0.5359	LR: 0.025000
Training Epoch: 68 [40704/50000]	Loss: 0.5389	LR: 0.025000
Training Epoch: 68 [40832/50000]	Loss: 0.4691	LR: 0.025000
Training Epoch: 68 [40960/50000]	Loss: 0.4902	LR: 0.025000
Training Epoch: 68 [41088/50000]	Loss: 0.3210	LR: 0.025000
Training Epoch: 68 [41216/50000]	Loss: 0.4891	LR: 0.025000
Training Epoch: 68 [41344/50000]	Loss: 0.6682	LR: 0.025000
Training Epoch: 68 [41472/50000]	Loss: 0.4575	LR: 0.025000
Training Epoch: 68 [41600/50000]	Loss: 0.8516	LR: 0.025000
Training Epoch: 68 [41728/50000]	Loss: 0.6357	LR: 0.025000
Training Epoch: 68 [41856/50000]	Loss: 0.3783	LR: 0.025000
Training Epoch: 68 [41984/50000]	Loss: 0.5424	LR: 0.025000
Training Epoch: 68 [42112/50000]	Loss: 0.8419	LR: 0.025000
Training Epoch: 68 [42240/50000]	Loss: 0.6028	LR: 0.025000
Training Epoch: 68 [42368/50000]	Loss: 0.4768	LR: 0.025000
Training Epoch: 68 [42496/50000]	Loss: 0.5723	LR: 0.025000
Training Epoch: 68 [42624/50000]	Loss: 0.4735	LR: 0.025000
Training Epoch: 68 [42752/50000]	Loss: 0.6751	LR: 0.025000
Training Epoch: 68 [42880/50000]	Loss: 0.5852	LR: 0.025000
Training Epoch: 68 [43008/50000]	Loss: 0.5903	LR: 0.025000
Training Epoch: 68 [43136/50000]	Loss: 0.5916	LR: 0.025000
Training Epoch: 68 [43264/50000]	Loss: 0.5399	LR: 0.025000
Training Epoch: 68 [43392/50000]	Loss: 0.6484	LR: 0.025000
Training Epoch: 68 [43520/50000]	Loss: 0.6062	LR: 0.025000
Training Epoch: 68 [43648/50000]	Loss: 0.4588	LR: 0.025000
Training Epoch: 68 [43776/50000]	Loss: 0.5947	LR: 0.025000
Training Epoch: 68 [43904/50000]	Loss: 0.6298	LR: 0.025000
Training Epoch: 68 [44032/50000]	Loss: 0.5173	LR: 0.025000
Training Epoch: 68 [44160/50000]	Loss: 0.6677	LR: 0.025000
Training Epoch: 68 [44288/50000]	Loss: 0.4549	LR: 0.025000
Training Epoch: 68 [44416/50000]	Loss: 0.5437	LR: 0.025000
Training Epoch: 68 [44544/50000]	Loss: 0.6376	LR: 0.025000
Training Epoch: 68 [44672/50000]	Loss: 0.4752	LR: 0.025000
Training Epoch: 68 [44800/50000]	Loss: 0.4104	LR: 0.025000
Training Epoch: 68 [44928/50000]	Loss: 0.5068	LR: 0.025000
Training Epoch: 68 [45056/50000]	Loss: 0.6207	LR: 0.025000
Training Epoch: 68 [45184/50000]	Loss: 0.4954	LR: 0.025000
Training Epoch: 68 [45312/50000]	Loss: 0.5127	LR: 0.025000
Training Epoch: 68 [45440/50000]	Loss: 0.4955	LR: 0.025000
Training Epoch: 68 [45568/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 68 [45696/50000]	Loss: 0.4766	LR: 0.025000
Training Epoch: 68 [45824/50000]	Loss: 0.5347	LR: 0.025000
Training Epoch: 68 [45952/50000]	Loss: 0.5452	LR: 0.025000
Training Epoch: 68 [46080/50000]	Loss: 0.5517	LR: 0.025000
Training Epoch: 68 [46208/50000]	Loss: 0.5316	LR: 0.025000
Training Epoch: 68 [46336/50000]	Loss: 0.6671	LR: 0.025000
Training Epoch: 68 [46464/50000]	Loss: 0.6133	LR: 0.025000
Training Epoch: 68 [46592/50000]	Loss: 0.6632	LR: 0.025000
Training Epoch: 68 [46720/50000]	Loss: 0.5185	LR: 0.025000
Training Epoch: 68 [46848/50000]	Loss: 0.5769	LR: 0.025000
Training Epoch: 68 [46976/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 68 [47104/50000]	Loss: 0.4436	LR: 0.025000
Training Epoch: 68 [47232/50000]	Loss: 0.4688	LR: 0.025000
Training Epoch: 68 [47360/50000]	Loss: 0.4011	LR: 0.025000
Training Epoch: 68 [47488/50000]	Loss: 0.4391	LR: 0.025000
Training Epoch: 68 [47616/50000]	Loss: 0.4925	LR: 0.025000
Training Epoch: 68 [47744/50000]	Loss: 0.5119	LR: 0.025000
Training Epoch: 68 [47872/50000]	Loss: 0.5322	LR: 0.025000
Training Epoch: 68 [48000/50000]	Loss: 0.5363	LR: 0.025000
Training Epoch: 68 [48128/50000]	Loss: 0.5802	LR: 0.025000
Training Epoch: 68 [48256/50000]	Loss: 0.5229	LR: 0.025000
Training Epoch: 68 [48384/50000]	Loss: 0.5423	LR: 0.025000
Training Epoch: 68 [48512/50000]	Loss: 0.8085	LR: 0.025000
Training Epoch: 68 [48640/50000]	Loss: 0.6202	LR: 0.025000
Training Epoch: 68 [48768/50000]	Loss: 0.4995	LR: 0.025000
Training Epoch: 68 [48896/50000]	Loss: 0.4973	LR: 0.025000
Training Epoch: 68 [49024/50000]	Loss: 0.7649	LR: 0.025000
Training Epoch: 68 [49152/50000]	Loss: 0.3916	LR: 0.025000
Training Epoch: 68 [49280/50000]	Loss: 0.5379	LR: 0.025000
Training Epoch: 68 [49408/50000]	Loss: 0.6630	LR: 0.025000
Training Epoch: 68 [49536/50000]	Loss: 0.5175	LR: 0.025000
Training Epoch: 68 [49664/50000]	Loss: 0.6816	LR: 0.025000
Training Epoch: 68 [49792/50000]	Loss: 0.4942	LR: 0.025000
Training Epoch: 68 [49920/50000]	Loss: 0.4948	LR: 0.025000
Training Epoch: 68 [50000/50000]	Loss: 0.6403	LR: 0.025000
epoch 68 training time consumed: 54.08s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  249564 GB |  249564 GB |
|       from large pool |  123392 KB |    1034 MB |  249318 GB |  249318 GB |
|       from small pool |   10798 KB |      13 MB |     245 GB |     245 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  249564 GB |  249564 GB |
|       from large pool |  123392 KB |    1034 MB |  249318 GB |  249318 GB |
|       from small pool |   10798 KB |      13 MB |     245 GB |     245 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  109823 GB |  109823 GB |
|       from large pool |  155136 KB |  433088 KB |  109551 GB |  109551 GB |
|       from small pool |    1490 KB |    3494 KB |     271 GB |     271 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    9629 K  |    9629 K  |
|       from large pool |      24    |      65    |    5026 K  |    5026 K  |
|       from small pool |     231    |     274    |    4603 K  |    4603 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    9629 K  |    9629 K  |
|       from large pool |      24    |      65    |    5026 K  |    5026 K  |
|       from small pool |     231    |     274    |    4603 K  |    4603 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4758 K  |    4758 K  |
|       from large pool |       9    |      14    |    2432 K  |    2432 K  |
|       from small pool |      12    |      16    |    2325 K  |    2325 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 68, Average loss: 0.0116, Accuracy: 0.6502, Time consumed:3.47s

Training Epoch: 69 [128/50000]	Loss: 0.4869	LR: 0.025000
Training Epoch: 69 [256/50000]	Loss: 0.3178	LR: 0.025000
Training Epoch: 69 [384/50000]	Loss: 0.4708	LR: 0.025000
Training Epoch: 69 [512/50000]	Loss: 0.4896	LR: 0.025000
Training Epoch: 69 [640/50000]	Loss: 0.4931	LR: 0.025000
Training Epoch: 69 [768/50000]	Loss: 0.3500	LR: 0.025000
Training Epoch: 69 [896/50000]	Loss: 0.4076	LR: 0.025000
Training Epoch: 69 [1024/50000]	Loss: 0.3791	LR: 0.025000
Training Epoch: 69 [1152/50000]	Loss: 0.4179	LR: 0.025000
Training Epoch: 69 [1280/50000]	Loss: 0.3907	LR: 0.025000
Training Epoch: 69 [1408/50000]	Loss: 0.5204	LR: 0.025000
Training Epoch: 69 [1536/50000]	Loss: 0.4561	LR: 0.025000
Training Epoch: 69 [1664/50000]	Loss: 0.4574	LR: 0.025000
Training Epoch: 69 [1792/50000]	Loss: 0.5185	LR: 0.025000
Training Epoch: 69 [1920/50000]	Loss: 0.4318	LR: 0.025000
Training Epoch: 69 [2048/50000]	Loss: 0.4657	LR: 0.025000
Training Epoch: 69 [2176/50000]	Loss: 0.4602	LR: 0.025000
Training Epoch: 69 [2304/50000]	Loss: 0.3305	LR: 0.025000
Training Epoch: 69 [2432/50000]	Loss: 0.5337	LR: 0.025000
Training Epoch: 69 [2560/50000]	Loss: 0.4431	LR: 0.025000
Training Epoch: 69 [2688/50000]	Loss: 0.2806	LR: 0.025000
Training Epoch: 69 [2816/50000]	Loss: 0.5702	LR: 0.025000
Training Epoch: 69 [2944/50000]	Loss: 0.3794	LR: 0.025000
Training Epoch: 69 [3072/50000]	Loss: 0.3665	LR: 0.025000
Training Epoch: 69 [3200/50000]	Loss: 0.3841	LR: 0.025000
Training Epoch: 69 [3328/50000]	Loss: 0.4540	LR: 0.025000
Training Epoch: 69 [3456/50000]	Loss: 0.4958	LR: 0.025000
Training Epoch: 69 [3584/50000]	Loss: 0.4495	LR: 0.025000
Training Epoch: 69 [3712/50000]	Loss: 0.4199	LR: 0.025000
Training Epoch: 69 [3840/50000]	Loss: 0.6423	LR: 0.025000
Training Epoch: 69 [3968/50000]	Loss: 0.5828	LR: 0.025000
Training Epoch: 69 [4096/50000]	Loss: 0.5623	LR: 0.025000
Training Epoch: 69 [4224/50000]	Loss: 0.4313	LR: 0.025000
Training Epoch: 69 [4352/50000]	Loss: 0.5255	LR: 0.025000
Training Epoch: 69 [4480/50000]	Loss: 0.3976	LR: 0.025000
Training Epoch: 69 [4608/50000]	Loss: 0.4583	LR: 0.025000
Training Epoch: 69 [4736/50000]	Loss: 0.4153	LR: 0.025000
Training Epoch: 69 [4864/50000]	Loss: 0.4319	LR: 0.025000
Training Epoch: 69 [4992/50000]	Loss: 0.3338	LR: 0.025000
Training Epoch: 69 [5120/50000]	Loss: 0.4979	LR: 0.025000
Training Epoch: 69 [5248/50000]	Loss: 0.3344	LR: 0.025000
Training Epoch: 69 [5376/50000]	Loss: 0.4135	LR: 0.025000
Training Epoch: 69 [5504/50000]	Loss: 0.4628	LR: 0.025000
Training Epoch: 69 [5632/50000]	Loss: 0.3240	LR: 0.025000
Training Epoch: 69 [5760/50000]	Loss: 0.3905	LR: 0.025000
Training Epoch: 69 [5888/50000]	Loss: 0.3693	LR: 0.025000
Training Epoch: 69 [6016/50000]	Loss: 0.4381	LR: 0.025000
Training Epoch: 69 [6144/50000]	Loss: 0.3355	LR: 0.025000
Training Epoch: 69 [6272/50000]	Loss: 0.3375	LR: 0.025000
Training Epoch: 69 [6400/50000]	Loss: 0.3127	LR: 0.025000
Training Epoch: 69 [6528/50000]	Loss: 0.4794	LR: 0.025000
Training Epoch: 69 [6656/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 69 [6784/50000]	Loss: 0.3266	LR: 0.025000
Training Epoch: 69 [6912/50000]	Loss: 0.3591	LR: 0.025000
Training Epoch: 69 [7040/50000]	Loss: 0.3911	LR: 0.025000
Training Epoch: 69 [7168/50000]	Loss: 0.5604	LR: 0.025000
Training Epoch: 69 [7296/50000]	Loss: 0.4614	LR: 0.025000
Training Epoch: 69 [7424/50000]	Loss: 0.4144	LR: 0.025000
Training Epoch: 69 [7552/50000]	Loss: 0.4360	LR: 0.025000
Training Epoch: 69 [7680/50000]	Loss: 0.3743	LR: 0.025000
Training Epoch: 69 [7808/50000]	Loss: 0.3471	LR: 0.025000
Training Epoch: 69 [7936/50000]	Loss: 0.2839	LR: 0.025000
Training Epoch: 69 [8064/50000]	Loss: 0.4693	LR: 0.025000
Training Epoch: 69 [8192/50000]	Loss: 0.5640	LR: 0.025000
Training Epoch: 69 [8320/50000]	Loss: 0.3223	LR: 0.025000
Training Epoch: 69 [8448/50000]	Loss: 0.4077	LR: 0.025000
Training Epoch: 69 [8576/50000]	Loss: 0.3624	LR: 0.025000
Training Epoch: 69 [8704/50000]	Loss: 0.3610	LR: 0.025000
Training Epoch: 69 [8832/50000]	Loss: 0.4024	LR: 0.025000
Training Epoch: 69 [8960/50000]	Loss: 0.3697	LR: 0.025000
Training Epoch: 69 [9088/50000]	Loss: 0.3748	LR: 0.025000
Training Epoch: 69 [9216/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 69 [9344/50000]	Loss: 0.5254	LR: 0.025000
Training Epoch: 69 [9472/50000]	Loss: 0.5247	LR: 0.025000
Training Epoch: 69 [9600/50000]	Loss: 0.3740	LR: 0.025000
Training Epoch: 69 [9728/50000]	Loss: 0.5648	LR: 0.025000
Training Epoch: 69 [9856/50000]	Loss: 0.3647	LR: 0.025000
Training Epoch: 69 [9984/50000]	Loss: 0.3718	LR: 0.025000
Training Epoch: 69 [10112/50000]	Loss: 0.2816	LR: 0.025000
Training Epoch: 69 [10240/50000]	Loss: 0.4473	LR: 0.025000
Training Epoch: 69 [10368/50000]	Loss: 0.3023	LR: 0.025000
Training Epoch: 69 [10496/50000]	Loss: 0.3750	LR: 0.025000
Training Epoch: 69 [10624/50000]	Loss: 0.4196	LR: 0.025000
Training Epoch: 69 [10752/50000]	Loss: 0.4197	LR: 0.025000
Training Epoch: 69 [10880/50000]	Loss: 0.3043	LR: 0.025000
Training Epoch: 69 [11008/50000]	Loss: 0.4465	LR: 0.025000
Training Epoch: 69 [11136/50000]	Loss: 0.4260	LR: 0.025000
Training Epoch: 69 [11264/50000]	Loss: 0.4600	LR: 0.025000
Training Epoch: 69 [11392/50000]	Loss: 0.3687	LR: 0.025000
Training Epoch: 69 [11520/50000]	Loss: 0.4294	LR: 0.025000
Training Epoch: 69 [11648/50000]	Loss: 0.4155	LR: 0.025000
Training Epoch: 69 [11776/50000]	Loss: 0.4003	LR: 0.025000
Training Epoch: 69 [11904/50000]	Loss: 0.5820	LR: 0.025000
Training Epoch: 69 [12032/50000]	Loss: 0.4791	LR: 0.025000
Training Epoch: 69 [12160/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 69 [12288/50000]	Loss: 0.4263	LR: 0.025000
Training Epoch: 69 [12416/50000]	Loss: 0.3841	LR: 0.025000
Training Epoch: 69 [12544/50000]	Loss: 0.4405	LR: 0.025000
Training Epoch: 69 [12672/50000]	Loss: 0.4189	LR: 0.025000
Training Epoch: 69 [12800/50000]	Loss: 0.3829	LR: 0.025000
Training Epoch: 69 [12928/50000]	Loss: 0.4919	LR: 0.025000
Training Epoch: 69 [13056/50000]	Loss: 0.3532	LR: 0.025000
Training Epoch: 69 [13184/50000]	Loss: 0.3375	LR: 0.025000
Training Epoch: 69 [13312/50000]	Loss: 0.3354	LR: 0.025000
Training Epoch: 69 [13440/50000]	Loss: 0.4255	LR: 0.025000
Training Epoch: 69 [13568/50000]	Loss: 0.3547	LR: 0.025000
Training Epoch: 69 [13696/50000]	Loss: 0.2479	LR: 0.025000
Training Epoch: 69 [13824/50000]	Loss: 0.5191	LR: 0.025000
Training Epoch: 69 [13952/50000]	Loss: 0.4414	LR: 0.025000
Training Epoch: 69 [14080/50000]	Loss: 0.4868	LR: 0.025000
Training Epoch: 69 [14208/50000]	Loss: 0.4174	LR: 0.025000
Training Epoch: 69 [14336/50000]	Loss: 0.4678	LR: 0.025000
Training Epoch: 69 [14464/50000]	Loss: 0.3639	LR: 0.025000
Training Epoch: 69 [14592/50000]	Loss: 0.4637	LR: 0.025000
Training Epoch: 69 [14720/50000]	Loss: 0.4951	LR: 0.025000
Training Epoch: 69 [14848/50000]	Loss: 0.5870	LR: 0.025000
Training Epoch: 69 [14976/50000]	Loss: 0.4425	LR: 0.025000
Training Epoch: 69 [15104/50000]	Loss: 0.4145	LR: 0.025000
Training Epoch: 69 [15232/50000]	Loss: 0.4031	LR: 0.025000
Training Epoch: 69 [15360/50000]	Loss: 0.2991	LR: 0.025000
Training Epoch: 69 [15488/50000]	Loss: 0.3774	LR: 0.025000
Training Epoch: 69 [15616/50000]	Loss: 0.4511	LR: 0.025000
Training Epoch: 69 [15744/50000]	Loss: 0.4751	LR: 0.025000
Training Epoch: 69 [15872/50000]	Loss: 0.5336	LR: 0.025000
Training Epoch: 69 [16000/50000]	Loss: 0.5261	LR: 0.025000
Training Epoch: 69 [16128/50000]	Loss: 0.4371	LR: 0.025000
Training Epoch: 69 [16256/50000]	Loss: 0.4859	LR: 0.025000
Training Epoch: 69 [16384/50000]	Loss: 0.3741	LR: 0.025000
Training Epoch: 69 [16512/50000]	Loss: 0.5165	LR: 0.025000
Training Epoch: 69 [16640/50000]	Loss: 0.3415	LR: 0.025000
Training Epoch: 69 [16768/50000]	Loss: 0.4639	LR: 0.025000
Training Epoch: 69 [16896/50000]	Loss: 0.5359	LR: 0.025000
Training Epoch: 69 [17024/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 69 [17152/50000]	Loss: 0.4759	LR: 0.025000
Training Epoch: 69 [17280/50000]	Loss: 0.4481	LR: 0.025000
Training Epoch: 69 [17408/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 69 [17536/50000]	Loss: 0.3815	LR: 0.025000
Training Epoch: 69 [17664/50000]	Loss: 0.4143	LR: 0.025000
Training Epoch: 69 [17792/50000]	Loss: 0.4430	LR: 0.025000
Training Epoch: 69 [17920/50000]	Loss: 0.5892	LR: 0.025000
Training Epoch: 69 [18048/50000]	Loss: 0.4191	LR: 0.025000
Training Epoch: 69 [18176/50000]	Loss: 0.3006	LR: 0.025000
Training Epoch: 69 [18304/50000]	Loss: 0.3850	LR: 0.025000
Training Epoch: 69 [18432/50000]	Loss: 0.4819	LR: 0.025000
Training Epoch: 69 [18560/50000]	Loss: 0.4457	LR: 0.025000
Training Epoch: 69 [18688/50000]	Loss: 0.3854	LR: 0.025000
Training Epoch: 69 [18816/50000]	Loss: 0.3839	LR: 0.025000
Training Epoch: 69 [18944/50000]	Loss: 0.6062	LR: 0.025000
Training Epoch: 69 [19072/50000]	Loss: 0.4095	LR: 0.025000
Training Epoch: 69 [19200/50000]	Loss: 0.3786	LR: 0.025000
Training Epoch: 69 [19328/50000]	Loss: 0.3065	LR: 0.025000
Training Epoch: 69 [19456/50000]	Loss: 0.4297	LR: 0.025000
Training Epoch: 69 [19584/50000]	Loss: 0.3724	LR: 0.025000
Training Epoch: 69 [19712/50000]	Loss: 0.4100	LR: 0.025000
Training Epoch: 69 [19840/50000]	Loss: 0.6212	LR: 0.025000
Training Epoch: 69 [19968/50000]	Loss: 0.3570	LR: 0.025000
Training Epoch: 69 [20096/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 69 [20224/50000]	Loss: 0.5434	LR: 0.025000
Training Epoch: 69 [20352/50000]	Loss: 0.5215	LR: 0.025000
Training Epoch: 69 [20480/50000]	Loss: 0.4799	LR: 0.025000
Training Epoch: 69 [20608/50000]	Loss: 0.4649	LR: 0.025000
Training Epoch: 69 [20736/50000]	Loss: 0.4732	LR: 0.025000
Training Epoch: 69 [20864/50000]	Loss: 0.4095	LR: 0.025000
Training Epoch: 69 [20992/50000]	Loss: 0.5203	LR: 0.025000
Training Epoch: 69 [21120/50000]	Loss: 0.3441	LR: 0.025000
Training Epoch: 69 [21248/50000]	Loss: 0.5202	LR: 0.025000
Training Epoch: 69 [21376/50000]	Loss: 0.5371	LR: 0.025000
Training Epoch: 69 [21504/50000]	Loss: 0.4360	LR: 0.025000
Training Epoch: 69 [21632/50000]	Loss: 0.4666	LR: 0.025000
Training Epoch: 69 [21760/50000]	Loss: 0.4294	LR: 0.025000
Training Epoch: 69 [21888/50000]	Loss: 0.3537	LR: 0.025000
Training Epoch: 69 [22016/50000]	Loss: 0.4117	LR: 0.025000
Training Epoch: 69 [22144/50000]	Loss: 0.4021	LR: 0.025000
Training Epoch: 69 [22272/50000]	Loss: 0.5780	LR: 0.025000
Training Epoch: 69 [22400/50000]	Loss: 0.4266	LR: 0.025000
Training Epoch: 69 [22528/50000]	Loss: 0.5251	LR: 0.025000
Training Epoch: 69 [22656/50000]	Loss: 0.3734	LR: 0.025000
Training Epoch: 69 [22784/50000]	Loss: 0.4214	LR: 0.025000
Training Epoch: 69 [22912/50000]	Loss: 0.4168	LR: 0.025000
Training Epoch: 69 [23040/50000]	Loss: 0.5652	LR: 0.025000
Training Epoch: 69 [23168/50000]	Loss: 0.3174	LR: 0.025000
Training Epoch: 69 [23296/50000]	Loss: 0.4609	LR: 0.025000
Training Epoch: 69 [23424/50000]	Loss: 0.4631	LR: 0.025000
Training Epoch: 69 [23552/50000]	Loss: 0.4598	LR: 0.025000
Training Epoch: 69 [23680/50000]	Loss: 0.7687	LR: 0.025000
Training Epoch: 69 [23808/50000]	Loss: 0.4219	LR: 0.025000
Training Epoch: 69 [23936/50000]	Loss: 0.5678	LR: 0.025000
Training Epoch: 69 [24064/50000]	Loss: 0.5618	LR: 0.025000
Training Epoch: 69 [24192/50000]	Loss: 0.4573	LR: 0.025000
Training Epoch: 69 [24320/50000]	Loss: 0.3910	LR: 0.025000
Training Epoch: 69 [24448/50000]	Loss: 0.3845	LR: 0.025000
Training Epoch: 69 [24576/50000]	Loss: 0.4305	LR: 0.025000
Training Epoch: 69 [24704/50000]	Loss: 0.6001	LR: 0.025000
Training Epoch: 69 [24832/50000]	Loss: 0.5167	LR: 0.025000
Training Epoch: 69 [24960/50000]	Loss: 0.4765	LR: 0.025000
Training Epoch: 69 [25088/50000]	Loss: 0.4927	LR: 0.025000
Training Epoch: 69 [25216/50000]	Loss: 0.4912	LR: 0.025000
Training Epoch: 69 [25344/50000]	Loss: 0.4562	LR: 0.025000
Training Epoch: 69 [25472/50000]	Loss: 0.3914	LR: 0.025000
Training Epoch: 69 [25600/50000]	Loss: 0.5144	LR: 0.025000
Training Epoch: 69 [25728/50000]	Loss: 0.5829	LR: 0.025000
Training Epoch: 69 [25856/50000]	Loss: 0.4560	LR: 0.025000
Training Epoch: 69 [25984/50000]	Loss: 0.5093	LR: 0.025000
Training Epoch: 69 [26112/50000]	Loss: 0.4899	LR: 0.025000
Training Epoch: 69 [26240/50000]	Loss: 0.4157	LR: 0.025000
Training Epoch: 69 [26368/50000]	Loss: 0.6146	LR: 0.025000
Training Epoch: 69 [26496/50000]	Loss: 0.3772	LR: 0.025000
Training Epoch: 69 [26624/50000]	Loss: 0.5482	LR: 0.025000
Training Epoch: 69 [26752/50000]	Loss: 0.5415	LR: 0.025000
Training Epoch: 69 [26880/50000]	Loss: 0.4219	LR: 0.025000
Training Epoch: 69 [27008/50000]	Loss: 0.4686	LR: 0.025000
Training Epoch: 69 [27136/50000]	Loss: 0.4325	LR: 0.025000
Training Epoch: 69 [27264/50000]	Loss: 0.4945	LR: 0.025000
Training Epoch: 69 [27392/50000]	Loss: 0.5207	LR: 0.025000
Training Epoch: 69 [27520/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 69 [27648/50000]	Loss: 0.4911	LR: 0.025000
Training Epoch: 69 [27776/50000]	Loss: 0.5324	LR: 0.025000
Training Epoch: 69 [27904/50000]	Loss: 0.4614	LR: 0.025000
Training Epoch: 69 [28032/50000]	Loss: 0.4560	LR: 0.025000
Training Epoch: 69 [28160/50000]	Loss: 0.4067	LR: 0.025000
Training Epoch: 69 [28288/50000]	Loss: 0.5852	LR: 0.025000
Training Epoch: 69 [28416/50000]	Loss: 0.6356	LR: 0.025000
Training Epoch: 69 [28544/50000]	Loss: 0.4418	LR: 0.025000
Training Epoch: 69 [28672/50000]	Loss: 0.4920	LR: 0.025000
Training Epoch: 69 [28800/50000]	Loss: 0.4883	LR: 0.025000
Training Epoch: 69 [28928/50000]	Loss: 0.2932	LR: 0.025000
Training Epoch: 69 [29056/50000]	Loss: 0.5040	LR: 0.025000
Training Epoch: 69 [29184/50000]	Loss: 0.5200	LR: 0.025000
Training Epoch: 69 [29312/50000]	Loss: 0.4908	LR: 0.025000
Training Epoch: 69 [29440/50000]	Loss: 0.4513	LR: 0.025000
Training Epoch: 69 [29568/50000]	Loss: 0.4954	LR: 0.025000
Training Epoch: 69 [29696/50000]	Loss: 0.6151	LR: 0.025000
Training Epoch: 69 [29824/50000]	Loss: 0.5556	LR: 0.025000
Training Epoch: 69 [29952/50000]	Loss: 0.6108	LR: 0.025000
Training Epoch: 69 [30080/50000]	Loss: 0.5290	LR: 0.025000
Training Epoch: 69 [30208/50000]	Loss: 0.4517	LR: 0.025000
Training Epoch: 69 [30336/50000]	Loss: 0.5501	LR: 0.025000
Training Epoch: 69 [30464/50000]	Loss: 0.3442	LR: 0.025000
Training Epoch: 69 [30592/50000]	Loss: 0.5297	LR: 0.025000
Training Epoch: 69 [30720/50000]	Loss: 0.3387	LR: 0.025000
Training Epoch: 69 [30848/50000]	Loss: 0.5770	LR: 0.025000
Training Epoch: 69 [30976/50000]	Loss: 0.3429	LR: 0.025000
Training Epoch: 69 [31104/50000]	Loss: 0.7101	LR: 0.025000
Training Epoch: 69 [31232/50000]	Loss: 0.5315	LR: 0.025000
Training Epoch: 69 [31360/50000]	Loss: 0.4133	LR: 0.025000
Training Epoch: 69 [31488/50000]	Loss: 0.4648	LR: 0.025000
Training Epoch: 69 [31616/50000]	Loss: 0.5648	LR: 0.025000
Training Epoch: 69 [31744/50000]	Loss: 0.3962	LR: 0.025000
Training Epoch: 69 [31872/50000]	Loss: 0.4995	LR: 0.025000
Training Epoch: 69 [32000/50000]	Loss: 0.6038	LR: 0.025000
Training Epoch: 69 [32128/50000]	Loss: 0.6273	LR: 0.025000
Training Epoch: 69 [32256/50000]	Loss: 0.6318	LR: 0.025000
Training Epoch: 69 [32384/50000]	Loss: 0.8030	LR: 0.025000
Training Epoch: 69 [32512/50000]	Loss: 0.4499	LR: 0.025000
Training Epoch: 69 [32640/50000]	Loss: 0.4471	LR: 0.025000
Training Epoch: 69 [32768/50000]	Loss: 0.4664	LR: 0.025000
Training Epoch: 69 [32896/50000]	Loss: 0.6962	LR: 0.025000
Training Epoch: 69 [33024/50000]	Loss: 0.4829	LR: 0.025000
Training Epoch: 69 [33152/50000]	Loss: 0.5793	LR: 0.025000
Training Epoch: 69 [33280/50000]	Loss: 0.5509	LR: 0.025000
Training Epoch: 69 [33408/50000]	Loss: 0.6042	LR: 0.025000
Training Epoch: 69 [33536/50000]	Loss: 0.5673	LR: 0.025000
Training Epoch: 69 [33664/50000]	Loss: 0.5357	LR: 0.025000
Training Epoch: 69 [33792/50000]	Loss: 0.4205	LR: 0.025000
Training Epoch: 69 [33920/50000]	Loss: 0.4245	LR: 0.025000
Training Epoch: 69 [34048/50000]	Loss: 0.5424	LR: 0.025000
Training Epoch: 69 [34176/50000]	Loss: 0.6084	LR: 0.025000
Training Epoch: 69 [34304/50000]	Loss: 0.4856	LR: 0.025000
Training Epoch: 69 [34432/50000]	Loss: 0.4601	LR: 0.025000
Training Epoch: 69 [34560/50000]	Loss: 0.3063	LR: 0.025000
Training Epoch: 69 [34688/50000]	Loss: 0.4298	LR: 0.025000
Training Epoch: 69 [34816/50000]	Loss: 0.5889	LR: 0.025000
Training Epoch: 69 [34944/50000]	Loss: 0.4851	LR: 0.025000
Training Epoch: 69 [35072/50000]	Loss: 0.6253	LR: 0.025000
Training Epoch: 69 [35200/50000]	Loss: 0.4465	LR: 0.025000
Training Epoch: 69 [35328/50000]	Loss: 0.5472	LR: 0.025000
Training Epoch: 69 [35456/50000]	Loss: 0.4781	LR: 0.025000
Training Epoch: 69 [35584/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 69 [35712/50000]	Loss: 0.5816	LR: 0.025000
Training Epoch: 69 [35840/50000]	Loss: 0.4495	LR: 0.025000
Training Epoch: 69 [35968/50000]	Loss: 0.6174	LR: 0.025000
Training Epoch: 69 [36096/50000]	Loss: 0.4311	LR: 0.025000
Training Epoch: 69 [36224/50000]	Loss: 0.5900	LR: 0.025000
Training Epoch: 69 [36352/50000]	Loss: 0.6107	LR: 0.025000
Training Epoch: 69 [36480/50000]	Loss: 0.5756	LR: 0.025000
Training Epoch: 69 [36608/50000]	Loss: 0.5208	LR: 0.025000
Training Epoch: 69 [36736/50000]	Loss: 0.5343	LR: 0.025000
Training Epoch: 69 [36864/50000]	Loss: 0.7118	LR: 0.025000
Training Epoch: 69 [36992/50000]	Loss: 0.4685	LR: 0.025000
Training Epoch: 69 [37120/50000]	Loss: 0.5432	LR: 0.025000
Training Epoch: 69 [37248/50000]	Loss: 0.3826	LR: 0.025000
Training Epoch: 69 [37376/50000]	Loss: 0.6354	LR: 0.025000
Training Epoch: 69 [37504/50000]	Loss: 0.4800	LR: 0.025000
Training Epoch: 69 [37632/50000]	Loss: 0.4217	LR: 0.025000
Training Epoch: 69 [37760/50000]	Loss: 0.4214	LR: 0.025000
Training Epoch: 69 [37888/50000]	Loss: 0.5428	LR: 0.025000
Training Epoch: 69 [38016/50000]	Loss: 0.5720	LR: 0.025000
Training Epoch: 69 [38144/50000]	Loss: 0.4327	LR: 0.025000
Training Epoch: 69 [38272/50000]	Loss: 0.6007	LR: 0.025000
Training Epoch: 69 [38400/50000]	Loss: 0.5146	LR: 0.025000
Training Epoch: 69 [38528/50000]	Loss: 0.5953	LR: 0.025000
Training Epoch: 69 [38656/50000]	Loss: 0.5225	LR: 0.025000
Training Epoch: 69 [38784/50000]	Loss: 0.5304	LR: 0.025000
Training Epoch: 69 [38912/50000]	Loss: 0.3152	LR: 0.025000
Training Epoch: 69 [39040/50000]	Loss: 0.4296	LR: 0.025000
Training Epoch: 69 [39168/50000]	Loss: 0.4781	LR: 0.025000
Training Epoch: 69 [39296/50000]	Loss: 0.5518	LR: 0.025000
Training Epoch: 69 [39424/50000]	Loss: 0.5723	LR: 0.025000
Training Epoch: 69 [39552/50000]	Loss: 0.5471	LR: 0.025000
Training Epoch: 69 [39680/50000]	Loss: 0.5838	LR: 0.025000
Training Epoch: 69 [39808/50000]	Loss: 0.5809	LR: 0.025000
Training Epoch: 69 [39936/50000]	Loss: 0.7871	LR: 0.025000
Training Epoch: 69 [40064/50000]	Loss: 0.4608	LR: 0.025000
Training Epoch: 69 [40192/50000]	Loss: 0.3819	LR: 0.025000
Training Epoch: 69 [40320/50000]	Loss: 0.5178	LR: 0.025000
Training Epoch: 69 [40448/50000]	Loss: 0.7424	LR: 0.025000
Training Epoch: 69 [40576/50000]	Loss: 0.4859	LR: 0.025000
Training Epoch: 69 [40704/50000]	Loss: 0.5246	LR: 0.025000
Training Epoch: 69 [40832/50000]	Loss: 0.5822	LR: 0.025000
Training Epoch: 69 [40960/50000]	Loss: 0.3965	LR: 0.025000
Training Epoch: 69 [41088/50000]	Loss: 0.4904	LR: 0.025000
Training Epoch: 69 [41216/50000]	Loss: 0.6442	LR: 0.025000
Training Epoch: 69 [41344/50000]	Loss: 0.5804	LR: 0.025000
Training Epoch: 69 [41472/50000]	Loss: 0.6708	LR: 0.025000
Training Epoch: 69 [41600/50000]	Loss: 0.5154	LR: 0.025000
Training Epoch: 69 [41728/50000]	Loss: 0.5525	LR: 0.025000
Training Epoch: 69 [41856/50000]	Loss: 0.5097	LR: 0.025000
Training Epoch: 69 [41984/50000]	Loss: 0.4778	LR: 0.025000
Training Epoch: 69 [42112/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 69 [42240/50000]	Loss: 0.5217	LR: 0.025000
Training Epoch: 69 [42368/50000]	Loss: 0.3491	LR: 0.025000
Training Epoch: 69 [42496/50000]	Loss: 0.4360	LR: 0.025000
Training Epoch: 69 [42624/50000]	Loss: 0.4842	LR: 0.025000
Training Epoch: 69 [42752/50000]	Loss: 0.4380	LR: 0.025000
Training Epoch: 69 [42880/50000]	Loss: 0.6031	LR: 0.025000
Training Epoch: 69 [43008/50000]	Loss: 0.5690	LR: 0.025000
Training Epoch: 69 [43136/50000]	Loss: 0.7128	LR: 0.025000
Training Epoch: 69 [43264/50000]	Loss: 0.4580	LR: 0.025000
Training Epoch: 69 [43392/50000]	Loss: 0.5673	LR: 0.025000
Training Epoch: 69 [43520/50000]	Loss: 0.4717	LR: 0.025000
Training Epoch: 69 [43648/50000]	Loss: 0.4601	LR: 0.025000
Training Epoch: 69 [43776/50000]	Loss: 0.4821	LR: 0.025000
Training Epoch: 69 [43904/50000]	Loss: 0.4675	LR: 0.025000
Training Epoch: 69 [44032/50000]	Loss: 0.4326	LR: 0.025000
Training Epoch: 69 [44160/50000]	Loss: 0.6684	LR: 0.025000
Training Epoch: 69 [44288/50000]	Loss: 0.5758	LR: 0.025000
Training Epoch: 69 [44416/50000]	Loss: 0.3690	LR: 0.025000
Training Epoch: 69 [44544/50000]	Loss: 0.4335	LR: 0.025000
Training Epoch: 69 [44672/50000]	Loss: 0.5627	LR: 0.025000
Training Epoch: 69 [44800/50000]	Loss: 0.3920	LR: 0.025000
Training Epoch: 69 [44928/50000]	Loss: 0.5065	LR: 0.025000
Training Epoch: 69 [45056/50000]	Loss: 0.6222	LR: 0.025000
Training Epoch: 69 [45184/50000]	Loss: 0.6411	LR: 0.025000
Training Epoch: 69 [45312/50000]	Loss: 0.5229	LR: 0.025000
Training Epoch: 69 [45440/50000]	Loss: 0.5132	LR: 0.025000
Training Epoch: 69 [45568/50000]	Loss: 0.5903	LR: 0.025000
Training Epoch: 69 [45696/50000]	Loss: 0.5542	LR: 0.025000
Training Epoch: 69 [45824/50000]	Loss: 0.2413	LR: 0.025000
Training Epoch: 69 [45952/50000]	Loss: 0.5593	LR: 0.025000
Training Epoch: 69 [46080/50000]	Loss: 0.6232	LR: 0.025000
Training Epoch: 69 [46208/50000]	Loss: 0.4466	LR: 0.025000
Training Epoch: 69 [46336/50000]	Loss: 0.5065	LR: 0.025000
Training Epoch: 69 [46464/50000]	Loss: 0.5501	LR: 0.025000
Training Epoch: 69 [46592/50000]	Loss: 0.5073	LR: 0.025000
Training Epoch: 69 [46720/50000]	Loss: 0.5076	LR: 0.025000
Training Epoch: 69 [46848/50000]	Loss: 0.5326	LR: 0.025000
Training Epoch: 69 [46976/50000]	Loss: 0.5472	LR: 0.025000
Training Epoch: 69 [47104/50000]	Loss: 0.6994	LR: 0.025000
Training Epoch: 69 [47232/50000]	Loss: 0.3650	LR: 0.025000
Training Epoch: 69 [47360/50000]	Loss: 0.5433	LR: 0.025000
Training Epoch: 69 [47488/50000]	Loss: 0.4817	LR: 0.025000
Training Epoch: 69 [47616/50000]	Loss: 0.5173	LR: 0.025000
Training Epoch: 69 [47744/50000]	Loss: 0.6427	LR: 0.025000
Training Epoch: 69 [47872/50000]	Loss: 0.5199	LR: 0.025000
Training Epoch: 69 [48000/50000]	Loss: 0.6229	LR: 0.025000
Training Epoch: 69 [48128/50000]	Loss: 0.4334	LR: 0.025000
Training Epoch: 69 [48256/50000]	Loss: 0.5939	LR: 0.025000
Training Epoch: 69 [48384/50000]	Loss: 0.6335	LR: 0.025000
Training Epoch: 69 [48512/50000]	Loss: 0.5009	LR: 0.025000
Training Epoch: 69 [48640/50000]	Loss: 0.4113	LR: 0.025000
Training Epoch: 69 [48768/50000]	Loss: 0.3302	LR: 0.025000
Training Epoch: 69 [48896/50000]	Loss: 0.5445	LR: 0.025000
Training Epoch: 69 [49024/50000]	Loss: 0.6094	LR: 0.025000
Training Epoch: 69 [49152/50000]	Loss: 0.4685	LR: 0.025000
Training Epoch: 69 [49280/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 69 [49408/50000]	Loss: 0.5498	LR: 0.025000
Training Epoch: 69 [49536/50000]	Loss: 0.4818	LR: 0.025000
Training Epoch: 69 [49664/50000]	Loss: 0.5726	LR: 0.025000
Training Epoch: 69 [49792/50000]	Loss: 0.5688	LR: 0.025000
Training Epoch: 69 [49920/50000]	Loss: 0.4806	LR: 0.025000
Training Epoch: 69 [50000/50000]	Loss: 0.5866	LR: 0.025000
epoch 69 training time consumed: 53.93s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  253234 GB |  253234 GB |
|       from large pool |  123392 KB |    1034 MB |  252984 GB |  252984 GB |
|       from small pool |   10798 KB |      13 MB |     249 GB |     249 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  253234 GB |  253234 GB |
|       from large pool |  123392 KB |    1034 MB |  252984 GB |  252984 GB |
|       from small pool |   10798 KB |      13 MB |     249 GB |     249 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  111438 GB |  111438 GB |
|       from large pool |  155136 KB |  433088 KB |  111162 GB |  111162 GB |
|       from small pool |    1490 KB |    3494 KB |     275 GB |     275 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    9771 K  |    9771 K  |
|       from large pool |      24    |      65    |    5100 K  |    5100 K  |
|       from small pool |     231    |     274    |    4671 K  |    4670 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    9771 K  |    9771 K  |
|       from large pool |      24    |      65    |    5100 K  |    5100 K  |
|       from small pool |     231    |     274    |    4671 K  |    4670 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4828 K  |    4828 K  |
|       from large pool |       9    |      14    |    2468 K  |    2468 K  |
|       from small pool |      12    |      16    |    2359 K  |    2359 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 69, Average loss: 0.0112, Accuracy: 0.6587, Time consumed:3.47s

Training Epoch: 70 [128/50000]	Loss: 0.4691	LR: 0.025000
Training Epoch: 70 [256/50000]	Loss: 0.3275	LR: 0.025000
Training Epoch: 70 [384/50000]	Loss: 0.3927	LR: 0.025000
Training Epoch: 70 [512/50000]	Loss: 0.4552	LR: 0.025000
Training Epoch: 70 [640/50000]	Loss: 0.2590	LR: 0.025000
Training Epoch: 70 [768/50000]	Loss: 0.3383	LR: 0.025000
Training Epoch: 70 [896/50000]	Loss: 0.3306	LR: 0.025000
Training Epoch: 70 [1024/50000]	Loss: 0.3667	LR: 0.025000
Training Epoch: 70 [1152/50000]	Loss: 0.4007	LR: 0.025000
Training Epoch: 70 [1280/50000]	Loss: 0.4490	LR: 0.025000
Training Epoch: 70 [1408/50000]	Loss: 0.4442	LR: 0.025000
Training Epoch: 70 [1536/50000]	Loss: 0.3540	LR: 0.025000
Training Epoch: 70 [1664/50000]	Loss: 0.3617	LR: 0.025000
Training Epoch: 70 [1792/50000]	Loss: 0.4685	LR: 0.025000
Training Epoch: 70 [1920/50000]	Loss: 0.2503	LR: 0.025000
Training Epoch: 70 [2048/50000]	Loss: 0.3482	LR: 0.025000
Training Epoch: 70 [2176/50000]	Loss: 0.3876	LR: 0.025000
Training Epoch: 70 [2304/50000]	Loss: 0.4756	LR: 0.025000
Training Epoch: 70 [2432/50000]	Loss: 0.4043	LR: 0.025000
Training Epoch: 70 [2560/50000]	Loss: 0.3158	LR: 0.025000
Training Epoch: 70 [2688/50000]	Loss: 0.4751	LR: 0.025000
Training Epoch: 70 [2816/50000]	Loss: 0.3946	LR: 0.025000
Training Epoch: 70 [2944/50000]	Loss: 0.5998	LR: 0.025000
Training Epoch: 70 [3072/50000]	Loss: 0.3533	LR: 0.025000
Training Epoch: 70 [3200/50000]	Loss: 0.3919	LR: 0.025000
Training Epoch: 70 [3328/50000]	Loss: 0.3163	LR: 0.025000
Training Epoch: 70 [3456/50000]	Loss: 0.2337	LR: 0.025000
Training Epoch: 70 [3584/50000]	Loss: 0.3782	LR: 0.025000
Training Epoch: 70 [3712/50000]	Loss: 0.4182	LR: 0.025000
Training Epoch: 70 [3840/50000]	Loss: 0.2891	LR: 0.025000
Training Epoch: 70 [3968/50000]	Loss: 0.3341	LR: 0.025000
Training Epoch: 70 [4096/50000]	Loss: 0.3836	LR: 0.025000
Training Epoch: 70 [4224/50000]	Loss: 0.3865	LR: 0.025000
Training Epoch: 70 [4352/50000]	Loss: 0.2845	LR: 0.025000
Training Epoch: 70 [4480/50000]	Loss: 0.4487	LR: 0.025000
Training Epoch: 70 [4608/50000]	Loss: 0.3258	LR: 0.025000
Training Epoch: 70 [4736/50000]	Loss: 0.4004	LR: 0.025000
Training Epoch: 70 [4864/50000]	Loss: 0.2796	LR: 0.025000
Training Epoch: 70 [4992/50000]	Loss: 0.3344	LR: 0.025000
Training Epoch: 70 [5120/50000]	Loss: 0.3399	LR: 0.025000
Training Epoch: 70 [5248/50000]	Loss: 0.3185	LR: 0.025000
Training Epoch: 70 [5376/50000]	Loss: 0.4524	LR: 0.025000
Training Epoch: 70 [5504/50000]	Loss: 0.4678	LR: 0.025000
Training Epoch: 70 [5632/50000]	Loss: 0.5442	LR: 0.025000
Training Epoch: 70 [5760/50000]	Loss: 0.3955	LR: 0.025000
Training Epoch: 70 [5888/50000]	Loss: 0.3177	LR: 0.025000
Training Epoch: 70 [6016/50000]	Loss: 0.4306	LR: 0.025000
Training Epoch: 70 [6144/50000]	Loss: 0.4341	LR: 0.025000
Training Epoch: 70 [6272/50000]	Loss: 0.4052	LR: 0.025000
Training Epoch: 70 [6400/50000]	Loss: 0.5319	LR: 0.025000
Training Epoch: 70 [6528/50000]	Loss: 0.4078	LR: 0.025000
Training Epoch: 70 [6656/50000]	Loss: 0.4021	LR: 0.025000
Training Epoch: 70 [6784/50000]	Loss: 0.3040	LR: 0.025000
Training Epoch: 70 [6912/50000]	Loss: 0.4212	LR: 0.025000
Training Epoch: 70 [7040/50000]	Loss: 0.4390	LR: 0.025000
Training Epoch: 70 [7168/50000]	Loss: 0.3484	LR: 0.025000
Training Epoch: 70 [7296/50000]	Loss: 0.3577	LR: 0.025000
Training Epoch: 70 [7424/50000]	Loss: 0.3476	LR: 0.025000
Training Epoch: 70 [7552/50000]	Loss: 0.3131	LR: 0.025000
Training Epoch: 70 [7680/50000]	Loss: 0.3092	LR: 0.025000
Training Epoch: 70 [7808/50000]	Loss: 0.4803	LR: 0.025000
Training Epoch: 70 [7936/50000]	Loss: 0.4062	LR: 0.025000
Training Epoch: 70 [8064/50000]	Loss: 0.4498	LR: 0.025000
Training Epoch: 70 [8192/50000]	Loss: 0.4227	LR: 0.025000
Training Epoch: 70 [8320/50000]	Loss: 0.5448	LR: 0.025000
Training Epoch: 70 [8448/50000]	Loss: 0.4059	LR: 0.025000
Training Epoch: 70 [8576/50000]	Loss: 0.3852	LR: 0.025000
Training Epoch: 70 [8704/50000]	Loss: 0.2626	LR: 0.025000
Training Epoch: 70 [8832/50000]	Loss: 0.3449	LR: 0.025000
Training Epoch: 70 [8960/50000]	Loss: 0.3752	LR: 0.025000
Training Epoch: 70 [9088/50000]	Loss: 0.5105	LR: 0.025000
Training Epoch: 70 [9216/50000]	Loss: 0.4402	LR: 0.025000
Training Epoch: 70 [9344/50000]	Loss: 0.4510	LR: 0.025000
Training Epoch: 70 [9472/50000]	Loss: 0.4112	LR: 0.025000
Training Epoch: 70 [9600/50000]	Loss: 0.3420	LR: 0.025000
Training Epoch: 70 [9728/50000]	Loss: 0.3651	LR: 0.025000
Training Epoch: 70 [9856/50000]	Loss: 0.3026	LR: 0.025000
Training Epoch: 70 [9984/50000]	Loss: 0.4145	LR: 0.025000
Training Epoch: 70 [10112/50000]	Loss: 0.4679	LR: 0.025000
Training Epoch: 70 [10240/50000]	Loss: 0.4615	LR: 0.025000
Training Epoch: 70 [10368/50000]	Loss: 0.4182	LR: 0.025000
Training Epoch: 70 [10496/50000]	Loss: 0.4183	LR: 0.025000
Training Epoch: 70 [10624/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 70 [10752/50000]	Loss: 0.2147	LR: 0.025000
Training Epoch: 70 [10880/50000]	Loss: 0.4250	LR: 0.025000
Training Epoch: 70 [11008/50000]	Loss: 0.5393	LR: 0.025000
Training Epoch: 70 [11136/50000]	Loss: 0.3876	LR: 0.025000
Training Epoch: 70 [11264/50000]	Loss: 0.4651	LR: 0.025000
Training Epoch: 70 [11392/50000]	Loss: 0.3838	LR: 0.025000
Training Epoch: 70 [11520/50000]	Loss: 0.4426	LR: 0.025000
Training Epoch: 70 [11648/50000]	Loss: 0.3351	LR: 0.025000
Training Epoch: 70 [11776/50000]	Loss: 0.2579	LR: 0.025000
Training Epoch: 70 [11904/50000]	Loss: 0.4050	LR: 0.025000
Training Epoch: 70 [12032/50000]	Loss: 0.3003	LR: 0.025000
Training Epoch: 70 [12160/50000]	Loss: 0.4768	LR: 0.025000
Training Epoch: 70 [12288/50000]	Loss: 0.4336	LR: 0.025000
Training Epoch: 70 [12416/50000]	Loss: 0.5646	LR: 0.025000
Training Epoch: 70 [12544/50000]	Loss: 0.3693	LR: 0.025000
Training Epoch: 70 [12672/50000]	Loss: 0.3852	LR: 0.025000
Training Epoch: 70 [12800/50000]	Loss: 0.3878	LR: 0.025000
Training Epoch: 70 [12928/50000]	Loss: 0.3428	LR: 0.025000
Training Epoch: 70 [13056/50000]	Loss: 0.4432	LR: 0.025000
Training Epoch: 70 [13184/50000]	Loss: 0.4452	LR: 0.025000
Training Epoch: 70 [13312/50000]	Loss: 0.3766	LR: 0.025000
Training Epoch: 70 [13440/50000]	Loss: 0.3493	LR: 0.025000
Training Epoch: 70 [13568/50000]	Loss: 0.4291	LR: 0.025000
Training Epoch: 70 [13696/50000]	Loss: 0.3984	LR: 0.025000
Training Epoch: 70 [13824/50000]	Loss: 0.4240	LR: 0.025000
Training Epoch: 70 [13952/50000]	Loss: 0.2109	LR: 0.025000
Training Epoch: 70 [14080/50000]	Loss: 0.3601	LR: 0.025000
Training Epoch: 70 [14208/50000]	Loss: 0.5038	LR: 0.025000
Training Epoch: 70 [14336/50000]	Loss: 0.3997	LR: 0.025000
Training Epoch: 70 [14464/50000]	Loss: 0.4836	LR: 0.025000
Training Epoch: 70 [14592/50000]	Loss: 0.4893	LR: 0.025000
Training Epoch: 70 [14720/50000]	Loss: 0.5040	LR: 0.025000
Training Epoch: 70 [14848/50000]	Loss: 0.4054	LR: 0.025000
Training Epoch: 70 [14976/50000]	Loss: 0.4236	LR: 0.025000
Training Epoch: 70 [15104/50000]	Loss: 0.3307	LR: 0.025000
Training Epoch: 70 [15232/50000]	Loss: 0.3342	LR: 0.025000
Training Epoch: 70 [15360/50000]	Loss: 0.4596	LR: 0.025000
Training Epoch: 70 [15488/50000]	Loss: 0.4302	LR: 0.025000
Training Epoch: 70 [15616/50000]	Loss: 0.4837	LR: 0.025000
Training Epoch: 70 [15744/50000]	Loss: 0.4174	LR: 0.025000
Training Epoch: 70 [15872/50000]	Loss: 0.3409	LR: 0.025000
Training Epoch: 70 [16000/50000]	Loss: 0.6092	LR: 0.025000
Training Epoch: 70 [16128/50000]	Loss: 0.4348	LR: 0.025000
Training Epoch: 70 [16256/50000]	Loss: 0.3295	LR: 0.025000
Training Epoch: 70 [16384/50000]	Loss: 0.4199	LR: 0.025000
Training Epoch: 70 [16512/50000]	Loss: 0.3126	LR: 0.025000
Training Epoch: 70 [16640/50000]	Loss: 0.4177	LR: 0.025000
Training Epoch: 70 [16768/50000]	Loss: 0.5155	LR: 0.025000
Training Epoch: 70 [16896/50000]	Loss: 0.3839	LR: 0.025000
Training Epoch: 70 [17024/50000]	Loss: 0.4627	LR: 0.025000
Training Epoch: 70 [17152/50000]	Loss: 0.3609	LR: 0.025000
Training Epoch: 70 [17280/50000]	Loss: 0.5014	LR: 0.025000
Training Epoch: 70 [17408/50000]	Loss: 0.4330	LR: 0.025000
Training Epoch: 70 [17536/50000]	Loss: 0.3690	LR: 0.025000
Training Epoch: 70 [17664/50000]	Loss: 0.3986	LR: 0.025000
Training Epoch: 70 [17792/50000]	Loss: 0.4447	LR: 0.025000
Training Epoch: 70 [17920/50000]	Loss: 0.3961	LR: 0.025000
Training Epoch: 70 [18048/50000]	Loss: 0.3900	LR: 0.025000
Training Epoch: 70 [18176/50000]	Loss: 0.3013	LR: 0.025000
Training Epoch: 70 [18304/50000]	Loss: 0.2923	LR: 0.025000
Training Epoch: 70 [18432/50000]	Loss: 0.4728	LR: 0.025000
Training Epoch: 70 [18560/50000]	Loss: 0.4012	LR: 0.025000
Training Epoch: 70 [18688/50000]	Loss: 0.4084	LR: 0.025000
Training Epoch: 70 [18816/50000]	Loss: 0.3714	LR: 0.025000
Training Epoch: 70 [18944/50000]	Loss: 0.5330	LR: 0.025000
Training Epoch: 70 [19072/50000]	Loss: 0.5450	LR: 0.025000
Training Epoch: 70 [19200/50000]	Loss: 0.3125	LR: 0.025000
Training Epoch: 70 [19328/50000]	Loss: 0.3681	LR: 0.025000
Training Epoch: 70 [19456/50000]	Loss: 0.4414	LR: 0.025000
Training Epoch: 70 [19584/50000]	Loss: 0.4665	LR: 0.025000
Training Epoch: 70 [19712/50000]	Loss: 0.4641	LR: 0.025000
Training Epoch: 70 [19840/50000]	Loss: 0.3429	LR: 0.025000
Training Epoch: 70 [19968/50000]	Loss: 0.4462	LR: 0.025000
Training Epoch: 70 [20096/50000]	Loss: 0.3756	LR: 0.025000
Training Epoch: 70 [20224/50000]	Loss: 0.4859	LR: 0.025000
Training Epoch: 70 [20352/50000]	Loss: 0.5368	LR: 0.025000
Training Epoch: 70 [20480/50000]	Loss: 0.4979	LR: 0.025000
Training Epoch: 70 [20608/50000]	Loss: 0.3270	LR: 0.025000
Training Epoch: 70 [20736/50000]	Loss: 0.4086	LR: 0.025000
Training Epoch: 70 [20864/50000]	Loss: 0.4071	LR: 0.025000
Training Epoch: 70 [20992/50000]	Loss: 0.4309	LR: 0.025000
Training Epoch: 70 [21120/50000]	Loss: 0.3581	LR: 0.025000
Training Epoch: 70 [21248/50000]	Loss: 0.4903	LR: 0.025000
Training Epoch: 70 [21376/50000]	Loss: 0.4798	LR: 0.025000
Training Epoch: 70 [21504/50000]	Loss: 0.3265	LR: 0.025000
Training Epoch: 70 [21632/50000]	Loss: 0.3598	LR: 0.025000
Training Epoch: 70 [21760/50000]	Loss: 0.5198	LR: 0.025000
Training Epoch: 70 [21888/50000]	Loss: 0.5164	LR: 0.025000
Training Epoch: 70 [22016/50000]	Loss: 0.3331	LR: 0.025000
Training Epoch: 70 [22144/50000]	Loss: 0.4003	LR: 0.025000
Training Epoch: 70 [22272/50000]	Loss: 0.4784	LR: 0.025000
Training Epoch: 70 [22400/50000]	Loss: 0.4670	LR: 0.025000
Training Epoch: 70 [22528/50000]	Loss: 0.3629	LR: 0.025000
Training Epoch: 70 [22656/50000]	Loss: 0.4506	LR: 0.025000
Training Epoch: 70 [22784/50000]	Loss: 0.4000	LR: 0.025000
Training Epoch: 70 [22912/50000]	Loss: 0.3635	LR: 0.025000
Training Epoch: 70 [23040/50000]	Loss: 0.5484	LR: 0.025000
Training Epoch: 70 [23168/50000]	Loss: 0.4514	LR: 0.025000
Training Epoch: 70 [23296/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 70 [23424/50000]	Loss: 0.5797	LR: 0.025000
Training Epoch: 70 [23552/50000]	Loss: 0.4142	LR: 0.025000
Training Epoch: 70 [23680/50000]	Loss: 0.4330	LR: 0.025000
Training Epoch: 70 [23808/50000]	Loss: 0.5427	LR: 0.025000
Training Epoch: 70 [23936/50000]	Loss: 0.4851	LR: 0.025000
Training Epoch: 70 [24064/50000]	Loss: 0.5227	LR: 0.025000
Training Epoch: 70 [24192/50000]	Loss: 0.5120	LR: 0.025000
Training Epoch: 70 [24320/50000]	Loss: 0.5370	LR: 0.025000
Training Epoch: 70 [24448/50000]	Loss: 0.5375	LR: 0.025000
Training Epoch: 70 [24576/50000]	Loss: 0.3574	LR: 0.025000
Training Epoch: 70 [24704/50000]	Loss: 0.4839	LR: 0.025000
Training Epoch: 70 [24832/50000]	Loss: 0.5127	LR: 0.025000
Training Epoch: 70 [24960/50000]	Loss: 0.5201	LR: 0.025000
Training Epoch: 70 [25088/50000]	Loss: 0.4212	LR: 0.025000
Training Epoch: 70 [25216/50000]	Loss: 0.5145	LR: 0.025000
Training Epoch: 70 [25344/50000]	Loss: 0.4889	LR: 0.025000
Training Epoch: 70 [25472/50000]	Loss: 0.4751	LR: 0.025000
Training Epoch: 70 [25600/50000]	Loss: 0.4669	LR: 0.025000
Training Epoch: 70 [25728/50000]	Loss: 0.4757	LR: 0.025000
Training Epoch: 70 [25856/50000]	Loss: 0.6172	LR: 0.025000
Training Epoch: 70 [25984/50000]	Loss: 0.4709	LR: 0.025000
Training Epoch: 70 [26112/50000]	Loss: 0.4606	LR: 0.025000
Training Epoch: 70 [26240/50000]	Loss: 0.5840	LR: 0.025000
Training Epoch: 70 [26368/50000]	Loss: 0.4268	LR: 0.025000
Training Epoch: 70 [26496/50000]	Loss: 0.6300	LR: 0.025000
Training Epoch: 70 [26624/50000]	Loss: 0.5733	LR: 0.025000
Training Epoch: 70 [26752/50000]	Loss: 0.5561	LR: 0.025000
Training Epoch: 70 [26880/50000]	Loss: 0.4606	LR: 0.025000
Training Epoch: 70 [27008/50000]	Loss: 0.4554	LR: 0.025000
Training Epoch: 70 [27136/50000]	Loss: 0.4033	LR: 0.025000
Training Epoch: 70 [27264/50000]	Loss: 0.6164	LR: 0.025000
Training Epoch: 70 [27392/50000]	Loss: 0.3747	LR: 0.025000
Training Epoch: 70 [27520/50000]	Loss: 0.5098	LR: 0.025000
Training Epoch: 70 [27648/50000]	Loss: 0.3977	LR: 0.025000
Training Epoch: 70 [27776/50000]	Loss: 0.7377	LR: 0.025000
Training Epoch: 70 [27904/50000]	Loss: 0.4449	LR: 0.025000
Training Epoch: 70 [28032/50000]	Loss: 0.4134	LR: 0.025000
Training Epoch: 70 [28160/50000]	Loss: 0.4361	LR: 0.025000
Training Epoch: 70 [28288/50000]	Loss: 0.6030	LR: 0.025000
Training Epoch: 70 [28416/50000]	Loss: 0.5348	LR: 0.025000
Training Epoch: 70 [28544/50000]	Loss: 0.5356	LR: 0.025000
Training Epoch: 70 [28672/50000]	Loss: 0.4718	LR: 0.025000
Training Epoch: 70 [28800/50000]	Loss: 0.3640	LR: 0.025000
Training Epoch: 70 [28928/50000]	Loss: 0.4229	LR: 0.025000
Training Epoch: 70 [29056/50000]	Loss: 0.4004	LR: 0.025000
Training Epoch: 70 [29184/50000]	Loss: 0.3632	LR: 0.025000
Training Epoch: 70 [29312/50000]	Loss: 0.5312	LR: 0.025000
Training Epoch: 70 [29440/50000]	Loss: 0.4050	LR: 0.025000
Training Epoch: 70 [29568/50000]	Loss: 0.3725	LR: 0.025000
Training Epoch: 70 [29696/50000]	Loss: 0.3970	LR: 0.025000
Training Epoch: 70 [29824/50000]	Loss: 0.5262	LR: 0.025000
Training Epoch: 70 [29952/50000]	Loss: 0.3717	LR: 0.025000
Training Epoch: 70 [30080/50000]	Loss: 0.6035	LR: 0.025000
Training Epoch: 70 [30208/50000]	Loss: 0.4631	LR: 0.025000
Training Epoch: 70 [30336/50000]	Loss: 0.4298	LR: 0.025000
Training Epoch: 70 [30464/50000]	Loss: 0.4996	LR: 0.025000
Training Epoch: 70 [30592/50000]	Loss: 0.4413	LR: 0.025000
Training Epoch: 70 [30720/50000]	Loss: 0.5356	LR: 0.025000
Training Epoch: 70 [30848/50000]	Loss: 0.5235	LR: 0.025000
Training Epoch: 70 [30976/50000]	Loss: 0.5309	LR: 0.025000
Training Epoch: 70 [31104/50000]	Loss: 0.7619	LR: 0.025000
Training Epoch: 70 [31232/50000]	Loss: 0.3589	LR: 0.025000
Training Epoch: 70 [31360/50000]	Loss: 0.3288	LR: 0.025000
Training Epoch: 70 [31488/50000]	Loss: 0.3859	LR: 0.025000
Training Epoch: 70 [31616/50000]	Loss: 0.5362	LR: 0.025000
Training Epoch: 70 [31744/50000]	Loss: 0.4587	LR: 0.025000
Training Epoch: 70 [31872/50000]	Loss: 0.6306	LR: 0.025000
Training Epoch: 70 [32000/50000]	Loss: 0.4710	LR: 0.025000
Training Epoch: 70 [32128/50000]	Loss: 0.3973	LR: 0.025000
Training Epoch: 70 [32256/50000]	Loss: 0.5734	LR: 0.025000
Training Epoch: 70 [32384/50000]	Loss: 0.4161	LR: 0.025000
Training Epoch: 70 [32512/50000]	Loss: 0.6707	LR: 0.025000
Training Epoch: 70 [32640/50000]	Loss: 0.6065	LR: 0.025000
Training Epoch: 70 [32768/50000]	Loss: 0.4525	LR: 0.025000
Training Epoch: 70 [32896/50000]	Loss: 0.4030	LR: 0.025000
Training Epoch: 70 [33024/50000]	Loss: 0.5590	LR: 0.025000
Training Epoch: 70 [33152/50000]	Loss: 0.5056	LR: 0.025000
Training Epoch: 70 [33280/50000]	Loss: 0.4395	LR: 0.025000
Training Epoch: 70 [33408/50000]	Loss: 0.6387	LR: 0.025000
Training Epoch: 70 [33536/50000]	Loss: 0.6032	LR: 0.025000
Training Epoch: 70 [33664/50000]	Loss: 0.3583	LR: 0.025000
Training Epoch: 70 [33792/50000]	Loss: 0.4399	LR: 0.025000
Training Epoch: 70 [33920/50000]	Loss: 0.6240	LR: 0.025000
Training Epoch: 70 [34048/50000]	Loss: 0.5237	LR: 0.025000
Training Epoch: 70 [34176/50000]	Loss: 0.5786	LR: 0.025000
Training Epoch: 70 [34304/50000]	Loss: 0.4497	LR: 0.025000
Training Epoch: 70 [34432/50000]	Loss: 0.4523	LR: 0.025000
Training Epoch: 70 [34560/50000]	Loss: 0.6182	LR: 0.025000
Training Epoch: 70 [34688/50000]	Loss: 0.4131	LR: 0.025000
Training Epoch: 70 [34816/50000]	Loss: 0.4096	LR: 0.025000
Training Epoch: 70 [34944/50000]	Loss: 0.4720	LR: 0.025000
Training Epoch: 70 [35072/50000]	Loss: 0.4977	LR: 0.025000
Training Epoch: 70 [35200/50000]	Loss: 0.3857	LR: 0.025000
Training Epoch: 70 [35328/50000]	Loss: 0.4985	LR: 0.025000
Training Epoch: 70 [35456/50000]	Loss: 0.6170	LR: 0.025000
Training Epoch: 70 [35584/50000]	Loss: 0.5530	LR: 0.025000
Training Epoch: 70 [35712/50000]	Loss: 0.3443	LR: 0.025000
Training Epoch: 70 [35840/50000]	Loss: 0.5348	LR: 0.025000
Training Epoch: 70 [35968/50000]	Loss: 0.5733	LR: 0.025000
Training Epoch: 70 [36096/50000]	Loss: 0.6061	LR: 0.025000
Training Epoch: 70 [36224/50000]	Loss: 0.5292	LR: 0.025000
Training Epoch: 70 [36352/50000]	Loss: 0.5316	LR: 0.025000
Training Epoch: 70 [36480/50000]	Loss: 0.4806	LR: 0.025000
Training Epoch: 70 [36608/50000]	Loss: 0.5662	LR: 0.025000
Training Epoch: 70 [36736/50000]	Loss: 0.4553	LR: 0.025000
Training Epoch: 70 [36864/50000]	Loss: 0.4077	LR: 0.025000
Training Epoch: 70 [36992/50000]	Loss: 0.5336	LR: 0.025000
Training Epoch: 70 [37120/50000]	Loss: 0.5813	LR: 0.025000
Training Epoch: 70 [37248/50000]	Loss: 0.4488	LR: 0.025000
Training Epoch: 70 [37376/50000]	Loss: 0.7147	LR: 0.025000
Training Epoch: 70 [37504/50000]	Loss: 0.4997	LR: 0.025000
Training Epoch: 70 [37632/50000]	Loss: 0.5298	LR: 0.025000
Training Epoch: 70 [37760/50000]	Loss: 0.5184	LR: 0.025000
Training Epoch: 70 [37888/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 70 [38016/50000]	Loss: 0.6139	LR: 0.025000
Training Epoch: 70 [38144/50000]	Loss: 0.4630	LR: 0.025000
Training Epoch: 70 [38272/50000]	Loss: 0.5017	LR: 0.025000
Training Epoch: 70 [38400/50000]	Loss: 0.4770	LR: 0.025000
Training Epoch: 70 [38528/50000]	Loss: 0.4087	LR: 0.025000
Training Epoch: 70 [38656/50000]	Loss: 0.4704	LR: 0.025000
Training Epoch: 70 [38784/50000]	Loss: 0.4083	LR: 0.025000
Training Epoch: 70 [38912/50000]	Loss: 0.6175	LR: 0.025000
Training Epoch: 70 [39040/50000]	Loss: 0.4205	LR: 0.025000
Training Epoch: 70 [39168/50000]	Loss: 0.4567	LR: 0.025000
Training Epoch: 70 [39296/50000]	Loss: 0.4083	LR: 0.025000
Training Epoch: 70 [39424/50000]	Loss: 0.5983	LR: 0.025000
Training Epoch: 70 [39552/50000]	Loss: 0.4934	LR: 0.025000
Training Epoch: 70 [39680/50000]	Loss: 0.6329	LR: 0.025000
Training Epoch: 70 [39808/50000]	Loss: 0.5225	LR: 0.025000
Training Epoch: 70 [39936/50000]	Loss: 0.5202	LR: 0.025000
Training Epoch: 70 [40064/50000]	Loss: 0.5950	LR: 0.025000
Training Epoch: 70 [40192/50000]	Loss: 0.6657	LR: 0.025000
Training Epoch: 70 [40320/50000]	Loss: 0.5190	LR: 0.025000
Training Epoch: 70 [40448/50000]	Loss: 0.5934	LR: 0.025000
Training Epoch: 70 [40576/50000]	Loss: 0.4902	LR: 0.025000
Training Epoch: 70 [40704/50000]	Loss: 0.5116	LR: 0.025000
Training Epoch: 70 [40832/50000]	Loss: 0.5042	LR: 0.025000
Training Epoch: 70 [40960/50000]	Loss: 0.5427	LR: 0.025000
Training Epoch: 70 [41088/50000]	Loss: 0.6412	LR: 0.025000
Training Epoch: 70 [41216/50000]	Loss: 0.5848	LR: 0.025000
Training Epoch: 70 [41344/50000]	Loss: 0.4515	LR: 0.025000
Training Epoch: 70 [41472/50000]	Loss: 0.5891	LR: 0.025000
Training Epoch: 70 [41600/50000]	Loss: 0.4996	LR: 0.025000
Training Epoch: 70 [41728/50000]	Loss: 0.5445	LR: 0.025000
Training Epoch: 70 [41856/50000]	Loss: 0.6538	LR: 0.025000
Training Epoch: 70 [41984/50000]	Loss: 0.4504	LR: 0.025000
Training Epoch: 70 [42112/50000]	Loss: 0.4609	LR: 0.025000
Training Epoch: 70 [42240/50000]	Loss: 0.3262	LR: 0.025000
Training Epoch: 70 [42368/50000]	Loss: 0.5011	LR: 0.025000
Training Epoch: 70 [42496/50000]	Loss: 0.6686	LR: 0.025000
Training Epoch: 70 [42624/50000]	Loss: 0.5614	LR: 0.025000
Training Epoch: 70 [42752/50000]	Loss: 0.5747	LR: 0.025000
Training Epoch: 70 [42880/50000]	Loss: 0.5219	LR: 0.025000
Training Epoch: 70 [43008/50000]	Loss: 0.4788	LR: 0.025000
Training Epoch: 70 [43136/50000]	Loss: 0.4723	LR: 0.025000
Training Epoch: 70 [43264/50000]	Loss: 0.4503	LR: 0.025000
Training Epoch: 70 [43392/50000]	Loss: 0.5700	LR: 0.025000
Training Epoch: 70 [43520/50000]	Loss: 0.8162	LR: 0.025000
Training Epoch: 70 [43648/50000]	Loss: 0.6589	LR: 0.025000
Training Epoch: 70 [43776/50000]	Loss: 0.4440	LR: 0.025000
Training Epoch: 70 [43904/50000]	Loss: 0.5352	LR: 0.025000
Training Epoch: 70 [44032/50000]	Loss: 0.4350	LR: 0.025000
Training Epoch: 70 [44160/50000]	Loss: 0.5515	LR: 0.025000
Training Epoch: 70 [44288/50000]	Loss: 0.3821	LR: 0.025000
Training Epoch: 70 [44416/50000]	Loss: 0.4862	LR: 0.025000
Training Epoch: 70 [44544/50000]	Loss: 0.4017	LR: 0.025000
Training Epoch: 70 [44672/50000]	Loss: 0.6230	LR: 0.025000
Training Epoch: 70 [44800/50000]	Loss: 0.5861	LR: 0.025000
Training Epoch: 70 [44928/50000]	Loss: 0.4148	LR: 0.025000
Training Epoch: 70 [45056/50000]	Loss: 0.5588	LR: 0.025000
Training Epoch: 70 [45184/50000]	Loss: 0.5274	LR: 0.025000
Training Epoch: 70 [45312/50000]	Loss: 0.6905	LR: 0.025000
Training Epoch: 70 [45440/50000]	Loss: 0.4384	LR: 0.025000
Training Epoch: 70 [45568/50000]	Loss: 0.5301	LR: 0.025000
Training Epoch: 70 [45696/50000]	Loss: 0.5109	LR: 0.025000
Training Epoch: 70 [45824/50000]	Loss: 0.4929	LR: 0.025000
Training Epoch: 70 [45952/50000]	Loss: 0.5183	LR: 0.025000
Training Epoch: 70 [46080/50000]	Loss: 0.7085	LR: 0.025000
Training Epoch: 70 [46208/50000]	Loss: 0.5936	LR: 0.025000
Training Epoch: 70 [46336/50000]	Loss: 0.4451	LR: 0.025000
Training Epoch: 70 [46464/50000]	Loss: 0.4407	LR: 0.025000
Training Epoch: 70 [46592/50000]	Loss: 0.5741	LR: 0.025000
Training Epoch: 70 [46720/50000]	Loss: 0.5741	LR: 0.025000
Training Epoch: 70 [46848/50000]	Loss: 0.5236	LR: 0.025000
Training Epoch: 70 [46976/50000]	Loss: 0.5793	LR: 0.025000
Training Epoch: 70 [47104/50000]	Loss: 0.4501	LR: 0.025000
Training Epoch: 70 [47232/50000]	Loss: 0.4583	LR: 0.025000
Training Epoch: 70 [47360/50000]	Loss: 0.5886	LR: 0.025000
Training Epoch: 70 [47488/50000]	Loss: 0.5431	LR: 0.025000
Training Epoch: 70 [47616/50000]	Loss: 0.5139	LR: 0.025000
Training Epoch: 70 [47744/50000]	Loss: 0.6145	LR: 0.025000
Training Epoch: 70 [47872/50000]	Loss: 0.6310	LR: 0.025000
Training Epoch: 70 [48000/50000]	Loss: 0.5874	LR: 0.025000
Training Epoch: 70 [48128/50000]	Loss: 0.6418	LR: 0.025000
Training Epoch: 70 [48256/50000]	Loss: 0.4247	LR: 0.025000
Training Epoch: 70 [48384/50000]	Loss: 0.5361	LR: 0.025000
Training Epoch: 70 [48512/50000]	Loss: 0.4785	LR: 0.025000
Training Epoch: 70 [48640/50000]	Loss: 0.5559	LR: 0.025000
Training Epoch: 70 [48768/50000]	Loss: 0.4941	LR: 0.025000
Training Epoch: 70 [48896/50000]	Loss: 0.3543	LR: 0.025000
Training Epoch: 70 [49024/50000]	Loss: 0.5216	LR: 0.025000
Training Epoch: 70 [49152/50000]	Loss: 0.4913	LR: 0.025000
Training Epoch: 70 [49280/50000]	Loss: 0.5826	LR: 0.025000
Training Epoch: 70 [49408/50000]	Loss: 0.4795	LR: 0.025000
Training Epoch: 70 [49536/50000]	Loss: 0.5230	LR: 0.025000
Training Epoch: 70 [49664/50000]	Loss: 0.4612	LR: 0.025000
Training Epoch: 70 [49792/50000]	Loss: 0.5665	LR: 0.025000
Training Epoch: 70 [49920/50000]	Loss: 0.5278	LR: 0.025000
Training Epoch: 70 [50000/50000]	Loss: 0.7139	LR: 0.025000
epoch 70 training time consumed: 53.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  256904 GB |  256904 GB |
|       from large pool |  123392 KB |    1034 MB |  256651 GB |  256651 GB |
|       from small pool |   10798 KB |      13 MB |     253 GB |     253 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  256904 GB |  256904 GB |
|       from large pool |  123392 KB |    1034 MB |  256651 GB |  256651 GB |
|       from small pool |   10798 KB |      13 MB |     253 GB |     253 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  113053 GB |  113053 GB |
|       from large pool |  155136 KB |  433088 KB |  112773 GB |  112773 GB |
|       from small pool |    1490 KB |    3494 KB |     279 GB |     279 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |    9913 K  |    9912 K  |
|       from large pool |      24    |      65    |    5174 K  |    5174 K  |
|       from small pool |     231    |     274    |    4738 K  |    4738 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |    9913 K  |    9912 K  |
|       from large pool |      24    |      65    |    5174 K  |    5174 K  |
|       from small pool |     231    |     274    |    4738 K  |    4738 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4898 K  |    4898 K  |
|       from large pool |       9    |      14    |    2504 K  |    2504 K  |
|       from small pool |      12    |      16    |    2393 K  |    2393 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 70, Average loss: 0.0107, Accuracy: 0.6686, Time consumed:3.45s

Training Epoch: 71 [128/50000]	Loss: 0.4522	LR: 0.025000
Training Epoch: 71 [256/50000]	Loss: 0.5081	LR: 0.025000
Training Epoch: 71 [384/50000]	Loss: 0.3171	LR: 0.025000
Training Epoch: 71 [512/50000]	Loss: 0.5310	LR: 0.025000
Training Epoch: 71 [640/50000]	Loss: 0.4933	LR: 0.025000
Training Epoch: 71 [768/50000]	Loss: 0.3812	LR: 0.025000
Training Epoch: 71 [896/50000]	Loss: 0.5748	LR: 0.025000
Training Epoch: 71 [1024/50000]	Loss: 0.4780	LR: 0.025000
Training Epoch: 71 [1152/50000]	Loss: 0.3393	LR: 0.025000
Training Epoch: 71 [1280/50000]	Loss: 0.5066	LR: 0.025000
Training Epoch: 71 [1408/50000]	Loss: 0.5674	LR: 0.025000
Training Epoch: 71 [1536/50000]	Loss: 0.4353	LR: 0.025000
Training Epoch: 71 [1664/50000]	Loss: 0.3266	LR: 0.025000
Training Epoch: 71 [1792/50000]	Loss: 0.4632	LR: 0.025000
Training Epoch: 71 [1920/50000]	Loss: 0.3928	LR: 0.025000
Training Epoch: 71 [2048/50000]	Loss: 0.4382	LR: 0.025000
Training Epoch: 71 [2176/50000]	Loss: 0.4062	LR: 0.025000
Training Epoch: 71 [2304/50000]	Loss: 0.5438	LR: 0.025000
Training Epoch: 71 [2432/50000]	Loss: 0.4859	LR: 0.025000
Training Epoch: 71 [2560/50000]	Loss: 0.4295	LR: 0.025000
Training Epoch: 71 [2688/50000]	Loss: 0.4635	LR: 0.025000
Training Epoch: 71 [2816/50000]	Loss: 0.4339	LR: 0.025000
Training Epoch: 71 [2944/50000]	Loss: 0.3923	LR: 0.025000
Training Epoch: 71 [3072/50000]	Loss: 0.3367	LR: 0.025000
Training Epoch: 71 [3200/50000]	Loss: 0.5127	LR: 0.025000
Training Epoch: 71 [3328/50000]	Loss: 0.3631	LR: 0.025000
Training Epoch: 71 [3456/50000]	Loss: 0.4404	LR: 0.025000
Training Epoch: 71 [3584/50000]	Loss: 0.4783	LR: 0.025000
Training Epoch: 71 [3712/50000]	Loss: 0.6123	LR: 0.025000
Training Epoch: 71 [3840/50000]	Loss: 0.3055	LR: 0.025000
Training Epoch: 71 [3968/50000]	Loss: 0.4425	LR: 0.025000
Training Epoch: 71 [4096/50000]	Loss: 0.4408	LR: 0.025000
Training Epoch: 71 [4224/50000]	Loss: 0.4802	LR: 0.025000
Training Epoch: 71 [4352/50000]	Loss: 0.4455	LR: 0.025000
Training Epoch: 71 [4480/50000]	Loss: 0.3132	LR: 0.025000
Training Epoch: 71 [4608/50000]	Loss: 0.3727	LR: 0.025000
Training Epoch: 71 [4736/50000]	Loss: 0.4229	LR: 0.025000
Training Epoch: 71 [4864/50000]	Loss: 0.4482	LR: 0.025000
Training Epoch: 71 [4992/50000]	Loss: 0.4454	LR: 0.025000
Training Epoch: 71 [5120/50000]	Loss: 0.4289	LR: 0.025000
Training Epoch: 71 [5248/50000]	Loss: 0.3907	LR: 0.025000
Training Epoch: 71 [5376/50000]	Loss: 0.2770	LR: 0.025000
Training Epoch: 71 [5504/50000]	Loss: 0.6527	LR: 0.025000
Training Epoch: 71 [5632/50000]	Loss: 0.4645	LR: 0.025000
Training Epoch: 71 [5760/50000]	Loss: 0.3410	LR: 0.025000
Training Epoch: 71 [5888/50000]	Loss: 0.3669	LR: 0.025000
Training Epoch: 71 [6016/50000]	Loss: 0.3658	LR: 0.025000
Training Epoch: 71 [6144/50000]	Loss: 0.3499	LR: 0.025000
Training Epoch: 71 [6272/50000]	Loss: 0.3152	LR: 0.025000
Training Epoch: 71 [6400/50000]	Loss: 0.3573	LR: 0.025000
Training Epoch: 71 [6528/50000]	Loss: 0.4323	LR: 0.025000
Training Epoch: 71 [6656/50000]	Loss: 0.5334	LR: 0.025000
Training Epoch: 71 [6784/50000]	Loss: 0.4673	LR: 0.025000
Training Epoch: 71 [6912/50000]	Loss: 0.5665	LR: 0.025000
Training Epoch: 71 [7040/50000]	Loss: 0.4048	LR: 0.025000
Training Epoch: 71 [7168/50000]	Loss: 0.3635	LR: 0.025000
Training Epoch: 71 [7296/50000]	Loss: 0.4625	LR: 0.025000
Training Epoch: 71 [7424/50000]	Loss: 0.4028	LR: 0.025000
Training Epoch: 71 [7552/50000]	Loss: 0.3840	LR: 0.025000
Training Epoch: 71 [7680/50000]	Loss: 0.5363	LR: 0.025000
Training Epoch: 71 [7808/50000]	Loss: 0.4579	LR: 0.025000
Training Epoch: 71 [7936/50000]	Loss: 0.4159	LR: 0.025000
Training Epoch: 71 [8064/50000]	Loss: 0.3739	LR: 0.025000
Training Epoch: 71 [8192/50000]	Loss: 0.3729	LR: 0.025000
Training Epoch: 71 [8320/50000]	Loss: 0.3346	LR: 0.025000
Training Epoch: 71 [8448/50000]	Loss: 0.4162	LR: 0.025000
Training Epoch: 71 [8576/50000]	Loss: 0.3488	LR: 0.025000
Training Epoch: 71 [8704/50000]	Loss: 0.4347	LR: 0.025000
Training Epoch: 71 [8832/50000]	Loss: 0.4360	LR: 0.025000
Training Epoch: 71 [8960/50000]	Loss: 0.3762	LR: 0.025000
Training Epoch: 71 [9088/50000]	Loss: 0.4743	LR: 0.025000
Training Epoch: 71 [9216/50000]	Loss: 0.3854	LR: 0.025000
Training Epoch: 71 [9344/50000]	Loss: 0.3961	LR: 0.025000
Training Epoch: 71 [9472/50000]	Loss: 0.3977	LR: 0.025000
Training Epoch: 71 [9600/50000]	Loss: 0.3777	LR: 0.025000
Training Epoch: 71 [9728/50000]	Loss: 0.5827	LR: 0.025000
Training Epoch: 71 [9856/50000]	Loss: 0.6709	LR: 0.025000
Training Epoch: 71 [9984/50000]	Loss: 0.5451	LR: 0.025000
Training Epoch: 71 [10112/50000]	Loss: 0.4186	LR: 0.025000
Training Epoch: 71 [10240/50000]	Loss: 0.4171	LR: 0.025000
Training Epoch: 71 [10368/50000]	Loss: 0.4389	LR: 0.025000
Training Epoch: 71 [10496/50000]	Loss: 0.4399	LR: 0.025000
Training Epoch: 71 [10624/50000]	Loss: 0.4932	LR: 0.025000
Training Epoch: 71 [10752/50000]	Loss: 0.4413	LR: 0.025000
Training Epoch: 71 [10880/50000]	Loss: 0.3742	LR: 0.025000
Training Epoch: 71 [11008/50000]	Loss: 0.3512	LR: 0.025000
Training Epoch: 71 [11136/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 71 [11264/50000]	Loss: 0.4134	LR: 0.025000
Training Epoch: 71 [11392/50000]	Loss: 0.3378	LR: 0.025000
Training Epoch: 71 [11520/50000]	Loss: 0.4872	LR: 0.025000
Training Epoch: 71 [11648/50000]	Loss: 0.4626	LR: 0.025000
Training Epoch: 71 [11776/50000]	Loss: 0.4174	LR: 0.025000
Training Epoch: 71 [11904/50000]	Loss: 0.5029	LR: 0.025000
Training Epoch: 71 [12032/50000]	Loss: 0.3113	LR: 0.025000
Training Epoch: 71 [12160/50000]	Loss: 0.4898	LR: 0.025000
Training Epoch: 71 [12288/50000]	Loss: 0.7219	LR: 0.025000
Training Epoch: 71 [12416/50000]	Loss: 0.4302	LR: 0.025000
Training Epoch: 71 [12544/50000]	Loss: 0.5021	LR: 0.025000
Training Epoch: 71 [12672/50000]	Loss: 0.4370	LR: 0.025000
Training Epoch: 71 [12800/50000]	Loss: 0.3987	LR: 0.025000
Training Epoch: 71 [12928/50000]	Loss: 0.3603	LR: 0.025000
Training Epoch: 71 [13056/50000]	Loss: 0.3851	LR: 0.025000
Training Epoch: 71 [13184/50000]	Loss: 0.4766	LR: 0.025000
Training Epoch: 71 [13312/50000]	Loss: 0.3745	LR: 0.025000
Training Epoch: 71 [13440/50000]	Loss: 0.4621	LR: 0.025000
Training Epoch: 71 [13568/50000]	Loss: 0.3270	LR: 0.025000
Training Epoch: 71 [13696/50000]	Loss: 0.3032	LR: 0.025000
Training Epoch: 71 [13824/50000]	Loss: 0.4245	LR: 0.025000
Training Epoch: 71 [13952/50000]	Loss: 0.3857	LR: 0.025000
Training Epoch: 71 [14080/50000]	Loss: 0.3641	LR: 0.025000
Training Epoch: 71 [14208/50000]	Loss: 0.4225	LR: 0.025000
Training Epoch: 71 [14336/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 71 [14464/50000]	Loss: 0.3020	LR: 0.025000
Training Epoch: 71 [14592/50000]	Loss: 0.4369	LR: 0.025000
Training Epoch: 71 [14720/50000]	Loss: 0.3635	LR: 0.025000
Training Epoch: 71 [14848/50000]	Loss: 0.4804	LR: 0.025000
Training Epoch: 71 [14976/50000]	Loss: 0.3961	LR: 0.025000
Training Epoch: 71 [15104/50000]	Loss: 0.4681	LR: 0.025000
Training Epoch: 71 [15232/50000]	Loss: 0.4168	LR: 0.025000
Training Epoch: 71 [15360/50000]	Loss: 0.4753	LR: 0.025000
Training Epoch: 71 [15488/50000]	Loss: 0.4369	LR: 0.025000
Training Epoch: 71 [15616/50000]	Loss: 0.5763	LR: 0.025000
Training Epoch: 71 [15744/50000]	Loss: 0.4723	LR: 0.025000
Training Epoch: 71 [15872/50000]	Loss: 0.4051	LR: 0.025000
Training Epoch: 71 [16000/50000]	Loss: 0.5261	LR: 0.025000
Training Epoch: 71 [16128/50000]	Loss: 0.5574	LR: 0.025000
Training Epoch: 71 [16256/50000]	Loss: 0.4208	LR: 0.025000
Training Epoch: 71 [16384/50000]	Loss: 0.5477	LR: 0.025000
Training Epoch: 71 [16512/50000]	Loss: 0.4344	LR: 0.025000
Training Epoch: 71 [16640/50000]	Loss: 0.4196	LR: 0.025000
Training Epoch: 71 [16768/50000]	Loss: 0.3793	LR: 0.025000
Training Epoch: 71 [16896/50000]	Loss: 0.4618	LR: 0.025000
Training Epoch: 71 [17024/50000]	Loss: 0.5009	LR: 0.025000
Training Epoch: 71 [17152/50000]	Loss: 0.3804	LR: 0.025000
Training Epoch: 71 [17280/50000]	Loss: 0.3656	LR: 0.025000
Training Epoch: 71 [17408/50000]	Loss: 0.4824	LR: 0.025000
Training Epoch: 71 [17536/50000]	Loss: 0.4039	LR: 0.025000
Training Epoch: 71 [17664/50000]	Loss: 0.5195	LR: 0.025000
Training Epoch: 71 [17792/50000]	Loss: 0.5061	LR: 0.025000
Training Epoch: 71 [17920/50000]	Loss: 0.3621	LR: 0.025000
Training Epoch: 71 [18048/50000]	Loss: 0.5871	LR: 0.025000
Training Epoch: 71 [18176/50000]	Loss: 0.3611	LR: 0.025000
Training Epoch: 71 [18304/50000]	Loss: 0.4143	LR: 0.025000
Training Epoch: 71 [18432/50000]	Loss: 0.4232	LR: 0.025000
Training Epoch: 71 [18560/50000]	Loss: 0.3308	LR: 0.025000
Training Epoch: 71 [18688/50000]	Loss: 0.3399	LR: 0.025000
Training Epoch: 71 [18816/50000]	Loss: 0.5825	LR: 0.025000
Training Epoch: 71 [18944/50000]	Loss: 0.3977	LR: 0.025000
Training Epoch: 71 [19072/50000]	Loss: 0.4052	LR: 0.025000
Training Epoch: 71 [19200/50000]	Loss: 0.6040	LR: 0.025000
Training Epoch: 71 [19328/50000]	Loss: 0.5380	LR: 0.025000
Training Epoch: 71 [19456/50000]	Loss: 0.3951	LR: 0.025000
Training Epoch: 71 [19584/50000]	Loss: 0.3152	LR: 0.025000
Training Epoch: 71 [19712/50000]	Loss: 0.2443	LR: 0.025000
Training Epoch: 71 [19840/50000]	Loss: 0.5818	LR: 0.025000
Training Epoch: 71 [19968/50000]	Loss: 0.4087	LR: 0.025000
Training Epoch: 71 [20096/50000]	Loss: 0.3635	LR: 0.025000
Training Epoch: 71 [20224/50000]	Loss: 0.3920	LR: 0.025000
Training Epoch: 71 [20352/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 71 [20480/50000]	Loss: 0.4073	LR: 0.025000
Training Epoch: 71 [20608/50000]	Loss: 0.4522	LR: 0.025000
Training Epoch: 71 [20736/50000]	Loss: 0.5742	LR: 0.025000
Training Epoch: 71 [20864/50000]	Loss: 0.3155	LR: 0.025000
Training Epoch: 71 [20992/50000]	Loss: 0.6002	LR: 0.025000
Training Epoch: 71 [21120/50000]	Loss: 0.4041	LR: 0.025000
Training Epoch: 71 [21248/50000]	Loss: 0.4335	LR: 0.025000
Training Epoch: 71 [21376/50000]	Loss: 0.4208	LR: 0.025000
Training Epoch: 71 [21504/50000]	Loss: 0.4587	LR: 0.025000
Training Epoch: 71 [21632/50000]	Loss: 0.3449	LR: 0.025000
Training Epoch: 71 [21760/50000]	Loss: 0.5362	LR: 0.025000
Training Epoch: 71 [21888/50000]	Loss: 0.3599	LR: 0.025000
Training Epoch: 71 [22016/50000]	Loss: 0.5579	LR: 0.025000
Training Epoch: 71 [22144/50000]	Loss: 0.3628	LR: 0.025000
Training Epoch: 71 [22272/50000]	Loss: 0.5859	LR: 0.025000
Training Epoch: 71 [22400/50000]	Loss: 0.4487	LR: 0.025000
Training Epoch: 71 [22528/50000]	Loss: 0.5329	LR: 0.025000
Training Epoch: 71 [22656/50000]	Loss: 0.5066	LR: 0.025000
Training Epoch: 71 [22784/50000]	Loss: 0.4676	LR: 0.025000
Training Epoch: 71 [22912/50000]	Loss: 0.3867	LR: 0.025000
Training Epoch: 71 [23040/50000]	Loss: 0.4677	LR: 0.025000
Training Epoch: 71 [23168/50000]	Loss: 0.5070	LR: 0.025000
Training Epoch: 71 [23296/50000]	Loss: 0.6848	LR: 0.025000
Training Epoch: 71 [23424/50000]	Loss: 0.4424	LR: 0.025000
Training Epoch: 71 [23552/50000]	Loss: 0.5100	LR: 0.025000
Training Epoch: 71 [23680/50000]	Loss: 0.6952	LR: 0.025000
Training Epoch: 71 [23808/50000]	Loss: 0.4304	LR: 0.025000
Training Epoch: 71 [23936/50000]	Loss: 0.5439	LR: 0.025000
Training Epoch: 71 [24064/50000]	Loss: 0.5176	LR: 0.025000
Training Epoch: 71 [24192/50000]	Loss: 0.4390	LR: 0.025000
Training Epoch: 71 [24320/50000]	Loss: 0.4836	LR: 0.025000
Training Epoch: 71 [24448/50000]	Loss: 0.4163	LR: 0.025000
Training Epoch: 71 [24576/50000]	Loss: 0.5605	LR: 0.025000
Training Epoch: 71 [24704/50000]	Loss: 0.4504	LR: 0.025000
Training Epoch: 71 [24832/50000]	Loss: 0.3163	LR: 0.025000
Training Epoch: 71 [24960/50000]	Loss: 0.3627	LR: 0.025000
Training Epoch: 71 [25088/50000]	Loss: 0.4391	LR: 0.025000
Training Epoch: 71 [25216/50000]	Loss: 0.4587	LR: 0.025000
Training Epoch: 71 [25344/50000]	Loss: 0.5581	LR: 0.025000
Training Epoch: 71 [25472/50000]	Loss: 0.4071	LR: 0.025000
Training Epoch: 71 [25600/50000]	Loss: 0.4643	LR: 0.025000
Training Epoch: 71 [25728/50000]	Loss: 0.3309	LR: 0.025000
Training Epoch: 71 [25856/50000]	Loss: 0.3942	LR: 0.025000
Training Epoch: 71 [25984/50000]	Loss: 0.5355	LR: 0.025000
Training Epoch: 71 [26112/50000]	Loss: 0.4328	LR: 0.025000
Training Epoch: 71 [26240/50000]	Loss: 0.3997	LR: 0.025000
Training Epoch: 71 [26368/50000]	Loss: 0.5293	LR: 0.025000
Training Epoch: 71 [26496/50000]	Loss: 0.3984	LR: 0.025000
Training Epoch: 71 [26624/50000]	Loss: 0.5017	LR: 0.025000
Training Epoch: 71 [26752/50000]	Loss: 0.3439	LR: 0.025000
Training Epoch: 71 [26880/50000]	Loss: 0.4566	LR: 0.025000
Training Epoch: 71 [27008/50000]	Loss: 0.5347	LR: 0.025000
Training Epoch: 71 [27136/50000]	Loss: 0.6752	LR: 0.025000
Training Epoch: 71 [27264/50000]	Loss: 0.3113	LR: 0.025000
Training Epoch: 71 [27392/50000]	Loss: 0.4025	LR: 0.025000
Training Epoch: 71 [27520/50000]	Loss: 0.5384	LR: 0.025000
Training Epoch: 71 [27648/50000]	Loss: 0.4610	LR: 0.025000
Training Epoch: 71 [27776/50000]	Loss: 0.6445	LR: 0.025000
Training Epoch: 71 [27904/50000]	Loss: 0.4444	LR: 0.025000
Training Epoch: 71 [28032/50000]	Loss: 0.4609	LR: 0.025000
Training Epoch: 71 [28160/50000]	Loss: 0.4733	LR: 0.025000
Training Epoch: 71 [28288/50000]	Loss: 0.4251	LR: 0.025000
Training Epoch: 71 [28416/50000]	Loss: 0.6565	LR: 0.025000
Training Epoch: 71 [28544/50000]	Loss: 0.4558	LR: 0.025000
Training Epoch: 71 [28672/50000]	Loss: 0.4395	LR: 0.025000
Training Epoch: 71 [28800/50000]	Loss: 0.3234	LR: 0.025000
Training Epoch: 71 [28928/50000]	Loss: 0.4684	LR: 0.025000
Training Epoch: 71 [29056/50000]	Loss: 0.6456	LR: 0.025000
Training Epoch: 71 [29184/50000]	Loss: 0.5641	LR: 0.025000
Training Epoch: 71 [29312/50000]	Loss: 0.4900	LR: 0.025000
Training Epoch: 71 [29440/50000]	Loss: 0.4922	LR: 0.025000
Training Epoch: 71 [29568/50000]	Loss: 0.5899	LR: 0.025000
Training Epoch: 71 [29696/50000]	Loss: 0.6968	LR: 0.025000
Training Epoch: 71 [29824/50000]	Loss: 0.5893	LR: 0.025000
Training Epoch: 71 [29952/50000]	Loss: 0.5349	LR: 0.025000
Training Epoch: 71 [30080/50000]	Loss: 0.3751	LR: 0.025000
Training Epoch: 71 [30208/50000]	Loss: 0.5175	LR: 0.025000
Training Epoch: 71 [30336/50000]	Loss: 0.4774	LR: 0.025000
Training Epoch: 71 [30464/50000]	Loss: 0.4746	LR: 0.025000
Training Epoch: 71 [30592/50000]	Loss: 0.3787	LR: 0.025000
Training Epoch: 71 [30720/50000]	Loss: 0.4560	LR: 0.025000
Training Epoch: 71 [30848/50000]	Loss: 0.6428	LR: 0.025000
Training Epoch: 71 [30976/50000]	Loss: 0.4222	LR: 0.025000
Training Epoch: 71 [31104/50000]	Loss: 0.4959	LR: 0.025000
Training Epoch: 71 [31232/50000]	Loss: 0.4235	LR: 0.025000
Training Epoch: 71 [31360/50000]	Loss: 0.5889	LR: 0.025000
Training Epoch: 71 [31488/50000]	Loss: 0.3577	LR: 0.025000
Training Epoch: 71 [31616/50000]	Loss: 0.3801	LR: 0.025000
Training Epoch: 71 [31744/50000]	Loss: 0.3808	LR: 0.025000
Training Epoch: 71 [31872/50000]	Loss: 0.4226	LR: 0.025000
Training Epoch: 71 [32000/50000]	Loss: 0.6075	LR: 0.025000
Training Epoch: 71 [32128/50000]	Loss: 0.4207	LR: 0.025000
Training Epoch: 71 [32256/50000]	Loss: 0.5244	LR: 0.025000
Training Epoch: 71 [32384/50000]	Loss: 0.5039	LR: 0.025000
Training Epoch: 71 [32512/50000]	Loss: 0.3464	LR: 0.025000
Training Epoch: 71 [32640/50000]	Loss: 0.5447	LR: 0.025000
Training Epoch: 71 [32768/50000]	Loss: 0.3498	LR: 0.025000
Training Epoch: 71 [32896/50000]	Loss: 0.5122	LR: 0.025000
Training Epoch: 71 [33024/50000]	Loss: 0.4325	LR: 0.025000
Training Epoch: 71 [33152/50000]	Loss: 0.4432	LR: 0.025000
Training Epoch: 71 [33280/50000]	Loss: 0.5170	LR: 0.025000
Training Epoch: 71 [33408/50000]	Loss: 0.4665	LR: 0.025000
Training Epoch: 71 [33536/50000]	Loss: 0.4381	LR: 0.025000
Training Epoch: 71 [33664/50000]	Loss: 0.6496	LR: 0.025000
Training Epoch: 71 [33792/50000]	Loss: 0.7379	LR: 0.025000
Training Epoch: 71 [33920/50000]	Loss: 0.4307	LR: 0.025000
Training Epoch: 71 [34048/50000]	Loss: 0.3675	LR: 0.025000
Training Epoch: 71 [34176/50000]	Loss: 0.4631	LR: 0.025000
Training Epoch: 71 [34304/50000]	Loss: 0.4668	LR: 0.025000
Training Epoch: 71 [34432/50000]	Loss: 0.5563	LR: 0.025000
Training Epoch: 71 [34560/50000]	Loss: 0.4228	LR: 0.025000
Training Epoch: 71 [34688/50000]	Loss: 0.3497	LR: 0.025000
Training Epoch: 71 [34816/50000]	Loss: 0.5487	LR: 0.025000
Training Epoch: 71 [34944/50000]	Loss: 0.4689	LR: 0.025000
Training Epoch: 71 [35072/50000]	Loss: 0.6301	LR: 0.025000
Training Epoch: 71 [35200/50000]	Loss: 0.4132	LR: 0.025000
Training Epoch: 71 [35328/50000]	Loss: 0.3947	LR: 0.025000
Training Epoch: 71 [35456/50000]	Loss: 0.3646	LR: 0.025000
Training Epoch: 71 [35584/50000]	Loss: 0.4911	LR: 0.025000
Training Epoch: 71 [35712/50000]	Loss: 0.3474	LR: 0.025000
Training Epoch: 71 [35840/50000]	Loss: 0.4507	LR: 0.025000
Training Epoch: 71 [35968/50000]	Loss: 0.5283	LR: 0.025000
Training Epoch: 71 [36096/50000]	Loss: 0.3917	LR: 0.025000
Training Epoch: 71 [36224/50000]	Loss: 0.3220	LR: 0.025000
Training Epoch: 71 [36352/50000]	Loss: 0.6025	LR: 0.025000
Training Epoch: 71 [36480/50000]	Loss: 0.5168	LR: 0.025000
Training Epoch: 71 [36608/50000]	Loss: 0.4856	LR: 0.025000
Training Epoch: 71 [36736/50000]	Loss: 0.5340	LR: 0.025000
Training Epoch: 71 [36864/50000]	Loss: 0.6018	LR: 0.025000
Training Epoch: 71 [36992/50000]	Loss: 0.5265	LR: 0.025000
Training Epoch: 71 [37120/50000]	Loss: 0.4654	LR: 0.025000
Training Epoch: 71 [37248/50000]	Loss: 0.5333	LR: 0.025000
Training Epoch: 71 [37376/50000]	Loss: 0.3931	LR: 0.025000
Training Epoch: 71 [37504/50000]	Loss: 0.4813	LR: 0.025000
Training Epoch: 71 [37632/50000]	Loss: 0.5257	LR: 0.025000
Training Epoch: 71 [37760/50000]	Loss: 0.4407	LR: 0.025000
Training Epoch: 71 [37888/50000]	Loss: 0.4244	LR: 0.025000
Training Epoch: 71 [38016/50000]	Loss: 0.4524	LR: 0.025000
Training Epoch: 71 [38144/50000]	Loss: 0.3910	LR: 0.025000
Training Epoch: 71 [38272/50000]	Loss: 0.4414	LR: 0.025000
Training Epoch: 71 [38400/50000]	Loss: 0.5246	LR: 0.025000
Training Epoch: 71 [38528/50000]	Loss: 0.4665	LR: 0.025000
Training Epoch: 71 [38656/50000]	Loss: 0.5076	LR: 0.025000
Training Epoch: 71 [38784/50000]	Loss: 0.3371	LR: 0.025000
Training Epoch: 71 [38912/50000]	Loss: 0.4739	LR: 0.025000
Training Epoch: 71 [39040/50000]	Loss: 0.5917	LR: 0.025000
Training Epoch: 71 [39168/50000]	Loss: 0.3659	LR: 0.025000
Training Epoch: 71 [39296/50000]	Loss: 0.4397	LR: 0.025000
Training Epoch: 71 [39424/50000]	Loss: 0.6503	LR: 0.025000
Training Epoch: 71 [39552/50000]	Loss: 0.4853	LR: 0.025000
Training Epoch: 71 [39680/50000]	Loss: 0.3922	LR: 0.025000
Training Epoch: 71 [39808/50000]	Loss: 0.5066	LR: 0.025000
Training Epoch: 71 [39936/50000]	Loss: 0.6456	LR: 0.025000
Training Epoch: 71 [40064/50000]	Loss: 0.3894	LR: 0.025000
Training Epoch: 71 [40192/50000]	Loss: 0.3556	LR: 0.025000
Training Epoch: 71 [40320/50000]	Loss: 0.4813	LR: 0.025000
Training Epoch: 71 [40448/50000]	Loss: 0.4858	LR: 0.025000
Training Epoch: 71 [40576/50000]	Loss: 0.4542	LR: 0.025000
Training Epoch: 71 [40704/50000]	Loss: 0.6272	LR: 0.025000
Training Epoch: 71 [40832/50000]	Loss: 0.3688	LR: 0.025000
Training Epoch: 71 [40960/50000]	Loss: 0.5406	LR: 0.025000
Training Epoch: 71 [41088/50000]	Loss: 0.5729	LR: 0.025000
Training Epoch: 71 [41216/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 71 [41344/50000]	Loss: 0.5801	LR: 0.025000
Training Epoch: 71 [41472/50000]	Loss: 0.4573	LR: 0.025000
Training Epoch: 71 [41600/50000]	Loss: 0.5102	LR: 0.025000
Training Epoch: 71 [41728/50000]	Loss: 0.5584	LR: 0.025000
Training Epoch: 71 [41856/50000]	Loss: 0.4133	LR: 0.025000
Training Epoch: 71 [41984/50000]	Loss: 0.5656	LR: 0.025000
Training Epoch: 71 [42112/50000]	Loss: 0.4672	LR: 0.025000
Training Epoch: 71 [42240/50000]	Loss: 0.5987	LR: 0.025000
Training Epoch: 71 [42368/50000]	Loss: 0.6410	LR: 0.025000
Training Epoch: 71 [42496/50000]	Loss: 0.5162	LR: 0.025000
Training Epoch: 71 [42624/50000]	Loss: 0.5296	LR: 0.025000
Training Epoch: 71 [42752/50000]	Loss: 0.5558	LR: 0.025000
Training Epoch: 71 [42880/50000]	Loss: 0.4762	LR: 0.025000
Training Epoch: 71 [43008/50000]	Loss: 0.5297	LR: 0.025000
Training Epoch: 71 [43136/50000]	Loss: 0.5002	LR: 0.025000
Training Epoch: 71 [43264/50000]	Loss: 0.5109	LR: 0.025000
Training Epoch: 71 [43392/50000]	Loss: 0.5287	LR: 0.025000
Training Epoch: 71 [43520/50000]	Loss: 0.6079	LR: 0.025000
Training Epoch: 71 [43648/50000]	Loss: 0.4568	LR: 0.025000
Training Epoch: 71 [43776/50000]	Loss: 0.4355	LR: 0.025000
Training Epoch: 71 [43904/50000]	Loss: 0.4661	LR: 0.025000
Training Epoch: 71 [44032/50000]	Loss: 0.5097	LR: 0.025000
Training Epoch: 71 [44160/50000]	Loss: 0.4155	LR: 0.025000
Training Epoch: 71 [44288/50000]	Loss: 0.4975	LR: 0.025000
Training Epoch: 71 [44416/50000]	Loss: 0.5523	LR: 0.025000
Training Epoch: 71 [44544/50000]	Loss: 0.5031	LR: 0.025000
Training Epoch: 71 [44672/50000]	Loss: 0.4092	LR: 0.025000
Training Epoch: 71 [44800/50000]	Loss: 0.4919	LR: 0.025000
Training Epoch: 71 [44928/50000]	Loss: 0.5017	LR: 0.025000
Training Epoch: 71 [45056/50000]	Loss: 0.5893	LR: 0.025000
Training Epoch: 71 [45184/50000]	Loss: 0.5939	LR: 0.025000
Training Epoch: 71 [45312/50000]	Loss: 0.5070	LR: 0.025000
Training Epoch: 71 [45440/50000]	Loss: 0.4501	LR: 0.025000
Training Epoch: 71 [45568/50000]	Loss: 0.2991	LR: 0.025000
Training Epoch: 71 [45696/50000]	Loss: 0.5182	LR: 0.025000
Training Epoch: 71 [45824/50000]	Loss: 0.6300	LR: 0.025000
Training Epoch: 71 [45952/50000]	Loss: 0.5132	LR: 0.025000
Training Epoch: 71 [46080/50000]	Loss: 0.5382	LR: 0.025000
Training Epoch: 71 [46208/50000]	Loss: 0.4346	LR: 0.025000
Training Epoch: 71 [46336/50000]	Loss: 0.4161	LR: 0.025000
Training Epoch: 71 [46464/50000]	Loss: 0.5080	LR: 0.025000
Training Epoch: 71 [46592/50000]	Loss: 0.5673	LR: 0.025000
Training Epoch: 71 [46720/50000]	Loss: 0.4985	LR: 0.025000
Training Epoch: 71 [46848/50000]	Loss: 0.5506	LR: 0.025000
Training Epoch: 71 [46976/50000]	Loss: 0.5881	LR: 0.025000
Training Epoch: 71 [47104/50000]	Loss: 0.5388	LR: 0.025000
Training Epoch: 71 [47232/50000]	Loss: 0.5885	LR: 0.025000
Training Epoch: 71 [47360/50000]	Loss: 0.6145	LR: 0.025000
Training Epoch: 71 [47488/50000]	Loss: 0.4900	LR: 0.025000
Training Epoch: 71 [47616/50000]	Loss: 0.6236	LR: 0.025000
Training Epoch: 71 [47744/50000]	Loss: 0.5237	LR: 0.025000
Training Epoch: 71 [47872/50000]	Loss: 0.4667	LR: 0.025000
Training Epoch: 71 [48000/50000]	Loss: 0.4409	LR: 0.025000
Training Epoch: 71 [48128/50000]	Loss: 0.6068	LR: 0.025000
Training Epoch: 71 [48256/50000]	Loss: 0.5179	LR: 0.025000
Training Epoch: 71 [48384/50000]	Loss: 0.6284	LR: 0.025000
Training Epoch: 71 [48512/50000]	Loss: 0.4891	LR: 0.025000
Training Epoch: 71 [48640/50000]	Loss: 0.4755	LR: 0.025000
Training Epoch: 71 [48768/50000]	Loss: 0.6003	LR: 0.025000
Training Epoch: 71 [48896/50000]	Loss: 0.4642	LR: 0.025000
Training Epoch: 71 [49024/50000]	Loss: 0.6660	LR: 0.025000
Training Epoch: 71 [49152/50000]	Loss: 0.5025	LR: 0.025000
Training Epoch: 71 [49280/50000]	Loss: 0.4775	LR: 0.025000
Training Epoch: 71 [49408/50000]	Loss: 0.5314	LR: 0.025000
Training Epoch: 71 [49536/50000]	Loss: 0.5815	LR: 0.025000
Training Epoch: 71 [49664/50000]	Loss: 0.6261	LR: 0.025000
Training Epoch: 71 [49792/50000]	Loss: 0.4494	LR: 0.025000
Training Epoch: 71 [49920/50000]	Loss: 0.4764	LR: 0.025000
Training Epoch: 71 [50000/50000]	Loss: 0.3166	LR: 0.025000
epoch 71 training time consumed: 53.96s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  260574 GB |  260574 GB |
|       from large pool |  123392 KB |    1034 MB |  260317 GB |  260317 GB |
|       from small pool |   10798 KB |      13 MB |     256 GB |     256 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  260574 GB |  260574 GB |
|       from large pool |  123392 KB |    1034 MB |  260317 GB |  260317 GB |
|       from small pool |   10798 KB |      13 MB |     256 GB |     256 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  114668 GB |  114668 GB |
|       from large pool |  155136 KB |  433088 KB |  114384 GB |  114384 GB |
|       from small pool |    1490 KB |    3494 KB |     283 GB |     283 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   10054 K  |   10054 K  |
|       from large pool |      24    |      65    |    5248 K  |    5248 K  |
|       from small pool |     231    |     274    |    4806 K  |    4806 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   10054 K  |   10054 K  |
|       from large pool |      24    |      65    |    5248 K  |    5248 K  |
|       from small pool |     231    |     274    |    4806 K  |    4806 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    4968 K  |    4968 K  |
|       from large pool |       9    |      14    |    2540 K  |    2540 K  |
|       from small pool |      12    |      16    |    2427 K  |    2427 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 71, Average loss: 0.0111, Accuracy: 0.6571, Time consumed:3.48s

Training Epoch: 72 [128/50000]	Loss: 0.3956	LR: 0.025000
Training Epoch: 72 [256/50000]	Loss: 0.5193	LR: 0.025000
Training Epoch: 72 [384/50000]	Loss: 0.3804	LR: 0.025000
Training Epoch: 72 [512/50000]	Loss: 0.5439	LR: 0.025000
Training Epoch: 72 [640/50000]	Loss: 0.4658	LR: 0.025000
Training Epoch: 72 [768/50000]	Loss: 0.4668	LR: 0.025000
Training Epoch: 72 [896/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 72 [1024/50000]	Loss: 0.4589	LR: 0.025000
Training Epoch: 72 [1152/50000]	Loss: 0.5874	LR: 0.025000
Training Epoch: 72 [1280/50000]	Loss: 0.4887	LR: 0.025000
Training Epoch: 72 [1408/50000]	Loss: 0.4536	LR: 0.025000
Training Epoch: 72 [1536/50000]	Loss: 0.6589	LR: 0.025000
Training Epoch: 72 [1664/50000]	Loss: 0.3483	LR: 0.025000
Training Epoch: 72 [1792/50000]	Loss: 0.4395	LR: 0.025000
Training Epoch: 72 [1920/50000]	Loss: 0.3663	LR: 0.025000
Training Epoch: 72 [2048/50000]	Loss: 0.3464	LR: 0.025000
Training Epoch: 72 [2176/50000]	Loss: 0.4289	LR: 0.025000
Training Epoch: 72 [2304/50000]	Loss: 0.4270	LR: 0.025000
Training Epoch: 72 [2432/50000]	Loss: 0.4987	LR: 0.025000
Training Epoch: 72 [2560/50000]	Loss: 0.3358	LR: 0.025000
Training Epoch: 72 [2688/50000]	Loss: 0.4485	LR: 0.025000
Training Epoch: 72 [2816/50000]	Loss: 0.4136	LR: 0.025000
Training Epoch: 72 [2944/50000]	Loss: 0.3714	LR: 0.025000
Training Epoch: 72 [3072/50000]	Loss: 0.4708	LR: 0.025000
Training Epoch: 72 [3200/50000]	Loss: 0.4195	LR: 0.025000
Training Epoch: 72 [3328/50000]	Loss: 0.4154	LR: 0.025000
Training Epoch: 72 [3456/50000]	Loss: 0.4908	LR: 0.025000
Training Epoch: 72 [3584/50000]	Loss: 0.4001	LR: 0.025000
Training Epoch: 72 [3712/50000]	Loss: 0.3063	LR: 0.025000
Training Epoch: 72 [3840/50000]	Loss: 0.3887	LR: 0.025000
Training Epoch: 72 [3968/50000]	Loss: 0.3576	LR: 0.025000
Training Epoch: 72 [4096/50000]	Loss: 0.3412	LR: 0.025000
Training Epoch: 72 [4224/50000]	Loss: 0.4452	LR: 0.025000
Training Epoch: 72 [4352/50000]	Loss: 0.4434	LR: 0.025000
Training Epoch: 72 [4480/50000]	Loss: 0.4289	LR: 0.025000
Training Epoch: 72 [4608/50000]	Loss: 0.4837	LR: 0.025000
Training Epoch: 72 [4736/50000]	Loss: 0.4491	LR: 0.025000
Training Epoch: 72 [4864/50000]	Loss: 0.4383	LR: 0.025000
Training Epoch: 72 [4992/50000]	Loss: 0.4471	LR: 0.025000
Training Epoch: 72 [5120/50000]	Loss: 0.4305	LR: 0.025000
Training Epoch: 72 [5248/50000]	Loss: 0.3766	LR: 0.025000
Training Epoch: 72 [5376/50000]	Loss: 0.4343	LR: 0.025000
Training Epoch: 72 [5504/50000]	Loss: 0.3889	LR: 0.025000
Training Epoch: 72 [5632/50000]	Loss: 0.4642	LR: 0.025000
Training Epoch: 72 [5760/50000]	Loss: 0.4107	LR: 0.025000
Training Epoch: 72 [5888/50000]	Loss: 0.4158	LR: 0.025000
Training Epoch: 72 [6016/50000]	Loss: 0.5164	LR: 0.025000
Training Epoch: 72 [6144/50000]	Loss: 0.4654	LR: 0.025000
Training Epoch: 72 [6272/50000]	Loss: 0.3484	LR: 0.025000
Training Epoch: 72 [6400/50000]	Loss: 0.3783	LR: 0.025000
Training Epoch: 72 [6528/50000]	Loss: 0.4254	LR: 0.025000
Training Epoch: 72 [6656/50000]	Loss: 0.4176	LR: 0.025000
Training Epoch: 72 [6784/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 72 [6912/50000]	Loss: 0.4035	LR: 0.025000
Training Epoch: 72 [7040/50000]	Loss: 0.4735	LR: 0.025000
Training Epoch: 72 [7168/50000]	Loss: 0.4090	LR: 0.025000
Training Epoch: 72 [7296/50000]	Loss: 0.5858	LR: 0.025000
Training Epoch: 72 [7424/50000]	Loss: 0.5387	LR: 0.025000
Training Epoch: 72 [7552/50000]	Loss: 0.5144	LR: 0.025000
Training Epoch: 72 [7680/50000]	Loss: 0.3960	LR: 0.025000
Training Epoch: 72 [7808/50000]	Loss: 0.4991	LR: 0.025000
Training Epoch: 72 [7936/50000]	Loss: 0.5362	LR: 0.025000
Training Epoch: 72 [8064/50000]	Loss: 0.4115	LR: 0.025000
Training Epoch: 72 [8192/50000]	Loss: 0.4205	LR: 0.025000
Training Epoch: 72 [8320/50000]	Loss: 0.4435	LR: 0.025000
Training Epoch: 72 [8448/50000]	Loss: 0.5200	LR: 0.025000
Training Epoch: 72 [8576/50000]	Loss: 0.4345	LR: 0.025000
Training Epoch: 72 [8704/50000]	Loss: 0.3348	LR: 0.025000
Training Epoch: 72 [8832/50000]	Loss: 0.5687	LR: 0.025000
Training Epoch: 72 [8960/50000]	Loss: 0.4445	LR: 0.025000
Training Epoch: 72 [9088/50000]	Loss: 0.4958	LR: 0.025000
Training Epoch: 72 [9216/50000]	Loss: 0.3095	LR: 0.025000
Training Epoch: 72 [9344/50000]	Loss: 0.4079	LR: 0.025000
Training Epoch: 72 [9472/50000]	Loss: 0.2927	LR: 0.025000
Training Epoch: 72 [9600/50000]	Loss: 0.3582	LR: 0.025000
Training Epoch: 72 [9728/50000]	Loss: 0.5008	LR: 0.025000
Training Epoch: 72 [9856/50000]	Loss: 0.5498	LR: 0.025000
Training Epoch: 72 [9984/50000]	Loss: 0.4845	LR: 0.025000
Training Epoch: 72 [10112/50000]	Loss: 0.4582	LR: 0.025000
Training Epoch: 72 [10240/50000]	Loss: 0.4184	LR: 0.025000
Training Epoch: 72 [10368/50000]	Loss: 0.4706	LR: 0.025000
Training Epoch: 72 [10496/50000]	Loss: 0.3452	LR: 0.025000
Training Epoch: 72 [10624/50000]	Loss: 0.4851	LR: 0.025000
Training Epoch: 72 [10752/50000]	Loss: 0.3634	LR: 0.025000
Training Epoch: 72 [10880/50000]	Loss: 0.4568	LR: 0.025000
Training Epoch: 72 [11008/50000]	Loss: 0.4504	LR: 0.025000
Training Epoch: 72 [11136/50000]	Loss: 0.3507	LR: 0.025000
Training Epoch: 72 [11264/50000]	Loss: 0.3496	LR: 0.025000
Training Epoch: 72 [11392/50000]	Loss: 0.3660	LR: 0.025000
Training Epoch: 72 [11520/50000]	Loss: 0.3447	LR: 0.025000
Training Epoch: 72 [11648/50000]	Loss: 0.3169	LR: 0.025000
Training Epoch: 72 [11776/50000]	Loss: 0.4413	LR: 0.025000
Training Epoch: 72 [11904/50000]	Loss: 0.2629	LR: 0.025000
Training Epoch: 72 [12032/50000]	Loss: 0.4322	LR: 0.025000
Training Epoch: 72 [12160/50000]	Loss: 0.4354	LR: 0.025000
Training Epoch: 72 [12288/50000]	Loss: 0.4525	LR: 0.025000
Training Epoch: 72 [12416/50000]	Loss: 0.3882	LR: 0.025000
Training Epoch: 72 [12544/50000]	Loss: 0.4032	LR: 0.025000
Training Epoch: 72 [12672/50000]	Loss: 0.5014	LR: 0.025000
Training Epoch: 72 [12800/50000]	Loss: 0.3812	LR: 0.025000
Training Epoch: 72 [12928/50000]	Loss: 0.6104	LR: 0.025000
Training Epoch: 72 [13056/50000]	Loss: 0.3247	LR: 0.025000
Training Epoch: 72 [13184/50000]	Loss: 0.5045	LR: 0.025000
Training Epoch: 72 [13312/50000]	Loss: 0.3116	LR: 0.025000
Training Epoch: 72 [13440/50000]	Loss: 0.3224	LR: 0.025000
Training Epoch: 72 [13568/50000]	Loss: 0.4148	LR: 0.025000
Training Epoch: 72 [13696/50000]	Loss: 0.3537	LR: 0.025000
Training Epoch: 72 [13824/50000]	Loss: 0.3587	LR: 0.025000
Training Epoch: 72 [13952/50000]	Loss: 0.3887	LR: 0.025000
Training Epoch: 72 [14080/50000]	Loss: 0.4445	LR: 0.025000
Training Epoch: 72 [14208/50000]	Loss: 0.5949	LR: 0.025000
Training Epoch: 72 [14336/50000]	Loss: 0.3882	LR: 0.025000
Training Epoch: 72 [14464/50000]	Loss: 0.3977	LR: 0.025000
Training Epoch: 72 [14592/50000]	Loss: 0.4351	LR: 0.025000
Training Epoch: 72 [14720/50000]	Loss: 0.3969	LR: 0.025000
Training Epoch: 72 [14848/50000]	Loss: 0.5359	LR: 0.025000
Training Epoch: 72 [14976/50000]	Loss: 0.3944	LR: 0.025000
Training Epoch: 72 [15104/50000]	Loss: 0.4738	LR: 0.025000
Training Epoch: 72 [15232/50000]	Loss: 0.3250	LR: 0.025000
Training Epoch: 72 [15360/50000]	Loss: 0.3317	LR: 0.025000
Training Epoch: 72 [15488/50000]	Loss: 0.3536	LR: 0.025000
Training Epoch: 72 [15616/50000]	Loss: 0.4558	LR: 0.025000
Training Epoch: 72 [15744/50000]	Loss: 0.3171	LR: 0.025000
Training Epoch: 72 [15872/50000]	Loss: 0.3806	LR: 0.025000
Training Epoch: 72 [16000/50000]	Loss: 0.4579	LR: 0.025000
Training Epoch: 72 [16128/50000]	Loss: 0.3490	LR: 0.025000
Training Epoch: 72 [16256/50000]	Loss: 0.2400	LR: 0.025000
Training Epoch: 72 [16384/50000]	Loss: 0.6183	LR: 0.025000
Training Epoch: 72 [16512/50000]	Loss: 0.3467	LR: 0.025000
Training Epoch: 72 [16640/50000]	Loss: 0.3930	LR: 0.025000
Training Epoch: 72 [16768/50000]	Loss: 0.5613	LR: 0.025000
Training Epoch: 72 [16896/50000]	Loss: 0.3888	LR: 0.025000
Training Epoch: 72 [17024/50000]	Loss: 0.4148	LR: 0.025000
Training Epoch: 72 [17152/50000]	Loss: 0.5013	LR: 0.025000
Training Epoch: 72 [17280/50000]	Loss: 0.5118	LR: 0.025000
Training Epoch: 72 [17408/50000]	Loss: 0.3658	LR: 0.025000
Training Epoch: 72 [17536/50000]	Loss: 0.4072	LR: 0.025000
Training Epoch: 72 [17664/50000]	Loss: 0.4083	LR: 0.025000
Training Epoch: 72 [17792/50000]	Loss: 0.4487	LR: 0.025000
Training Epoch: 72 [17920/50000]	Loss: 0.4452	LR: 0.025000
Training Epoch: 72 [18048/50000]	Loss: 0.3866	LR: 0.025000
Training Epoch: 72 [18176/50000]	Loss: 0.3995	LR: 0.025000
Training Epoch: 72 [18304/50000]	Loss: 0.4606	LR: 0.025000
Training Epoch: 72 [18432/50000]	Loss: 0.5104	LR: 0.025000
Training Epoch: 72 [18560/50000]	Loss: 0.4757	LR: 0.025000
Training Epoch: 72 [18688/50000]	Loss: 0.3987	LR: 0.025000
Training Epoch: 72 [18816/50000]	Loss: 0.3820	LR: 0.025000
Training Epoch: 72 [18944/50000]	Loss: 0.4533	LR: 0.025000
Training Epoch: 72 [19072/50000]	Loss: 0.5019	LR: 0.025000
Training Epoch: 72 [19200/50000]	Loss: 0.4334	LR: 0.025000
Training Epoch: 72 [19328/50000]	Loss: 0.5398	LR: 0.025000
Training Epoch: 72 [19456/50000]	Loss: 0.4191	LR: 0.025000
Training Epoch: 72 [19584/50000]	Loss: 0.5668	LR: 0.025000
Training Epoch: 72 [19712/50000]	Loss: 0.4225	LR: 0.025000
Training Epoch: 72 [19840/50000]	Loss: 0.4178	LR: 0.025000
Training Epoch: 72 [19968/50000]	Loss: 0.5641	LR: 0.025000
Training Epoch: 72 [20096/50000]	Loss: 0.3827	LR: 0.025000
Training Epoch: 72 [20224/50000]	Loss: 0.3797	LR: 0.025000
Training Epoch: 72 [20352/50000]	Loss: 0.4731	LR: 0.025000
Training Epoch: 72 [20480/50000]	Loss: 0.4673	LR: 0.025000
Training Epoch: 72 [20608/50000]	Loss: 0.5063	LR: 0.025000
Training Epoch: 72 [20736/50000]	Loss: 0.5240	LR: 0.025000
Training Epoch: 72 [20864/50000]	Loss: 0.3939	LR: 0.025000
Training Epoch: 72 [20992/50000]	Loss: 0.4320	LR: 0.025000
Training Epoch: 72 [21120/50000]	Loss: 0.5623	LR: 0.025000
Training Epoch: 72 [21248/50000]	Loss: 0.4373	LR: 0.025000
Training Epoch: 72 [21376/50000]	Loss: 0.4830	LR: 0.025000
Training Epoch: 72 [21504/50000]	Loss: 0.4738	LR: 0.025000
Training Epoch: 72 [21632/50000]	Loss: 0.4408	LR: 0.025000
Training Epoch: 72 [21760/50000]	Loss: 0.2766	LR: 0.025000
Training Epoch: 72 [21888/50000]	Loss: 0.3425	LR: 0.025000
Training Epoch: 72 [22016/50000]	Loss: 0.4937	LR: 0.025000
Training Epoch: 72 [22144/50000]	Loss: 0.4884	LR: 0.025000
Training Epoch: 72 [22272/50000]	Loss: 0.4694	LR: 0.025000
Training Epoch: 72 [22400/50000]	Loss: 0.4508	LR: 0.025000
Training Epoch: 72 [22528/50000]	Loss: 0.5437	LR: 0.025000
Training Epoch: 72 [22656/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 72 [22784/50000]	Loss: 0.5317	LR: 0.025000
Training Epoch: 72 [22912/50000]	Loss: 0.4218	LR: 0.025000
Training Epoch: 72 [23040/50000]	Loss: 0.4915	LR: 0.025000
Training Epoch: 72 [23168/50000]	Loss: 0.4113	LR: 0.025000
Training Epoch: 72 [23296/50000]	Loss: 0.4758	LR: 0.025000
Training Epoch: 72 [23424/50000]	Loss: 0.4048	LR: 0.025000
Training Epoch: 72 [23552/50000]	Loss: 0.3571	LR: 0.025000
Training Epoch: 72 [23680/50000]	Loss: 0.4764	LR: 0.025000
Training Epoch: 72 [23808/50000]	Loss: 0.4770	LR: 0.025000
Training Epoch: 72 [23936/50000]	Loss: 0.5946	LR: 0.025000
Training Epoch: 72 [24064/50000]	Loss: 0.3970	LR: 0.025000
Training Epoch: 72 [24192/50000]	Loss: 0.5412	LR: 0.025000
Training Epoch: 72 [24320/50000]	Loss: 0.6593	LR: 0.025000
Training Epoch: 72 [24448/50000]	Loss: 0.3250	LR: 0.025000
Training Epoch: 72 [24576/50000]	Loss: 0.5029	LR: 0.025000
Training Epoch: 72 [24704/50000]	Loss: 0.4412	LR: 0.025000
Training Epoch: 72 [24832/50000]	Loss: 0.5950	LR: 0.025000
Training Epoch: 72 [24960/50000]	Loss: 0.4627	LR: 0.025000
Training Epoch: 72 [25088/50000]	Loss: 0.4275	LR: 0.025000
Training Epoch: 72 [25216/50000]	Loss: 0.4237	LR: 0.025000
Training Epoch: 72 [25344/50000]	Loss: 0.5041	LR: 0.025000
Training Epoch: 72 [25472/50000]	Loss: 0.5269	LR: 0.025000
Training Epoch: 72 [25600/50000]	Loss: 0.5382	LR: 0.025000
Training Epoch: 72 [25728/50000]	Loss: 0.4478	LR: 0.025000
Training Epoch: 72 [25856/50000]	Loss: 0.6477	LR: 0.025000
Training Epoch: 72 [25984/50000]	Loss: 0.5963	LR: 0.025000
Training Epoch: 72 [26112/50000]	Loss: 0.3368	LR: 0.025000
Training Epoch: 72 [26240/50000]	Loss: 0.5127	LR: 0.025000
Training Epoch: 72 [26368/50000]	Loss: 0.4815	LR: 0.025000
Training Epoch: 72 [26496/50000]	Loss: 0.4675	LR: 0.025000
Training Epoch: 72 [26624/50000]	Loss: 0.3606	LR: 0.025000
Training Epoch: 72 [26752/50000]	Loss: 0.4454	LR: 0.025000
Training Epoch: 72 [26880/50000]	Loss: 0.3900	LR: 0.025000
Training Epoch: 72 [27008/50000]	Loss: 0.5311	LR: 0.025000
Training Epoch: 72 [27136/50000]	Loss: 0.4564	LR: 0.025000
Training Epoch: 72 [27264/50000]	Loss: 0.4522	LR: 0.025000
Training Epoch: 72 [27392/50000]	Loss: 0.4289	LR: 0.025000
Training Epoch: 72 [27520/50000]	Loss: 0.4403	LR: 0.025000
Training Epoch: 72 [27648/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 72 [27776/50000]	Loss: 0.4345	LR: 0.025000
Training Epoch: 72 [27904/50000]	Loss: 0.4270	LR: 0.025000
Training Epoch: 72 [28032/50000]	Loss: 0.7215	LR: 0.025000
Training Epoch: 72 [28160/50000]	Loss: 0.3553	LR: 0.025000
Training Epoch: 72 [28288/50000]	Loss: 0.4014	LR: 0.025000
Training Epoch: 72 [28416/50000]	Loss: 0.5331	LR: 0.025000
Training Epoch: 72 [28544/50000]	Loss: 0.5167	LR: 0.025000
Training Epoch: 72 [28672/50000]	Loss: 0.3851	LR: 0.025000
Training Epoch: 72 [28800/50000]	Loss: 0.3343	LR: 0.025000
Training Epoch: 72 [28928/50000]	Loss: 0.5376	LR: 0.025000
Training Epoch: 72 [29056/50000]	Loss: 0.3697	LR: 0.025000
Training Epoch: 72 [29184/50000]	Loss: 0.4143	LR: 0.025000
Training Epoch: 72 [29312/50000]	Loss: 0.4208	LR: 0.025000
Training Epoch: 72 [29440/50000]	Loss: 0.4121	LR: 0.025000
Training Epoch: 72 [29568/50000]	Loss: 0.4376	LR: 0.025000
Training Epoch: 72 [29696/50000]	Loss: 0.2942	LR: 0.025000
Training Epoch: 72 [29824/50000]	Loss: 0.5044	LR: 0.025000
Training Epoch: 72 [29952/50000]	Loss: 0.2791	LR: 0.025000
Training Epoch: 72 [30080/50000]	Loss: 0.4900	LR: 0.025000
Training Epoch: 72 [30208/50000]	Loss: 0.3921	LR: 0.025000
Training Epoch: 72 [30336/50000]	Loss: 0.3881	LR: 0.025000
Training Epoch: 72 [30464/50000]	Loss: 0.5457	LR: 0.025000
Training Epoch: 72 [30592/50000]	Loss: 0.4517	LR: 0.025000
Training Epoch: 72 [30720/50000]	Loss: 0.3886	LR: 0.025000
Training Epoch: 72 [30848/50000]	Loss: 0.5851	LR: 0.025000
Training Epoch: 72 [30976/50000]	Loss: 0.4869	LR: 0.025000
Training Epoch: 72 [31104/50000]	Loss: 0.5743	LR: 0.025000
Training Epoch: 72 [31232/50000]	Loss: 0.3909	LR: 0.025000
Training Epoch: 72 [31360/50000]	Loss: 0.4347	LR: 0.025000
Training Epoch: 72 [31488/50000]	Loss: 0.5013	LR: 0.025000
Training Epoch: 72 [31616/50000]	Loss: 0.4198	LR: 0.025000
Training Epoch: 72 [31744/50000]	Loss: 0.3615	LR: 0.025000
Training Epoch: 72 [31872/50000]	Loss: 0.5510	LR: 0.025000
Training Epoch: 72 [32000/50000]	Loss: 0.4455	LR: 0.025000
Training Epoch: 72 [32128/50000]	Loss: 0.3395	LR: 0.025000
Training Epoch: 72 [32256/50000]	Loss: 0.5100	LR: 0.025000
Training Epoch: 72 [32384/50000]	Loss: 0.5910	LR: 0.025000
Training Epoch: 72 [32512/50000]	Loss: 0.5229	LR: 0.025000
Training Epoch: 72 [32640/50000]	Loss: 0.6126	LR: 0.025000
Training Epoch: 72 [32768/50000]	Loss: 0.4935	LR: 0.025000
Training Epoch: 72 [32896/50000]	Loss: 0.4540	LR: 0.025000
Training Epoch: 72 [33024/50000]	Loss: 0.3455	LR: 0.025000
Training Epoch: 72 [33152/50000]	Loss: 0.7184	LR: 0.025000
Training Epoch: 72 [33280/50000]	Loss: 0.5717	LR: 0.025000
Training Epoch: 72 [33408/50000]	Loss: 0.4515	LR: 0.025000
Training Epoch: 72 [33536/50000]	Loss: 0.5826	LR: 0.025000
Training Epoch: 72 [33664/50000]	Loss: 0.6154	LR: 0.025000
Training Epoch: 72 [33792/50000]	Loss: 0.5388	LR: 0.025000
Training Epoch: 72 [33920/50000]	Loss: 0.4889	LR: 0.025000
Training Epoch: 72 [34048/50000]	Loss: 0.5292	LR: 0.025000
Training Epoch: 72 [34176/50000]	Loss: 0.3970	LR: 0.025000
Training Epoch: 72 [34304/50000]	Loss: 0.4313	LR: 0.025000
Training Epoch: 72 [34432/50000]	Loss: 0.3862	LR: 0.025000
Training Epoch: 72 [34560/50000]	Loss: 0.5252	LR: 0.025000
Training Epoch: 72 [34688/50000]	Loss: 0.4477	LR: 0.025000
Training Epoch: 72 [34816/50000]	Loss: 0.3928	LR: 0.025000
Training Epoch: 72 [34944/50000]	Loss: 0.4637	LR: 0.025000
Training Epoch: 72 [35072/50000]	Loss: 0.5519	LR: 0.025000
Training Epoch: 72 [35200/50000]	Loss: 0.4901	LR: 0.025000
Training Epoch: 72 [35328/50000]	Loss: 0.5359	LR: 0.025000
Training Epoch: 72 [35456/50000]	Loss: 0.4418	LR: 0.025000
Training Epoch: 72 [35584/50000]	Loss: 0.3719	LR: 0.025000
Training Epoch: 72 [35712/50000]	Loss: 0.4126	LR: 0.025000
Training Epoch: 72 [35840/50000]	Loss: 0.3921	LR: 0.025000
Training Epoch: 72 [35968/50000]	Loss: 0.6317	LR: 0.025000
Training Epoch: 72 [36096/50000]	Loss: 0.5205	LR: 0.025000
Training Epoch: 72 [36224/50000]	Loss: 0.5142	LR: 0.025000
Training Epoch: 72 [36352/50000]	Loss: 0.5302	LR: 0.025000
Training Epoch: 72 [36480/50000]	Loss: 0.5743	LR: 0.025000
Training Epoch: 72 [36608/50000]	Loss: 0.4439	LR: 0.025000
Training Epoch: 72 [36736/50000]	Loss: 0.5060	LR: 0.025000
Training Epoch: 72 [36864/50000]	Loss: 0.4280	LR: 0.025000
Training Epoch: 72 [36992/50000]	Loss: 0.3574	LR: 0.025000
Training Epoch: 72 [37120/50000]	Loss: 0.4974	LR: 0.025000
Training Epoch: 72 [37248/50000]	Loss: 0.5501	LR: 0.025000
Training Epoch: 72 [37376/50000]	Loss: 0.3713	LR: 0.025000
Training Epoch: 72 [37504/50000]	Loss: 0.7489	LR: 0.025000
Training Epoch: 72 [37632/50000]	Loss: 0.6175	LR: 0.025000
Training Epoch: 72 [37760/50000]	Loss: 0.5408	LR: 0.025000
Training Epoch: 72 [37888/50000]	Loss: 0.3976	LR: 0.025000
Training Epoch: 72 [38016/50000]	Loss: 0.4327	LR: 0.025000
Training Epoch: 72 [38144/50000]	Loss: 0.5392	LR: 0.025000
Training Epoch: 72 [38272/50000]	Loss: 0.6229	LR: 0.025000
Training Epoch: 72 [38400/50000]	Loss: 0.6867	LR: 0.025000
Training Epoch: 72 [38528/50000]	Loss: 0.5518	LR: 0.025000
Training Epoch: 72 [38656/50000]	Loss: 0.6717	LR: 0.025000
Training Epoch: 72 [38784/50000]	Loss: 0.4561	LR: 0.025000
Training Epoch: 72 [38912/50000]	Loss: 0.5517	LR: 0.025000
Training Epoch: 72 [39040/50000]	Loss: 0.2664	LR: 0.025000
Training Epoch: 72 [39168/50000]	Loss: 0.4557	LR: 0.025000
Training Epoch: 72 [39296/50000]	Loss: 0.6007	LR: 0.025000
Training Epoch: 72 [39424/50000]	Loss: 0.5815	LR: 0.025000
Training Epoch: 72 [39552/50000]	Loss: 0.5410	LR: 0.025000
Training Epoch: 72 [39680/50000]	Loss: 0.4845	LR: 0.025000
Training Epoch: 72 [39808/50000]	Loss: 0.5411	LR: 0.025000
Training Epoch: 72 [39936/50000]	Loss: 0.5346	LR: 0.025000
Training Epoch: 72 [40064/50000]	Loss: 0.4017	LR: 0.025000
Training Epoch: 72 [40192/50000]	Loss: 0.6121	LR: 0.025000
Training Epoch: 72 [40320/50000]	Loss: 0.4147	LR: 0.025000
Training Epoch: 72 [40448/50000]	Loss: 0.4350	LR: 0.025000
Training Epoch: 72 [40576/50000]	Loss: 0.4610	LR: 0.025000
Training Epoch: 72 [40704/50000]	Loss: 0.4963	LR: 0.025000
Training Epoch: 72 [40832/50000]	Loss: 0.4206	LR: 0.025000
Training Epoch: 72 [40960/50000]	Loss: 0.5352	LR: 0.025000
Training Epoch: 72 [41088/50000]	Loss: 0.5783	LR: 0.025000
Training Epoch: 72 [41216/50000]	Loss: 0.4714	LR: 0.025000
Training Epoch: 72 [41344/50000]	Loss: 0.5156	LR: 0.025000
Training Epoch: 72 [41472/50000]	Loss: 0.5744	LR: 0.025000
Training Epoch: 72 [41600/50000]	Loss: 0.5434	LR: 0.025000
Training Epoch: 72 [41728/50000]	Loss: 0.5731	LR: 0.025000
Training Epoch: 72 [41856/50000]	Loss: 0.3559	LR: 0.025000
Training Epoch: 72 [41984/50000]	Loss: 0.4548	LR: 0.025000
Training Epoch: 72 [42112/50000]	Loss: 0.4924	LR: 0.025000
Training Epoch: 72 [42240/50000]	Loss: 0.5936	LR: 0.025000
Training Epoch: 72 [42368/50000]	Loss: 0.3930	LR: 0.025000
Training Epoch: 72 [42496/50000]	Loss: 0.5074	LR: 0.025000
Training Epoch: 72 [42624/50000]	Loss: 0.4805	LR: 0.025000
Training Epoch: 72 [42752/50000]	Loss: 0.5468	LR: 0.025000
Training Epoch: 72 [42880/50000]	Loss: 0.5617	LR: 0.025000
Training Epoch: 72 [43008/50000]	Loss: 0.6437	LR: 0.025000
Training Epoch: 72 [43136/50000]	Loss: 0.3997	LR: 0.025000
Training Epoch: 72 [43264/50000]	Loss: 0.5910	LR: 0.025000
Training Epoch: 72 [43392/50000]	Loss: 0.6958	LR: 0.025000
Training Epoch: 72 [43520/50000]	Loss: 0.6349	LR: 0.025000
Training Epoch: 72 [43648/50000]	Loss: 0.3990	LR: 0.025000
Training Epoch: 72 [43776/50000]	Loss: 0.5057	LR: 0.025000
Training Epoch: 72 [43904/50000]	Loss: 0.5323	LR: 0.025000
Training Epoch: 72 [44032/50000]	Loss: 0.6748	LR: 0.025000
Training Epoch: 72 [44160/50000]	Loss: 0.5700	LR: 0.025000
Training Epoch: 72 [44288/50000]	Loss: 0.6220	LR: 0.025000
Training Epoch: 72 [44416/50000]	Loss: 0.6012	LR: 0.025000
Training Epoch: 72 [44544/50000]	Loss: 0.4258	LR: 0.025000
Training Epoch: 72 [44672/50000]	Loss: 0.4368	LR: 0.025000
Training Epoch: 72 [44800/50000]	Loss: 0.3771	LR: 0.025000
Training Epoch: 72 [44928/50000]	Loss: 0.5029	LR: 0.025000
Training Epoch: 72 [45056/50000]	Loss: 0.5283	LR: 0.025000
Training Epoch: 72 [45184/50000]	Loss: 0.4688	LR: 0.025000
Training Epoch: 72 [45312/50000]	Loss: 0.5755	LR: 0.025000
Training Epoch: 72 [45440/50000]	Loss: 0.5435	LR: 0.025000
Training Epoch: 72 [45568/50000]	Loss: 0.5635	LR: 0.025000
Training Epoch: 72 [45696/50000]	Loss: 0.4474	LR: 0.025000
Training Epoch: 72 [45824/50000]	Loss: 0.4060	LR: 0.025000
Training Epoch: 72 [45952/50000]	Loss: 0.6106	LR: 0.025000
Training Epoch: 72 [46080/50000]	Loss: 0.4821	LR: 0.025000
Training Epoch: 72 [46208/50000]	Loss: 0.4988	LR: 0.025000
Training Epoch: 72 [46336/50000]	Loss: 0.5135	LR: 0.025000
Training Epoch: 72 [46464/50000]	Loss: 0.5635	LR: 0.025000
Training Epoch: 72 [46592/50000]	Loss: 0.4554	LR: 0.025000
Training Epoch: 72 [46720/50000]	Loss: 0.4965	LR: 0.025000
Training Epoch: 72 [46848/50000]	Loss: 0.5280	LR: 0.025000
Training Epoch: 72 [46976/50000]	Loss: 0.5443	LR: 0.025000
Training Epoch: 72 [47104/50000]	Loss: 0.4536	LR: 0.025000
Training Epoch: 72 [47232/50000]	Loss: 0.3126	LR: 0.025000
Training Epoch: 72 [47360/50000]	Loss: 0.6204	LR: 0.025000
Training Epoch: 72 [47488/50000]	Loss: 0.7177	LR: 0.025000
Training Epoch: 72 [47616/50000]	Loss: 0.4436	LR: 0.025000
Training Epoch: 72 [47744/50000]	Loss: 0.4019	LR: 0.025000
Training Epoch: 72 [47872/50000]	Loss: 0.6398	LR: 0.025000
Training Epoch: 72 [48000/50000]	Loss: 0.5304	LR: 0.025000
Training Epoch: 72 [48128/50000]	Loss: 0.4639	LR: 0.025000
Training Epoch: 72 [48256/50000]	Loss: 0.5320	LR: 0.025000
Training Epoch: 72 [48384/50000]	Loss: 0.4615	LR: 0.025000
Training Epoch: 72 [48512/50000]	Loss: 0.5492	LR: 0.025000
Training Epoch: 72 [48640/50000]	Loss: 0.4333	LR: 0.025000
Training Epoch: 72 [48768/50000]	Loss: 0.4628	LR: 0.025000
Training Epoch: 72 [48896/50000]	Loss: 0.5617	LR: 0.025000
Training Epoch: 72 [49024/50000]	Loss: 0.5608	LR: 0.025000
Training Epoch: 72 [49152/50000]	Loss: 0.3676	LR: 0.025000
Training Epoch: 72 [49280/50000]	Loss: 0.4883	LR: 0.025000
Training Epoch: 72 [49408/50000]	Loss: 0.4528	LR: 0.025000
Training Epoch: 72 [49536/50000]	Loss: 0.4733	LR: 0.025000
Training Epoch: 72 [49664/50000]	Loss: 0.4758	LR: 0.025000
Training Epoch: 72 [49792/50000]	Loss: 0.5398	LR: 0.025000
Training Epoch: 72 [49920/50000]	Loss: 0.5719	LR: 0.025000
Training Epoch: 72 [50000/50000]	Loss: 0.3877	LR: 0.025000
epoch 72 training time consumed: 53.97s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  264244 GB |  264244 GB |
|       from large pool |  123392 KB |    1034 MB |  263984 GB |  263983 GB |
|       from small pool |   10798 KB |      13 MB |     260 GB |     260 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  264244 GB |  264244 GB |
|       from large pool |  123392 KB |    1034 MB |  263984 GB |  263983 GB |
|       from small pool |   10798 KB |      13 MB |     260 GB |     260 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  116283 GB |  116283 GB |
|       from large pool |  155136 KB |  433088 KB |  115995 GB |  115995 GB |
|       from small pool |    1490 KB |    3494 KB |     287 GB |     287 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   10196 K  |   10196 K  |
|       from large pool |      24    |      65    |    5322 K  |    5322 K  |
|       from small pool |     231    |     274    |    4874 K  |    4873 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   10196 K  |   10196 K  |
|       from large pool |      24    |      65    |    5322 K  |    5322 K  |
|       from small pool |     231    |     274    |    4874 K  |    4873 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5038 K  |    5037 K  |
|       from large pool |       9    |      14    |    2576 K  |    2576 K  |
|       from small pool |      12    |      16    |    2461 K  |    2461 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 72, Average loss: 0.0112, Accuracy: 0.6593, Time consumed:3.46s

Training Epoch: 73 [128/50000]	Loss: 0.4390	LR: 0.025000
Training Epoch: 73 [256/50000]	Loss: 0.3563	LR: 0.025000
Training Epoch: 73 [384/50000]	Loss: 0.3601	LR: 0.025000
Training Epoch: 73 [512/50000]	Loss: 0.4211	LR: 0.025000
Training Epoch: 73 [640/50000]	Loss: 0.4428	LR: 0.025000
Training Epoch: 73 [768/50000]	Loss: 0.4350	LR: 0.025000
Training Epoch: 73 [896/50000]	Loss: 0.4639	LR: 0.025000
Training Epoch: 73 [1024/50000]	Loss: 0.4989	LR: 0.025000
Training Epoch: 73 [1152/50000]	Loss: 0.3623	LR: 0.025000
Training Epoch: 73 [1280/50000]	Loss: 0.5021	LR: 0.025000
Training Epoch: 73 [1408/50000]	Loss: 0.4513	LR: 0.025000
Training Epoch: 73 [1536/50000]	Loss: 0.4034	LR: 0.025000
Training Epoch: 73 [1664/50000]	Loss: 0.4720	LR: 0.025000
Training Epoch: 73 [1792/50000]	Loss: 0.3036	LR: 0.025000
Training Epoch: 73 [1920/50000]	Loss: 0.3352	LR: 0.025000
Training Epoch: 73 [2048/50000]	Loss: 0.4127	LR: 0.025000
Training Epoch: 73 [2176/50000]	Loss: 0.4000	LR: 0.025000
Training Epoch: 73 [2304/50000]	Loss: 0.3406	LR: 0.025000
Training Epoch: 73 [2432/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 73 [2560/50000]	Loss: 0.4769	LR: 0.025000
Training Epoch: 73 [2688/50000]	Loss: 0.3471	LR: 0.025000
Training Epoch: 73 [2816/50000]	Loss: 0.4252	LR: 0.025000
Training Epoch: 73 [2944/50000]	Loss: 0.4534	LR: 0.025000
Training Epoch: 73 [3072/50000]	Loss: 0.4010	LR: 0.025000
Training Epoch: 73 [3200/50000]	Loss: 0.3739	LR: 0.025000
Training Epoch: 73 [3328/50000]	Loss: 0.4471	LR: 0.025000
Training Epoch: 73 [3456/50000]	Loss: 0.2513	LR: 0.025000
Training Epoch: 73 [3584/50000]	Loss: 0.3846	LR: 0.025000
Training Epoch: 73 [3712/50000]	Loss: 0.4275	LR: 0.025000
Training Epoch: 73 [3840/50000]	Loss: 0.4901	LR: 0.025000
Training Epoch: 73 [3968/50000]	Loss: 0.3287	LR: 0.025000
Training Epoch: 73 [4096/50000]	Loss: 0.5238	LR: 0.025000
Training Epoch: 73 [4224/50000]	Loss: 0.4236	LR: 0.025000
Training Epoch: 73 [4352/50000]	Loss: 0.3064	LR: 0.025000
Training Epoch: 73 [4480/50000]	Loss: 0.3398	LR: 0.025000
Training Epoch: 73 [4608/50000]	Loss: 0.3541	LR: 0.025000
Training Epoch: 73 [4736/50000]	Loss: 0.3717	LR: 0.025000
Training Epoch: 73 [4864/50000]	Loss: 0.4260	LR: 0.025000
Training Epoch: 73 [4992/50000]	Loss: 0.4785	LR: 0.025000
Training Epoch: 73 [5120/50000]	Loss: 0.4226	LR: 0.025000
Training Epoch: 73 [5248/50000]	Loss: 0.2747	LR: 0.025000
Training Epoch: 73 [5376/50000]	Loss: 0.4236	LR: 0.025000
Training Epoch: 73 [5504/50000]	Loss: 0.4005	LR: 0.025000
Training Epoch: 73 [5632/50000]	Loss: 0.3929	LR: 0.025000
Training Epoch: 73 [5760/50000]	Loss: 0.3769	LR: 0.025000
Training Epoch: 73 [5888/50000]	Loss: 0.3905	LR: 0.025000
Training Epoch: 73 [6016/50000]	Loss: 0.5035	LR: 0.025000
Training Epoch: 73 [6144/50000]	Loss: 0.4909	LR: 0.025000
Training Epoch: 73 [6272/50000]	Loss: 0.4935	LR: 0.025000
Training Epoch: 73 [6400/50000]	Loss: 0.5549	LR: 0.025000
Training Epoch: 73 [6528/50000]	Loss: 0.3137	LR: 0.025000
Training Epoch: 73 [6656/50000]	Loss: 0.3923	LR: 0.025000
Training Epoch: 73 [6784/50000]	Loss: 0.4038	LR: 0.025000
Training Epoch: 73 [6912/50000]	Loss: 0.3942	LR: 0.025000
Training Epoch: 73 [7040/50000]	Loss: 0.4409	LR: 0.025000
Training Epoch: 73 [7168/50000]	Loss: 0.4499	LR: 0.025000
Training Epoch: 73 [7296/50000]	Loss: 0.4226	LR: 0.025000
Training Epoch: 73 [7424/50000]	Loss: 0.3317	LR: 0.025000
Training Epoch: 73 [7552/50000]	Loss: 0.4321	LR: 0.025000
Training Epoch: 73 [7680/50000]	Loss: 0.4101	LR: 0.025000
Training Epoch: 73 [7808/50000]	Loss: 0.3300	LR: 0.025000
Training Epoch: 73 [7936/50000]	Loss: 0.4091	LR: 0.025000
Training Epoch: 73 [8064/50000]	Loss: 0.4085	LR: 0.025000
Training Epoch: 73 [8192/50000]	Loss: 0.5099	LR: 0.025000
Training Epoch: 73 [8320/50000]	Loss: 0.4605	LR: 0.025000
Training Epoch: 73 [8448/50000]	Loss: 0.3787	LR: 0.025000
Training Epoch: 73 [8576/50000]	Loss: 0.4351	LR: 0.025000
Training Epoch: 73 [8704/50000]	Loss: 0.3193	LR: 0.025000
Training Epoch: 73 [8832/50000]	Loss: 0.4695	LR: 0.025000
Training Epoch: 73 [8960/50000]	Loss: 0.3167	LR: 0.025000
Training Epoch: 73 [9088/50000]	Loss: 0.6055	LR: 0.025000
Training Epoch: 73 [9216/50000]	Loss: 0.4039	LR: 0.025000
Training Epoch: 73 [9344/50000]	Loss: 0.3656	LR: 0.025000
Training Epoch: 73 [9472/50000]	Loss: 0.4816	LR: 0.025000
Training Epoch: 73 [9600/50000]	Loss: 0.4022	LR: 0.025000
Training Epoch: 73 [9728/50000]	Loss: 0.4556	LR: 0.025000
Training Epoch: 73 [9856/50000]	Loss: 0.3531	LR: 0.025000
Training Epoch: 73 [9984/50000]	Loss: 0.3412	LR: 0.025000
Training Epoch: 73 [10112/50000]	Loss: 0.4312	LR: 0.025000
Training Epoch: 73 [10240/50000]	Loss: 0.4604	LR: 0.025000
Training Epoch: 73 [10368/50000]	Loss: 0.3251	LR: 0.025000
Training Epoch: 73 [10496/50000]	Loss: 0.4175	LR: 0.025000
Training Epoch: 73 [10624/50000]	Loss: 0.4047	LR: 0.025000
Training Epoch: 73 [10752/50000]	Loss: 0.4237	LR: 0.025000
Training Epoch: 73 [10880/50000]	Loss: 0.6805	LR: 0.025000
Training Epoch: 73 [11008/50000]	Loss: 0.4701	LR: 0.025000
Training Epoch: 73 [11136/50000]	Loss: 0.4492	LR: 0.025000
Training Epoch: 73 [11264/50000]	Loss: 0.3943	LR: 0.025000
Training Epoch: 73 [11392/50000]	Loss: 0.3760	LR: 0.025000
Training Epoch: 73 [11520/50000]	Loss: 0.3849	LR: 0.025000
Training Epoch: 73 [11648/50000]	Loss: 0.5026	LR: 0.025000
Training Epoch: 73 [11776/50000]	Loss: 0.3391	LR: 0.025000
Training Epoch: 73 [11904/50000]	Loss: 0.3696	LR: 0.025000
Training Epoch: 73 [12032/50000]	Loss: 0.4140	LR: 0.025000
Training Epoch: 73 [12160/50000]	Loss: 0.4401	LR: 0.025000
Training Epoch: 73 [12288/50000]	Loss: 0.5733	LR: 0.025000
Training Epoch: 73 [12416/50000]	Loss: 0.4630	LR: 0.025000
Training Epoch: 73 [12544/50000]	Loss: 0.5986	LR: 0.025000
Training Epoch: 73 [12672/50000]	Loss: 0.4038	LR: 0.025000
Training Epoch: 73 [12800/50000]	Loss: 0.4484	LR: 0.025000
Training Epoch: 73 [12928/50000]	Loss: 0.3778	LR: 0.025000
Training Epoch: 73 [13056/50000]	Loss: 0.3297	LR: 0.025000
Training Epoch: 73 [13184/50000]	Loss: 0.5098	LR: 0.025000
Training Epoch: 73 [13312/50000]	Loss: 0.3618	LR: 0.025000
Training Epoch: 73 [13440/50000]	Loss: 0.4816	LR: 0.025000
Training Epoch: 73 [13568/50000]	Loss: 0.3892	LR: 0.025000
Training Epoch: 73 [13696/50000]	Loss: 0.3812	LR: 0.025000
Training Epoch: 73 [13824/50000]	Loss: 0.5244	LR: 0.025000
Training Epoch: 73 [13952/50000]	Loss: 0.3027	LR: 0.025000
Training Epoch: 73 [14080/50000]	Loss: 0.4817	LR: 0.025000
Training Epoch: 73 [14208/50000]	Loss: 0.3493	LR: 0.025000
Training Epoch: 73 [14336/50000]	Loss: 0.3476	LR: 0.025000
Training Epoch: 73 [14464/50000]	Loss: 0.3562	LR: 0.025000
Training Epoch: 73 [14592/50000]	Loss: 0.4755	LR: 0.025000
Training Epoch: 73 [14720/50000]	Loss: 0.4439	LR: 0.025000
Training Epoch: 73 [14848/50000]	Loss: 0.3874	LR: 0.025000
Training Epoch: 73 [14976/50000]	Loss: 0.4856	LR: 0.025000
Training Epoch: 73 [15104/50000]	Loss: 0.3597	LR: 0.025000
Training Epoch: 73 [15232/50000]	Loss: 0.5404	LR: 0.025000
Training Epoch: 73 [15360/50000]	Loss: 0.3949	LR: 0.025000
Training Epoch: 73 [15488/50000]	Loss: 0.4121	LR: 0.025000
Training Epoch: 73 [15616/50000]	Loss: 0.4548	LR: 0.025000
Training Epoch: 73 [15744/50000]	Loss: 0.5023	LR: 0.025000
Training Epoch: 73 [15872/50000]	Loss: 0.4457	LR: 0.025000
Training Epoch: 73 [16000/50000]	Loss: 0.5235	LR: 0.025000
Training Epoch: 73 [16128/50000]	Loss: 0.4457	LR: 0.025000
Training Epoch: 73 [16256/50000]	Loss: 0.3500	LR: 0.025000
Training Epoch: 73 [16384/50000]	Loss: 0.4308	LR: 0.025000
Training Epoch: 73 [16512/50000]	Loss: 0.5219	LR: 0.025000
Training Epoch: 73 [16640/50000]	Loss: 0.4318	LR: 0.025000
Training Epoch: 73 [16768/50000]	Loss: 0.3680	LR: 0.025000
Training Epoch: 73 [16896/50000]	Loss: 0.4061	LR: 0.025000
Training Epoch: 73 [17024/50000]	Loss: 0.4065	LR: 0.025000
Training Epoch: 73 [17152/50000]	Loss: 0.4645	LR: 0.025000
Training Epoch: 73 [17280/50000]	Loss: 0.4359	LR: 0.025000
Training Epoch: 73 [17408/50000]	Loss: 0.4463	LR: 0.025000
Training Epoch: 73 [17536/50000]	Loss: 0.5094	LR: 0.025000
Training Epoch: 73 [17664/50000]	Loss: 0.5190	LR: 0.025000
Training Epoch: 73 [17792/50000]	Loss: 0.3579	LR: 0.025000
Training Epoch: 73 [17920/50000]	Loss: 0.4499	LR: 0.025000
Training Epoch: 73 [18048/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 73 [18176/50000]	Loss: 0.4018	LR: 0.025000
Training Epoch: 73 [18304/50000]	Loss: 0.5003	LR: 0.025000
Training Epoch: 73 [18432/50000]	Loss: 0.3465	LR: 0.025000
Training Epoch: 73 [18560/50000]	Loss: 0.4420	LR: 0.025000
Training Epoch: 73 [18688/50000]	Loss: 0.4702	LR: 0.025000
Training Epoch: 73 [18816/50000]	Loss: 0.6203	LR: 0.025000
Training Epoch: 73 [18944/50000]	Loss: 0.4018	LR: 0.025000
Training Epoch: 73 [19072/50000]	Loss: 0.4381	LR: 0.025000
Training Epoch: 73 [19200/50000]	Loss: 0.4115	LR: 0.025000
Training Epoch: 73 [19328/50000]	Loss: 0.4833	LR: 0.025000
Training Epoch: 73 [19456/50000]	Loss: 0.4499	LR: 0.025000
Training Epoch: 73 [19584/50000]	Loss: 0.3883	LR: 0.025000
Training Epoch: 73 [19712/50000]	Loss: 0.3626	LR: 0.025000
Training Epoch: 73 [19840/50000]	Loss: 0.4504	LR: 0.025000
Training Epoch: 73 [19968/50000]	Loss: 0.4345	LR: 0.025000
Training Epoch: 73 [20096/50000]	Loss: 0.4001	LR: 0.025000
Training Epoch: 73 [20224/50000]	Loss: 0.4713	LR: 0.025000
Training Epoch: 73 [20352/50000]	Loss: 0.5340	LR: 0.025000
Training Epoch: 73 [20480/50000]	Loss: 0.5083	LR: 0.025000
Training Epoch: 73 [20608/50000]	Loss: 0.5131	LR: 0.025000
Training Epoch: 73 [20736/50000]	Loss: 0.3841	LR: 0.025000
Training Epoch: 73 [20864/50000]	Loss: 0.4637	LR: 0.025000
Training Epoch: 73 [20992/50000]	Loss: 0.4802	LR: 0.025000
Training Epoch: 73 [21120/50000]	Loss: 0.5478	LR: 0.025000
Training Epoch: 73 [21248/50000]	Loss: 0.3190	LR: 0.025000
Training Epoch: 73 [21376/50000]	Loss: 0.4751	LR: 0.025000
Training Epoch: 73 [21504/50000]	Loss: 0.4947	LR: 0.025000
Training Epoch: 73 [21632/50000]	Loss: 0.3686	LR: 0.025000
Training Epoch: 73 [21760/50000]	Loss: 0.5320	LR: 0.025000
Training Epoch: 73 [21888/50000]	Loss: 0.5020	LR: 0.025000
Training Epoch: 73 [22016/50000]	Loss: 0.4752	LR: 0.025000
Training Epoch: 73 [22144/50000]	Loss: 0.4160	LR: 0.025000
Training Epoch: 73 [22272/50000]	Loss: 0.3593	LR: 0.025000
Training Epoch: 73 [22400/50000]	Loss: 0.3924	LR: 0.025000
Training Epoch: 73 [22528/50000]	Loss: 0.5772	LR: 0.025000
Training Epoch: 73 [22656/50000]	Loss: 0.5502	LR: 0.025000
Training Epoch: 73 [22784/50000]	Loss: 0.5090	LR: 0.025000
Training Epoch: 73 [22912/50000]	Loss: 0.3890	LR: 0.025000
Training Epoch: 73 [23040/50000]	Loss: 0.5328	LR: 0.025000
Training Epoch: 73 [23168/50000]	Loss: 0.3631	LR: 0.025000
Training Epoch: 73 [23296/50000]	Loss: 0.4752	LR: 0.025000
Training Epoch: 73 [23424/50000]	Loss: 0.5089	LR: 0.025000
Training Epoch: 73 [23552/50000]	Loss: 0.5372	LR: 0.025000
Training Epoch: 73 [23680/50000]	Loss: 0.4146	LR: 0.025000
Training Epoch: 73 [23808/50000]	Loss: 0.3563	LR: 0.025000
Training Epoch: 73 [23936/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 73 [24064/50000]	Loss: 0.4918	LR: 0.025000
Training Epoch: 73 [24192/50000]	Loss: 0.5146	LR: 0.025000
Training Epoch: 73 [24320/50000]	Loss: 0.5786	LR: 0.025000
Training Epoch: 73 [24448/50000]	Loss: 0.6358	LR: 0.025000
Training Epoch: 73 [24576/50000]	Loss: 0.3864	LR: 0.025000
Training Epoch: 73 [24704/50000]	Loss: 0.4077	LR: 0.025000
Training Epoch: 73 [24832/50000]	Loss: 0.4361	LR: 0.025000
Training Epoch: 73 [24960/50000]	Loss: 0.4016	LR: 0.025000
Training Epoch: 73 [25088/50000]	Loss: 0.4492	LR: 0.025000
Training Epoch: 73 [25216/50000]	Loss: 0.3680	LR: 0.025000
Training Epoch: 73 [25344/50000]	Loss: 0.5436	LR: 0.025000
Training Epoch: 73 [25472/50000]	Loss: 0.5424	LR: 0.025000
Training Epoch: 73 [25600/50000]	Loss: 0.5719	LR: 0.025000
Training Epoch: 73 [25728/50000]	Loss: 0.3775	LR: 0.025000
Training Epoch: 73 [25856/50000]	Loss: 0.3944	LR: 0.025000
Training Epoch: 73 [25984/50000]	Loss: 0.4060	LR: 0.025000
Training Epoch: 73 [26112/50000]	Loss: 0.6102	LR: 0.025000
Training Epoch: 73 [26240/50000]	Loss: 0.5959	LR: 0.025000
Training Epoch: 73 [26368/50000]	Loss: 0.4416	LR: 0.025000
Training Epoch: 73 [26496/50000]	Loss: 0.5906	LR: 0.025000
Training Epoch: 73 [26624/50000]	Loss: 0.3394	LR: 0.025000
Training Epoch: 73 [26752/50000]	Loss: 0.5691	LR: 0.025000
Training Epoch: 73 [26880/50000]	Loss: 0.4685	LR: 0.025000
Training Epoch: 73 [27008/50000]	Loss: 0.6069	LR: 0.025000
Training Epoch: 73 [27136/50000]	Loss: 0.5189	LR: 0.025000
Training Epoch: 73 [27264/50000]	Loss: 0.6193	LR: 0.025000
Training Epoch: 73 [27392/50000]	Loss: 0.5858	LR: 0.025000
Training Epoch: 73 [27520/50000]	Loss: 0.3706	LR: 0.025000
Training Epoch: 73 [27648/50000]	Loss: 0.3791	LR: 0.025000
Training Epoch: 73 [27776/50000]	Loss: 0.4843	LR: 0.025000
Training Epoch: 73 [27904/50000]	Loss: 0.4581	LR: 0.025000
Training Epoch: 73 [28032/50000]	Loss: 0.4598	LR: 0.025000
Training Epoch: 73 [28160/50000]	Loss: 0.5116	LR: 0.025000
Training Epoch: 73 [28288/50000]	Loss: 0.4714	LR: 0.025000
Training Epoch: 73 [28416/50000]	Loss: 0.5166	LR: 0.025000
Training Epoch: 73 [28544/50000]	Loss: 0.3780	LR: 0.025000
Training Epoch: 73 [28672/50000]	Loss: 0.5121	LR: 0.025000
Training Epoch: 73 [28800/50000]	Loss: 0.5667	LR: 0.025000
Training Epoch: 73 [28928/50000]	Loss: 0.4370	LR: 0.025000
Training Epoch: 73 [29056/50000]	Loss: 0.5079	LR: 0.025000
Training Epoch: 73 [29184/50000]	Loss: 0.5202	LR: 0.025000
Training Epoch: 73 [29312/50000]	Loss: 0.3934	LR: 0.025000
Training Epoch: 73 [29440/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 73 [29568/50000]	Loss: 0.3907	LR: 0.025000
Training Epoch: 73 [29696/50000]	Loss: 0.6361	LR: 0.025000
Training Epoch: 73 [29824/50000]	Loss: 0.4692	LR: 0.025000
Training Epoch: 73 [29952/50000]	Loss: 0.4265	LR: 0.025000
Training Epoch: 73 [30080/50000]	Loss: 0.5217	LR: 0.025000
Training Epoch: 73 [30208/50000]	Loss: 0.5343	LR: 0.025000
Training Epoch: 73 [30336/50000]	Loss: 0.5459	LR: 0.025000
Training Epoch: 73 [30464/50000]	Loss: 0.3761	LR: 0.025000
Training Epoch: 73 [30592/50000]	Loss: 0.5704	LR: 0.025000
Training Epoch: 73 [30720/50000]	Loss: 0.5555	LR: 0.025000
Training Epoch: 73 [30848/50000]	Loss: 0.6072	LR: 0.025000
Training Epoch: 73 [30976/50000]	Loss: 0.4305	LR: 0.025000
Training Epoch: 73 [31104/50000]	Loss: 0.4622	LR: 0.025000
Training Epoch: 73 [31232/50000]	Loss: 0.4779	LR: 0.025000
Training Epoch: 73 [31360/50000]	Loss: 0.5019	LR: 0.025000
Training Epoch: 73 [31488/50000]	Loss: 0.5430	LR: 0.025000
Training Epoch: 73 [31616/50000]	Loss: 0.5183	LR: 0.025000
Training Epoch: 73 [31744/50000]	Loss: 0.4343	LR: 0.025000
Training Epoch: 73 [31872/50000]	Loss: 0.4519	LR: 0.025000
Training Epoch: 73 [32000/50000]	Loss: 0.6235	LR: 0.025000
Training Epoch: 73 [32128/50000]	Loss: 0.6338	LR: 0.025000
Training Epoch: 73 [32256/50000]	Loss: 0.5266	LR: 0.025000
Training Epoch: 73 [32384/50000]	Loss: 0.5010	LR: 0.025000
Training Epoch: 73 [32512/50000]	Loss: 0.6838	LR: 0.025000
Training Epoch: 73 [32640/50000]	Loss: 0.4860	LR: 0.025000
Training Epoch: 73 [32768/50000]	Loss: 0.5179	LR: 0.025000
Training Epoch: 73 [32896/50000]	Loss: 0.6173	LR: 0.025000
Training Epoch: 73 [33024/50000]	Loss: 0.5236	LR: 0.025000
Training Epoch: 73 [33152/50000]	Loss: 0.4633	LR: 0.025000
Training Epoch: 73 [33280/50000]	Loss: 0.6312	LR: 0.025000
Training Epoch: 73 [33408/50000]	Loss: 0.3403	LR: 0.025000
Training Epoch: 73 [33536/50000]	Loss: 0.4387	LR: 0.025000
Training Epoch: 73 [33664/50000]	Loss: 0.4573	LR: 0.025000
Training Epoch: 73 [33792/50000]	Loss: 0.5147	LR: 0.025000
Training Epoch: 73 [33920/50000]	Loss: 0.5099	LR: 0.025000
Training Epoch: 73 [34048/50000]	Loss: 0.5525	LR: 0.025000
Training Epoch: 73 [34176/50000]	Loss: 0.4830	LR: 0.025000
Training Epoch: 73 [34304/50000]	Loss: 0.4181	LR: 0.025000
Training Epoch: 73 [34432/50000]	Loss: 0.3913	LR: 0.025000
Training Epoch: 73 [34560/50000]	Loss: 0.5291	LR: 0.025000
Training Epoch: 73 [34688/50000]	Loss: 0.4708	LR: 0.025000
Training Epoch: 73 [34816/50000]	Loss: 0.6304	LR: 0.025000
Training Epoch: 73 [34944/50000]	Loss: 0.4594	LR: 0.025000
Training Epoch: 73 [35072/50000]	Loss: 0.6117	LR: 0.025000
Training Epoch: 73 [35200/50000]	Loss: 0.4210	LR: 0.025000
Training Epoch: 73 [35328/50000]	Loss: 0.4944	LR: 0.025000
Training Epoch: 73 [35456/50000]	Loss: 0.3131	LR: 0.025000
Training Epoch: 73 [35584/50000]	Loss: 0.5806	LR: 0.025000
Training Epoch: 73 [35712/50000]	Loss: 0.3678	LR: 0.025000
Training Epoch: 73 [35840/50000]	Loss: 0.4142	LR: 0.025000
Training Epoch: 73 [35968/50000]	Loss: 0.5536	LR: 0.025000
Training Epoch: 73 [36096/50000]	Loss: 0.5320	LR: 0.025000
Training Epoch: 73 [36224/50000]	Loss: 0.5239	LR: 0.025000
Training Epoch: 73 [36352/50000]	Loss: 0.4933	LR: 0.025000
Training Epoch: 73 [36480/50000]	Loss: 0.5056	LR: 0.025000
Training Epoch: 73 [36608/50000]	Loss: 0.5171	LR: 0.025000
Training Epoch: 73 [36736/50000]	Loss: 0.5487	LR: 0.025000
Training Epoch: 73 [36864/50000]	Loss: 0.3748	LR: 0.025000
Training Epoch: 73 [36992/50000]	Loss: 0.4888	LR: 0.025000
Training Epoch: 73 [37120/50000]	Loss: 0.4272	LR: 0.025000
Training Epoch: 73 [37248/50000]	Loss: 0.5138	LR: 0.025000
Training Epoch: 73 [37376/50000]	Loss: 0.5826	LR: 0.025000
Training Epoch: 73 [37504/50000]	Loss: 0.5149	LR: 0.025000
Training Epoch: 73 [37632/50000]	Loss: 0.4465	LR: 0.025000
Training Epoch: 73 [37760/50000]	Loss: 0.4729	LR: 0.025000
Training Epoch: 73 [37888/50000]	Loss: 0.4641	LR: 0.025000
Training Epoch: 73 [38016/50000]	Loss: 0.6246	LR: 0.025000
Training Epoch: 73 [38144/50000]	Loss: 0.5118	LR: 0.025000
Training Epoch: 73 [38272/50000]	Loss: 0.4751	LR: 0.025000
Training Epoch: 73 [38400/50000]	Loss: 0.6195	LR: 0.025000
Training Epoch: 73 [38528/50000]	Loss: 0.4402	LR: 0.025000
Training Epoch: 73 [38656/50000]	Loss: 0.5774	LR: 0.025000
Training Epoch: 73 [38784/50000]	Loss: 0.4791	LR: 0.025000
Training Epoch: 73 [38912/50000]	Loss: 0.5697	LR: 0.025000
Training Epoch: 73 [39040/50000]	Loss: 0.5007	LR: 0.025000
Training Epoch: 73 [39168/50000]	Loss: 0.3473	LR: 0.025000
Training Epoch: 73 [39296/50000]	Loss: 0.4377	LR: 0.025000
Training Epoch: 73 [39424/50000]	Loss: 0.4551	LR: 0.025000
Training Epoch: 73 [39552/50000]	Loss: 0.6340	LR: 0.025000
Training Epoch: 73 [39680/50000]	Loss: 0.4501	LR: 0.025000
Training Epoch: 73 [39808/50000]	Loss: 0.4343	LR: 0.025000
Training Epoch: 73 [39936/50000]	Loss: 0.3824	LR: 0.025000
Training Epoch: 73 [40064/50000]	Loss: 0.5927	LR: 0.025000
Training Epoch: 73 [40192/50000]	Loss: 0.3736	LR: 0.025000
Training Epoch: 73 [40320/50000]	Loss: 0.3981	LR: 0.025000
Training Epoch: 73 [40448/50000]	Loss: 0.5945	LR: 0.025000
Training Epoch: 73 [40576/50000]	Loss: 0.5040	LR: 0.025000
Training Epoch: 73 [40704/50000]	Loss: 0.6283	LR: 0.025000
Training Epoch: 73 [40832/50000]	Loss: 0.4733	LR: 0.025000
Training Epoch: 73 [40960/50000]	Loss: 0.6011	LR: 0.025000
Training Epoch: 73 [41088/50000]	Loss: 0.6655	LR: 0.025000
Training Epoch: 73 [41216/50000]	Loss: 0.4323	LR: 0.025000
Training Epoch: 73 [41344/50000]	Loss: 0.5438	LR: 0.025000
Training Epoch: 73 [41472/50000]	Loss: 0.5409	LR: 0.025000
Training Epoch: 73 [41600/50000]	Loss: 0.5184	LR: 0.025000
Training Epoch: 73 [41728/50000]	Loss: 0.5445	LR: 0.025000
Training Epoch: 73 [41856/50000]	Loss: 0.5337	LR: 0.025000
Training Epoch: 73 [41984/50000]	Loss: 0.6419	LR: 0.025000
Training Epoch: 73 [42112/50000]	Loss: 0.5232	LR: 0.025000
Training Epoch: 73 [42240/50000]	Loss: 0.3521	LR: 0.025000
Training Epoch: 73 [42368/50000]	Loss: 0.5373	LR: 0.025000
Training Epoch: 73 [42496/50000]	Loss: 0.4037	LR: 0.025000
Training Epoch: 73 [42624/50000]	Loss: 0.4979	LR: 0.025000
Training Epoch: 73 [42752/50000]	Loss: 0.6954	LR: 0.025000
Training Epoch: 73 [42880/50000]	Loss: 0.5397	LR: 0.025000
Training Epoch: 73 [43008/50000]	Loss: 0.4203	LR: 0.025000
Training Epoch: 73 [43136/50000]	Loss: 0.4568	LR: 0.025000
Training Epoch: 73 [43264/50000]	Loss: 0.6470	LR: 0.025000
Training Epoch: 73 [43392/50000]	Loss: 0.4858	LR: 0.025000
Training Epoch: 73 [43520/50000]	Loss: 0.5532	LR: 0.025000
Training Epoch: 73 [43648/50000]	Loss: 0.3859	LR: 0.025000
Training Epoch: 73 [43776/50000]	Loss: 0.3861	LR: 0.025000
Training Epoch: 73 [43904/50000]	Loss: 0.4510	LR: 0.025000
Training Epoch: 73 [44032/50000]	Loss: 0.5991	LR: 0.025000
Training Epoch: 73 [44160/50000]	Loss: 0.5182	LR: 0.025000
Training Epoch: 73 [44288/50000]	Loss: 0.4164	LR: 0.025000
Training Epoch: 73 [44416/50000]	Loss: 0.4623	LR: 0.025000
Training Epoch: 73 [44544/50000]	Loss: 0.3936	LR: 0.025000
Training Epoch: 73 [44672/50000]	Loss: 0.5757	LR: 0.025000
Training Epoch: 73 [44800/50000]	Loss: 0.5380	LR: 0.025000
Training Epoch: 73 [44928/50000]	Loss: 0.5724	LR: 0.025000
Training Epoch: 73 [45056/50000]	Loss: 0.5385	LR: 0.025000
Training Epoch: 73 [45184/50000]	Loss: 0.5526	LR: 0.025000
Training Epoch: 73 [45312/50000]	Loss: 0.5713	LR: 0.025000
Training Epoch: 73 [45440/50000]	Loss: 0.6071	LR: 0.025000
Training Epoch: 73 [45568/50000]	Loss: 0.4452	LR: 0.025000
Training Epoch: 73 [45696/50000]	Loss: 0.4937	LR: 0.025000
Training Epoch: 73 [45824/50000]	Loss: 0.5626	LR: 0.025000
Training Epoch: 73 [45952/50000]	Loss: 0.5641	LR: 0.025000
Training Epoch: 73 [46080/50000]	Loss: 0.5034	LR: 0.025000
Training Epoch: 73 [46208/50000]	Loss: 0.5688	LR: 0.025000
Training Epoch: 73 [46336/50000]	Loss: 0.4986	LR: 0.025000
Training Epoch: 73 [46464/50000]	Loss: 0.5625	LR: 0.025000
Training Epoch: 73 [46592/50000]	Loss: 0.5152	LR: 0.025000
Training Epoch: 73 [46720/50000]	Loss: 0.5606	LR: 0.025000
Training Epoch: 73 [46848/50000]	Loss: 0.4176	LR: 0.025000
Training Epoch: 73 [46976/50000]	Loss: 0.5001	LR: 0.025000
Training Epoch: 73 [47104/50000]	Loss: 0.4219	LR: 0.025000
Training Epoch: 73 [47232/50000]	Loss: 0.6331	LR: 0.025000
Training Epoch: 73 [47360/50000]	Loss: 0.5898	LR: 0.025000
Training Epoch: 73 [47488/50000]	Loss: 0.5858	LR: 0.025000
Training Epoch: 73 [47616/50000]	Loss: 0.5236	LR: 0.025000
Training Epoch: 73 [47744/50000]	Loss: 0.4374	LR: 0.025000
Training Epoch: 73 [47872/50000]	Loss: 0.4090	LR: 0.025000
Training Epoch: 73 [48000/50000]	Loss: 0.4808	LR: 0.025000
Training Epoch: 73 [48128/50000]	Loss: 0.6611	LR: 0.025000
Training Epoch: 73 [48256/50000]	Loss: 0.6084	LR: 0.025000
Training Epoch: 73 [48384/50000]	Loss: 0.4371	LR: 0.025000
Training Epoch: 73 [48512/50000]	Loss: 0.3462	LR: 0.025000
Training Epoch: 73 [48640/50000]	Loss: 0.6157	LR: 0.025000
Training Epoch: 73 [48768/50000]	Loss: 0.5049	LR: 0.025000
Training Epoch: 73 [48896/50000]	Loss: 0.5675	LR: 0.025000
Training Epoch: 73 [49024/50000]	Loss: 0.4882	LR: 0.025000
Training Epoch: 73 [49152/50000]	Loss: 0.5064	LR: 0.025000
Training Epoch: 73 [49280/50000]	Loss: 0.6578	LR: 0.025000
Training Epoch: 73 [49408/50000]	Loss: 0.4875	LR: 0.025000
Training Epoch: 73 [49536/50000]	Loss: 0.6866	LR: 0.025000
Training Epoch: 73 [49664/50000]	Loss: 0.3742	LR: 0.025000
Training Epoch: 73 [49792/50000]	Loss: 0.4353	LR: 0.025000
Training Epoch: 73 [49920/50000]	Loss: 0.5424	LR: 0.025000
Training Epoch: 73 [50000/50000]	Loss: 0.5444	LR: 0.025000
epoch 73 training time consumed: 53.89s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  267914 GB |  267914 GB |
|       from large pool |  123392 KB |    1034 MB |  267650 GB |  267650 GB |
|       from small pool |   10798 KB |      13 MB |     263 GB |     263 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  267914 GB |  267914 GB |
|       from large pool |  123392 KB |    1034 MB |  267650 GB |  267650 GB |
|       from small pool |   10798 KB |      13 MB |     263 GB |     263 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  117898 GB |  117898 GB |
|       from large pool |  155136 KB |  433088 KB |  117606 GB |  117606 GB |
|       from small pool |    1490 KB |    3494 KB |     291 GB |     291 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   10338 K  |   10337 K  |
|       from large pool |      24    |      65    |    5396 K  |    5396 K  |
|       from small pool |     231    |     274    |    4941 K  |    4941 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   10338 K  |   10337 K  |
|       from large pool |      24    |      65    |    5396 K  |    5396 K  |
|       from small pool |     231    |     274    |    4941 K  |    4941 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5107 K  |    5107 K  |
|       from large pool |       9    |      14    |    2611 K  |    2611 K  |
|       from small pool |      12    |      16    |    2496 K  |    2496 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 73, Average loss: 0.0105, Accuracy: 0.6707, Time consumed:3.44s

Training Epoch: 74 [128/50000]	Loss: 0.3325	LR: 0.025000
Training Epoch: 74 [256/50000]	Loss: 0.5254	LR: 0.025000
Training Epoch: 74 [384/50000]	Loss: 0.4132	LR: 0.025000
Training Epoch: 74 [512/50000]	Loss: 0.4654	LR: 0.025000
Training Epoch: 74 [640/50000]	Loss: 0.4148	LR: 0.025000
Training Epoch: 74 [768/50000]	Loss: 0.4607	LR: 0.025000
Training Epoch: 74 [896/50000]	Loss: 0.3800	LR: 0.025000
Training Epoch: 74 [1024/50000]	Loss: 0.4586	LR: 0.025000
Training Epoch: 74 [1152/50000]	Loss: 0.4301	LR: 0.025000
Training Epoch: 74 [1280/50000]	Loss: 0.4174	LR: 0.025000
Training Epoch: 74 [1408/50000]	Loss: 0.3291	LR: 0.025000
Training Epoch: 74 [1536/50000]	Loss: 0.3797	LR: 0.025000
Training Epoch: 74 [1664/50000]	Loss: 0.4695	LR: 0.025000
Training Epoch: 74 [1792/50000]	Loss: 0.3815	LR: 0.025000
Training Epoch: 74 [1920/50000]	Loss: 0.6021	LR: 0.025000
Training Epoch: 74 [2048/50000]	Loss: 0.5161	LR: 0.025000
Training Epoch: 74 [2176/50000]	Loss: 0.3448	LR: 0.025000
Training Epoch: 74 [2304/50000]	Loss: 0.4353	LR: 0.025000
Training Epoch: 74 [2432/50000]	Loss: 0.3955	LR: 0.025000
Training Epoch: 74 [2560/50000]	Loss: 0.4524	LR: 0.025000
Training Epoch: 74 [2688/50000]	Loss: 0.3292	LR: 0.025000
Training Epoch: 74 [2816/50000]	Loss: 0.4991	LR: 0.025000
Training Epoch: 74 [2944/50000]	Loss: 0.4542	LR: 0.025000
Training Epoch: 74 [3072/50000]	Loss: 0.3377	LR: 0.025000
Training Epoch: 74 [3200/50000]	Loss: 0.3453	LR: 0.025000
Training Epoch: 74 [3328/50000]	Loss: 0.3127	LR: 0.025000
Training Epoch: 74 [3456/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 74 [3584/50000]	Loss: 0.3864	LR: 0.025000
Training Epoch: 74 [3712/50000]	Loss: 0.3181	LR: 0.025000
Training Epoch: 74 [3840/50000]	Loss: 0.3854	LR: 0.025000
Training Epoch: 74 [3968/50000]	Loss: 0.3518	LR: 0.025000
Training Epoch: 74 [4096/50000]	Loss: 0.4344	LR: 0.025000
Training Epoch: 74 [4224/50000]	Loss: 0.4847	LR: 0.025000
Training Epoch: 74 [4352/50000]	Loss: 0.5062	LR: 0.025000
Training Epoch: 74 [4480/50000]	Loss: 0.4541	LR: 0.025000
Training Epoch: 74 [4608/50000]	Loss: 0.3778	LR: 0.025000
Training Epoch: 74 [4736/50000]	Loss: 0.2733	LR: 0.025000
Training Epoch: 74 [4864/50000]	Loss: 0.4697	LR: 0.025000
Training Epoch: 74 [4992/50000]	Loss: 0.3755	LR: 0.025000
Training Epoch: 74 [5120/50000]	Loss: 0.3323	LR: 0.025000
Training Epoch: 74 [5248/50000]	Loss: 0.5169	LR: 0.025000
Training Epoch: 74 [5376/50000]	Loss: 0.4171	LR: 0.025000
Training Epoch: 74 [5504/50000]	Loss: 0.4935	LR: 0.025000
Training Epoch: 74 [5632/50000]	Loss: 0.3439	LR: 0.025000
Training Epoch: 74 [5760/50000]	Loss: 0.4392	LR: 0.025000
Training Epoch: 74 [5888/50000]	Loss: 0.3111	LR: 0.025000
Training Epoch: 74 [6016/50000]	Loss: 0.4320	LR: 0.025000
Training Epoch: 74 [6144/50000]	Loss: 0.3040	LR: 0.025000
Training Epoch: 74 [6272/50000]	Loss: 0.5067	LR: 0.025000
Training Epoch: 74 [6400/50000]	Loss: 0.4124	LR: 0.025000
Training Epoch: 74 [6528/50000]	Loss: 0.4409	LR: 0.025000
Training Epoch: 74 [6656/50000]	Loss: 0.3558	LR: 0.025000
Training Epoch: 74 [6784/50000]	Loss: 0.2836	LR: 0.025000
Training Epoch: 74 [6912/50000]	Loss: 0.3756	LR: 0.025000
Training Epoch: 74 [7040/50000]	Loss: 0.3330	LR: 0.025000
Training Epoch: 74 [7168/50000]	Loss: 0.4761	LR: 0.025000
Training Epoch: 74 [7296/50000]	Loss: 0.4309	LR: 0.025000
Training Epoch: 74 [7424/50000]	Loss: 0.4595	LR: 0.025000
Training Epoch: 74 [7552/50000]	Loss: 0.4944	LR: 0.025000
Training Epoch: 74 [7680/50000]	Loss: 0.4152	LR: 0.025000
Training Epoch: 74 [7808/50000]	Loss: 0.3002	LR: 0.025000
Training Epoch: 74 [7936/50000]	Loss: 0.3135	LR: 0.025000
Training Epoch: 74 [8064/50000]	Loss: 0.3272	LR: 0.025000
Training Epoch: 74 [8192/50000]	Loss: 0.4975	LR: 0.025000
Training Epoch: 74 [8320/50000]	Loss: 0.3342	LR: 0.025000
Training Epoch: 74 [8448/50000]	Loss: 0.3623	LR: 0.025000
Training Epoch: 74 [8576/50000]	Loss: 0.3837	LR: 0.025000
Training Epoch: 74 [8704/50000]	Loss: 0.4034	LR: 0.025000
Training Epoch: 74 [8832/50000]	Loss: 0.4441	LR: 0.025000
Training Epoch: 74 [8960/50000]	Loss: 0.3672	LR: 0.025000
Training Epoch: 74 [9088/50000]	Loss: 0.3323	LR: 0.025000
Training Epoch: 74 [9216/50000]	Loss: 0.4359	LR: 0.025000
Training Epoch: 74 [9344/50000]	Loss: 0.5405	LR: 0.025000
Training Epoch: 74 [9472/50000]	Loss: 0.3251	LR: 0.025000
Training Epoch: 74 [9600/50000]	Loss: 0.4193	LR: 0.025000
Training Epoch: 74 [9728/50000]	Loss: 0.4872	LR: 0.025000
Training Epoch: 74 [9856/50000]	Loss: 0.3339	LR: 0.025000
Training Epoch: 74 [9984/50000]	Loss: 0.3485	LR: 0.025000
Training Epoch: 74 [10112/50000]	Loss: 0.5408	LR: 0.025000
Training Epoch: 74 [10240/50000]	Loss: 0.4657	LR: 0.025000
Training Epoch: 74 [10368/50000]	Loss: 0.4029	LR: 0.025000
Training Epoch: 74 [10496/50000]	Loss: 0.3855	LR: 0.025000
Training Epoch: 74 [10624/50000]	Loss: 0.4484	LR: 0.025000
Training Epoch: 74 [10752/50000]	Loss: 0.3016	LR: 0.025000
Training Epoch: 74 [10880/50000]	Loss: 0.4441	LR: 0.025000
Training Epoch: 74 [11008/50000]	Loss: 0.4587	LR: 0.025000
Training Epoch: 74 [11136/50000]	Loss: 0.5903	LR: 0.025000
Training Epoch: 74 [11264/50000]	Loss: 0.3377	LR: 0.025000
Training Epoch: 74 [11392/50000]	Loss: 0.3385	LR: 0.025000
Training Epoch: 74 [11520/50000]	Loss: 0.2900	LR: 0.025000
Training Epoch: 74 [11648/50000]	Loss: 0.3254	LR: 0.025000
Training Epoch: 74 [11776/50000]	Loss: 0.3710	LR: 0.025000
Training Epoch: 74 [11904/50000]	Loss: 0.3260	LR: 0.025000
Training Epoch: 74 [12032/50000]	Loss: 0.3805	LR: 0.025000
Training Epoch: 74 [12160/50000]	Loss: 0.3274	LR: 0.025000
Training Epoch: 74 [12288/50000]	Loss: 0.3792	LR: 0.025000
Training Epoch: 74 [12416/50000]	Loss: 0.3175	LR: 0.025000
Training Epoch: 74 [12544/50000]	Loss: 0.2846	LR: 0.025000
Training Epoch: 74 [12672/50000]	Loss: 0.4817	LR: 0.025000
Training Epoch: 74 [12800/50000]	Loss: 0.3504	LR: 0.025000
Training Epoch: 74 [12928/50000]	Loss: 0.3400	LR: 0.025000
Training Epoch: 74 [13056/50000]	Loss: 0.3877	LR: 0.025000
Training Epoch: 74 [13184/50000]	Loss: 0.5462	LR: 0.025000
Training Epoch: 74 [13312/50000]	Loss: 0.2964	LR: 0.025000
Training Epoch: 74 [13440/50000]	Loss: 0.4626	LR: 0.025000
Training Epoch: 74 [13568/50000]	Loss: 0.3370	LR: 0.025000
Training Epoch: 74 [13696/50000]	Loss: 0.4086	LR: 0.025000
Training Epoch: 74 [13824/50000]	Loss: 0.3348	LR: 0.025000
Training Epoch: 74 [13952/50000]	Loss: 0.6076	LR: 0.025000
Training Epoch: 74 [14080/50000]	Loss: 0.3412	LR: 0.025000
Training Epoch: 74 [14208/50000]	Loss: 0.4634	LR: 0.025000
Training Epoch: 74 [14336/50000]	Loss: 0.4851	LR: 0.025000
Training Epoch: 74 [14464/50000]	Loss: 0.3951	LR: 0.025000
Training Epoch: 74 [14592/50000]	Loss: 0.3386	LR: 0.025000
Training Epoch: 74 [14720/50000]	Loss: 0.5310	LR: 0.025000
Training Epoch: 74 [14848/50000]	Loss: 0.4506	LR: 0.025000
Training Epoch: 74 [14976/50000]	Loss: 0.3876	LR: 0.025000
Training Epoch: 74 [15104/50000]	Loss: 0.5281	LR: 0.025000
Training Epoch: 74 [15232/50000]	Loss: 0.4230	LR: 0.025000
Training Epoch: 74 [15360/50000]	Loss: 0.5581	LR: 0.025000
Training Epoch: 74 [15488/50000]	Loss: 0.4516	LR: 0.025000
Training Epoch: 74 [15616/50000]	Loss: 0.3010	LR: 0.025000
Training Epoch: 74 [15744/50000]	Loss: 0.4300	LR: 0.025000
Training Epoch: 74 [15872/50000]	Loss: 0.5418	LR: 0.025000
Training Epoch: 74 [16000/50000]	Loss: 0.4145	LR: 0.025000
Training Epoch: 74 [16128/50000]	Loss: 0.3064	LR: 0.025000
Training Epoch: 74 [16256/50000]	Loss: 0.4298	LR: 0.025000
Training Epoch: 74 [16384/50000]	Loss: 0.5096	LR: 0.025000
Training Epoch: 74 [16512/50000]	Loss: 0.4353	LR: 0.025000
Training Epoch: 74 [16640/50000]	Loss: 0.4729	LR: 0.025000
Training Epoch: 74 [16768/50000]	Loss: 0.3917	LR: 0.025000
Training Epoch: 74 [16896/50000]	Loss: 0.3780	LR: 0.025000
Training Epoch: 74 [17024/50000]	Loss: 0.4465	LR: 0.025000
Training Epoch: 74 [17152/50000]	Loss: 0.4233	LR: 0.025000
Training Epoch: 74 [17280/50000]	Loss: 0.4729	LR: 0.025000
Training Epoch: 74 [17408/50000]	Loss: 0.3382	LR: 0.025000
Training Epoch: 74 [17536/50000]	Loss: 0.4318	LR: 0.025000
Training Epoch: 74 [17664/50000]	Loss: 0.3581	LR: 0.025000
Training Epoch: 74 [17792/50000]	Loss: 0.3897	LR: 0.025000
Training Epoch: 74 [17920/50000]	Loss: 0.4991	LR: 0.025000
Training Epoch: 74 [18048/50000]	Loss: 0.4093	LR: 0.025000
Training Epoch: 74 [18176/50000]	Loss: 0.3843	LR: 0.025000
Training Epoch: 74 [18304/50000]	Loss: 0.2745	LR: 0.025000
Training Epoch: 74 [18432/50000]	Loss: 0.3148	LR: 0.025000
Training Epoch: 74 [18560/50000]	Loss: 0.4867	LR: 0.025000
Training Epoch: 74 [18688/50000]	Loss: 0.3133	LR: 0.025000
Training Epoch: 74 [18816/50000]	Loss: 0.4351	LR: 0.025000
Training Epoch: 74 [18944/50000]	Loss: 0.3777	LR: 0.025000
Training Epoch: 74 [19072/50000]	Loss: 0.4919	LR: 0.025000
Training Epoch: 74 [19200/50000]	Loss: 0.3664	LR: 0.025000
Training Epoch: 74 [19328/50000]	Loss: 0.4319	LR: 0.025000
Training Epoch: 74 [19456/50000]	Loss: 0.4514	LR: 0.025000
Training Epoch: 74 [19584/50000]	Loss: 0.5601	LR: 0.025000
Training Epoch: 74 [19712/50000]	Loss: 0.3928	LR: 0.025000
Training Epoch: 74 [19840/50000]	Loss: 0.5089	LR: 0.025000
Training Epoch: 74 [19968/50000]	Loss: 0.5046	LR: 0.025000
Training Epoch: 74 [20096/50000]	Loss: 0.3537	LR: 0.025000
Training Epoch: 74 [20224/50000]	Loss: 0.3059	LR: 0.025000
Training Epoch: 74 [20352/50000]	Loss: 0.4452	LR: 0.025000
Training Epoch: 74 [20480/50000]	Loss: 0.4474	LR: 0.025000
Training Epoch: 74 [20608/50000]	Loss: 0.3212	LR: 0.025000
Training Epoch: 74 [20736/50000]	Loss: 0.4437	LR: 0.025000
Training Epoch: 74 [20864/50000]	Loss: 0.4339	LR: 0.025000
Training Epoch: 74 [20992/50000]	Loss: 0.5531	LR: 0.025000
Training Epoch: 74 [21120/50000]	Loss: 0.3788	LR: 0.025000
Training Epoch: 74 [21248/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 74 [21376/50000]	Loss: 0.4925	LR: 0.025000
Training Epoch: 74 [21504/50000]	Loss: 0.4293	LR: 0.025000
Training Epoch: 74 [21632/50000]	Loss: 0.5235	LR: 0.025000
Training Epoch: 74 [21760/50000]	Loss: 0.2923	LR: 0.025000
Training Epoch: 74 [21888/50000]	Loss: 0.4032	LR: 0.025000
Training Epoch: 74 [22016/50000]	Loss: 0.5343	LR: 0.025000
Training Epoch: 74 [22144/50000]	Loss: 0.3296	LR: 0.025000
Training Epoch: 74 [22272/50000]	Loss: 0.4296	LR: 0.025000
Training Epoch: 74 [22400/50000]	Loss: 0.3819	LR: 0.025000
Training Epoch: 74 [22528/50000]	Loss: 0.5444	LR: 0.025000
Training Epoch: 74 [22656/50000]	Loss: 0.4493	LR: 0.025000
Training Epoch: 74 [22784/50000]	Loss: 0.5617	LR: 0.025000
Training Epoch: 74 [22912/50000]	Loss: 0.4331	LR: 0.025000
Training Epoch: 74 [23040/50000]	Loss: 0.4085	LR: 0.025000
Training Epoch: 74 [23168/50000]	Loss: 0.4371	LR: 0.025000
Training Epoch: 74 [23296/50000]	Loss: 0.5729	LR: 0.025000
Training Epoch: 74 [23424/50000]	Loss: 0.5048	LR: 0.025000
Training Epoch: 74 [23552/50000]	Loss: 0.3309	LR: 0.025000
Training Epoch: 74 [23680/50000]	Loss: 0.3214	LR: 0.025000
Training Epoch: 74 [23808/50000]	Loss: 0.5240	LR: 0.025000
Training Epoch: 74 [23936/50000]	Loss: 0.4138	LR: 0.025000
Training Epoch: 74 [24064/50000]	Loss: 0.3771	LR: 0.025000
Training Epoch: 74 [24192/50000]	Loss: 0.5173	LR: 0.025000
Training Epoch: 74 [24320/50000]	Loss: 0.3620	LR: 0.025000
Training Epoch: 74 [24448/50000]	Loss: 0.2696	LR: 0.025000
Training Epoch: 74 [24576/50000]	Loss: 0.3768	LR: 0.025000
Training Epoch: 74 [24704/50000]	Loss: 0.4124	LR: 0.025000
Training Epoch: 74 [24832/50000]	Loss: 0.5231	LR: 0.025000
Training Epoch: 74 [24960/50000]	Loss: 0.2981	LR: 0.025000
Training Epoch: 74 [25088/50000]	Loss: 0.5245	LR: 0.025000
Training Epoch: 74 [25216/50000]	Loss: 0.4628	LR: 0.025000
Training Epoch: 74 [25344/50000]	Loss: 0.5815	LR: 0.025000
Training Epoch: 74 [25472/50000]	Loss: 0.5878	LR: 0.025000
Training Epoch: 74 [25600/50000]	Loss: 0.5144	LR: 0.025000
Training Epoch: 74 [25728/50000]	Loss: 0.5235	LR: 0.025000
Training Epoch: 74 [25856/50000]	Loss: 0.3827	LR: 0.025000
Training Epoch: 74 [25984/50000]	Loss: 0.4013	LR: 0.025000
Training Epoch: 74 [26112/50000]	Loss: 0.3849	LR: 0.025000
Training Epoch: 74 [26240/50000]	Loss: 0.3489	LR: 0.025000
Training Epoch: 74 [26368/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 74 [26496/50000]	Loss: 0.6012	LR: 0.025000
Training Epoch: 74 [26624/50000]	Loss: 0.5358	LR: 0.025000
Training Epoch: 74 [26752/50000]	Loss: 0.4179	LR: 0.025000
Training Epoch: 74 [26880/50000]	Loss: 0.4485	LR: 0.025000
Training Epoch: 74 [27008/50000]	Loss: 0.5566	LR: 0.025000
Training Epoch: 74 [27136/50000]	Loss: 0.4333	LR: 0.025000
Training Epoch: 74 [27264/50000]	Loss: 0.5207	LR: 0.025000
Training Epoch: 74 [27392/50000]	Loss: 0.5399	LR: 0.025000
Training Epoch: 74 [27520/50000]	Loss: 0.3861	LR: 0.025000
Training Epoch: 74 [27648/50000]	Loss: 0.4527	LR: 0.025000
Training Epoch: 74 [27776/50000]	Loss: 0.4650	LR: 0.025000
Training Epoch: 74 [27904/50000]	Loss: 0.2956	LR: 0.025000
Training Epoch: 74 [28032/50000]	Loss: 0.3346	LR: 0.025000
Training Epoch: 74 [28160/50000]	Loss: 0.4699	LR: 0.025000
Training Epoch: 74 [28288/50000]	Loss: 0.4923	LR: 0.025000
Training Epoch: 74 [28416/50000]	Loss: 0.5143	LR: 0.025000
Training Epoch: 74 [28544/50000]	Loss: 0.4275	LR: 0.025000
Training Epoch: 74 [28672/50000]	Loss: 0.4838	LR: 0.025000
Training Epoch: 74 [28800/50000]	Loss: 0.5235	LR: 0.025000
Training Epoch: 74 [28928/50000]	Loss: 0.5007	LR: 0.025000
Training Epoch: 74 [29056/50000]	Loss: 0.4618	LR: 0.025000
Training Epoch: 74 [29184/50000]	Loss: 0.5937	LR: 0.025000
Training Epoch: 74 [29312/50000]	Loss: 0.4401	LR: 0.025000
Training Epoch: 74 [29440/50000]	Loss: 0.4839	LR: 0.025000
Training Epoch: 74 [29568/50000]	Loss: 0.4378	LR: 0.025000
Training Epoch: 74 [29696/50000]	Loss: 0.4880	LR: 0.025000
Training Epoch: 74 [29824/50000]	Loss: 0.3262	LR: 0.025000
Training Epoch: 74 [29952/50000]	Loss: 0.5537	LR: 0.025000
Training Epoch: 74 [30080/50000]	Loss: 0.5570	LR: 0.025000
Training Epoch: 74 [30208/50000]	Loss: 0.4340	LR: 0.025000
Training Epoch: 74 [30336/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 74 [30464/50000]	Loss: 0.6623	LR: 0.025000
Training Epoch: 74 [30592/50000]	Loss: 0.3836	LR: 0.025000
Training Epoch: 74 [30720/50000]	Loss: 0.4713	LR: 0.025000
Training Epoch: 74 [30848/50000]	Loss: 0.2371	LR: 0.025000
Training Epoch: 74 [30976/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 74 [31104/50000]	Loss: 0.4595	LR: 0.025000
Training Epoch: 74 [31232/50000]	Loss: 0.4152	LR: 0.025000
Training Epoch: 74 [31360/50000]	Loss: 0.4783	LR: 0.025000
Training Epoch: 74 [31488/50000]	Loss: 0.5291	LR: 0.025000
Training Epoch: 74 [31616/50000]	Loss: 0.4137	LR: 0.025000
Training Epoch: 74 [31744/50000]	Loss: 0.5098	LR: 0.025000
Training Epoch: 74 [31872/50000]	Loss: 0.5847	LR: 0.025000
Training Epoch: 74 [32000/50000]	Loss: 0.3642	LR: 0.025000
Training Epoch: 74 [32128/50000]	Loss: 0.4518	LR: 0.025000
Training Epoch: 74 [32256/50000]	Loss: 0.4585	LR: 0.025000
Training Epoch: 74 [32384/50000]	Loss: 0.5208	LR: 0.025000
Training Epoch: 74 [32512/50000]	Loss: 0.4017	LR: 0.025000
Training Epoch: 74 [32640/50000]	Loss: 0.4941	LR: 0.025000
Training Epoch: 74 [32768/50000]	Loss: 0.3623	LR: 0.025000
Training Epoch: 74 [32896/50000]	Loss: 0.6112	LR: 0.025000
Training Epoch: 74 [33024/50000]	Loss: 0.5000	LR: 0.025000
Training Epoch: 74 [33152/50000]	Loss: 0.4510	LR: 0.025000
Training Epoch: 74 [33280/50000]	Loss: 0.4862	LR: 0.025000
Training Epoch: 74 [33408/50000]	Loss: 0.7084	LR: 0.025000
Training Epoch: 74 [33536/50000]	Loss: 0.4756	LR: 0.025000
Training Epoch: 74 [33664/50000]	Loss: 0.4759	LR: 0.025000
Training Epoch: 74 [33792/50000]	Loss: 0.4640	LR: 0.025000
Training Epoch: 74 [33920/50000]	Loss: 0.5750	LR: 0.025000
Training Epoch: 74 [34048/50000]	Loss: 0.4254	LR: 0.025000
Training Epoch: 74 [34176/50000]	Loss: 0.4168	LR: 0.025000
Training Epoch: 74 [34304/50000]	Loss: 0.4299	LR: 0.025000
Training Epoch: 74 [34432/50000]	Loss: 0.4614	LR: 0.025000
Training Epoch: 74 [34560/50000]	Loss: 0.4549	LR: 0.025000
Training Epoch: 74 [34688/50000]	Loss: 0.5070	LR: 0.025000
Training Epoch: 74 [34816/50000]	Loss: 0.5168	LR: 0.025000
Training Epoch: 74 [34944/50000]	Loss: 0.5156	LR: 0.025000
Training Epoch: 74 [35072/50000]	Loss: 0.6017	LR: 0.025000
Training Epoch: 74 [35200/50000]	Loss: 0.3631	LR: 0.025000
Training Epoch: 74 [35328/50000]	Loss: 0.4514	LR: 0.025000
Training Epoch: 74 [35456/50000]	Loss: 0.5702	LR: 0.025000
Training Epoch: 74 [35584/50000]	Loss: 0.6242	LR: 0.025000
Training Epoch: 74 [35712/50000]	Loss: 0.4235	LR: 0.025000
Training Epoch: 74 [35840/50000]	Loss: 0.4421	LR: 0.025000
Training Epoch: 74 [35968/50000]	Loss: 0.4487	LR: 0.025000
Training Epoch: 74 [36096/50000]	Loss: 0.4604	LR: 0.025000
Training Epoch: 74 [36224/50000]	Loss: 0.4875	LR: 0.025000
Training Epoch: 74 [36352/50000]	Loss: 0.5166	LR: 0.025000
Training Epoch: 74 [36480/50000]	Loss: 0.4536	LR: 0.025000
Training Epoch: 74 [36608/50000]	Loss: 0.3224	LR: 0.025000
Training Epoch: 74 [36736/50000]	Loss: 0.3846	LR: 0.025000
Training Epoch: 74 [36864/50000]	Loss: 0.4358	LR: 0.025000
Training Epoch: 74 [36992/50000]	Loss: 0.4761	LR: 0.025000
Training Epoch: 74 [37120/50000]	Loss: 0.4515	LR: 0.025000
Training Epoch: 74 [37248/50000]	Loss: 0.5275	LR: 0.025000
Training Epoch: 74 [37376/50000]	Loss: 0.3257	LR: 0.025000
Training Epoch: 74 [37504/50000]	Loss: 0.4065	LR: 0.025000
Training Epoch: 74 [37632/50000]	Loss: 0.4827	LR: 0.025000
Training Epoch: 74 [37760/50000]	Loss: 0.3472	LR: 0.025000
Training Epoch: 74 [37888/50000]	Loss: 0.3101	LR: 0.025000
Training Epoch: 74 [38016/50000]	Loss: 0.3445	LR: 0.025000
Training Epoch: 74 [38144/50000]	Loss: 0.5498	LR: 0.025000
Training Epoch: 74 [38272/50000]	Loss: 0.4331	LR: 0.025000
Training Epoch: 74 [38400/50000]	Loss: 0.3815	LR: 0.025000
Training Epoch: 74 [38528/50000]	Loss: 0.4073	LR: 0.025000
Training Epoch: 74 [38656/50000]	Loss: 0.5120	LR: 0.025000
Training Epoch: 74 [38784/50000]	Loss: 0.4224	LR: 0.025000
Training Epoch: 74 [38912/50000]	Loss: 0.3930	LR: 0.025000
Training Epoch: 74 [39040/50000]	Loss: 0.5313	LR: 0.025000
Training Epoch: 74 [39168/50000]	Loss: 0.3696	LR: 0.025000
Training Epoch: 74 [39296/50000]	Loss: 0.5754	LR: 0.025000
Training Epoch: 74 [39424/50000]	Loss: 0.4045	LR: 0.025000
Training Epoch: 74 [39552/50000]	Loss: 0.3445	LR: 0.025000
Training Epoch: 74 [39680/50000]	Loss: 0.4779	LR: 0.025000
Training Epoch: 74 [39808/50000]	Loss: 0.3564	LR: 0.025000
Training Epoch: 74 [39936/50000]	Loss: 0.4037	LR: 0.025000
Training Epoch: 74 [40064/50000]	Loss: 0.4533	LR: 0.025000
Training Epoch: 74 [40192/50000]	Loss: 0.4338	LR: 0.025000
Training Epoch: 74 [40320/50000]	Loss: 0.5047	LR: 0.025000
Training Epoch: 74 [40448/50000]	Loss: 0.4728	LR: 0.025000
Training Epoch: 74 [40576/50000]	Loss: 0.4584	LR: 0.025000
Training Epoch: 74 [40704/50000]	Loss: 0.4425	LR: 0.025000
Training Epoch: 74 [40832/50000]	Loss: 0.4224	LR: 0.025000
Training Epoch: 74 [40960/50000]	Loss: 0.4053	LR: 0.025000
Training Epoch: 74 [41088/50000]	Loss: 0.4101	LR: 0.025000
Training Epoch: 74 [41216/50000]	Loss: 0.4630	LR: 0.025000
Training Epoch: 74 [41344/50000]	Loss: 0.6151	LR: 0.025000
Training Epoch: 74 [41472/50000]	Loss: 0.5712	LR: 0.025000
Training Epoch: 74 [41600/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 74 [41728/50000]	Loss: 0.4476	LR: 0.025000
Training Epoch: 74 [41856/50000]	Loss: 0.4631	LR: 0.025000
Training Epoch: 74 [41984/50000]	Loss: 0.5281	LR: 0.025000
Training Epoch: 74 [42112/50000]	Loss: 0.3805	LR: 0.025000
Training Epoch: 74 [42240/50000]	Loss: 0.5983	LR: 0.025000
Training Epoch: 74 [42368/50000]	Loss: 0.4082	LR: 0.025000
Training Epoch: 74 [42496/50000]	Loss: 0.4339	LR: 0.025000
Training Epoch: 74 [42624/50000]	Loss: 0.4104	LR: 0.025000
Training Epoch: 74 [42752/50000]	Loss: 0.4387	LR: 0.025000
Training Epoch: 74 [42880/50000]	Loss: 0.5056	LR: 0.025000
Training Epoch: 74 [43008/50000]	Loss: 0.6697	LR: 0.025000
Training Epoch: 74 [43136/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 74 [43264/50000]	Loss: 0.4623	LR: 0.025000
Training Epoch: 74 [43392/50000]	Loss: 0.5078	LR: 0.025000
Training Epoch: 74 [43520/50000]	Loss: 0.5086	LR: 0.025000
Training Epoch: 74 [43648/50000]	Loss: 0.3341	LR: 0.025000
Training Epoch: 74 [43776/50000]	Loss: 0.5195	LR: 0.025000
Training Epoch: 74 [43904/50000]	Loss: 0.5297	LR: 0.025000
Training Epoch: 74 [44032/50000]	Loss: 0.4043	LR: 0.025000
Training Epoch: 74 [44160/50000]	Loss: 0.5258	LR: 0.025000
Training Epoch: 74 [44288/50000]	Loss: 0.4418	LR: 0.025000
Training Epoch: 74 [44416/50000]	Loss: 0.5268	LR: 0.025000
Training Epoch: 74 [44544/50000]	Loss: 0.3435	LR: 0.025000
Training Epoch: 74 [44672/50000]	Loss: 0.3904	LR: 0.025000
Training Epoch: 74 [44800/50000]	Loss: 0.5211	LR: 0.025000
Training Epoch: 74 [44928/50000]	Loss: 0.5771	LR: 0.025000
Training Epoch: 74 [45056/50000]	Loss: 0.4777	LR: 0.025000
Training Epoch: 74 [45184/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 74 [45312/50000]	Loss: 0.4684	LR: 0.025000
Training Epoch: 74 [45440/50000]	Loss: 0.3986	LR: 0.025000
Training Epoch: 74 [45568/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 74 [45696/50000]	Loss: 0.4039	LR: 0.025000
Training Epoch: 74 [45824/50000]	Loss: 0.4550	LR: 0.025000
Training Epoch: 74 [45952/50000]	Loss: 0.6700	LR: 0.025000
Training Epoch: 74 [46080/50000]	Loss: 0.5783	LR: 0.025000
Training Epoch: 74 [46208/50000]	Loss: 0.6078	LR: 0.025000
Training Epoch: 74 [46336/50000]	Loss: 0.5697	LR: 0.025000
Training Epoch: 74 [46464/50000]	Loss: 0.6964	LR: 0.025000
Training Epoch: 74 [46592/50000]	Loss: 0.4344	LR: 0.025000
Training Epoch: 74 [46720/50000]	Loss: 0.5201	LR: 0.025000
Training Epoch: 74 [46848/50000]	Loss: 0.3996	LR: 0.025000
Training Epoch: 74 [46976/50000]	Loss: 0.3669	LR: 0.025000
Training Epoch: 74 [47104/50000]	Loss: 0.5219	LR: 0.025000
Training Epoch: 74 [47232/50000]	Loss: 0.5729	LR: 0.025000
Training Epoch: 74 [47360/50000]	Loss: 0.4586	LR: 0.025000
Training Epoch: 74 [47488/50000]	Loss: 0.6961	LR: 0.025000
Training Epoch: 74 [47616/50000]	Loss: 0.4735	LR: 0.025000
Training Epoch: 74 [47744/50000]	Loss: 0.3131	LR: 0.025000
Training Epoch: 74 [47872/50000]	Loss: 0.4992	LR: 0.025000
Training Epoch: 74 [48000/50000]	Loss: 0.5159	LR: 0.025000
Training Epoch: 74 [48128/50000]	Loss: 0.3828	LR: 0.025000
Training Epoch: 74 [48256/50000]	Loss: 0.5927	LR: 0.025000
Training Epoch: 74 [48384/50000]	Loss: 0.4434	LR: 0.025000
Training Epoch: 74 [48512/50000]	Loss: 0.5597	LR: 0.025000
Training Epoch: 74 [48640/50000]	Loss: 0.3924	LR: 0.025000
Training Epoch: 74 [48768/50000]	Loss: 0.5027	LR: 0.025000
Training Epoch: 74 [48896/50000]	Loss: 0.4012	LR: 0.025000
Training Epoch: 74 [49024/50000]	Loss: 0.6575	LR: 0.025000
Training Epoch: 74 [49152/50000]	Loss: 0.4789	LR: 0.025000
Training Epoch: 74 [49280/50000]	Loss: 0.5835	LR: 0.025000
Training Epoch: 74 [49408/50000]	Loss: 0.5720	LR: 0.025000
Training Epoch: 74 [49536/50000]	Loss: 0.5194	LR: 0.025000
Training Epoch: 74 [49664/50000]	Loss: 0.4031	LR: 0.025000
Training Epoch: 74 [49792/50000]	Loss: 0.4197	LR: 0.025000
Training Epoch: 74 [49920/50000]	Loss: 0.4712	LR: 0.025000
Training Epoch: 74 [50000/50000]	Loss: 0.5890	LR: 0.025000
epoch 74 training time consumed: 53.97s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  271584 GB |  271584 GB |
|       from large pool |  123392 KB |    1034 MB |  271316 GB |  271316 GB |
|       from small pool |   10798 KB |      13 MB |     267 GB |     267 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  271584 GB |  271584 GB |
|       from large pool |  123392 KB |    1034 MB |  271316 GB |  271316 GB |
|       from small pool |   10798 KB |      13 MB |     267 GB |     267 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  119513 GB |  119513 GB |
|       from large pool |  155136 KB |  433088 KB |  119217 GB |  119217 GB |
|       from small pool |    1490 KB |    3494 KB |     295 GB |     295 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   10479 K  |   10479 K  |
|       from large pool |      24    |      65    |    5470 K  |    5470 K  |
|       from small pool |     231    |     274    |    5009 K  |    5009 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   10479 K  |   10479 K  |
|       from large pool |      24    |      65    |    5470 K  |    5470 K  |
|       from small pool |     231    |     274    |    5009 K  |    5009 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5177 K  |    5177 K  |
|       from large pool |       9    |      14    |    2647 K  |    2647 K  |
|       from small pool |      12    |      16    |    2530 K  |    2530 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 74, Average loss: 0.0121, Accuracy: 0.6375, Time consumed:3.48s

Training Epoch: 75 [128/50000]	Loss: 0.5941	LR: 0.025000
Training Epoch: 75 [256/50000]	Loss: 0.3931	LR: 0.025000
Training Epoch: 75 [384/50000]	Loss: 0.5976	LR: 0.025000
Training Epoch: 75 [512/50000]	Loss: 0.3873	LR: 0.025000
Training Epoch: 75 [640/50000]	Loss: 0.3621	LR: 0.025000
Training Epoch: 75 [768/50000]	Loss: 0.4352	LR: 0.025000
Training Epoch: 75 [896/50000]	Loss: 0.3876	LR: 0.025000
Training Epoch: 75 [1024/50000]	Loss: 0.3294	LR: 0.025000
Training Epoch: 75 [1152/50000]	Loss: 0.3797	LR: 0.025000
Training Epoch: 75 [1280/50000]	Loss: 0.4062	LR: 0.025000
Training Epoch: 75 [1408/50000]	Loss: 0.4504	LR: 0.025000
Training Epoch: 75 [1536/50000]	Loss: 0.6689	LR: 0.025000
Training Epoch: 75 [1664/50000]	Loss: 0.5000	LR: 0.025000
Training Epoch: 75 [1792/50000]	Loss: 0.3603	LR: 0.025000
Training Epoch: 75 [1920/50000]	Loss: 0.3680	LR: 0.025000
Training Epoch: 75 [2048/50000]	Loss: 0.4332	LR: 0.025000
Training Epoch: 75 [2176/50000]	Loss: 0.3797	LR: 0.025000
Training Epoch: 75 [2304/50000]	Loss: 0.3992	LR: 0.025000
Training Epoch: 75 [2432/50000]	Loss: 0.4417	LR: 0.025000
Training Epoch: 75 [2560/50000]	Loss: 0.3570	LR: 0.025000
Training Epoch: 75 [2688/50000]	Loss: 0.4513	LR: 0.025000
Training Epoch: 75 [2816/50000]	Loss: 0.4982	LR: 0.025000
Training Epoch: 75 [2944/50000]	Loss: 0.5336	LR: 0.025000
Training Epoch: 75 [3072/50000]	Loss: 0.3571	LR: 0.025000
Training Epoch: 75 [3200/50000]	Loss: 0.3410	LR: 0.025000
Training Epoch: 75 [3328/50000]	Loss: 0.5512	LR: 0.025000
Training Epoch: 75 [3456/50000]	Loss: 0.5189	LR: 0.025000
Training Epoch: 75 [3584/50000]	Loss: 0.3527	LR: 0.025000
Training Epoch: 75 [3712/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 75 [3840/50000]	Loss: 0.3901	LR: 0.025000
Training Epoch: 75 [3968/50000]	Loss: 0.3442	LR: 0.025000
Training Epoch: 75 [4096/50000]	Loss: 0.3360	LR: 0.025000
Training Epoch: 75 [4224/50000]	Loss: 0.4549	LR: 0.025000
Training Epoch: 75 [4352/50000]	Loss: 0.4003	LR: 0.025000
Training Epoch: 75 [4480/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 75 [4608/50000]	Loss: 0.3659	LR: 0.025000
Training Epoch: 75 [4736/50000]	Loss: 0.4177	LR: 0.025000
Training Epoch: 75 [4864/50000]	Loss: 0.3993	LR: 0.025000
Training Epoch: 75 [4992/50000]	Loss: 0.4142	LR: 0.025000
Training Epoch: 75 [5120/50000]	Loss: 0.4892	LR: 0.025000
Training Epoch: 75 [5248/50000]	Loss: 0.3373	LR: 0.025000
Training Epoch: 75 [5376/50000]	Loss: 0.4650	LR: 0.025000
Training Epoch: 75 [5504/50000]	Loss: 0.3890	LR: 0.025000
Training Epoch: 75 [5632/50000]	Loss: 0.2860	LR: 0.025000
Training Epoch: 75 [5760/50000]	Loss: 0.3557	LR: 0.025000
Training Epoch: 75 [5888/50000]	Loss: 0.3695	LR: 0.025000
Training Epoch: 75 [6016/50000]	Loss: 0.3809	LR: 0.025000
Training Epoch: 75 [6144/50000]	Loss: 0.5275	LR: 0.025000
Training Epoch: 75 [6272/50000]	Loss: 0.3501	LR: 0.025000
Training Epoch: 75 [6400/50000]	Loss: 0.6485	LR: 0.025000
Training Epoch: 75 [6528/50000]	Loss: 0.3746	LR: 0.025000
Training Epoch: 75 [6656/50000]	Loss: 0.3083	LR: 0.025000
Training Epoch: 75 [6784/50000]	Loss: 0.3533	LR: 0.025000
Training Epoch: 75 [6912/50000]	Loss: 0.3496	LR: 0.025000
Training Epoch: 75 [7040/50000]	Loss: 0.3532	LR: 0.025000
Training Epoch: 75 [7168/50000]	Loss: 0.3850	LR: 0.025000
Training Epoch: 75 [7296/50000]	Loss: 0.3308	LR: 0.025000
Training Epoch: 75 [7424/50000]	Loss: 0.4003	LR: 0.025000
Training Epoch: 75 [7552/50000]	Loss: 0.5116	LR: 0.025000
Training Epoch: 75 [7680/50000]	Loss: 0.4977	LR: 0.025000
Training Epoch: 75 [7808/50000]	Loss: 0.4118	LR: 0.025000
Training Epoch: 75 [7936/50000]	Loss: 0.4931	LR: 0.025000
Training Epoch: 75 [8064/50000]	Loss: 0.3139	LR: 0.025000
Training Epoch: 75 [8192/50000]	Loss: 0.3380	LR: 0.025000
Training Epoch: 75 [8320/50000]	Loss: 0.4998	LR: 0.025000
Training Epoch: 75 [8448/50000]	Loss: 0.6045	LR: 0.025000
Training Epoch: 75 [8576/50000]	Loss: 0.3364	LR: 0.025000
Training Epoch: 75 [8704/50000]	Loss: 0.3381	LR: 0.025000
Training Epoch: 75 [8832/50000]	Loss: 0.3328	LR: 0.025000
Training Epoch: 75 [8960/50000]	Loss: 0.4218	LR: 0.025000
Training Epoch: 75 [9088/50000]	Loss: 0.4138	LR: 0.025000
Training Epoch: 75 [9216/50000]	Loss: 0.3427	LR: 0.025000
Training Epoch: 75 [9344/50000]	Loss: 0.5342	LR: 0.025000
Training Epoch: 75 [9472/50000]	Loss: 0.4439	LR: 0.025000
Training Epoch: 75 [9600/50000]	Loss: 0.3492	LR: 0.025000
Training Epoch: 75 [9728/50000]	Loss: 0.4158	LR: 0.025000
Training Epoch: 75 [9856/50000]	Loss: 0.4215	LR: 0.025000
Training Epoch: 75 [9984/50000]	Loss: 0.4769	LR: 0.025000
Training Epoch: 75 [10112/50000]	Loss: 0.3940	LR: 0.025000
Training Epoch: 75 [10240/50000]	Loss: 0.4168	LR: 0.025000
Training Epoch: 75 [10368/50000]	Loss: 0.4649	LR: 0.025000
Training Epoch: 75 [10496/50000]	Loss: 0.4520	LR: 0.025000
Training Epoch: 75 [10624/50000]	Loss: 0.3309	LR: 0.025000
Training Epoch: 75 [10752/50000]	Loss: 0.5212	LR: 0.025000
Training Epoch: 75 [10880/50000]	Loss: 0.4815	LR: 0.025000
Training Epoch: 75 [11008/50000]	Loss: 0.4529	LR: 0.025000
Training Epoch: 75 [11136/50000]	Loss: 0.2659	LR: 0.025000
Training Epoch: 75 [11264/50000]	Loss: 0.4535	LR: 0.025000
Training Epoch: 75 [11392/50000]	Loss: 0.3919	LR: 0.025000
Training Epoch: 75 [11520/50000]	Loss: 0.4220	LR: 0.025000
Training Epoch: 75 [11648/50000]	Loss: 0.5256	LR: 0.025000
Training Epoch: 75 [11776/50000]	Loss: 0.4154	LR: 0.025000
Training Epoch: 75 [11904/50000]	Loss: 0.3064	LR: 0.025000
Training Epoch: 75 [12032/50000]	Loss: 0.5423	LR: 0.025000
Training Epoch: 75 [12160/50000]	Loss: 0.5832	LR: 0.025000
Training Epoch: 75 [12288/50000]	Loss: 0.3992	LR: 0.025000
Training Epoch: 75 [12416/50000]	Loss: 0.3789	LR: 0.025000
Training Epoch: 75 [12544/50000]	Loss: 0.4438	LR: 0.025000
Training Epoch: 75 [12672/50000]	Loss: 0.3549	LR: 0.025000
Training Epoch: 75 [12800/50000]	Loss: 0.4094	LR: 0.025000
Training Epoch: 75 [12928/50000]	Loss: 0.4706	LR: 0.025000
Training Epoch: 75 [13056/50000]	Loss: 0.4681	LR: 0.025000
Training Epoch: 75 [13184/50000]	Loss: 0.4476	LR: 0.025000
Training Epoch: 75 [13312/50000]	Loss: 0.4663	LR: 0.025000
Training Epoch: 75 [13440/50000]	Loss: 0.4931	LR: 0.025000
Training Epoch: 75 [13568/50000]	Loss: 0.5495	LR: 0.025000
Training Epoch: 75 [13696/50000]	Loss: 0.3543	LR: 0.025000
Training Epoch: 75 [13824/50000]	Loss: 0.3665	LR: 0.025000
Training Epoch: 75 [13952/50000]	Loss: 0.4436	LR: 0.025000
Training Epoch: 75 [14080/50000]	Loss: 0.3590	LR: 0.025000
Training Epoch: 75 [14208/50000]	Loss: 0.5960	LR: 0.025000
Training Epoch: 75 [14336/50000]	Loss: 0.3196	LR: 0.025000
Training Epoch: 75 [14464/50000]	Loss: 0.2772	LR: 0.025000
Training Epoch: 75 [14592/50000]	Loss: 0.6182	LR: 0.025000
Training Epoch: 75 [14720/50000]	Loss: 0.4984	LR: 0.025000
Training Epoch: 75 [14848/50000]	Loss: 0.5498	LR: 0.025000
Training Epoch: 75 [14976/50000]	Loss: 0.3862	LR: 0.025000
Training Epoch: 75 [15104/50000]	Loss: 0.5958	LR: 0.025000
Training Epoch: 75 [15232/50000]	Loss: 0.3658	LR: 0.025000
Training Epoch: 75 [15360/50000]	Loss: 0.4626	LR: 0.025000
Training Epoch: 75 [15488/50000]	Loss: 0.3480	LR: 0.025000
Training Epoch: 75 [15616/50000]	Loss: 0.4330	LR: 0.025000
Training Epoch: 75 [15744/50000]	Loss: 0.5645	LR: 0.025000
Training Epoch: 75 [15872/50000]	Loss: 0.4335	LR: 0.025000
Training Epoch: 75 [16000/50000]	Loss: 0.4259	LR: 0.025000
Training Epoch: 75 [16128/50000]	Loss: 0.4636	LR: 0.025000
Training Epoch: 75 [16256/50000]	Loss: 0.3755	LR: 0.025000
Training Epoch: 75 [16384/50000]	Loss: 0.3836	LR: 0.025000
Training Epoch: 75 [16512/50000]	Loss: 0.4368	LR: 0.025000
Training Epoch: 75 [16640/50000]	Loss: 0.6303	LR: 0.025000
Training Epoch: 75 [16768/50000]	Loss: 0.5667	LR: 0.025000
Training Epoch: 75 [16896/50000]	Loss: 0.5699	LR: 0.025000
Training Epoch: 75 [17024/50000]	Loss: 0.5218	LR: 0.025000
Training Epoch: 75 [17152/50000]	Loss: 0.3649	LR: 0.025000
Training Epoch: 75 [17280/50000]	Loss: 0.4625	LR: 0.025000
Training Epoch: 75 [17408/50000]	Loss: 0.4627	LR: 0.025000
Training Epoch: 75 [17536/50000]	Loss: 0.4677	LR: 0.025000
Training Epoch: 75 [17664/50000]	Loss: 0.5312	LR: 0.025000
Training Epoch: 75 [17792/50000]	Loss: 0.4744	LR: 0.025000
Training Epoch: 75 [17920/50000]	Loss: 0.5416	LR: 0.025000
Training Epoch: 75 [18048/50000]	Loss: 0.4603	LR: 0.025000
Training Epoch: 75 [18176/50000]	Loss: 0.3911	LR: 0.025000
Training Epoch: 75 [18304/50000]	Loss: 0.5297	LR: 0.025000
Training Epoch: 75 [18432/50000]	Loss: 0.4727	LR: 0.025000
Training Epoch: 75 [18560/50000]	Loss: 0.4190	LR: 0.025000
Training Epoch: 75 [18688/50000]	Loss: 0.4201	LR: 0.025000
Training Epoch: 75 [18816/50000]	Loss: 0.4325	LR: 0.025000
Training Epoch: 75 [18944/50000]	Loss: 0.6978	LR: 0.025000
Training Epoch: 75 [19072/50000]	Loss: 0.5016	LR: 0.025000
Training Epoch: 75 [19200/50000]	Loss: 0.4394	LR: 0.025000
Training Epoch: 75 [19328/50000]	Loss: 0.5539	LR: 0.025000
Training Epoch: 75 [19456/50000]	Loss: 0.4663	LR: 0.025000
Training Epoch: 75 [19584/50000]	Loss: 0.5024	LR: 0.025000
Training Epoch: 75 [19712/50000]	Loss: 0.3676	LR: 0.025000
Training Epoch: 75 [19840/50000]	Loss: 0.4733	LR: 0.025000
Training Epoch: 75 [19968/50000]	Loss: 0.4079	LR: 0.025000
Training Epoch: 75 [20096/50000]	Loss: 0.3417	LR: 0.025000
Training Epoch: 75 [20224/50000]	Loss: 0.5007	LR: 0.025000
Training Epoch: 75 [20352/50000]	Loss: 0.2824	LR: 0.025000
Training Epoch: 75 [20480/50000]	Loss: 0.6308	LR: 0.025000
Training Epoch: 75 [20608/50000]	Loss: 0.4097	LR: 0.025000
Training Epoch: 75 [20736/50000]	Loss: 0.4796	LR: 0.025000
Training Epoch: 75 [20864/50000]	Loss: 0.4774	LR: 0.025000
Training Epoch: 75 [20992/50000]	Loss: 0.4960	LR: 0.025000
Training Epoch: 75 [21120/50000]	Loss: 0.3278	LR: 0.025000
Training Epoch: 75 [21248/50000]	Loss: 0.5229	LR: 0.025000
Training Epoch: 75 [21376/50000]	Loss: 0.4418	LR: 0.025000
Training Epoch: 75 [21504/50000]	Loss: 0.5838	LR: 0.025000
Training Epoch: 75 [21632/50000]	Loss: 0.3611	LR: 0.025000
Training Epoch: 75 [21760/50000]	Loss: 0.4420	LR: 0.025000
Training Epoch: 75 [21888/50000]	Loss: 0.4333	LR: 0.025000
Training Epoch: 75 [22016/50000]	Loss: 0.5701	LR: 0.025000
Training Epoch: 75 [22144/50000]	Loss: 0.4847	LR: 0.025000
Training Epoch: 75 [22272/50000]	Loss: 0.4495	LR: 0.025000
Training Epoch: 75 [22400/50000]	Loss: 0.4623	LR: 0.025000
Training Epoch: 75 [22528/50000]	Loss: 0.3664	LR: 0.025000
Training Epoch: 75 [22656/50000]	Loss: 0.4515	LR: 0.025000
Training Epoch: 75 [22784/50000]	Loss: 0.4892	LR: 0.025000
Training Epoch: 75 [22912/50000]	Loss: 0.4575	LR: 0.025000
Training Epoch: 75 [23040/50000]	Loss: 0.4811	LR: 0.025000
Training Epoch: 75 [23168/50000]	Loss: 0.4848	LR: 0.025000
Training Epoch: 75 [23296/50000]	Loss: 0.5935	LR: 0.025000
Training Epoch: 75 [23424/50000]	Loss: 0.5179	LR: 0.025000
Training Epoch: 75 [23552/50000]	Loss: 0.5878	LR: 0.025000
Training Epoch: 75 [23680/50000]	Loss: 0.5127	LR: 0.025000
Training Epoch: 75 [23808/50000]	Loss: 0.6184	LR: 0.025000
Training Epoch: 75 [23936/50000]	Loss: 0.6663	LR: 0.025000
Training Epoch: 75 [24064/50000]	Loss: 0.4926	LR: 0.025000
Training Epoch: 75 [24192/50000]	Loss: 0.4439	LR: 0.025000
Training Epoch: 75 [24320/50000]	Loss: 0.4442	LR: 0.025000
Training Epoch: 75 [24448/50000]	Loss: 0.5479	LR: 0.025000
Training Epoch: 75 [24576/50000]	Loss: 0.5565	LR: 0.025000
Training Epoch: 75 [24704/50000]	Loss: 0.3887	LR: 0.025000
Training Epoch: 75 [24832/50000]	Loss: 0.6197	LR: 0.025000
Training Epoch: 75 [24960/50000]	Loss: 0.4191	LR: 0.025000
Training Epoch: 75 [25088/50000]	Loss: 0.5778	LR: 0.025000
Training Epoch: 75 [25216/50000]	Loss: 0.3626	LR: 0.025000
Training Epoch: 75 [25344/50000]	Loss: 0.3869	LR: 0.025000
Training Epoch: 75 [25472/50000]	Loss: 0.3688	LR: 0.025000
Training Epoch: 75 [25600/50000]	Loss: 0.4650	LR: 0.025000
Training Epoch: 75 [25728/50000]	Loss: 0.5404	LR: 0.025000
Training Epoch: 75 [25856/50000]	Loss: 0.4705	LR: 0.025000
Training Epoch: 75 [25984/50000]	Loss: 0.4687	LR: 0.025000
Training Epoch: 75 [26112/50000]	Loss: 0.4978	LR: 0.025000
Training Epoch: 75 [26240/50000]	Loss: 0.4358	LR: 0.025000
Training Epoch: 75 [26368/50000]	Loss: 0.5227	LR: 0.025000
Training Epoch: 75 [26496/50000]	Loss: 0.4328	LR: 0.025000
Training Epoch: 75 [26624/50000]	Loss: 0.4395	LR: 0.025000
Training Epoch: 75 [26752/50000]	Loss: 0.4659	LR: 0.025000
Training Epoch: 75 [26880/50000]	Loss: 0.3797	LR: 0.025000
Training Epoch: 75 [27008/50000]	Loss: 0.4147	LR: 0.025000
Training Epoch: 75 [27136/50000]	Loss: 0.4442	LR: 0.025000
Training Epoch: 75 [27264/50000]	Loss: 0.4923	LR: 0.025000
Training Epoch: 75 [27392/50000]	Loss: 0.6533	LR: 0.025000
Training Epoch: 75 [27520/50000]	Loss: 0.5327	LR: 0.025000
Training Epoch: 75 [27648/50000]	Loss: 0.3192	LR: 0.025000
Training Epoch: 75 [27776/50000]	Loss: 0.4977	LR: 0.025000
Training Epoch: 75 [27904/50000]	Loss: 0.4488	LR: 0.025000
Training Epoch: 75 [28032/50000]	Loss: 0.5684	LR: 0.025000
Training Epoch: 75 [28160/50000]	Loss: 0.6302	LR: 0.025000
Training Epoch: 75 [28288/50000]	Loss: 0.5844	LR: 0.025000
Training Epoch: 75 [28416/50000]	Loss: 0.6468	LR: 0.025000
Training Epoch: 75 [28544/50000]	Loss: 0.4431	LR: 0.025000
Training Epoch: 75 [28672/50000]	Loss: 0.5567	LR: 0.025000
Training Epoch: 75 [28800/50000]	Loss: 0.5367	LR: 0.025000
Training Epoch: 75 [28928/50000]	Loss: 0.4851	LR: 0.025000
Training Epoch: 75 [29056/50000]	Loss: 0.4937	LR: 0.025000
Training Epoch: 75 [29184/50000]	Loss: 0.5794	LR: 0.025000
Training Epoch: 75 [29312/50000]	Loss: 0.5615	LR: 0.025000
Training Epoch: 75 [29440/50000]	Loss: 0.5251	LR: 0.025000
Training Epoch: 75 [29568/50000]	Loss: 0.4761	LR: 0.025000
Training Epoch: 75 [29696/50000]	Loss: 0.5171	LR: 0.025000
Training Epoch: 75 [29824/50000]	Loss: 0.6309	LR: 0.025000
Training Epoch: 75 [29952/50000]	Loss: 0.5548	LR: 0.025000
Training Epoch: 75 [30080/50000]	Loss: 0.4363	LR: 0.025000
Training Epoch: 75 [30208/50000]	Loss: 0.5250	LR: 0.025000
Training Epoch: 75 [30336/50000]	Loss: 0.4827	LR: 0.025000
Training Epoch: 75 [30464/50000]	Loss: 0.5324	LR: 0.025000
Training Epoch: 75 [30592/50000]	Loss: 0.4823	LR: 0.025000
Training Epoch: 75 [30720/50000]	Loss: 0.5611	LR: 0.025000
Training Epoch: 75 [30848/50000]	Loss: 0.5405	LR: 0.025000
Training Epoch: 75 [30976/50000]	Loss: 0.4622	LR: 0.025000
Training Epoch: 75 [31104/50000]	Loss: 0.5504	LR: 0.025000
Training Epoch: 75 [31232/50000]	Loss: 0.4554	LR: 0.025000
Training Epoch: 75 [31360/50000]	Loss: 0.5677	LR: 0.025000
Training Epoch: 75 [31488/50000]	Loss: 0.5292	LR: 0.025000
Training Epoch: 75 [31616/50000]	Loss: 0.5651	LR: 0.025000
Training Epoch: 75 [31744/50000]	Loss: 0.4687	LR: 0.025000
Training Epoch: 75 [31872/50000]	Loss: 0.5205	LR: 0.025000
Training Epoch: 75 [32000/50000]	Loss: 0.4394	LR: 0.025000
Training Epoch: 75 [32128/50000]	Loss: 0.4633	LR: 0.025000
Training Epoch: 75 [32256/50000]	Loss: 0.6082	LR: 0.025000
Training Epoch: 75 [32384/50000]	Loss: 0.4596	LR: 0.025000
Training Epoch: 75 [32512/50000]	Loss: 0.5033	LR: 0.025000
Training Epoch: 75 [32640/50000]	Loss: 0.4958	LR: 0.025000
Training Epoch: 75 [32768/50000]	Loss: 0.4789	LR: 0.025000
Training Epoch: 75 [32896/50000]	Loss: 0.4874	LR: 0.025000
Training Epoch: 75 [33024/50000]	Loss: 0.5835	LR: 0.025000
Training Epoch: 75 [33152/50000]	Loss: 0.3887	LR: 0.025000
Training Epoch: 75 [33280/50000]	Loss: 0.5216	LR: 0.025000
Training Epoch: 75 [33408/50000]	Loss: 0.4009	LR: 0.025000
Training Epoch: 75 [33536/50000]	Loss: 0.3743	LR: 0.025000
Training Epoch: 75 [33664/50000]	Loss: 0.4306	LR: 0.025000
Training Epoch: 75 [33792/50000]	Loss: 0.4179	LR: 0.025000
Training Epoch: 75 [33920/50000]	Loss: 0.5514	LR: 0.025000
Training Epoch: 75 [34048/50000]	Loss: 0.4876	LR: 0.025000
Training Epoch: 75 [34176/50000]	Loss: 0.4417	LR: 0.025000
Training Epoch: 75 [34304/50000]	Loss: 0.4593	LR: 0.025000
Training Epoch: 75 [34432/50000]	Loss: 0.5970	LR: 0.025000
Training Epoch: 75 [34560/50000]	Loss: 0.5373	LR: 0.025000
Training Epoch: 75 [34688/50000]	Loss: 0.4524	LR: 0.025000
Training Epoch: 75 [34816/50000]	Loss: 0.7365	LR: 0.025000
Training Epoch: 75 [34944/50000]	Loss: 0.3935	LR: 0.025000
Training Epoch: 75 [35072/50000]	Loss: 0.7055	LR: 0.025000
Training Epoch: 75 [35200/50000]	Loss: 0.6273	LR: 0.025000
Training Epoch: 75 [35328/50000]	Loss: 0.4055	LR: 0.025000
Training Epoch: 75 [35456/50000]	Loss: 0.4825	LR: 0.025000
Training Epoch: 75 [35584/50000]	Loss: 0.5009	LR: 0.025000
Training Epoch: 75 [35712/50000]	Loss: 0.5311	LR: 0.025000
Training Epoch: 75 [35840/50000]	Loss: 0.4813	LR: 0.025000
Training Epoch: 75 [35968/50000]	Loss: 0.5915	LR: 0.025000
Training Epoch: 75 [36096/50000]	Loss: 0.4376	LR: 0.025000
Training Epoch: 75 [36224/50000]	Loss: 0.4724	LR: 0.025000
Training Epoch: 75 [36352/50000]	Loss: 0.5722	LR: 0.025000
Training Epoch: 75 [36480/50000]	Loss: 0.5839	LR: 0.025000
Training Epoch: 75 [36608/50000]	Loss: 0.4916	LR: 0.025000
Training Epoch: 75 [36736/50000]	Loss: 0.4650	LR: 0.025000
Training Epoch: 75 [36864/50000]	Loss: 0.4764	LR: 0.025000
Training Epoch: 75 [36992/50000]	Loss: 0.3690	LR: 0.025000
Training Epoch: 75 [37120/50000]	Loss: 0.5160	LR: 0.025000
Training Epoch: 75 [37248/50000]	Loss: 0.6272	LR: 0.025000
Training Epoch: 75 [37376/50000]	Loss: 0.4541	LR: 0.025000
Training Epoch: 75 [37504/50000]	Loss: 0.5586	LR: 0.025000
Training Epoch: 75 [37632/50000]	Loss: 0.5152	LR: 0.025000
Training Epoch: 75 [37760/50000]	Loss: 0.4305	LR: 0.025000
Training Epoch: 75 [37888/50000]	Loss: 0.5765	LR: 0.025000
Training Epoch: 75 [38016/50000]	Loss: 0.4533	LR: 0.025000
Training Epoch: 75 [38144/50000]	Loss: 0.5487	LR: 0.025000
Training Epoch: 75 [38272/50000]	Loss: 0.5427	LR: 0.025000
Training Epoch: 75 [38400/50000]	Loss: 0.4629	LR: 0.025000
Training Epoch: 75 [38528/50000]	Loss: 0.2962	LR: 0.025000
Training Epoch: 75 [38656/50000]	Loss: 0.5272	LR: 0.025000
Training Epoch: 75 [38784/50000]	Loss: 0.4304	LR: 0.025000
Training Epoch: 75 [38912/50000]	Loss: 0.5568	LR: 0.025000
Training Epoch: 75 [39040/50000]	Loss: 0.4327	LR: 0.025000
Training Epoch: 75 [39168/50000]	Loss: 0.4671	LR: 0.025000
Training Epoch: 75 [39296/50000]	Loss: 0.4617	LR: 0.025000
Training Epoch: 75 [39424/50000]	Loss: 0.5103	LR: 0.025000
Training Epoch: 75 [39552/50000]	Loss: 0.5517	LR: 0.025000
Training Epoch: 75 [39680/50000]	Loss: 0.6021	LR: 0.025000
Training Epoch: 75 [39808/50000]	Loss: 0.3760	LR: 0.025000
Training Epoch: 75 [39936/50000]	Loss: 0.5435	LR: 0.025000
Training Epoch: 75 [40064/50000]	Loss: 0.4190	LR: 0.025000
Training Epoch: 75 [40192/50000]	Loss: 0.4328	LR: 0.025000
Training Epoch: 75 [40320/50000]	Loss: 0.4854	LR: 0.025000
Training Epoch: 75 [40448/50000]	Loss: 0.4035	LR: 0.025000
Training Epoch: 75 [40576/50000]	Loss: 0.2795	LR: 0.025000
Training Epoch: 75 [40704/50000]	Loss: 0.4820	LR: 0.025000
Training Epoch: 75 [40832/50000]	Loss: 0.4503	LR: 0.025000
Training Epoch: 75 [40960/50000]	Loss: 0.4343	LR: 0.025000
Training Epoch: 75 [41088/50000]	Loss: 0.4423	LR: 0.025000
Training Epoch: 75 [41216/50000]	Loss: 0.5529	LR: 0.025000
Training Epoch: 75 [41344/50000]	Loss: 0.4648	LR: 0.025000
Training Epoch: 75 [41472/50000]	Loss: 0.6399	LR: 0.025000
Training Epoch: 75 [41600/50000]	Loss: 0.5575	LR: 0.025000
Training Epoch: 75 [41728/50000]	Loss: 0.5192	LR: 0.025000
Training Epoch: 75 [41856/50000]	Loss: 0.5578	LR: 0.025000
Training Epoch: 75 [41984/50000]	Loss: 0.4573	LR: 0.025000
Training Epoch: 75 [42112/50000]	Loss: 0.4255	LR: 0.025000
Training Epoch: 75 [42240/50000]	Loss: 0.4304	LR: 0.025000
Training Epoch: 75 [42368/50000]	Loss: 0.6217	LR: 0.025000
Training Epoch: 75 [42496/50000]	Loss: 0.4276	LR: 0.025000
Training Epoch: 75 [42624/50000]	Loss: 0.4457	LR: 0.025000
Training Epoch: 75 [42752/50000]	Loss: 0.4979	LR: 0.025000
Training Epoch: 75 [42880/50000]	Loss: 0.3767	LR: 0.025000
Training Epoch: 75 [43008/50000]	Loss: 0.3325	LR: 0.025000
Training Epoch: 75 [43136/50000]	Loss: 0.5261	LR: 0.025000
Training Epoch: 75 [43264/50000]	Loss: 0.4771	LR: 0.025000
Training Epoch: 75 [43392/50000]	Loss: 0.5625	LR: 0.025000
Training Epoch: 75 [43520/50000]	Loss: 0.7010	LR: 0.025000
Training Epoch: 75 [43648/50000]	Loss: 0.5638	LR: 0.025000
Training Epoch: 75 [43776/50000]	Loss: 0.5942	LR: 0.025000
Training Epoch: 75 [43904/50000]	Loss: 0.4883	LR: 0.025000
Training Epoch: 75 [44032/50000]	Loss: 0.5611	LR: 0.025000
Training Epoch: 75 [44160/50000]	Loss: 0.4495	LR: 0.025000
Training Epoch: 75 [44288/50000]	Loss: 0.4662	LR: 0.025000
Training Epoch: 75 [44416/50000]	Loss: 0.5835	LR: 0.025000
Training Epoch: 75 [44544/50000]	Loss: 0.3468	LR: 0.025000
Training Epoch: 75 [44672/50000]	Loss: 0.3770	LR: 0.025000
Training Epoch: 75 [44800/50000]	Loss: 0.5631	LR: 0.025000
Training Epoch: 75 [44928/50000]	Loss: 0.4950	LR: 0.025000
Training Epoch: 75 [45056/50000]	Loss: 0.5508	LR: 0.025000
Training Epoch: 75 [45184/50000]	Loss: 0.4903	LR: 0.025000
Training Epoch: 75 [45312/50000]	Loss: 0.5191	LR: 0.025000
Training Epoch: 75 [45440/50000]	Loss: 0.5201	LR: 0.025000
Training Epoch: 75 [45568/50000]	Loss: 0.4965	LR: 0.025000
Training Epoch: 75 [45696/50000]	Loss: 0.6030	LR: 0.025000
Training Epoch: 75 [45824/50000]	Loss: 0.6921	LR: 0.025000
Training Epoch: 75 [45952/50000]	Loss: 0.4435	LR: 0.025000
Training Epoch: 75 [46080/50000]	Loss: 0.6957	LR: 0.025000
Training Epoch: 75 [46208/50000]	Loss: 0.5604	LR: 0.025000
Training Epoch: 75 [46336/50000]	Loss: 0.6548	LR: 0.025000
Training Epoch: 75 [46464/50000]	Loss: 0.4674	LR: 0.025000
Training Epoch: 75 [46592/50000]	Loss: 0.4553	LR: 0.025000
Training Epoch: 75 [46720/50000]	Loss: 0.5546	LR: 0.025000
Training Epoch: 75 [46848/50000]	Loss: 0.5242	LR: 0.025000
Training Epoch: 75 [46976/50000]	Loss: 0.4789	LR: 0.025000
Training Epoch: 75 [47104/50000]	Loss: 0.5333	LR: 0.025000
Training Epoch: 75 [47232/50000]	Loss: 0.4411	LR: 0.025000
Training Epoch: 75 [47360/50000]	Loss: 0.5689	LR: 0.025000
Training Epoch: 75 [47488/50000]	Loss: 0.4786	LR: 0.025000
Training Epoch: 75 [47616/50000]	Loss: 0.6042	LR: 0.025000
Training Epoch: 75 [47744/50000]	Loss: 0.5965	LR: 0.025000
Training Epoch: 75 [47872/50000]	Loss: 0.5350	LR: 0.025000
Training Epoch: 75 [48000/50000]	Loss: 0.5083	LR: 0.025000
Training Epoch: 75 [48128/50000]	Loss: 0.5897	LR: 0.025000
Training Epoch: 75 [48256/50000]	Loss: 0.4984	LR: 0.025000
Training Epoch: 75 [48384/50000]	Loss: 0.4143	LR: 0.025000
Training Epoch: 75 [48512/50000]	Loss: 0.5639	LR: 0.025000
Training Epoch: 75 [48640/50000]	Loss: 0.7087	LR: 0.025000
Training Epoch: 75 [48768/50000]	Loss: 0.5141	LR: 0.025000
Training Epoch: 75 [48896/50000]	Loss: 0.4773	LR: 0.025000
Training Epoch: 75 [49024/50000]	Loss: 0.5887	LR: 0.025000
Training Epoch: 75 [49152/50000]	Loss: 0.5024	LR: 0.025000
Training Epoch: 75 [49280/50000]	Loss: 0.6844	LR: 0.025000
Training Epoch: 75 [49408/50000]	Loss: 0.5563	LR: 0.025000
Training Epoch: 75 [49536/50000]	Loss: 0.5564	LR: 0.025000
Training Epoch: 75 [49664/50000]	Loss: 0.4297	LR: 0.025000
Training Epoch: 75 [49792/50000]	Loss: 0.5323	LR: 0.025000
Training Epoch: 75 [49920/50000]	Loss: 0.6152	LR: 0.025000
Training Epoch: 75 [50000/50000]	Loss: 0.5575	LR: 0.025000
epoch 75 training time consumed: 53.85s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  275254 GB |  275254 GB |
|       from large pool |  123392 KB |    1034 MB |  274983 GB |  274983 GB |
|       from small pool |   10798 KB |      13 MB |     271 GB |     271 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  275254 GB |  275254 GB |
|       from large pool |  123392 KB |    1034 MB |  274983 GB |  274983 GB |
|       from small pool |   10798 KB |      13 MB |     271 GB |     271 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  121128 GB |  121128 GB |
|       from large pool |  155136 KB |  433088 KB |  120828 GB |  120828 GB |
|       from small pool |    1490 KB |    3494 KB |     299 GB |     299 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   10621 K  |   10620 K  |
|       from large pool |      24    |      65    |    5544 K  |    5544 K  |
|       from small pool |     231    |     274    |    5077 K  |    5076 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   10621 K  |   10620 K  |
|       from large pool |      24    |      65    |    5544 K  |    5544 K  |
|       from small pool |     231    |     274    |    5077 K  |    5076 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5247 K  |    5247 K  |
|       from large pool |       9    |      14    |    2683 K  |    2683 K  |
|       from small pool |      12    |      16    |    2564 K  |    2564 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 75, Average loss: 0.0116, Accuracy: 0.6440, Time consumed:3.46s

Training Epoch: 76 [128/50000]	Loss: 0.4991	LR: 0.025000
Training Epoch: 76 [256/50000]	Loss: 0.4600	LR: 0.025000
Training Epoch: 76 [384/50000]	Loss: 0.5460	LR: 0.025000
Training Epoch: 76 [512/50000]	Loss: 0.4695	LR: 0.025000
Training Epoch: 76 [640/50000]	Loss: 0.4488	LR: 0.025000
Training Epoch: 76 [768/50000]	Loss: 0.3847	LR: 0.025000
Training Epoch: 76 [896/50000]	Loss: 0.4104	LR: 0.025000
Training Epoch: 76 [1024/50000]	Loss: 0.4331	LR: 0.025000
Training Epoch: 76 [1152/50000]	Loss: 0.3734	LR: 0.025000
Training Epoch: 76 [1280/50000]	Loss: 0.3315	LR: 0.025000
Training Epoch: 76 [1408/50000]	Loss: 0.6663	LR: 0.025000
Training Epoch: 76 [1536/50000]	Loss: 0.4784	LR: 0.025000
Training Epoch: 76 [1664/50000]	Loss: 0.4925	LR: 0.025000
Training Epoch: 76 [1792/50000]	Loss: 0.4192	LR: 0.025000
Training Epoch: 76 [1920/50000]	Loss: 0.5923	LR: 0.025000
Training Epoch: 76 [2048/50000]	Loss: 0.3757	LR: 0.025000
Training Epoch: 76 [2176/50000]	Loss: 0.3161	LR: 0.025000
Training Epoch: 76 [2304/50000]	Loss: 0.4141	LR: 0.025000
Training Epoch: 76 [2432/50000]	Loss: 0.4253	LR: 0.025000
Training Epoch: 76 [2560/50000]	Loss: 0.4731	LR: 0.025000
Training Epoch: 76 [2688/50000]	Loss: 0.4719	LR: 0.025000
Training Epoch: 76 [2816/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 76 [2944/50000]	Loss: 0.5708	LR: 0.025000
Training Epoch: 76 [3072/50000]	Loss: 0.3554	LR: 0.025000
Training Epoch: 76 [3200/50000]	Loss: 0.4493	LR: 0.025000
Training Epoch: 76 [3328/50000]	Loss: 0.4714	LR: 0.025000
Training Epoch: 76 [3456/50000]	Loss: 0.3216	LR: 0.025000
Training Epoch: 76 [3584/50000]	Loss: 0.2331	LR: 0.025000
Training Epoch: 76 [3712/50000]	Loss: 0.3639	LR: 0.025000
Training Epoch: 76 [3840/50000]	Loss: 0.5404	LR: 0.025000
Training Epoch: 76 [3968/50000]	Loss: 0.4137	LR: 0.025000
Training Epoch: 76 [4096/50000]	Loss: 0.4320	LR: 0.025000
Training Epoch: 76 [4224/50000]	Loss: 0.3601	LR: 0.025000
Training Epoch: 76 [4352/50000]	Loss: 0.3896	LR: 0.025000
Training Epoch: 76 [4480/50000]	Loss: 0.4577	LR: 0.025000
Training Epoch: 76 [4608/50000]	Loss: 0.3487	LR: 0.025000
Training Epoch: 76 [4736/50000]	Loss: 0.3585	LR: 0.025000
Training Epoch: 76 [4864/50000]	Loss: 0.3441	LR: 0.025000
Training Epoch: 76 [4992/50000]	Loss: 0.3858	LR: 0.025000
Training Epoch: 76 [5120/50000]	Loss: 0.3924	LR: 0.025000
Training Epoch: 76 [5248/50000]	Loss: 0.4499	LR: 0.025000
Training Epoch: 76 [5376/50000]	Loss: 0.4107	LR: 0.025000
Training Epoch: 76 [5504/50000]	Loss: 0.4620	LR: 0.025000
Training Epoch: 76 [5632/50000]	Loss: 0.4662	LR: 0.025000
Training Epoch: 76 [5760/50000]	Loss: 0.3894	LR: 0.025000
Training Epoch: 76 [5888/50000]	Loss: 0.4228	LR: 0.025000
Training Epoch: 76 [6016/50000]	Loss: 0.2955	LR: 0.025000
Training Epoch: 76 [6144/50000]	Loss: 0.5774	LR: 0.025000
Training Epoch: 76 [6272/50000]	Loss: 0.3819	LR: 0.025000
Training Epoch: 76 [6400/50000]	Loss: 0.4933	LR: 0.025000
Training Epoch: 76 [6528/50000]	Loss: 0.3970	LR: 0.025000
Training Epoch: 76 [6656/50000]	Loss: 0.5149	LR: 0.025000
Training Epoch: 76 [6784/50000]	Loss: 0.3640	LR: 0.025000
Training Epoch: 76 [6912/50000]	Loss: 0.4695	LR: 0.025000
Training Epoch: 76 [7040/50000]	Loss: 0.4804	LR: 0.025000
Training Epoch: 76 [7168/50000]	Loss: 0.3489	LR: 0.025000
Training Epoch: 76 [7296/50000]	Loss: 0.3836	LR: 0.025000
Training Epoch: 76 [7424/50000]	Loss: 0.3163	LR: 0.025000
Training Epoch: 76 [7552/50000]	Loss: 0.4041	LR: 0.025000
Training Epoch: 76 [7680/50000]	Loss: 0.4454	LR: 0.025000
Training Epoch: 76 [7808/50000]	Loss: 0.4433	LR: 0.025000
Training Epoch: 76 [7936/50000]	Loss: 0.6242	LR: 0.025000
Training Epoch: 76 [8064/50000]	Loss: 0.4608	LR: 0.025000
Training Epoch: 76 [8192/50000]	Loss: 0.3163	LR: 0.025000
Training Epoch: 76 [8320/50000]	Loss: 0.3551	LR: 0.025000
Training Epoch: 76 [8448/50000]	Loss: 0.2997	LR: 0.025000
Training Epoch: 76 [8576/50000]	Loss: 0.4445	LR: 0.025000
Training Epoch: 76 [8704/50000]	Loss: 0.5710	LR: 0.025000
Training Epoch: 76 [8832/50000]	Loss: 0.2836	LR: 0.025000
Training Epoch: 76 [8960/50000]	Loss: 0.4178	LR: 0.025000
Training Epoch: 76 [9088/50000]	Loss: 0.3588	LR: 0.025000
Training Epoch: 76 [9216/50000]	Loss: 0.4437	LR: 0.025000
Training Epoch: 76 [9344/50000]	Loss: 0.4184	LR: 0.025000
Training Epoch: 76 [9472/50000]	Loss: 0.3350	LR: 0.025000
Training Epoch: 76 [9600/50000]	Loss: 0.4007	LR: 0.025000
Training Epoch: 76 [9728/50000]	Loss: 0.4422	LR: 0.025000
Training Epoch: 76 [9856/50000]	Loss: 0.3871	LR: 0.025000
Training Epoch: 76 [9984/50000]	Loss: 0.4523	LR: 0.025000
Training Epoch: 76 [10112/50000]	Loss: 0.3045	LR: 0.025000
Training Epoch: 76 [10240/50000]	Loss: 0.3979	LR: 0.025000
Training Epoch: 76 [10368/50000]	Loss: 0.5511	LR: 0.025000
Training Epoch: 76 [10496/50000]	Loss: 0.4419	LR: 0.025000
Training Epoch: 76 [10624/50000]	Loss: 0.4502	LR: 0.025000
Training Epoch: 76 [10752/50000]	Loss: 0.4802	LR: 0.025000
Training Epoch: 76 [10880/50000]	Loss: 0.4672	LR: 0.025000
Training Epoch: 76 [11008/50000]	Loss: 0.4017	LR: 0.025000
Training Epoch: 76 [11136/50000]	Loss: 0.2839	LR: 0.025000
Training Epoch: 76 [11264/50000]	Loss: 0.3741	LR: 0.025000
Training Epoch: 76 [11392/50000]	Loss: 0.3790	LR: 0.025000
Training Epoch: 76 [11520/50000]	Loss: 0.3676	LR: 0.025000
Training Epoch: 76 [11648/50000]	Loss: 0.4440	LR: 0.025000
Training Epoch: 76 [11776/50000]	Loss: 0.4072	LR: 0.025000
Training Epoch: 76 [11904/50000]	Loss: 0.4343	LR: 0.025000
Training Epoch: 76 [12032/50000]	Loss: 0.3958	LR: 0.025000
Training Epoch: 76 [12160/50000]	Loss: 0.5021	LR: 0.025000
Training Epoch: 76 [12288/50000]	Loss: 0.4371	LR: 0.025000
Training Epoch: 76 [12416/50000]	Loss: 0.2995	LR: 0.025000
Training Epoch: 76 [12544/50000]	Loss: 0.2351	LR: 0.025000
Training Epoch: 76 [12672/50000]	Loss: 0.3572	LR: 0.025000
Training Epoch: 76 [12800/50000]	Loss: 0.4004	LR: 0.025000
Training Epoch: 76 [12928/50000]	Loss: 0.2867	LR: 0.025000
Training Epoch: 76 [13056/50000]	Loss: 0.4536	LR: 0.025000
Training Epoch: 76 [13184/50000]	Loss: 0.3743	LR: 0.025000
Training Epoch: 76 [13312/50000]	Loss: 0.4704	LR: 0.025000
Training Epoch: 76 [13440/50000]	Loss: 0.4765	LR: 0.025000
Training Epoch: 76 [13568/50000]	Loss: 0.3369	LR: 0.025000
Training Epoch: 76 [13696/50000]	Loss: 0.4744	LR: 0.025000
Training Epoch: 76 [13824/50000]	Loss: 0.4409	LR: 0.025000
Training Epoch: 76 [13952/50000]	Loss: 0.3390	LR: 0.025000
Training Epoch: 76 [14080/50000]	Loss: 0.3662	LR: 0.025000
Training Epoch: 76 [14208/50000]	Loss: 0.3210	LR: 0.025000
Training Epoch: 76 [14336/50000]	Loss: 0.4051	LR: 0.025000
Training Epoch: 76 [14464/50000]	Loss: 0.3480	LR: 0.025000
Training Epoch: 76 [14592/50000]	Loss: 0.3228	LR: 0.025000
Training Epoch: 76 [14720/50000]	Loss: 0.4059	LR: 0.025000
Training Epoch: 76 [14848/50000]	Loss: 0.3405	LR: 0.025000
Training Epoch: 76 [14976/50000]	Loss: 0.4292	LR: 0.025000
Training Epoch: 76 [15104/50000]	Loss: 0.4640	LR: 0.025000
Training Epoch: 76 [15232/50000]	Loss: 0.3417	LR: 0.025000
Training Epoch: 76 [15360/50000]	Loss: 0.3688	LR: 0.025000
Training Epoch: 76 [15488/50000]	Loss: 0.4673	LR: 0.025000
Training Epoch: 76 [15616/50000]	Loss: 0.4847	LR: 0.025000
Training Epoch: 76 [15744/50000]	Loss: 0.5601	LR: 0.025000
Training Epoch: 76 [15872/50000]	Loss: 0.4117	LR: 0.025000
Training Epoch: 76 [16000/50000]	Loss: 0.3349	LR: 0.025000
Training Epoch: 76 [16128/50000]	Loss: 0.4946	LR: 0.025000
Training Epoch: 76 [16256/50000]	Loss: 0.4015	LR: 0.025000
Training Epoch: 76 [16384/50000]	Loss: 0.2616	LR: 0.025000
Training Epoch: 76 [16512/50000]	Loss: 0.3932	LR: 0.025000
Training Epoch: 76 [16640/50000]	Loss: 0.4578	LR: 0.025000
Training Epoch: 76 [16768/50000]	Loss: 0.5379	LR: 0.025000
Training Epoch: 76 [16896/50000]	Loss: 0.5307	LR: 0.025000
Training Epoch: 76 [17024/50000]	Loss: 0.4760	LR: 0.025000
Training Epoch: 76 [17152/50000]	Loss: 0.4168	LR: 0.025000
Training Epoch: 76 [17280/50000]	Loss: 0.4299	LR: 0.025000
Training Epoch: 76 [17408/50000]	Loss: 0.3749	LR: 0.025000
Training Epoch: 76 [17536/50000]	Loss: 0.4537	LR: 0.025000
Training Epoch: 76 [17664/50000]	Loss: 0.4657	LR: 0.025000
Training Epoch: 76 [17792/50000]	Loss: 0.4586	LR: 0.025000
Training Epoch: 76 [17920/50000]	Loss: 0.3960	LR: 0.025000
Training Epoch: 76 [18048/50000]	Loss: 0.3341	LR: 0.025000
Training Epoch: 76 [18176/50000]	Loss: 0.3891	LR: 0.025000
Training Epoch: 76 [18304/50000]	Loss: 0.4386	LR: 0.025000
Training Epoch: 76 [18432/50000]	Loss: 0.4489	LR: 0.025000
Training Epoch: 76 [18560/50000]	Loss: 0.4607	LR: 0.025000
Training Epoch: 76 [18688/50000]	Loss: 0.2766	LR: 0.025000
Training Epoch: 76 [18816/50000]	Loss: 0.5275	LR: 0.025000
Training Epoch: 76 [18944/50000]	Loss: 0.4169	LR: 0.025000
Training Epoch: 76 [19072/50000]	Loss: 0.3342	LR: 0.025000
Training Epoch: 76 [19200/50000]	Loss: 0.2551	LR: 0.025000
Training Epoch: 76 [19328/50000]	Loss: 0.5542	LR: 0.025000
Training Epoch: 76 [19456/50000]	Loss: 0.3673	LR: 0.025000
Training Epoch: 76 [19584/50000]	Loss: 0.4301	LR: 0.025000
Training Epoch: 76 [19712/50000]	Loss: 0.3986	LR: 0.025000
Training Epoch: 76 [19840/50000]	Loss: 0.5140	LR: 0.025000
Training Epoch: 76 [19968/50000]	Loss: 0.3539	LR: 0.025000
Training Epoch: 76 [20096/50000]	Loss: 0.4367	LR: 0.025000
Training Epoch: 76 [20224/50000]	Loss: 0.3578	LR: 0.025000
Training Epoch: 76 [20352/50000]	Loss: 0.4542	LR: 0.025000
Training Epoch: 76 [20480/50000]	Loss: 0.4187	LR: 0.025000
Training Epoch: 76 [20608/50000]	Loss: 0.5131	LR: 0.025000
Training Epoch: 76 [20736/50000]	Loss: 0.3477	LR: 0.025000
Training Epoch: 76 [20864/50000]	Loss: 0.5149	LR: 0.025000
Training Epoch: 76 [20992/50000]	Loss: 0.3969	LR: 0.025000
Training Epoch: 76 [21120/50000]	Loss: 0.3956	LR: 0.025000
Training Epoch: 76 [21248/50000]	Loss: 0.5127	LR: 0.025000
Training Epoch: 76 [21376/50000]	Loss: 0.2731	LR: 0.025000
Training Epoch: 76 [21504/50000]	Loss: 0.4633	LR: 0.025000
Training Epoch: 76 [21632/50000]	Loss: 0.3463	LR: 0.025000
Training Epoch: 76 [21760/50000]	Loss: 0.5507	LR: 0.025000
Training Epoch: 76 [21888/50000]	Loss: 0.3597	LR: 0.025000
Training Epoch: 76 [22016/50000]	Loss: 0.4873	LR: 0.025000
Training Epoch: 76 [22144/50000]	Loss: 0.3208	LR: 0.025000
Training Epoch: 76 [22272/50000]	Loss: 0.5274	LR: 0.025000
Training Epoch: 76 [22400/50000]	Loss: 0.3566	LR: 0.025000
Training Epoch: 76 [22528/50000]	Loss: 0.4617	LR: 0.025000
Training Epoch: 76 [22656/50000]	Loss: 0.5038	LR: 0.025000
Training Epoch: 76 [22784/50000]	Loss: 0.2623	LR: 0.025000
Training Epoch: 76 [22912/50000]	Loss: 0.3279	LR: 0.025000
Training Epoch: 76 [23040/50000]	Loss: 0.5402	LR: 0.025000
Training Epoch: 76 [23168/50000]	Loss: 0.4615	LR: 0.025000
Training Epoch: 76 [23296/50000]	Loss: 0.4461	LR: 0.025000
Training Epoch: 76 [23424/50000]	Loss: 0.3230	LR: 0.025000
Training Epoch: 76 [23552/50000]	Loss: 0.5222	LR: 0.025000
Training Epoch: 76 [23680/50000]	Loss: 0.5809	LR: 0.025000
Training Epoch: 76 [23808/50000]	Loss: 0.4415	LR: 0.025000
Training Epoch: 76 [23936/50000]	Loss: 0.3829	LR: 0.025000
Training Epoch: 76 [24064/50000]	Loss: 0.4292	LR: 0.025000
Training Epoch: 76 [24192/50000]	Loss: 0.5555	LR: 0.025000
Training Epoch: 76 [24320/50000]	Loss: 0.3767	LR: 0.025000
Training Epoch: 76 [24448/50000]	Loss: 0.5229	LR: 0.025000
Training Epoch: 76 [24576/50000]	Loss: 0.2860	LR: 0.025000
Training Epoch: 76 [24704/50000]	Loss: 0.4829	LR: 0.025000
Training Epoch: 76 [24832/50000]	Loss: 0.4686	LR: 0.025000
Training Epoch: 76 [24960/50000]	Loss: 0.4717	LR: 0.025000
Training Epoch: 76 [25088/50000]	Loss: 0.5350	LR: 0.025000
Training Epoch: 76 [25216/50000]	Loss: 0.5640	LR: 0.025000
Training Epoch: 76 [25344/50000]	Loss: 0.5139	LR: 0.025000
Training Epoch: 76 [25472/50000]	Loss: 0.4550	LR: 0.025000
Training Epoch: 76 [25600/50000]	Loss: 0.5041	LR: 0.025000
Training Epoch: 76 [25728/50000]	Loss: 0.4554	LR: 0.025000
Training Epoch: 76 [25856/50000]	Loss: 0.4574	LR: 0.025000
Training Epoch: 76 [25984/50000]	Loss: 0.4605	LR: 0.025000
Training Epoch: 76 [26112/50000]	Loss: 0.4888	LR: 0.025000
Training Epoch: 76 [26240/50000]	Loss: 0.5142	LR: 0.025000
Training Epoch: 76 [26368/50000]	Loss: 0.4676	LR: 0.025000
Training Epoch: 76 [26496/50000]	Loss: 0.3515	LR: 0.025000
Training Epoch: 76 [26624/50000]	Loss: 0.4485	LR: 0.025000
Training Epoch: 76 [26752/50000]	Loss: 0.4140	LR: 0.025000
Training Epoch: 76 [26880/50000]	Loss: 0.3484	LR: 0.025000
Training Epoch: 76 [27008/50000]	Loss: 0.3981	LR: 0.025000
Training Epoch: 76 [27136/50000]	Loss: 0.4484	LR: 0.025000
Training Epoch: 76 [27264/50000]	Loss: 0.4657	LR: 0.025000
Training Epoch: 76 [27392/50000]	Loss: 0.3301	LR: 0.025000
Training Epoch: 76 [27520/50000]	Loss: 0.4403	LR: 0.025000
Training Epoch: 76 [27648/50000]	Loss: 0.4897	LR: 0.025000
Training Epoch: 76 [27776/50000]	Loss: 0.4641	LR: 0.025000
Training Epoch: 76 [27904/50000]	Loss: 0.3610	LR: 0.025000
Training Epoch: 76 [28032/50000]	Loss: 0.5232	LR: 0.025000
Training Epoch: 76 [28160/50000]	Loss: 0.3771	LR: 0.025000
Training Epoch: 76 [28288/50000]	Loss: 0.4542	LR: 0.025000
Training Epoch: 76 [28416/50000]	Loss: 0.3146	LR: 0.025000
Training Epoch: 76 [28544/50000]	Loss: 0.5843	LR: 0.025000
Training Epoch: 76 [28672/50000]	Loss: 0.5097	LR: 0.025000
Training Epoch: 76 [28800/50000]	Loss: 0.3925	LR: 0.025000
Training Epoch: 76 [28928/50000]	Loss: 0.4549	LR: 0.025000
Training Epoch: 76 [29056/50000]	Loss: 0.3332	LR: 0.025000
Training Epoch: 76 [29184/50000]	Loss: 0.4425	LR: 0.025000
Training Epoch: 76 [29312/50000]	Loss: 0.6112	LR: 0.025000
Training Epoch: 76 [29440/50000]	Loss: 0.4184	LR: 0.025000
Training Epoch: 76 [29568/50000]	Loss: 0.5774	LR: 0.025000
Training Epoch: 76 [29696/50000]	Loss: 0.3564	LR: 0.025000
Training Epoch: 76 [29824/50000]	Loss: 0.4524	LR: 0.025000
Training Epoch: 76 [29952/50000]	Loss: 0.3241	LR: 0.025000
Training Epoch: 76 [30080/50000]	Loss: 0.4183	LR: 0.025000
Training Epoch: 76 [30208/50000]	Loss: 0.3747	LR: 0.025000
Training Epoch: 76 [30336/50000]	Loss: 0.4533	LR: 0.025000
Training Epoch: 76 [30464/50000]	Loss: 0.5350	LR: 0.025000
Training Epoch: 76 [30592/50000]	Loss: 0.5011	LR: 0.025000
Training Epoch: 76 [30720/50000]	Loss: 0.4916	LR: 0.025000
Training Epoch: 76 [30848/50000]	Loss: 0.4697	LR: 0.025000
Training Epoch: 76 [30976/50000]	Loss: 0.2934	LR: 0.025000
Training Epoch: 76 [31104/50000]	Loss: 0.5119	LR: 0.025000
Training Epoch: 76 [31232/50000]	Loss: 0.3151	LR: 0.025000
Training Epoch: 76 [31360/50000]	Loss: 0.3246	LR: 0.025000
Training Epoch: 76 [31488/50000]	Loss: 0.3286	LR: 0.025000
Training Epoch: 76 [31616/50000]	Loss: 0.4111	LR: 0.025000
Training Epoch: 76 [31744/50000]	Loss: 0.4655	LR: 0.025000
Training Epoch: 76 [31872/50000]	Loss: 0.4252	LR: 0.025000
Training Epoch: 76 [32000/50000]	Loss: 0.5223	LR: 0.025000
Training Epoch: 76 [32128/50000]	Loss: 0.3465	LR: 0.025000
Training Epoch: 76 [32256/50000]	Loss: 0.7102	LR: 0.025000
Training Epoch: 76 [32384/50000]	Loss: 0.4665	LR: 0.025000
Training Epoch: 76 [32512/50000]	Loss: 0.3725	LR: 0.025000
Training Epoch: 76 [32640/50000]	Loss: 0.3554	LR: 0.025000
Training Epoch: 76 [32768/50000]	Loss: 0.5358	LR: 0.025000
Training Epoch: 76 [32896/50000]	Loss: 0.4565	LR: 0.025000
Training Epoch: 76 [33024/50000]	Loss: 0.5355	LR: 0.025000
Training Epoch: 76 [33152/50000]	Loss: 0.4338	LR: 0.025000
Training Epoch: 76 [33280/50000]	Loss: 0.4117	LR: 0.025000
Training Epoch: 76 [33408/50000]	Loss: 0.3399	LR: 0.025000
Training Epoch: 76 [33536/50000]	Loss: 0.4029	LR: 0.025000
Training Epoch: 76 [33664/50000]	Loss: 0.5216	LR: 0.025000
Training Epoch: 76 [33792/50000]	Loss: 0.5439	LR: 0.025000
Training Epoch: 76 [33920/50000]	Loss: 0.4880	LR: 0.025000
Training Epoch: 76 [34048/50000]	Loss: 0.6455	LR: 0.025000
Training Epoch: 76 [34176/50000]	Loss: 0.4953	LR: 0.025000
Training Epoch: 76 [34304/50000]	Loss: 0.4643	LR: 0.025000
Training Epoch: 76 [34432/50000]	Loss: 0.4059	LR: 0.025000
Training Epoch: 76 [34560/50000]	Loss: 0.4483	LR: 0.025000
Training Epoch: 76 [34688/50000]	Loss: 0.4176	LR: 0.025000
Training Epoch: 76 [34816/50000]	Loss: 0.3505	LR: 0.025000
Training Epoch: 76 [34944/50000]	Loss: 0.4651	LR: 0.025000
Training Epoch: 76 [35072/50000]	Loss: 0.5160	LR: 0.025000
Training Epoch: 76 [35200/50000]	Loss: 0.6126	LR: 0.025000
Training Epoch: 76 [35328/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 76 [35456/50000]	Loss: 0.4547	LR: 0.025000
Training Epoch: 76 [35584/50000]	Loss: 0.4654	LR: 0.025000
Training Epoch: 76 [35712/50000]	Loss: 0.4116	LR: 0.025000
Training Epoch: 76 [35840/50000]	Loss: 0.5207	LR: 0.025000
Training Epoch: 76 [35968/50000]	Loss: 0.5371	LR: 0.025000
Training Epoch: 76 [36096/50000]	Loss: 0.3969	LR: 0.025000
Training Epoch: 76 [36224/50000]	Loss: 0.4939	LR: 0.025000
Training Epoch: 76 [36352/50000]	Loss: 0.3952	LR: 0.025000
Training Epoch: 76 [36480/50000]	Loss: 0.4888	LR: 0.025000
Training Epoch: 76 [36608/50000]	Loss: 0.4868	LR: 0.025000
Training Epoch: 76 [36736/50000]	Loss: 0.5053	LR: 0.025000
Training Epoch: 76 [36864/50000]	Loss: 0.3617	LR: 0.025000
Training Epoch: 76 [36992/50000]	Loss: 0.3411	LR: 0.025000
Training Epoch: 76 [37120/50000]	Loss: 0.4523	LR: 0.025000
Training Epoch: 76 [37248/50000]	Loss: 0.4546	LR: 0.025000
Training Epoch: 76 [37376/50000]	Loss: 0.3915	LR: 0.025000
Training Epoch: 76 [37504/50000]	Loss: 0.5156	LR: 0.025000
Training Epoch: 76 [37632/50000]	Loss: 0.6051	LR: 0.025000
Training Epoch: 76 [37760/50000]	Loss: 0.5961	LR: 0.025000
Training Epoch: 76 [37888/50000]	Loss: 0.6071	LR: 0.025000
Training Epoch: 76 [38016/50000]	Loss: 0.4154	LR: 0.025000
Training Epoch: 76 [38144/50000]	Loss: 0.4048	LR: 0.025000
Training Epoch: 76 [38272/50000]	Loss: 0.3660	LR: 0.025000
Training Epoch: 76 [38400/50000]	Loss: 0.4792	LR: 0.025000
Training Epoch: 76 [38528/50000]	Loss: 0.4877	LR: 0.025000
Training Epoch: 76 [38656/50000]	Loss: 0.5052	LR: 0.025000
Training Epoch: 76 [38784/50000]	Loss: 0.4772	LR: 0.025000
Training Epoch: 76 [38912/50000]	Loss: 0.5465	LR: 0.025000
Training Epoch: 76 [39040/50000]	Loss: 0.2369	LR: 0.025000
Training Epoch: 76 [39168/50000]	Loss: 0.4242	LR: 0.025000
Training Epoch: 76 [39296/50000]	Loss: 0.3098	LR: 0.025000
Training Epoch: 76 [39424/50000]	Loss: 0.3100	LR: 0.025000
Training Epoch: 76 [39552/50000]	Loss: 0.4956	LR: 0.025000
Training Epoch: 76 [39680/50000]	Loss: 0.5383	LR: 0.025000
Training Epoch: 76 [39808/50000]	Loss: 0.3594	LR: 0.025000
Training Epoch: 76 [39936/50000]	Loss: 0.5109	LR: 0.025000
Training Epoch: 76 [40064/50000]	Loss: 0.3949	LR: 0.025000
Training Epoch: 76 [40192/50000]	Loss: 0.6456	LR: 0.025000
Training Epoch: 76 [40320/50000]	Loss: 0.5677	LR: 0.025000
Training Epoch: 76 [40448/50000]	Loss: 0.4095	LR: 0.025000
Training Epoch: 76 [40576/50000]	Loss: 0.5266	LR: 0.025000
Training Epoch: 76 [40704/50000]	Loss: 0.4842	LR: 0.025000
Training Epoch: 76 [40832/50000]	Loss: 0.4811	LR: 0.025000
Training Epoch: 76 [40960/50000]	Loss: 0.4768	LR: 0.025000
Training Epoch: 76 [41088/50000]	Loss: 0.4430	LR: 0.025000
Training Epoch: 76 [41216/50000]	Loss: 0.3521	LR: 0.025000
Training Epoch: 76 [41344/50000]	Loss: 0.5655	LR: 0.025000
Training Epoch: 76 [41472/50000]	Loss: 0.3572	LR: 0.025000
Training Epoch: 76 [41600/50000]	Loss: 0.4227	LR: 0.025000
Training Epoch: 76 [41728/50000]	Loss: 0.5121	LR: 0.025000
Training Epoch: 76 [41856/50000]	Loss: 0.5050	LR: 0.025000
Training Epoch: 76 [41984/50000]	Loss: 0.4021	LR: 0.025000
Training Epoch: 76 [42112/50000]	Loss: 0.3992	LR: 0.025000
Training Epoch: 76 [42240/50000]	Loss: 0.4944	LR: 0.025000
Training Epoch: 76 [42368/50000]	Loss: 0.4656	LR: 0.025000
Training Epoch: 76 [42496/50000]	Loss: 0.5160	LR: 0.025000
Training Epoch: 76 [42624/50000]	Loss: 0.4848	LR: 0.025000
Training Epoch: 76 [42752/50000]	Loss: 0.5468	LR: 0.025000
Training Epoch: 76 [42880/50000]	Loss: 0.5004	LR: 0.025000
Training Epoch: 76 [43008/50000]	Loss: 0.5160	LR: 0.025000
Training Epoch: 76 [43136/50000]	Loss: 0.4475	LR: 0.025000
Training Epoch: 76 [43264/50000]	Loss: 0.5793	LR: 0.025000
Training Epoch: 76 [43392/50000]	Loss: 0.4173	LR: 0.025000
Training Epoch: 76 [43520/50000]	Loss: 0.4142	LR: 0.025000
Training Epoch: 76 [43648/50000]	Loss: 0.4549	LR: 0.025000
Training Epoch: 76 [43776/50000]	Loss: 0.6101	LR: 0.025000
Training Epoch: 76 [43904/50000]	Loss: 0.5034	LR: 0.025000
Training Epoch: 76 [44032/50000]	Loss: 0.4995	LR: 0.025000
Training Epoch: 76 [44160/50000]	Loss: 0.4958	LR: 0.025000
Training Epoch: 76 [44288/50000]	Loss: 0.5489	LR: 0.025000
Training Epoch: 76 [44416/50000]	Loss: 0.4030	LR: 0.025000
Training Epoch: 76 [44544/50000]	Loss: 0.4568	LR: 0.025000
Training Epoch: 76 [44672/50000]	Loss: 0.4904	LR: 0.025000
Training Epoch: 76 [44800/50000]	Loss: 0.4203	LR: 0.025000
Training Epoch: 76 [44928/50000]	Loss: 0.4381	LR: 0.025000
Training Epoch: 76 [45056/50000]	Loss: 0.5644	LR: 0.025000
Training Epoch: 76 [45184/50000]	Loss: 0.4021	LR: 0.025000
Training Epoch: 76 [45312/50000]	Loss: 0.4260	LR: 0.025000
Training Epoch: 76 [45440/50000]	Loss: 0.4558	LR: 0.025000
Training Epoch: 76 [45568/50000]	Loss: 0.4462	LR: 0.025000
Training Epoch: 76 [45696/50000]	Loss: 0.5866	LR: 0.025000
Training Epoch: 76 [45824/50000]	Loss: 0.5105	LR: 0.025000
Training Epoch: 76 [45952/50000]	Loss: 0.5028	LR: 0.025000
Training Epoch: 76 [46080/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 76 [46208/50000]	Loss: 0.3584	LR: 0.025000
Training Epoch: 76 [46336/50000]	Loss: 0.3532	LR: 0.025000
Training Epoch: 76 [46464/50000]	Loss: 0.5829	LR: 0.025000
Training Epoch: 76 [46592/50000]	Loss: 0.4393	LR: 0.025000
Training Epoch: 76 [46720/50000]	Loss: 0.5432	LR: 0.025000
Training Epoch: 76 [46848/50000]	Loss: 0.5261	LR: 0.025000
Training Epoch: 76 [46976/50000]	Loss: 0.5608	LR: 0.025000
Training Epoch: 76 [47104/50000]	Loss: 0.7814	LR: 0.025000
Training Epoch: 76 [47232/50000]	Loss: 0.4409	LR: 0.025000
Training Epoch: 76 [47360/50000]	Loss: 0.5769	LR: 0.025000
Training Epoch: 76 [47488/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 76 [47616/50000]	Loss: 0.5176	LR: 0.025000
Training Epoch: 76 [47744/50000]	Loss: 0.4939	LR: 0.025000
Training Epoch: 76 [47872/50000]	Loss: 0.5819	LR: 0.025000
Training Epoch: 76 [48000/50000]	Loss: 0.5128	LR: 0.025000
Training Epoch: 76 [48128/50000]	Loss: 0.4680	LR: 0.025000
Training Epoch: 76 [48256/50000]	Loss: 0.5289	LR: 0.025000
Training Epoch: 76 [48384/50000]	Loss: 0.4853	LR: 0.025000
Training Epoch: 76 [48512/50000]	Loss: 0.5482	LR: 0.025000
Training Epoch: 76 [48640/50000]	Loss: 0.6007	LR: 0.025000
Training Epoch: 76 [48768/50000]	Loss: 0.4509	LR: 0.025000
Training Epoch: 76 [48896/50000]	Loss: 0.6311	LR: 0.025000
Training Epoch: 76 [49024/50000]	Loss: 0.4342	LR: 0.025000
Training Epoch: 76 [49152/50000]	Loss: 0.5213	LR: 0.025000
Training Epoch: 76 [49280/50000]	Loss: 0.4207	LR: 0.025000
Training Epoch: 76 [49408/50000]	Loss: 0.4098	LR: 0.025000
Training Epoch: 76 [49536/50000]	Loss: 0.6022	LR: 0.025000
Training Epoch: 76 [49664/50000]	Loss: 0.5396	LR: 0.025000
Training Epoch: 76 [49792/50000]	Loss: 0.7091	LR: 0.025000
Training Epoch: 76 [49920/50000]	Loss: 0.6538	LR: 0.025000
Training Epoch: 76 [50000/50000]	Loss: 0.4194	LR: 0.025000
epoch 76 training time consumed: 53.93s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  278924 GB |  278924 GB |
|       from large pool |  123392 KB |    1034 MB |  278649 GB |  278649 GB |
|       from small pool |   10798 KB |      13 MB |     274 GB |     274 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  278924 GB |  278924 GB |
|       from large pool |  123392 KB |    1034 MB |  278649 GB |  278649 GB |
|       from small pool |   10798 KB |      13 MB |     274 GB |     274 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  122743 GB |  122743 GB |
|       from large pool |  155136 KB |  433088 KB |  122439 GB |  122439 GB |
|       from small pool |    1490 KB |    3494 KB |     303 GB |     303 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   10762 K  |   10762 K  |
|       from large pool |      24    |      65    |    5617 K  |    5617 K  |
|       from small pool |     231    |     274    |    5144 K  |    5144 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   10762 K  |   10762 K  |
|       from large pool |      24    |      65    |    5617 K  |    5617 K  |
|       from small pool |     231    |     274    |    5144 K  |    5144 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5317 K  |    5317 K  |
|       from large pool |       9    |      14    |    2719 K  |    2719 K  |
|       from small pool |      12    |      16    |    2598 K  |    2598 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 76, Average loss: 0.0116, Accuracy: 0.6535, Time consumed:3.45s

Training Epoch: 77 [128/50000]	Loss: 0.2469	LR: 0.025000
Training Epoch: 77 [256/50000]	Loss: 0.4396	LR: 0.025000
Training Epoch: 77 [384/50000]	Loss: 0.4531	LR: 0.025000
Training Epoch: 77 [512/50000]	Loss: 0.4806	LR: 0.025000
Training Epoch: 77 [640/50000]	Loss: 0.4161	LR: 0.025000
Training Epoch: 77 [768/50000]	Loss: 0.3753	LR: 0.025000
Training Epoch: 77 [896/50000]	Loss: 0.3825	LR: 0.025000
Training Epoch: 77 [1024/50000]	Loss: 0.4515	LR: 0.025000
Training Epoch: 77 [1152/50000]	Loss: 0.2665	LR: 0.025000
Training Epoch: 77 [1280/50000]	Loss: 0.4368	LR: 0.025000
Training Epoch: 77 [1408/50000]	Loss: 0.3852	LR: 0.025000
Training Epoch: 77 [1536/50000]	Loss: 0.6168	LR: 0.025000
Training Epoch: 77 [1664/50000]	Loss: 0.4017	LR: 0.025000
Training Epoch: 77 [1792/50000]	Loss: 0.3725	LR: 0.025000
Training Epoch: 77 [1920/50000]	Loss: 0.4365	LR: 0.025000
Training Epoch: 77 [2048/50000]	Loss: 0.3756	LR: 0.025000
Training Epoch: 77 [2176/50000]	Loss: 0.4095	LR: 0.025000
Training Epoch: 77 [2304/50000]	Loss: 0.4915	LR: 0.025000
Training Epoch: 77 [2432/50000]	Loss: 0.4863	LR: 0.025000
Training Epoch: 77 [2560/50000]	Loss: 0.3877	LR: 0.025000
Training Epoch: 77 [2688/50000]	Loss: 0.3737	LR: 0.025000
Training Epoch: 77 [2816/50000]	Loss: 0.4398	LR: 0.025000
Training Epoch: 77 [2944/50000]	Loss: 0.5147	LR: 0.025000
Training Epoch: 77 [3072/50000]	Loss: 0.4158	LR: 0.025000
Training Epoch: 77 [3200/50000]	Loss: 0.3819	LR: 0.025000
Training Epoch: 77 [3328/50000]	Loss: 0.2648	LR: 0.025000
Training Epoch: 77 [3456/50000]	Loss: 0.3245	LR: 0.025000
Training Epoch: 77 [3584/50000]	Loss: 0.6350	LR: 0.025000
Training Epoch: 77 [3712/50000]	Loss: 0.4146	LR: 0.025000
Training Epoch: 77 [3840/50000]	Loss: 0.3534	LR: 0.025000
Training Epoch: 77 [3968/50000]	Loss: 0.3790	LR: 0.025000
Training Epoch: 77 [4096/50000]	Loss: 0.2937	LR: 0.025000
Training Epoch: 77 [4224/50000]	Loss: 0.5593	LR: 0.025000
Training Epoch: 77 [4352/50000]	Loss: 0.4243	LR: 0.025000
Training Epoch: 77 [4480/50000]	Loss: 0.3543	LR: 0.025000
Training Epoch: 77 [4608/50000]	Loss: 0.5477	LR: 0.025000
Training Epoch: 77 [4736/50000]	Loss: 0.4185	LR: 0.025000
Training Epoch: 77 [4864/50000]	Loss: 0.4929	LR: 0.025000
Training Epoch: 77 [4992/50000]	Loss: 0.4677	LR: 0.025000
Training Epoch: 77 [5120/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 77 [5248/50000]	Loss: 0.4708	LR: 0.025000
Training Epoch: 77 [5376/50000]	Loss: 0.6552	LR: 0.025000
Training Epoch: 77 [5504/50000]	Loss: 0.3468	LR: 0.025000
Training Epoch: 77 [5632/50000]	Loss: 0.4447	LR: 0.025000
Training Epoch: 77 [5760/50000]	Loss: 0.4472	LR: 0.025000
Training Epoch: 77 [5888/50000]	Loss: 0.4366	LR: 0.025000
Training Epoch: 77 [6016/50000]	Loss: 0.3629	LR: 0.025000
Training Epoch: 77 [6144/50000]	Loss: 0.4692	LR: 0.025000
Training Epoch: 77 [6272/50000]	Loss: 0.3828	LR: 0.025000
Training Epoch: 77 [6400/50000]	Loss: 0.3328	LR: 0.025000
Training Epoch: 77 [6528/50000]	Loss: 0.3012	LR: 0.025000
Training Epoch: 77 [6656/50000]	Loss: 0.4376	LR: 0.025000
Training Epoch: 77 [6784/50000]	Loss: 0.4841	LR: 0.025000
Training Epoch: 77 [6912/50000]	Loss: 0.3766	LR: 0.025000
Training Epoch: 77 [7040/50000]	Loss: 0.3087	LR: 0.025000
Training Epoch: 77 [7168/50000]	Loss: 0.5411	LR: 0.025000
Training Epoch: 77 [7296/50000]	Loss: 0.3989	LR: 0.025000
Training Epoch: 77 [7424/50000]	Loss: 0.4905	LR: 0.025000
Training Epoch: 77 [7552/50000]	Loss: 0.4053	LR: 0.025000
Training Epoch: 77 [7680/50000]	Loss: 0.3246	LR: 0.025000
Training Epoch: 77 [7808/50000]	Loss: 0.3569	LR: 0.025000
Training Epoch: 77 [7936/50000]	Loss: 0.3209	LR: 0.025000
Training Epoch: 77 [8064/50000]	Loss: 0.4375	LR: 0.025000
Training Epoch: 77 [8192/50000]	Loss: 0.3068	LR: 0.025000
Training Epoch: 77 [8320/50000]	Loss: 0.3037	LR: 0.025000
Training Epoch: 77 [8448/50000]	Loss: 0.4413	LR: 0.025000
Training Epoch: 77 [8576/50000]	Loss: 0.2748	LR: 0.025000
Training Epoch: 77 [8704/50000]	Loss: 0.5287	LR: 0.025000
Training Epoch: 77 [8832/50000]	Loss: 0.4100	LR: 0.025000
Training Epoch: 77 [8960/50000]	Loss: 0.4024	LR: 0.025000
Training Epoch: 77 [9088/50000]	Loss: 0.2694	LR: 0.025000
Training Epoch: 77 [9216/50000]	Loss: 0.4627	LR: 0.025000
Training Epoch: 77 [9344/50000]	Loss: 0.4132	LR: 0.025000
Training Epoch: 77 [9472/50000]	Loss: 0.4092	LR: 0.025000
Training Epoch: 77 [9600/50000]	Loss: 0.2838	LR: 0.025000
Training Epoch: 77 [9728/50000]	Loss: 0.3870	LR: 0.025000
Training Epoch: 77 [9856/50000]	Loss: 0.4096	LR: 0.025000
Training Epoch: 77 [9984/50000]	Loss: 0.3990	LR: 0.025000
Training Epoch: 77 [10112/50000]	Loss: 0.3503	LR: 0.025000
Training Epoch: 77 [10240/50000]	Loss: 0.4131	LR: 0.025000
Training Epoch: 77 [10368/50000]	Loss: 0.4438	LR: 0.025000
Training Epoch: 77 [10496/50000]	Loss: 0.4620	LR: 0.025000
Training Epoch: 77 [10624/50000]	Loss: 0.2728	LR: 0.025000
Training Epoch: 77 [10752/50000]	Loss: 0.4203	LR: 0.025000
Training Epoch: 77 [10880/50000]	Loss: 0.4027	LR: 0.025000
Training Epoch: 77 [11008/50000]	Loss: 0.2736	LR: 0.025000
Training Epoch: 77 [11136/50000]	Loss: 0.4991	LR: 0.025000
Training Epoch: 77 [11264/50000]	Loss: 0.4454	LR: 0.025000
Training Epoch: 77 [11392/50000]	Loss: 0.3849	LR: 0.025000
Training Epoch: 77 [11520/50000]	Loss: 0.3859	LR: 0.025000
Training Epoch: 77 [11648/50000]	Loss: 0.4370	LR: 0.025000
Training Epoch: 77 [11776/50000]	Loss: 0.3958	LR: 0.025000
Training Epoch: 77 [11904/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 77 [12032/50000]	Loss: 0.4142	LR: 0.025000
Training Epoch: 77 [12160/50000]	Loss: 0.3176	LR: 0.025000
Training Epoch: 77 [12288/50000]	Loss: 0.4071	LR: 0.025000
Training Epoch: 77 [12416/50000]	Loss: 0.4265	LR: 0.025000
Training Epoch: 77 [12544/50000]	Loss: 0.3094	LR: 0.025000
Training Epoch: 77 [12672/50000]	Loss: 0.4538	LR: 0.025000
Training Epoch: 77 [12800/50000]	Loss: 0.3374	LR: 0.025000
Training Epoch: 77 [12928/50000]	Loss: 0.4250	LR: 0.025000
Training Epoch: 77 [13056/50000]	Loss: 0.4791	LR: 0.025000
Training Epoch: 77 [13184/50000]	Loss: 0.3730	LR: 0.025000
Training Epoch: 77 [13312/50000]	Loss: 0.5105	LR: 0.025000
Training Epoch: 77 [13440/50000]	Loss: 0.5019	LR: 0.025000
Training Epoch: 77 [13568/50000]	Loss: 0.3172	LR: 0.025000
Training Epoch: 77 [13696/50000]	Loss: 0.4625	LR: 0.025000
Training Epoch: 77 [13824/50000]	Loss: 0.4503	LR: 0.025000
Training Epoch: 77 [13952/50000]	Loss: 0.3685	LR: 0.025000
Training Epoch: 77 [14080/50000]	Loss: 0.4278	LR: 0.025000
Training Epoch: 77 [14208/50000]	Loss: 0.3489	LR: 0.025000
Training Epoch: 77 [14336/50000]	Loss: 0.2267	LR: 0.025000
Training Epoch: 77 [14464/50000]	Loss: 0.3767	LR: 0.025000
Training Epoch: 77 [14592/50000]	Loss: 0.3501	LR: 0.025000
Training Epoch: 77 [14720/50000]	Loss: 0.5673	LR: 0.025000
Training Epoch: 77 [14848/50000]	Loss: 0.4671	LR: 0.025000
Training Epoch: 77 [14976/50000]	Loss: 0.4114	LR: 0.025000
Training Epoch: 77 [15104/50000]	Loss: 0.3754	LR: 0.025000
Training Epoch: 77 [15232/50000]	Loss: 0.4306	LR: 0.025000
Training Epoch: 77 [15360/50000]	Loss: 0.4067	LR: 0.025000
Training Epoch: 77 [15488/50000]	Loss: 0.4795	LR: 0.025000
Training Epoch: 77 [15616/50000]	Loss: 0.3791	LR: 0.025000
Training Epoch: 77 [15744/50000]	Loss: 0.3089	LR: 0.025000
Training Epoch: 77 [15872/50000]	Loss: 0.4614	LR: 0.025000
Training Epoch: 77 [16000/50000]	Loss: 0.4352	LR: 0.025000
Training Epoch: 77 [16128/50000]	Loss: 0.4910	LR: 0.025000
Training Epoch: 77 [16256/50000]	Loss: 0.3534	LR: 0.025000
Training Epoch: 77 [16384/50000]	Loss: 0.4301	LR: 0.025000
Training Epoch: 77 [16512/50000]	Loss: 0.2997	LR: 0.025000
Training Epoch: 77 [16640/50000]	Loss: 0.4562	LR: 0.025000
Training Epoch: 77 [16768/50000]	Loss: 0.4794	LR: 0.025000
Training Epoch: 77 [16896/50000]	Loss: 0.3297	LR: 0.025000
Training Epoch: 77 [17024/50000]	Loss: 0.4483	LR: 0.025000
Training Epoch: 77 [17152/50000]	Loss: 0.4012	LR: 0.025000
Training Epoch: 77 [17280/50000]	Loss: 0.3953	LR: 0.025000
Training Epoch: 77 [17408/50000]	Loss: 0.4338	LR: 0.025000
Training Epoch: 77 [17536/50000]	Loss: 0.3610	LR: 0.025000
Training Epoch: 77 [17664/50000]	Loss: 0.3412	LR: 0.025000
Training Epoch: 77 [17792/50000]	Loss: 0.4194	LR: 0.025000
Training Epoch: 77 [17920/50000]	Loss: 0.3699	LR: 0.025000
Training Epoch: 77 [18048/50000]	Loss: 0.5031	LR: 0.025000
Training Epoch: 77 [18176/50000]	Loss: 0.3387	LR: 0.025000
Training Epoch: 77 [18304/50000]	Loss: 0.4446	LR: 0.025000
Training Epoch: 77 [18432/50000]	Loss: 0.4527	LR: 0.025000
Training Epoch: 77 [18560/50000]	Loss: 0.3602	LR: 0.025000
Training Epoch: 77 [18688/50000]	Loss: 0.4689	LR: 0.025000
Training Epoch: 77 [18816/50000]	Loss: 0.5414	LR: 0.025000
Training Epoch: 77 [18944/50000]	Loss: 0.3761	LR: 0.025000
Training Epoch: 77 [19072/50000]	Loss: 0.5180	LR: 0.025000
Training Epoch: 77 [19200/50000]	Loss: 0.4915	LR: 0.025000
Training Epoch: 77 [19328/50000]	Loss: 0.4450	LR: 0.025000
Training Epoch: 77 [19456/50000]	Loss: 0.4651	LR: 0.025000
Training Epoch: 77 [19584/50000]	Loss: 0.4574	LR: 0.025000
Training Epoch: 77 [19712/50000]	Loss: 0.3851	LR: 0.025000
Training Epoch: 77 [19840/50000]	Loss: 0.5629	LR: 0.025000
Training Epoch: 77 [19968/50000]	Loss: 0.2318	LR: 0.025000
Training Epoch: 77 [20096/50000]	Loss: 0.4825	LR: 0.025000
Training Epoch: 77 [20224/50000]	Loss: 0.4098	LR: 0.025000
Training Epoch: 77 [20352/50000]	Loss: 0.3883	LR: 0.025000
Training Epoch: 77 [20480/50000]	Loss: 0.4453	LR: 0.025000
Training Epoch: 77 [20608/50000]	Loss: 0.3396	LR: 0.025000
Training Epoch: 77 [20736/50000]	Loss: 0.4704	LR: 0.025000
Training Epoch: 77 [20864/50000]	Loss: 0.5086	LR: 0.025000
Training Epoch: 77 [20992/50000]	Loss: 0.3777	LR: 0.025000
Training Epoch: 77 [21120/50000]	Loss: 0.4168	LR: 0.025000
Training Epoch: 77 [21248/50000]	Loss: 0.4308	LR: 0.025000
Training Epoch: 77 [21376/50000]	Loss: 0.4346	LR: 0.025000
Training Epoch: 77 [21504/50000]	Loss: 0.3254	LR: 0.025000
Training Epoch: 77 [21632/50000]	Loss: 0.4725	LR: 0.025000
Training Epoch: 77 [21760/50000]	Loss: 0.2903	LR: 0.025000
Training Epoch: 77 [21888/50000]	Loss: 0.3681	LR: 0.025000
Training Epoch: 77 [22016/50000]	Loss: 0.4482	LR: 0.025000
Training Epoch: 77 [22144/50000]	Loss: 0.4314	LR: 0.025000
Training Epoch: 77 [22272/50000]	Loss: 0.4338	LR: 0.025000
Training Epoch: 77 [22400/50000]	Loss: 0.3254	LR: 0.025000
Training Epoch: 77 [22528/50000]	Loss: 0.3713	LR: 0.025000
Training Epoch: 77 [22656/50000]	Loss: 0.3788	LR: 0.025000
Training Epoch: 77 [22784/50000]	Loss: 0.4506	LR: 0.025000
Training Epoch: 77 [22912/50000]	Loss: 0.4554	LR: 0.025000
Training Epoch: 77 [23040/50000]	Loss: 0.4611	LR: 0.025000
Training Epoch: 77 [23168/50000]	Loss: 0.5638	LR: 0.025000
Training Epoch: 77 [23296/50000]	Loss: 0.3452	LR: 0.025000
Training Epoch: 77 [23424/50000]	Loss: 0.3435	LR: 0.025000
Training Epoch: 77 [23552/50000]	Loss: 0.3386	LR: 0.025000
Training Epoch: 77 [23680/50000]	Loss: 0.2676	LR: 0.025000
Training Epoch: 77 [23808/50000]	Loss: 0.5015	LR: 0.025000
Training Epoch: 77 [23936/50000]	Loss: 0.4583	LR: 0.025000
Training Epoch: 77 [24064/50000]	Loss: 0.5848	LR: 0.025000
Training Epoch: 77 [24192/50000]	Loss: 0.3304	LR: 0.025000
Training Epoch: 77 [24320/50000]	Loss: 0.4662	LR: 0.025000
Training Epoch: 77 [24448/50000]	Loss: 0.4170	LR: 0.025000
Training Epoch: 77 [24576/50000]	Loss: 0.4107	LR: 0.025000
Training Epoch: 77 [24704/50000]	Loss: 0.3894	LR: 0.025000
Training Epoch: 77 [24832/50000]	Loss: 0.3700	LR: 0.025000
Training Epoch: 77 [24960/50000]	Loss: 0.4231	LR: 0.025000
Training Epoch: 77 [25088/50000]	Loss: 0.4541	LR: 0.025000
Training Epoch: 77 [25216/50000]	Loss: 0.6774	LR: 0.025000
Training Epoch: 77 [25344/50000]	Loss: 0.5152	LR: 0.025000
Training Epoch: 77 [25472/50000]	Loss: 0.4245	LR: 0.025000
Training Epoch: 77 [25600/50000]	Loss: 0.4122	LR: 0.025000
Training Epoch: 77 [25728/50000]	Loss: 0.5022	LR: 0.025000
Training Epoch: 77 [25856/50000]	Loss: 0.5784	LR: 0.025000
Training Epoch: 77 [25984/50000]	Loss: 0.4078	LR: 0.025000
Training Epoch: 77 [26112/50000]	Loss: 0.4505	LR: 0.025000
Training Epoch: 77 [26240/50000]	Loss: 0.3682	LR: 0.025000
Training Epoch: 77 [26368/50000]	Loss: 0.4452	LR: 0.025000
Training Epoch: 77 [26496/50000]	Loss: 0.4227	LR: 0.025000
Training Epoch: 77 [26624/50000]	Loss: 0.3807	LR: 0.025000
Training Epoch: 77 [26752/50000]	Loss: 0.4629	LR: 0.025000
Training Epoch: 77 [26880/50000]	Loss: 0.4554	LR: 0.025000
Training Epoch: 77 [27008/50000]	Loss: 0.5096	LR: 0.025000
Training Epoch: 77 [27136/50000]	Loss: 0.4217	LR: 0.025000
Training Epoch: 77 [27264/50000]	Loss: 0.4431	LR: 0.025000
Training Epoch: 77 [27392/50000]	Loss: 0.5440	LR: 0.025000
Training Epoch: 77 [27520/50000]	Loss: 0.3844	LR: 0.025000
Training Epoch: 77 [27648/50000]	Loss: 0.4569	LR: 0.025000
Training Epoch: 77 [27776/50000]	Loss: 0.3826	LR: 0.025000
Training Epoch: 77 [27904/50000]	Loss: 0.5767	LR: 0.025000
Training Epoch: 77 [28032/50000]	Loss: 0.4595	LR: 0.025000
Training Epoch: 77 [28160/50000]	Loss: 0.5878	LR: 0.025000
Training Epoch: 77 [28288/50000]	Loss: 0.3587	LR: 0.025000
Training Epoch: 77 [28416/50000]	Loss: 0.4126	LR: 0.025000
Training Epoch: 77 [28544/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 77 [28672/50000]	Loss: 0.4945	LR: 0.025000
Training Epoch: 77 [28800/50000]	Loss: 0.4523	LR: 0.025000
Training Epoch: 77 [28928/50000]	Loss: 0.5800	LR: 0.025000
Training Epoch: 77 [29056/50000]	Loss: 0.4280	LR: 0.025000
Training Epoch: 77 [29184/50000]	Loss: 0.5551	LR: 0.025000
Training Epoch: 77 [29312/50000]	Loss: 0.5296	LR: 0.025000
Training Epoch: 77 [29440/50000]	Loss: 0.5617	LR: 0.025000
Training Epoch: 77 [29568/50000]	Loss: 0.3951	LR: 0.025000
Training Epoch: 77 [29696/50000]	Loss: 0.5443	LR: 0.025000
Training Epoch: 77 [29824/50000]	Loss: 0.3726	LR: 0.025000
Training Epoch: 77 [29952/50000]	Loss: 0.4862	LR: 0.025000
Training Epoch: 77 [30080/50000]	Loss: 0.3901	LR: 0.025000
Training Epoch: 77 [30208/50000]	Loss: 0.3856	LR: 0.025000
Training Epoch: 77 [30336/50000]	Loss: 0.4404	LR: 0.025000
Training Epoch: 77 [30464/50000]	Loss: 0.4572	LR: 0.025000
Training Epoch: 77 [30592/50000]	Loss: 0.5114	LR: 0.025000
Training Epoch: 77 [30720/50000]	Loss: 0.5288	LR: 0.025000
Training Epoch: 77 [30848/50000]	Loss: 0.3693	LR: 0.025000
Training Epoch: 77 [30976/50000]	Loss: 0.4366	LR: 0.025000
Training Epoch: 77 [31104/50000]	Loss: 0.4604	LR: 0.025000
Training Epoch: 77 [31232/50000]	Loss: 0.4977	LR: 0.025000
Training Epoch: 77 [31360/50000]	Loss: 0.6476	LR: 0.025000
Training Epoch: 77 [31488/50000]	Loss: 0.4503	LR: 0.025000
Training Epoch: 77 [31616/50000]	Loss: 0.6125	LR: 0.025000
Training Epoch: 77 [31744/50000]	Loss: 0.5784	LR: 0.025000
Training Epoch: 77 [31872/50000]	Loss: 0.4141	LR: 0.025000
Training Epoch: 77 [32000/50000]	Loss: 0.3756	LR: 0.025000
Training Epoch: 77 [32128/50000]	Loss: 0.4361	LR: 0.025000
Training Epoch: 77 [32256/50000]	Loss: 0.3618	LR: 0.025000
Training Epoch: 77 [32384/50000]	Loss: 0.4198	LR: 0.025000
Training Epoch: 77 [32512/50000]	Loss: 0.5484	LR: 0.025000
Training Epoch: 77 [32640/50000]	Loss: 0.5392	LR: 0.025000
Training Epoch: 77 [32768/50000]	Loss: 0.5796	LR: 0.025000
Training Epoch: 77 [32896/50000]	Loss: 0.4835	LR: 0.025000
Training Epoch: 77 [33024/50000]	Loss: 0.4332	LR: 0.025000
Training Epoch: 77 [33152/50000]	Loss: 0.5731	LR: 0.025000
Training Epoch: 77 [33280/50000]	Loss: 0.4409	LR: 0.025000
Training Epoch: 77 [33408/50000]	Loss: 0.5369	LR: 0.025000
Training Epoch: 77 [33536/50000]	Loss: 0.4349	LR: 0.025000
Training Epoch: 77 [33664/50000]	Loss: 0.4598	LR: 0.025000
Training Epoch: 77 [33792/50000]	Loss: 0.3483	LR: 0.025000
Training Epoch: 77 [33920/50000]	Loss: 0.6170	LR: 0.025000
Training Epoch: 77 [34048/50000]	Loss: 0.5203	LR: 0.025000
Training Epoch: 77 [34176/50000]	Loss: 0.4560	LR: 0.025000
Training Epoch: 77 [34304/50000]	Loss: 0.4568	LR: 0.025000
Training Epoch: 77 [34432/50000]	Loss: 0.4209	LR: 0.025000
Training Epoch: 77 [34560/50000]	Loss: 0.4655	LR: 0.025000
Training Epoch: 77 [34688/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 77 [34816/50000]	Loss: 0.5482	LR: 0.025000
Training Epoch: 77 [34944/50000]	Loss: 0.5761	LR: 0.025000
Training Epoch: 77 [35072/50000]	Loss: 0.5492	LR: 0.025000
Training Epoch: 77 [35200/50000]	Loss: 0.4256	LR: 0.025000
Training Epoch: 77 [35328/50000]	Loss: 0.5015	LR: 0.025000
Training Epoch: 77 [35456/50000]	Loss: 0.4684	LR: 0.025000
Training Epoch: 77 [35584/50000]	Loss: 0.3743	LR: 0.025000
Training Epoch: 77 [35712/50000]	Loss: 0.5535	LR: 0.025000
Training Epoch: 77 [35840/50000]	Loss: 0.4476	LR: 0.025000
Training Epoch: 77 [35968/50000]	Loss: 0.5030	LR: 0.025000
Training Epoch: 77 [36096/50000]	Loss: 0.5403	LR: 0.025000
Training Epoch: 77 [36224/50000]	Loss: 0.5648	LR: 0.025000
Training Epoch: 77 [36352/50000]	Loss: 0.5112	LR: 0.025000
Training Epoch: 77 [36480/50000]	Loss: 0.5712	LR: 0.025000
Training Epoch: 77 [36608/50000]	Loss: 0.4070	LR: 0.025000
Training Epoch: 77 [36736/50000]	Loss: 0.4021	LR: 0.025000
Training Epoch: 77 [36864/50000]	Loss: 0.4038	LR: 0.025000
Training Epoch: 77 [36992/50000]	Loss: 0.4011	LR: 0.025000
Training Epoch: 77 [37120/50000]	Loss: 0.4998	LR: 0.025000
Training Epoch: 77 [37248/50000]	Loss: 0.5684	LR: 0.025000
Training Epoch: 77 [37376/50000]	Loss: 0.5558	LR: 0.025000
Training Epoch: 77 [37504/50000]	Loss: 0.4790	LR: 0.025000
Training Epoch: 77 [37632/50000]	Loss: 0.3404	LR: 0.025000
Training Epoch: 77 [37760/50000]	Loss: 0.4740	LR: 0.025000
Training Epoch: 77 [37888/50000]	Loss: 0.5725	LR: 0.025000
Training Epoch: 77 [38016/50000]	Loss: 0.6546	LR: 0.025000
Training Epoch: 77 [38144/50000]	Loss: 0.5081	LR: 0.025000
Training Epoch: 77 [38272/50000]	Loss: 0.4935	LR: 0.025000
Training Epoch: 77 [38400/50000]	Loss: 0.4994	LR: 0.025000
Training Epoch: 77 [38528/50000]	Loss: 0.4339	LR: 0.025000
Training Epoch: 77 [38656/50000]	Loss: 0.3937	LR: 0.025000
Training Epoch: 77 [38784/50000]	Loss: 0.4463	LR: 0.025000
Training Epoch: 77 [38912/50000]	Loss: 0.3574	LR: 0.025000
Training Epoch: 77 [39040/50000]	Loss: 0.4332	LR: 0.025000
Training Epoch: 77 [39168/50000]	Loss: 0.4270	LR: 0.025000
Training Epoch: 77 [39296/50000]	Loss: 0.5749	LR: 0.025000
Training Epoch: 77 [39424/50000]	Loss: 0.6262	LR: 0.025000
Training Epoch: 77 [39552/50000]	Loss: 0.4427	LR: 0.025000
Training Epoch: 77 [39680/50000]	Loss: 0.4831	LR: 0.025000
Training Epoch: 77 [39808/50000]	Loss: 0.5540	LR: 0.025000
Training Epoch: 77 [39936/50000]	Loss: 0.5373	LR: 0.025000
Training Epoch: 77 [40064/50000]	Loss: 0.5460	LR: 0.025000
Training Epoch: 77 [40192/50000]	Loss: 0.5430	LR: 0.025000
Training Epoch: 77 [40320/50000]	Loss: 0.4815	LR: 0.025000
Training Epoch: 77 [40448/50000]	Loss: 0.4441	LR: 0.025000
Training Epoch: 77 [40576/50000]	Loss: 0.5367	LR: 0.025000
Training Epoch: 77 [40704/50000]	Loss: 0.3800	LR: 0.025000
Training Epoch: 77 [40832/50000]	Loss: 0.5748	LR: 0.025000
Training Epoch: 77 [40960/50000]	Loss: 0.5081	LR: 0.025000
Training Epoch: 77 [41088/50000]	Loss: 0.4546	LR: 0.025000
Training Epoch: 77 [41216/50000]	Loss: 0.5656	LR: 0.025000
Training Epoch: 77 [41344/50000]	Loss: 0.5593	LR: 0.025000
Training Epoch: 77 [41472/50000]	Loss: 0.5358	LR: 0.025000
Training Epoch: 77 [41600/50000]	Loss: 0.5858	LR: 0.025000
Training Epoch: 77 [41728/50000]	Loss: 0.5330	LR: 0.025000
Training Epoch: 77 [41856/50000]	Loss: 0.3842	LR: 0.025000
Training Epoch: 77 [41984/50000]	Loss: 0.3384	LR: 0.025000
Training Epoch: 77 [42112/50000]	Loss: 0.5016	LR: 0.025000
Training Epoch: 77 [42240/50000]	Loss: 0.6154	LR: 0.025000
Training Epoch: 77 [42368/50000]	Loss: 0.4392	LR: 0.025000
Training Epoch: 77 [42496/50000]	Loss: 0.5686	LR: 0.025000
Training Epoch: 77 [42624/50000]	Loss: 0.4572	LR: 0.025000
Training Epoch: 77 [42752/50000]	Loss: 0.5230	LR: 0.025000
Training Epoch: 77 [42880/50000]	Loss: 0.4358	LR: 0.025000
Training Epoch: 77 [43008/50000]	Loss: 0.3845	LR: 0.025000
Training Epoch: 77 [43136/50000]	Loss: 0.5605	LR: 0.025000
Training Epoch: 77 [43264/50000]	Loss: 0.4453	LR: 0.025000
Training Epoch: 77 [43392/50000]	Loss: 0.4657	LR: 0.025000
Training Epoch: 77 [43520/50000]	Loss: 0.6174	LR: 0.025000
Training Epoch: 77 [43648/50000]	Loss: 0.4965	LR: 0.025000
Training Epoch: 77 [43776/50000]	Loss: 0.5911	LR: 0.025000
Training Epoch: 77 [43904/50000]	Loss: 0.4916	LR: 0.025000
Training Epoch: 77 [44032/50000]	Loss: 0.6249	LR: 0.025000
Training Epoch: 77 [44160/50000]	Loss: 0.6409	LR: 0.025000
Training Epoch: 77 [44288/50000]	Loss: 0.5110	LR: 0.025000
Training Epoch: 77 [44416/50000]	Loss: 0.5593	LR: 0.025000
Training Epoch: 77 [44544/50000]	Loss: 0.5427	LR: 0.025000
Training Epoch: 77 [44672/50000]	Loss: 0.5236	LR: 0.025000
Training Epoch: 77 [44800/50000]	Loss: 0.4030	LR: 0.025000
Training Epoch: 77 [44928/50000]	Loss: 0.4882	LR: 0.025000
Training Epoch: 77 [45056/50000]	Loss: 0.6055	LR: 0.025000
Training Epoch: 77 [45184/50000]	Loss: 0.5061	LR: 0.025000
Training Epoch: 77 [45312/50000]	Loss: 0.4495	LR: 0.025000
Training Epoch: 77 [45440/50000]	Loss: 0.5141	LR: 0.025000
Training Epoch: 77 [45568/50000]	Loss: 0.4545	LR: 0.025000
Training Epoch: 77 [45696/50000]	Loss: 0.4534	LR: 0.025000
Training Epoch: 77 [45824/50000]	Loss: 0.5547	LR: 0.025000
Training Epoch: 77 [45952/50000]	Loss: 0.7335	LR: 0.025000
Training Epoch: 77 [46080/50000]	Loss: 0.4664	LR: 0.025000
Training Epoch: 77 [46208/50000]	Loss: 0.5697	LR: 0.025000
Training Epoch: 77 [46336/50000]	Loss: 0.5857	LR: 0.025000
Training Epoch: 77 [46464/50000]	Loss: 0.7099	LR: 0.025000
Training Epoch: 77 [46592/50000]	Loss: 0.6492	LR: 0.025000
Training Epoch: 77 [46720/50000]	Loss: 0.4679	LR: 0.025000
Training Epoch: 77 [46848/50000]	Loss: 0.5022	LR: 0.025000
Training Epoch: 77 [46976/50000]	Loss: 0.4462	LR: 0.025000
Training Epoch: 77 [47104/50000]	Loss: 0.5414	LR: 0.025000
Training Epoch: 77 [47232/50000]	Loss: 0.4164	LR: 0.025000
Training Epoch: 77 [47360/50000]	Loss: 0.5745	LR: 0.025000
Training Epoch: 77 [47488/50000]	Loss: 0.4737	LR: 0.025000
Training Epoch: 77 [47616/50000]	Loss: 0.5283	LR: 0.025000
Training Epoch: 77 [47744/50000]	Loss: 0.3760	LR: 0.025000
Training Epoch: 77 [47872/50000]	Loss: 0.5045	LR: 0.025000
Training Epoch: 77 [48000/50000]	Loss: 0.5991	LR: 0.025000
Training Epoch: 77 [48128/50000]	Loss: 0.4255	LR: 0.025000
Training Epoch: 77 [48256/50000]	Loss: 0.4855	LR: 0.025000
Training Epoch: 77 [48384/50000]	Loss: 0.6657	LR: 0.025000
Training Epoch: 77 [48512/50000]	Loss: 0.4910	LR: 0.025000
Training Epoch: 77 [48640/50000]	Loss: 0.5882	LR: 0.025000
Training Epoch: 77 [48768/50000]	Loss: 0.5125	LR: 0.025000
Training Epoch: 77 [48896/50000]	Loss: 0.4208	LR: 0.025000
Training Epoch: 77 [49024/50000]	Loss: 0.5172	LR: 0.025000
Training Epoch: 77 [49152/50000]	Loss: 0.6918	LR: 0.025000
Training Epoch: 77 [49280/50000]	Loss: 0.3660	LR: 0.025000
Training Epoch: 77 [49408/50000]	Loss: 0.4247	LR: 0.025000
Training Epoch: 77 [49536/50000]	Loss: 0.5833	LR: 0.025000
Training Epoch: 77 [49664/50000]	Loss: 0.4030	LR: 0.025000
Training Epoch: 77 [49792/50000]	Loss: 0.5861	LR: 0.025000
Training Epoch: 77 [49920/50000]	Loss: 0.3895	LR: 0.025000
Training Epoch: 77 [50000/50000]	Loss: 0.4852	LR: 0.025000
epoch 77 training time consumed: 53.90s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  282594 GB |  282594 GB |
|       from large pool |  123392 KB |    1034 MB |  282316 GB |  282316 GB |
|       from small pool |   10798 KB |      13 MB |     278 GB |     278 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  282594 GB |  282594 GB |
|       from large pool |  123392 KB |    1034 MB |  282316 GB |  282316 GB |
|       from small pool |   10798 KB |      13 MB |     278 GB |     278 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  124358 GB |  124358 GB |
|       from large pool |  155136 KB |  433088 KB |  124050 GB |  124050 GB |
|       from small pool |    1490 KB |    3494 KB |     307 GB |     307 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   10904 K  |   10904 K  |
|       from large pool |      24    |      65    |    5691 K  |    5691 K  |
|       from small pool |     231    |     274    |    5212 K  |    5212 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   10904 K  |   10904 K  |
|       from large pool |      24    |      65    |    5691 K  |    5691 K  |
|       from small pool |     231    |     274    |    5212 K  |    5212 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5387 K  |    5387 K  |
|       from large pool |       9    |      14    |    2754 K  |    2754 K  |
|       from small pool |      12    |      16    |    2632 K  |    2632 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 77, Average loss: 0.0106, Accuracy: 0.6762, Time consumed:3.46s

Training Epoch: 78 [128/50000]	Loss: 0.2922	LR: 0.025000
Training Epoch: 78 [256/50000]	Loss: 0.4370	LR: 0.025000
Training Epoch: 78 [384/50000]	Loss: 0.3120	LR: 0.025000
Training Epoch: 78 [512/50000]	Loss: 0.4727	LR: 0.025000
Training Epoch: 78 [640/50000]	Loss: 0.4642	LR: 0.025000
Training Epoch: 78 [768/50000]	Loss: 0.4504	LR: 0.025000
Training Epoch: 78 [896/50000]	Loss: 0.3699	LR: 0.025000
Training Epoch: 78 [1024/50000]	Loss: 0.4694	LR: 0.025000
Training Epoch: 78 [1152/50000]	Loss: 0.4467	LR: 0.025000
Training Epoch: 78 [1280/50000]	Loss: 0.4905	LR: 0.025000
Training Epoch: 78 [1408/50000]	Loss: 0.3507	LR: 0.025000
Training Epoch: 78 [1536/50000]	Loss: 0.4432	LR: 0.025000
Training Epoch: 78 [1664/50000]	Loss: 0.3598	LR: 0.025000
Training Epoch: 78 [1792/50000]	Loss: 0.5689	LR: 0.025000
Training Epoch: 78 [1920/50000]	Loss: 0.4661	LR: 0.025000
Training Epoch: 78 [2048/50000]	Loss: 0.4242	LR: 0.025000
Training Epoch: 78 [2176/50000]	Loss: 0.2946	LR: 0.025000
Training Epoch: 78 [2304/50000]	Loss: 0.4281	LR: 0.025000
Training Epoch: 78 [2432/50000]	Loss: 0.4155	LR: 0.025000
Training Epoch: 78 [2560/50000]	Loss: 0.5164	LR: 0.025000
Training Epoch: 78 [2688/50000]	Loss: 0.3207	LR: 0.025000
Training Epoch: 78 [2816/50000]	Loss: 0.4190	LR: 0.025000
Training Epoch: 78 [2944/50000]	Loss: 0.4166	LR: 0.025000
Training Epoch: 78 [3072/50000]	Loss: 0.4752	LR: 0.025000
Training Epoch: 78 [3200/50000]	Loss: 0.4263	LR: 0.025000
Training Epoch: 78 [3328/50000]	Loss: 0.4578	LR: 0.025000
Training Epoch: 78 [3456/50000]	Loss: 0.4318	LR: 0.025000
Training Epoch: 78 [3584/50000]	Loss: 0.3876	LR: 0.025000
Training Epoch: 78 [3712/50000]	Loss: 0.4787	LR: 0.025000
Training Epoch: 78 [3840/50000]	Loss: 0.3484	LR: 0.025000
Training Epoch: 78 [3968/50000]	Loss: 0.4737	LR: 0.025000
Training Epoch: 78 [4096/50000]	Loss: 0.5014	LR: 0.025000
Training Epoch: 78 [4224/50000]	Loss: 0.3087	LR: 0.025000
Training Epoch: 78 [4352/50000]	Loss: 0.5673	LR: 0.025000
Training Epoch: 78 [4480/50000]	Loss: 0.4444	LR: 0.025000
Training Epoch: 78 [4608/50000]	Loss: 0.3767	LR: 0.025000
Training Epoch: 78 [4736/50000]	Loss: 0.4030	LR: 0.025000
Training Epoch: 78 [4864/50000]	Loss: 0.5101	LR: 0.025000
Training Epoch: 78 [4992/50000]	Loss: 0.3651	LR: 0.025000
Training Epoch: 78 [5120/50000]	Loss: 0.4311	LR: 0.025000
Training Epoch: 78 [5248/50000]	Loss: 0.5340	LR: 0.025000
Training Epoch: 78 [5376/50000]	Loss: 0.5085	LR: 0.025000
Training Epoch: 78 [5504/50000]	Loss: 0.3573	LR: 0.025000
Training Epoch: 78 [5632/50000]	Loss: 0.4325	LR: 0.025000
Training Epoch: 78 [5760/50000]	Loss: 0.4010	LR: 0.025000
Training Epoch: 78 [5888/50000]	Loss: 0.3525	LR: 0.025000
Training Epoch: 78 [6016/50000]	Loss: 0.4147	LR: 0.025000
Training Epoch: 78 [6144/50000]	Loss: 0.4353	LR: 0.025000
Training Epoch: 78 [6272/50000]	Loss: 0.3985	LR: 0.025000
Training Epoch: 78 [6400/50000]	Loss: 0.4182	LR: 0.025000
Training Epoch: 78 [6528/50000]	Loss: 0.5190	LR: 0.025000
Training Epoch: 78 [6656/50000]	Loss: 0.4805	LR: 0.025000
Training Epoch: 78 [6784/50000]	Loss: 0.3259	LR: 0.025000
Training Epoch: 78 [6912/50000]	Loss: 0.3782	LR: 0.025000
Training Epoch: 78 [7040/50000]	Loss: 0.3969	LR: 0.025000
Training Epoch: 78 [7168/50000]	Loss: 0.2967	LR: 0.025000
Training Epoch: 78 [7296/50000]	Loss: 0.4481	LR: 0.025000
Training Epoch: 78 [7424/50000]	Loss: 0.3481	LR: 0.025000
Training Epoch: 78 [7552/50000]	Loss: 0.3352	LR: 0.025000
Training Epoch: 78 [7680/50000]	Loss: 0.4538	LR: 0.025000
Training Epoch: 78 [7808/50000]	Loss: 0.2810	LR: 0.025000
Training Epoch: 78 [7936/50000]	Loss: 0.2675	LR: 0.025000
Training Epoch: 78 [8064/50000]	Loss: 0.4509	LR: 0.025000
Training Epoch: 78 [8192/50000]	Loss: 0.4015	LR: 0.025000
Training Epoch: 78 [8320/50000]	Loss: 0.4089	LR: 0.025000
Training Epoch: 78 [8448/50000]	Loss: 0.5745	LR: 0.025000
Training Epoch: 78 [8576/50000]	Loss: 0.4166	LR: 0.025000
Training Epoch: 78 [8704/50000]	Loss: 0.3694	LR: 0.025000
Training Epoch: 78 [8832/50000]	Loss: 0.4731	LR: 0.025000
Training Epoch: 78 [8960/50000]	Loss: 0.3647	LR: 0.025000
Training Epoch: 78 [9088/50000]	Loss: 0.4480	LR: 0.025000
Training Epoch: 78 [9216/50000]	Loss: 0.4387	LR: 0.025000
Training Epoch: 78 [9344/50000]	Loss: 0.4120	LR: 0.025000
Training Epoch: 78 [9472/50000]	Loss: 0.4366	LR: 0.025000
Training Epoch: 78 [9600/50000]	Loss: 0.5077	LR: 0.025000
Training Epoch: 78 [9728/50000]	Loss: 0.3046	LR: 0.025000
Training Epoch: 78 [9856/50000]	Loss: 0.3882	LR: 0.025000
Training Epoch: 78 [9984/50000]	Loss: 0.2910	LR: 0.025000
Training Epoch: 78 [10112/50000]	Loss: 0.2663	LR: 0.025000
Training Epoch: 78 [10240/50000]	Loss: 0.3809	LR: 0.025000
Training Epoch: 78 [10368/50000]	Loss: 0.3388	LR: 0.025000
Training Epoch: 78 [10496/50000]	Loss: 0.4191	LR: 0.025000
Training Epoch: 78 [10624/50000]	Loss: 0.4765	LR: 0.025000
Training Epoch: 78 [10752/50000]	Loss: 0.5552	LR: 0.025000
Training Epoch: 78 [10880/50000]	Loss: 0.5175	LR: 0.025000
Training Epoch: 78 [11008/50000]	Loss: 0.5590	LR: 0.025000
Training Epoch: 78 [11136/50000]	Loss: 0.5011	LR: 0.025000
Training Epoch: 78 [11264/50000]	Loss: 0.4496	LR: 0.025000
Training Epoch: 78 [11392/50000]	Loss: 0.4539	LR: 0.025000
Training Epoch: 78 [11520/50000]	Loss: 0.5455	LR: 0.025000
Training Epoch: 78 [11648/50000]	Loss: 0.3676	LR: 0.025000
Training Epoch: 78 [11776/50000]	Loss: 0.3992	LR: 0.025000
Training Epoch: 78 [11904/50000]	Loss: 0.2029	LR: 0.025000
Training Epoch: 78 [12032/50000]	Loss: 0.2420	LR: 0.025000
Training Epoch: 78 [12160/50000]	Loss: 0.5255	LR: 0.025000
Training Epoch: 78 [12288/50000]	Loss: 0.3116	LR: 0.025000
Training Epoch: 78 [12416/50000]	Loss: 0.4009	LR: 0.025000
Training Epoch: 78 [12544/50000]	Loss: 0.4306	LR: 0.025000
Training Epoch: 78 [12672/50000]	Loss: 0.3614	LR: 0.025000
Training Epoch: 78 [12800/50000]	Loss: 0.3928	LR: 0.025000
Training Epoch: 78 [12928/50000]	Loss: 0.3672	LR: 0.025000
Training Epoch: 78 [13056/50000]	Loss: 0.4376	LR: 0.025000
Training Epoch: 78 [13184/50000]	Loss: 0.4497	LR: 0.025000
Training Epoch: 78 [13312/50000]	Loss: 0.3640	LR: 0.025000
Training Epoch: 78 [13440/50000]	Loss: 0.3656	LR: 0.025000
Training Epoch: 78 [13568/50000]	Loss: 0.4892	LR: 0.025000
Training Epoch: 78 [13696/50000]	Loss: 0.3146	LR: 0.025000
Training Epoch: 78 [13824/50000]	Loss: 0.4926	LR: 0.025000
Training Epoch: 78 [13952/50000]	Loss: 0.2860	LR: 0.025000
Training Epoch: 78 [14080/50000]	Loss: 0.2527	LR: 0.025000
Training Epoch: 78 [14208/50000]	Loss: 0.2523	LR: 0.025000
Training Epoch: 78 [14336/50000]	Loss: 0.3781	LR: 0.025000
Training Epoch: 78 [14464/50000]	Loss: 0.5051	LR: 0.025000
Training Epoch: 78 [14592/50000]	Loss: 0.5686	LR: 0.025000
Training Epoch: 78 [14720/50000]	Loss: 0.4046	LR: 0.025000
Training Epoch: 78 [14848/50000]	Loss: 0.4770	LR: 0.025000
Training Epoch: 78 [14976/50000]	Loss: 0.4114	LR: 0.025000
Training Epoch: 78 [15104/50000]	Loss: 0.4739	LR: 0.025000
Training Epoch: 78 [15232/50000]	Loss: 0.4667	LR: 0.025000
Training Epoch: 78 [15360/50000]	Loss: 0.3002	LR: 0.025000
Training Epoch: 78 [15488/50000]	Loss: 0.2992	LR: 0.025000
Training Epoch: 78 [15616/50000]	Loss: 0.6053	LR: 0.025000
Training Epoch: 78 [15744/50000]	Loss: 0.3517	LR: 0.025000
Training Epoch: 78 [15872/50000]	Loss: 0.4852	LR: 0.025000
Training Epoch: 78 [16000/50000]	Loss: 0.4124	LR: 0.025000
Training Epoch: 78 [16128/50000]	Loss: 0.3312	LR: 0.025000
Training Epoch: 78 [16256/50000]	Loss: 0.3309	LR: 0.025000
Training Epoch: 78 [16384/50000]	Loss: 0.3865	LR: 0.025000
Training Epoch: 78 [16512/50000]	Loss: 0.3447	LR: 0.025000
Training Epoch: 78 [16640/50000]	Loss: 0.3733	LR: 0.025000
Training Epoch: 78 [16768/50000]	Loss: 0.4703	LR: 0.025000
Training Epoch: 78 [16896/50000]	Loss: 0.4261	LR: 0.025000
Training Epoch: 78 [17024/50000]	Loss: 0.5143	LR: 0.025000
Training Epoch: 78 [17152/50000]	Loss: 0.4067	LR: 0.025000
Training Epoch: 78 [17280/50000]	Loss: 0.5405	LR: 0.025000
Training Epoch: 78 [17408/50000]	Loss: 0.3588	LR: 0.025000
Training Epoch: 78 [17536/50000]	Loss: 0.3697	LR: 0.025000
Training Epoch: 78 [17664/50000]	Loss: 0.3714	LR: 0.025000
Training Epoch: 78 [17792/50000]	Loss: 0.4473	LR: 0.025000
Training Epoch: 78 [17920/50000]	Loss: 0.4724	LR: 0.025000
Training Epoch: 78 [18048/50000]	Loss: 0.3734	LR: 0.025000
Training Epoch: 78 [18176/50000]	Loss: 0.3171	LR: 0.025000
Training Epoch: 78 [18304/50000]	Loss: 0.3844	LR: 0.025000
Training Epoch: 78 [18432/50000]	Loss: 0.3892	LR: 0.025000
Training Epoch: 78 [18560/50000]	Loss: 0.3935	LR: 0.025000
Training Epoch: 78 [18688/50000]	Loss: 0.2906	LR: 0.025000
Training Epoch: 78 [18816/50000]	Loss: 0.4376	LR: 0.025000
Training Epoch: 78 [18944/50000]	Loss: 0.3746	LR: 0.025000
Training Epoch: 78 [19072/50000]	Loss: 0.4600	LR: 0.025000
Training Epoch: 78 [19200/50000]	Loss: 0.5295	LR: 0.025000
Training Epoch: 78 [19328/50000]	Loss: 0.4350	LR: 0.025000
Training Epoch: 78 [19456/50000]	Loss: 0.3818	LR: 0.025000
Training Epoch: 78 [19584/50000]	Loss: 0.4363	LR: 0.025000
Training Epoch: 78 [19712/50000]	Loss: 0.4406	LR: 0.025000
Training Epoch: 78 [19840/50000]	Loss: 0.4077	LR: 0.025000
Training Epoch: 78 [19968/50000]	Loss: 0.4816	LR: 0.025000
Training Epoch: 78 [20096/50000]	Loss: 0.6390	LR: 0.025000
Training Epoch: 78 [20224/50000]	Loss: 0.3974	LR: 0.025000
Training Epoch: 78 [20352/50000]	Loss: 0.4601	LR: 0.025000
Training Epoch: 78 [20480/50000]	Loss: 0.4689	LR: 0.025000
Training Epoch: 78 [20608/50000]	Loss: 0.3181	LR: 0.025000
Training Epoch: 78 [20736/50000]	Loss: 0.4637	LR: 0.025000
Training Epoch: 78 [20864/50000]	Loss: 0.5576	LR: 0.025000
Training Epoch: 78 [20992/50000]	Loss: 0.3614	LR: 0.025000
Training Epoch: 78 [21120/50000]	Loss: 0.4296	LR: 0.025000
Training Epoch: 78 [21248/50000]	Loss: 0.5645	LR: 0.025000
Training Epoch: 78 [21376/50000]	Loss: 0.5222	LR: 0.025000
Training Epoch: 78 [21504/50000]	Loss: 0.4845	LR: 0.025000
Training Epoch: 78 [21632/50000]	Loss: 0.6139	LR: 0.025000
Training Epoch: 78 [21760/50000]	Loss: 0.4133	LR: 0.025000
Training Epoch: 78 [21888/50000]	Loss: 0.5946	LR: 0.025000
Training Epoch: 78 [22016/50000]	Loss: 0.2484	LR: 0.025000
Training Epoch: 78 [22144/50000]	Loss: 0.4268	LR: 0.025000
Training Epoch: 78 [22272/50000]	Loss: 0.4212	LR: 0.025000
Training Epoch: 78 [22400/50000]	Loss: 0.3639	LR: 0.025000
Training Epoch: 78 [22528/50000]	Loss: 0.3632	LR: 0.025000
Training Epoch: 78 [22656/50000]	Loss: 0.4039	LR: 0.025000
Training Epoch: 78 [22784/50000]	Loss: 0.4274	LR: 0.025000
Training Epoch: 78 [22912/50000]	Loss: 0.2979	LR: 0.025000
Training Epoch: 78 [23040/50000]	Loss: 0.4894	LR: 0.025000
Training Epoch: 78 [23168/50000]	Loss: 0.4782	LR: 0.025000
Training Epoch: 78 [23296/50000]	Loss: 0.3539	LR: 0.025000
Training Epoch: 78 [23424/50000]	Loss: 0.4273	LR: 0.025000
Training Epoch: 78 [23552/50000]	Loss: 0.3780	LR: 0.025000
Training Epoch: 78 [23680/50000]	Loss: 0.3439	LR: 0.025000
Training Epoch: 78 [23808/50000]	Loss: 0.3996	LR: 0.025000
Training Epoch: 78 [23936/50000]	Loss: 0.3806	LR: 0.025000
Training Epoch: 78 [24064/50000]	Loss: 0.3621	LR: 0.025000
Training Epoch: 78 [24192/50000]	Loss: 0.5068	LR: 0.025000
Training Epoch: 78 [24320/50000]	Loss: 0.4695	LR: 0.025000
Training Epoch: 78 [24448/50000]	Loss: 0.4248	LR: 0.025000
Training Epoch: 78 [24576/50000]	Loss: 0.5109	LR: 0.025000
Training Epoch: 78 [24704/50000]	Loss: 0.3380	LR: 0.025000
Training Epoch: 78 [24832/50000]	Loss: 0.4587	LR: 0.025000
Training Epoch: 78 [24960/50000]	Loss: 0.5497	LR: 0.025000
Training Epoch: 78 [25088/50000]	Loss: 0.5250	LR: 0.025000
Training Epoch: 78 [25216/50000]	Loss: 0.3769	LR: 0.025000
Training Epoch: 78 [25344/50000]	Loss: 0.5688	LR: 0.025000
Training Epoch: 78 [25472/50000]	Loss: 0.3751	LR: 0.025000
Training Epoch: 78 [25600/50000]	Loss: 0.4812	LR: 0.025000
Training Epoch: 78 [25728/50000]	Loss: 0.5255	LR: 0.025000
Training Epoch: 78 [25856/50000]	Loss: 0.4501	LR: 0.025000
Training Epoch: 78 [25984/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 78 [26112/50000]	Loss: 0.4255	LR: 0.025000
Training Epoch: 78 [26240/50000]	Loss: 0.3998	LR: 0.025000
Training Epoch: 78 [26368/50000]	Loss: 0.2702	LR: 0.025000
Training Epoch: 78 [26496/50000]	Loss: 0.4131	LR: 0.025000
Training Epoch: 78 [26624/50000]	Loss: 0.5192	LR: 0.025000
Training Epoch: 78 [26752/50000]	Loss: 0.5809	LR: 0.025000
Training Epoch: 78 [26880/50000]	Loss: 0.6033	LR: 0.025000
Training Epoch: 78 [27008/50000]	Loss: 0.6048	LR: 0.025000
Training Epoch: 78 [27136/50000]	Loss: 0.3907	LR: 0.025000
Training Epoch: 78 [27264/50000]	Loss: 0.5737	LR: 0.025000
Training Epoch: 78 [27392/50000]	Loss: 0.4736	LR: 0.025000
Training Epoch: 78 [27520/50000]	Loss: 0.3047	LR: 0.025000
Training Epoch: 78 [27648/50000]	Loss: 0.3622	LR: 0.025000
Training Epoch: 78 [27776/50000]	Loss: 0.3966	LR: 0.025000
Training Epoch: 78 [27904/50000]	Loss: 0.3586	LR: 0.025000
Training Epoch: 78 [28032/50000]	Loss: 0.3827	LR: 0.025000
Training Epoch: 78 [28160/50000]	Loss: 0.5559	LR: 0.025000
Training Epoch: 78 [28288/50000]	Loss: 0.4207	LR: 0.025000
Training Epoch: 78 [28416/50000]	Loss: 0.4531	LR: 0.025000
Training Epoch: 78 [28544/50000]	Loss: 0.3975	LR: 0.025000
Training Epoch: 78 [28672/50000]	Loss: 0.4167	LR: 0.025000
Training Epoch: 78 [28800/50000]	Loss: 0.3740	LR: 0.025000
Training Epoch: 78 [28928/50000]	Loss: 0.3757	LR: 0.025000
Training Epoch: 78 [29056/50000]	Loss: 0.6037	LR: 0.025000
Training Epoch: 78 [29184/50000]	Loss: 0.4116	LR: 0.025000
Training Epoch: 78 [29312/50000]	Loss: 0.3327	LR: 0.025000
Training Epoch: 78 [29440/50000]	Loss: 0.5316	LR: 0.025000
Training Epoch: 78 [29568/50000]	Loss: 0.3933	LR: 0.025000
Training Epoch: 78 [29696/50000]	Loss: 0.4948	LR: 0.025000
Training Epoch: 78 [29824/50000]	Loss: 0.5434	LR: 0.025000
Training Epoch: 78 [29952/50000]	Loss: 0.4680	LR: 0.025000
Training Epoch: 78 [30080/50000]	Loss: 0.4161	LR: 0.025000
Training Epoch: 78 [30208/50000]	Loss: 0.4402	LR: 0.025000
Training Epoch: 78 [30336/50000]	Loss: 0.3825	LR: 0.025000
Training Epoch: 78 [30464/50000]	Loss: 0.5374	LR: 0.025000
Training Epoch: 78 [30592/50000]	Loss: 0.4235	LR: 0.025000
Training Epoch: 78 [30720/50000]	Loss: 0.4456	LR: 0.025000
Training Epoch: 78 [30848/50000]	Loss: 0.4681	LR: 0.025000
Training Epoch: 78 [30976/50000]	Loss: 0.5604	LR: 0.025000
Training Epoch: 78 [31104/50000]	Loss: 0.3996	LR: 0.025000
Training Epoch: 78 [31232/50000]	Loss: 0.3990	LR: 0.025000
Training Epoch: 78 [31360/50000]	Loss: 0.2970	LR: 0.025000
Training Epoch: 78 [31488/50000]	Loss: 0.4317	LR: 0.025000
Training Epoch: 78 [31616/50000]	Loss: 0.3739	LR: 0.025000
Training Epoch: 78 [31744/50000]	Loss: 0.5865	LR: 0.025000
Training Epoch: 78 [31872/50000]	Loss: 0.4309	LR: 0.025000
Training Epoch: 78 [32000/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 78 [32128/50000]	Loss: 0.4078	LR: 0.025000
Training Epoch: 78 [32256/50000]	Loss: 0.4088	LR: 0.025000
Training Epoch: 78 [32384/50000]	Loss: 0.4112	LR: 0.025000
Training Epoch: 78 [32512/50000]	Loss: 0.4247	LR: 0.025000
Training Epoch: 78 [32640/50000]	Loss: 0.4707	LR: 0.025000
Training Epoch: 78 [32768/50000]	Loss: 0.3873	LR: 0.025000
Training Epoch: 78 [32896/50000]	Loss: 0.4882	LR: 0.025000
Training Epoch: 78 [33024/50000]	Loss: 0.4143	LR: 0.025000
Training Epoch: 78 [33152/50000]	Loss: 0.6628	LR: 0.025000
Training Epoch: 78 [33280/50000]	Loss: 0.4910	LR: 0.025000
Training Epoch: 78 [33408/50000]	Loss: 0.4786	LR: 0.025000
Training Epoch: 78 [33536/50000]	Loss: 0.4249	LR: 0.025000
Training Epoch: 78 [33664/50000]	Loss: 0.3009	LR: 0.025000
Training Epoch: 78 [33792/50000]	Loss: 0.5898	LR: 0.025000
Training Epoch: 78 [33920/50000]	Loss: 0.6572	LR: 0.025000
Training Epoch: 78 [34048/50000]	Loss: 0.3777	LR: 0.025000
Training Epoch: 78 [34176/50000]	Loss: 0.4238	LR: 0.025000
Training Epoch: 78 [34304/50000]	Loss: 0.4523	LR: 0.025000
Training Epoch: 78 [34432/50000]	Loss: 0.5735	LR: 0.025000
Training Epoch: 78 [34560/50000]	Loss: 0.3952	LR: 0.025000
Training Epoch: 78 [34688/50000]	Loss: 0.5130	LR: 0.025000
Training Epoch: 78 [34816/50000]	Loss: 0.4115	LR: 0.025000
Training Epoch: 78 [34944/50000]	Loss: 0.5697	LR: 0.025000
Training Epoch: 78 [35072/50000]	Loss: 0.3968	LR: 0.025000
Training Epoch: 78 [35200/50000]	Loss: 0.4817	LR: 0.025000
Training Epoch: 78 [35328/50000]	Loss: 0.6167	LR: 0.025000
Training Epoch: 78 [35456/50000]	Loss: 0.5812	LR: 0.025000
Training Epoch: 78 [35584/50000]	Loss: 0.4025	LR: 0.025000
Training Epoch: 78 [35712/50000]	Loss: 0.5128	LR: 0.025000
Training Epoch: 78 [35840/50000]	Loss: 0.4595	LR: 0.025000
Training Epoch: 78 [35968/50000]	Loss: 0.3704	LR: 0.025000
Training Epoch: 78 [36096/50000]	Loss: 0.4966	LR: 0.025000
Training Epoch: 78 [36224/50000]	Loss: 0.5459	LR: 0.025000
Training Epoch: 78 [36352/50000]	Loss: 0.3428	LR: 0.025000
Training Epoch: 78 [36480/50000]	Loss: 0.5755	LR: 0.025000
Training Epoch: 78 [36608/50000]	Loss: 0.4468	LR: 0.025000
Training Epoch: 78 [36736/50000]	Loss: 0.4397	LR: 0.025000
Training Epoch: 78 [36864/50000]	Loss: 0.4492	LR: 0.025000
Training Epoch: 78 [36992/50000]	Loss: 0.5499	LR: 0.025000
Training Epoch: 78 [37120/50000]	Loss: 0.5541	LR: 0.025000
Training Epoch: 78 [37248/50000]	Loss: 0.5324	LR: 0.025000
Training Epoch: 78 [37376/50000]	Loss: 0.4823	LR: 0.025000
Training Epoch: 78 [37504/50000]	Loss: 0.3723	LR: 0.025000
Training Epoch: 78 [37632/50000]	Loss: 0.2890	LR: 0.025000
Training Epoch: 78 [37760/50000]	Loss: 0.3637	LR: 0.025000
Training Epoch: 78 [37888/50000]	Loss: 0.5068	LR: 0.025000
Training Epoch: 78 [38016/50000]	Loss: 0.5007	LR: 0.025000
Training Epoch: 78 [38144/50000]	Loss: 0.5162	LR: 0.025000
Training Epoch: 78 [38272/50000]	Loss: 0.4366	LR: 0.025000
Training Epoch: 78 [38400/50000]	Loss: 0.4755	LR: 0.025000
Training Epoch: 78 [38528/50000]	Loss: 0.5081	LR: 0.025000
Training Epoch: 78 [38656/50000]	Loss: 0.5942	LR: 0.025000
Training Epoch: 78 [38784/50000]	Loss: 0.6583	LR: 0.025000
Training Epoch: 78 [38912/50000]	Loss: 0.5452	LR: 0.025000
Training Epoch: 78 [39040/50000]	Loss: 0.6528	LR: 0.025000
Training Epoch: 78 [39168/50000]	Loss: 0.6086	LR: 0.025000
Training Epoch: 78 [39296/50000]	Loss: 0.6373	LR: 0.025000
Training Epoch: 78 [39424/50000]	Loss: 0.5002	LR: 0.025000
Training Epoch: 78 [39552/50000]	Loss: 0.3650	LR: 0.025000
Training Epoch: 78 [39680/50000]	Loss: 0.5398	LR: 0.025000
Training Epoch: 78 [39808/50000]	Loss: 0.3480	LR: 0.025000
Training Epoch: 78 [39936/50000]	Loss: 0.4804	LR: 0.025000
Training Epoch: 78 [40064/50000]	Loss: 0.5180	LR: 0.025000
Training Epoch: 78 [40192/50000]	Loss: 0.5449	LR: 0.025000
Training Epoch: 78 [40320/50000]	Loss: 0.6074	LR: 0.025000
Training Epoch: 78 [40448/50000]	Loss: 0.4814	LR: 0.025000
Training Epoch: 78 [40576/50000]	Loss: 0.4090	LR: 0.025000
Training Epoch: 78 [40704/50000]	Loss: 0.4994	LR: 0.025000
Training Epoch: 78 [40832/50000]	Loss: 0.4742	LR: 0.025000
Training Epoch: 78 [40960/50000]	Loss: 0.5493	LR: 0.025000
Training Epoch: 78 [41088/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 78 [41216/50000]	Loss: 0.6071	LR: 0.025000
Training Epoch: 78 [41344/50000]	Loss: 0.5721	LR: 0.025000
Training Epoch: 78 [41472/50000]	Loss: 0.5162	LR: 0.025000
Training Epoch: 78 [41600/50000]	Loss: 0.5485	LR: 0.025000
Training Epoch: 78 [41728/50000]	Loss: 0.6033	LR: 0.025000
Training Epoch: 78 [41856/50000]	Loss: 0.4476	LR: 0.025000
Training Epoch: 78 [41984/50000]	Loss: 0.5423	LR: 0.025000
Training Epoch: 78 [42112/50000]	Loss: 0.6386	LR: 0.025000
Training Epoch: 78 [42240/50000]	Loss: 0.5796	LR: 0.025000
Training Epoch: 78 [42368/50000]	Loss: 0.5519	LR: 0.025000
Training Epoch: 78 [42496/50000]	Loss: 0.4953	LR: 0.025000
Training Epoch: 78 [42624/50000]	Loss: 0.4647	LR: 0.025000
Training Epoch: 78 [42752/50000]	Loss: 0.6530	LR: 0.025000
Training Epoch: 78 [42880/50000]	Loss: 0.4904	LR: 0.025000
Training Epoch: 78 [43008/50000]	Loss: 0.5537	LR: 0.025000
Training Epoch: 78 [43136/50000]	Loss: 0.4489	LR: 0.025000
Training Epoch: 78 [43264/50000]	Loss: 0.6769	LR: 0.025000
Training Epoch: 78 [43392/50000]	Loss: 0.5441	LR: 0.025000
Training Epoch: 78 [43520/50000]	Loss: 0.4313	LR: 0.025000
Training Epoch: 78 [43648/50000]	Loss: 0.5345	LR: 0.025000
Training Epoch: 78 [43776/50000]	Loss: 0.5950	LR: 0.025000
Training Epoch: 78 [43904/50000]	Loss: 0.7433	LR: 0.025000
Training Epoch: 78 [44032/50000]	Loss: 0.5640	LR: 0.025000
Training Epoch: 78 [44160/50000]	Loss: 0.4287	LR: 0.025000
Training Epoch: 78 [44288/50000]	Loss: 0.5687	LR: 0.025000
Training Epoch: 78 [44416/50000]	Loss: 0.4163	LR: 0.025000
Training Epoch: 78 [44544/50000]	Loss: 0.4936	LR: 0.025000
Training Epoch: 78 [44672/50000]	Loss: 0.5579	LR: 0.025000
Training Epoch: 78 [44800/50000]	Loss: 0.4955	LR: 0.025000
Training Epoch: 78 [44928/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 78 [45056/50000]	Loss: 0.3006	LR: 0.025000
Training Epoch: 78 [45184/50000]	Loss: 0.4110	LR: 0.025000
Training Epoch: 78 [45312/50000]	Loss: 0.5167	LR: 0.025000
Training Epoch: 78 [45440/50000]	Loss: 0.6254	LR: 0.025000
Training Epoch: 78 [45568/50000]	Loss: 0.5560	LR: 0.025000
Training Epoch: 78 [45696/50000]	Loss: 0.5284	LR: 0.025000
Training Epoch: 78 [45824/50000]	Loss: 0.6083	LR: 0.025000
Training Epoch: 78 [45952/50000]	Loss: 0.4824	LR: 0.025000
Training Epoch: 78 [46080/50000]	Loss: 0.5462	LR: 0.025000
Training Epoch: 78 [46208/50000]	Loss: 0.4651	LR: 0.025000
Training Epoch: 78 [46336/50000]	Loss: 0.4377	LR: 0.025000
Training Epoch: 78 [46464/50000]	Loss: 0.3722	LR: 0.025000
Training Epoch: 78 [46592/50000]	Loss: 0.4516	LR: 0.025000
Training Epoch: 78 [46720/50000]	Loss: 0.5962	LR: 0.025000
Training Epoch: 78 [46848/50000]	Loss: 0.4500	LR: 0.025000
Training Epoch: 78 [46976/50000]	Loss: 0.6939	LR: 0.025000
Training Epoch: 78 [47104/50000]	Loss: 0.4460	LR: 0.025000
Training Epoch: 78 [47232/50000]	Loss: 0.5308	LR: 0.025000
Training Epoch: 78 [47360/50000]	Loss: 0.4765	LR: 0.025000
Training Epoch: 78 [47488/50000]	Loss: 0.5436	LR: 0.025000
Training Epoch: 78 [47616/50000]	Loss: 0.5353	LR: 0.025000
Training Epoch: 78 [47744/50000]	Loss: 0.4252	LR: 0.025000
Training Epoch: 78 [47872/50000]	Loss: 0.4632	LR: 0.025000
Training Epoch: 78 [48000/50000]	Loss: 0.4081	LR: 0.025000
Training Epoch: 78 [48128/50000]	Loss: 0.4164	LR: 0.025000
Training Epoch: 78 [48256/50000]	Loss: 0.6735	LR: 0.025000
Training Epoch: 78 [48384/50000]	Loss: 0.4246	LR: 0.025000
Training Epoch: 78 [48512/50000]	Loss: 0.5749	LR: 0.025000
Training Epoch: 78 [48640/50000]	Loss: 0.4224	LR: 0.025000
Training Epoch: 78 [48768/50000]	Loss: 0.4732	LR: 0.025000
Training Epoch: 78 [48896/50000]	Loss: 0.4607	LR: 0.025000
Training Epoch: 78 [49024/50000]	Loss: 0.4450	LR: 0.025000
Training Epoch: 78 [49152/50000]	Loss: 0.5329	LR: 0.025000
Training Epoch: 78 [49280/50000]	Loss: 0.5045	LR: 0.025000
Training Epoch: 78 [49408/50000]	Loss: 0.4441	LR: 0.025000
Training Epoch: 78 [49536/50000]	Loss: 0.3523	LR: 0.025000
Training Epoch: 78 [49664/50000]	Loss: 0.4474	LR: 0.025000
Training Epoch: 78 [49792/50000]	Loss: 0.5677	LR: 0.025000
Training Epoch: 78 [49920/50000]	Loss: 0.4962	LR: 0.025000
Training Epoch: 78 [50000/50000]	Loss: 0.4637	LR: 0.025000
epoch 78 training time consumed: 53.95s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  286264 GB |  286264 GB |
|       from large pool |  123392 KB |    1034 MB |  285982 GB |  285982 GB |
|       from small pool |   10798 KB |      13 MB |     282 GB |     282 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  286264 GB |  286264 GB |
|       from large pool |  123392 KB |    1034 MB |  285982 GB |  285982 GB |
|       from small pool |   10798 KB |      13 MB |     282 GB |     282 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  125973 GB |  125973 GB |
|       from large pool |  155136 KB |  433088 KB |  125661 GB |  125661 GB |
|       from small pool |    1490 KB |    3494 KB |     311 GB |     311 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   11046 K  |   11045 K  |
|       from large pool |      24    |      65    |    5765 K  |    5765 K  |
|       from small pool |     231    |     274    |    5280 K  |    5280 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   11046 K  |   11045 K  |
|       from large pool |      24    |      65    |    5765 K  |    5765 K  |
|       from small pool |     231    |     274    |    5280 K  |    5280 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5457 K  |    5457 K  |
|       from large pool |       9    |      14    |    2790 K  |    2790 K  |
|       from small pool |      12    |      16    |    2666 K  |    2666 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 78, Average loss: 0.0111, Accuracy: 0.6601, Time consumed:3.46s

Training Epoch: 79 [128/50000]	Loss: 0.4264	LR: 0.025000
Training Epoch: 79 [256/50000]	Loss: 0.3440	LR: 0.025000
Training Epoch: 79 [384/50000]	Loss: 0.4427	LR: 0.025000
Training Epoch: 79 [512/50000]	Loss: 0.5373	LR: 0.025000
Training Epoch: 79 [640/50000]	Loss: 0.4497	LR: 0.025000
Training Epoch: 79 [768/50000]	Loss: 0.3038	LR: 0.025000
Training Epoch: 79 [896/50000]	Loss: 0.4194	LR: 0.025000
Training Epoch: 79 [1024/50000]	Loss: 0.4501	LR: 0.025000
Training Epoch: 79 [1152/50000]	Loss: 0.5806	LR: 0.025000
Training Epoch: 79 [1280/50000]	Loss: 0.4328	LR: 0.025000
Training Epoch: 79 [1408/50000]	Loss: 0.4875	LR: 0.025000
Training Epoch: 79 [1536/50000]	Loss: 0.4316	LR: 0.025000
Training Epoch: 79 [1664/50000]	Loss: 0.3414	LR: 0.025000
Training Epoch: 79 [1792/50000]	Loss: 0.4961	LR: 0.025000
Training Epoch: 79 [1920/50000]	Loss: 0.2982	LR: 0.025000
Training Epoch: 79 [2048/50000]	Loss: 0.2957	LR: 0.025000
Training Epoch: 79 [2176/50000]	Loss: 0.4658	LR: 0.025000
Training Epoch: 79 [2304/50000]	Loss: 0.3447	LR: 0.025000
Training Epoch: 79 [2432/50000]	Loss: 0.4107	LR: 0.025000
Training Epoch: 79 [2560/50000]	Loss: 0.4122	LR: 0.025000
Training Epoch: 79 [2688/50000]	Loss: 0.3667	LR: 0.025000
Training Epoch: 79 [2816/50000]	Loss: 0.4229	LR: 0.025000
Training Epoch: 79 [2944/50000]	Loss: 0.3743	LR: 0.025000
Training Epoch: 79 [3072/50000]	Loss: 0.4832	LR: 0.025000
Training Epoch: 79 [3200/50000]	Loss: 0.3322	LR: 0.025000
Training Epoch: 79 [3328/50000]	Loss: 0.4466	LR: 0.025000
Training Epoch: 79 [3456/50000]	Loss: 0.3268	LR: 0.025000
Training Epoch: 79 [3584/50000]	Loss: 0.2960	LR: 0.025000
Training Epoch: 79 [3712/50000]	Loss: 0.3897	LR: 0.025000
Training Epoch: 79 [3840/50000]	Loss: 0.3051	LR: 0.025000
Training Epoch: 79 [3968/50000]	Loss: 0.2689	LR: 0.025000
Training Epoch: 79 [4096/50000]	Loss: 0.3945	LR: 0.025000
Training Epoch: 79 [4224/50000]	Loss: 0.4689	LR: 0.025000
Training Epoch: 79 [4352/50000]	Loss: 0.3498	LR: 0.025000
Training Epoch: 79 [4480/50000]	Loss: 0.3640	LR: 0.025000
Training Epoch: 79 [4608/50000]	Loss: 0.4925	LR: 0.025000
Training Epoch: 79 [4736/50000]	Loss: 0.3558	LR: 0.025000
Training Epoch: 79 [4864/50000]	Loss: 0.4866	LR: 0.025000
Training Epoch: 79 [4992/50000]	Loss: 0.4840	LR: 0.025000
Training Epoch: 79 [5120/50000]	Loss: 0.5067	LR: 0.025000
Training Epoch: 79 [5248/50000]	Loss: 0.3985	LR: 0.025000
Training Epoch: 79 [5376/50000]	Loss: 0.4180	LR: 0.025000
Training Epoch: 79 [5504/50000]	Loss: 0.3945	LR: 0.025000
Training Epoch: 79 [5632/50000]	Loss: 0.4683	LR: 0.025000
Training Epoch: 79 [5760/50000]	Loss: 0.4930	LR: 0.025000
Training Epoch: 79 [5888/50000]	Loss: 0.4368	LR: 0.025000
Training Epoch: 79 [6016/50000]	Loss: 0.4647	LR: 0.025000
Training Epoch: 79 [6144/50000]	Loss: 0.2895	LR: 0.025000
Training Epoch: 79 [6272/50000]	Loss: 0.3327	LR: 0.025000
Training Epoch: 79 [6400/50000]	Loss: 0.4796	LR: 0.025000
Training Epoch: 79 [6528/50000]	Loss: 0.3119	LR: 0.025000
Training Epoch: 79 [6656/50000]	Loss: 0.3975	LR: 0.025000
Training Epoch: 79 [6784/50000]	Loss: 0.3356	LR: 0.025000
Training Epoch: 79 [6912/50000]	Loss: 0.4751	LR: 0.025000
Training Epoch: 79 [7040/50000]	Loss: 0.3323	LR: 0.025000
Training Epoch: 79 [7168/50000]	Loss: 0.4351	LR: 0.025000
Training Epoch: 79 [7296/50000]	Loss: 0.4480	LR: 0.025000
Training Epoch: 79 [7424/50000]	Loss: 0.3605	LR: 0.025000
Training Epoch: 79 [7552/50000]	Loss: 0.4311	LR: 0.025000
Training Epoch: 79 [7680/50000]	Loss: 0.4041	LR: 0.025000
Training Epoch: 79 [7808/50000]	Loss: 0.5054	LR: 0.025000
Training Epoch: 79 [7936/50000]	Loss: 0.3656	LR: 0.025000
Training Epoch: 79 [8064/50000]	Loss: 0.4209	LR: 0.025000
Training Epoch: 79 [8192/50000]	Loss: 0.3095	LR: 0.025000
Training Epoch: 79 [8320/50000]	Loss: 0.3929	LR: 0.025000
Training Epoch: 79 [8448/50000]	Loss: 0.4241	LR: 0.025000
Training Epoch: 79 [8576/50000]	Loss: 0.4327	LR: 0.025000
Training Epoch: 79 [8704/50000]	Loss: 0.2973	LR: 0.025000
Training Epoch: 79 [8832/50000]	Loss: 0.4397	LR: 0.025000
Training Epoch: 79 [8960/50000]	Loss: 0.4468	LR: 0.025000
Training Epoch: 79 [9088/50000]	Loss: 0.3648	LR: 0.025000
Training Epoch: 79 [9216/50000]	Loss: 0.4215	LR: 0.025000
Training Epoch: 79 [9344/50000]	Loss: 0.4276	LR: 0.025000
Training Epoch: 79 [9472/50000]	Loss: 0.3330	LR: 0.025000
Training Epoch: 79 [9600/50000]	Loss: 0.4398	LR: 0.025000
Training Epoch: 79 [9728/50000]	Loss: 0.4304	LR: 0.025000
Training Epoch: 79 [9856/50000]	Loss: 0.5541	LR: 0.025000
Training Epoch: 79 [9984/50000]	Loss: 0.4264	LR: 0.025000
Training Epoch: 79 [10112/50000]	Loss: 0.3834	LR: 0.025000
Training Epoch: 79 [10240/50000]	Loss: 0.3618	LR: 0.025000
Training Epoch: 79 [10368/50000]	Loss: 0.3522	LR: 0.025000
Training Epoch: 79 [10496/50000]	Loss: 0.3659	LR: 0.025000
Training Epoch: 79 [10624/50000]	Loss: 0.4273	LR: 0.025000
Training Epoch: 79 [10752/50000]	Loss: 0.5487	LR: 0.025000
Training Epoch: 79 [10880/50000]	Loss: 0.3514	LR: 0.025000
Training Epoch: 79 [11008/50000]	Loss: 0.2991	LR: 0.025000
Training Epoch: 79 [11136/50000]	Loss: 0.4328	LR: 0.025000
Training Epoch: 79 [11264/50000]	Loss: 0.3648	LR: 0.025000
Training Epoch: 79 [11392/50000]	Loss: 0.3764	LR: 0.025000
Training Epoch: 79 [11520/50000]	Loss: 0.4430	LR: 0.025000
Training Epoch: 79 [11648/50000]	Loss: 0.5227	LR: 0.025000
Training Epoch: 79 [11776/50000]	Loss: 0.5278	LR: 0.025000
Training Epoch: 79 [11904/50000]	Loss: 0.3153	LR: 0.025000
Training Epoch: 79 [12032/50000]	Loss: 0.4190	LR: 0.025000
Training Epoch: 79 [12160/50000]	Loss: 0.4443	LR: 0.025000
Training Epoch: 79 [12288/50000]	Loss: 0.4354	LR: 0.025000
Training Epoch: 79 [12416/50000]	Loss: 0.5036	LR: 0.025000
Training Epoch: 79 [12544/50000]	Loss: 0.3973	LR: 0.025000
Training Epoch: 79 [12672/50000]	Loss: 0.2576	LR: 0.025000
Training Epoch: 79 [12800/50000]	Loss: 0.3359	LR: 0.025000
Training Epoch: 79 [12928/50000]	Loss: 0.3765	LR: 0.025000
Training Epoch: 79 [13056/50000]	Loss: 0.3978	LR: 0.025000
Training Epoch: 79 [13184/50000]	Loss: 0.3210	LR: 0.025000
Training Epoch: 79 [13312/50000]	Loss: 0.4246	LR: 0.025000
Training Epoch: 79 [13440/50000]	Loss: 0.3852	LR: 0.025000
Training Epoch: 79 [13568/50000]	Loss: 0.4003	LR: 0.025000
Training Epoch: 79 [13696/50000]	Loss: 0.4652	LR: 0.025000
Training Epoch: 79 [13824/50000]	Loss: 0.4823	LR: 0.025000
Training Epoch: 79 [13952/50000]	Loss: 0.3495	LR: 0.025000
Training Epoch: 79 [14080/50000]	Loss: 0.3716	LR: 0.025000
Training Epoch: 79 [14208/50000]	Loss: 0.4055	LR: 0.025000
Training Epoch: 79 [14336/50000]	Loss: 0.3684	LR: 0.025000
Training Epoch: 79 [14464/50000]	Loss: 0.3810	LR: 0.025000
Training Epoch: 79 [14592/50000]	Loss: 0.4688	LR: 0.025000
Training Epoch: 79 [14720/50000]	Loss: 0.5611	LR: 0.025000
Training Epoch: 79 [14848/50000]	Loss: 0.3922	LR: 0.025000
Training Epoch: 79 [14976/50000]	Loss: 0.4111	LR: 0.025000
Training Epoch: 79 [15104/50000]	Loss: 0.4964	LR: 0.025000
Training Epoch: 79 [15232/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 79 [15360/50000]	Loss: 0.4917	LR: 0.025000
Training Epoch: 79 [15488/50000]	Loss: 0.3327	LR: 0.025000
Training Epoch: 79 [15616/50000]	Loss: 0.3662	LR: 0.025000
Training Epoch: 79 [15744/50000]	Loss: 0.4363	LR: 0.025000
Training Epoch: 79 [15872/50000]	Loss: 0.3567	LR: 0.025000
Training Epoch: 79 [16000/50000]	Loss: 0.3983	LR: 0.025000
Training Epoch: 79 [16128/50000]	Loss: 0.3654	LR: 0.025000
Training Epoch: 79 [16256/50000]	Loss: 0.4498	LR: 0.025000
Training Epoch: 79 [16384/50000]	Loss: 0.3469	LR: 0.025000
Training Epoch: 79 [16512/50000]	Loss: 0.3978	LR: 0.025000
Training Epoch: 79 [16640/50000]	Loss: 0.4106	LR: 0.025000
Training Epoch: 79 [16768/50000]	Loss: 0.3458	LR: 0.025000
Training Epoch: 79 [16896/50000]	Loss: 0.4467	LR: 0.025000
Training Epoch: 79 [17024/50000]	Loss: 0.4498	LR: 0.025000
Training Epoch: 79 [17152/50000]	Loss: 0.2844	LR: 0.025000
Training Epoch: 79 [17280/50000]	Loss: 0.3856	LR: 0.025000
Training Epoch: 79 [17408/50000]	Loss: 0.3031	LR: 0.025000
Training Epoch: 79 [17536/50000]	Loss: 0.3353	LR: 0.025000
Training Epoch: 79 [17664/50000]	Loss: 0.3846	LR: 0.025000
Training Epoch: 79 [17792/50000]	Loss: 0.3261	LR: 0.025000
Training Epoch: 79 [17920/50000]	Loss: 0.4608	LR: 0.025000
Training Epoch: 79 [18048/50000]	Loss: 0.4983	LR: 0.025000
Training Epoch: 79 [18176/50000]	Loss: 0.5272	LR: 0.025000
Training Epoch: 79 [18304/50000]	Loss: 0.5140	LR: 0.025000
Training Epoch: 79 [18432/50000]	Loss: 0.4033	LR: 0.025000
Training Epoch: 79 [18560/50000]	Loss: 0.3888	LR: 0.025000
Training Epoch: 79 [18688/50000]	Loss: 0.3255	LR: 0.025000
Training Epoch: 79 [18816/50000]	Loss: 0.2660	LR: 0.025000
Training Epoch: 79 [18944/50000]	Loss: 0.4430	LR: 0.025000
Training Epoch: 79 [19072/50000]	Loss: 0.3095	LR: 0.025000
Training Epoch: 79 [19200/50000]	Loss: 0.4753	LR: 0.025000
Training Epoch: 79 [19328/50000]	Loss: 0.4981	LR: 0.025000
Training Epoch: 79 [19456/50000]	Loss: 0.4854	LR: 0.025000
Training Epoch: 79 [19584/50000]	Loss: 0.3729	LR: 0.025000
Training Epoch: 79 [19712/50000]	Loss: 0.5139	LR: 0.025000
Training Epoch: 79 [19840/50000]	Loss: 0.4850	LR: 0.025000
Training Epoch: 79 [19968/50000]	Loss: 0.3406	LR: 0.025000
Training Epoch: 79 [20096/50000]	Loss: 0.3987	LR: 0.025000
Training Epoch: 79 [20224/50000]	Loss: 0.3379	LR: 0.025000
Training Epoch: 79 [20352/50000]	Loss: 0.4302	LR: 0.025000
Training Epoch: 79 [20480/50000]	Loss: 0.4888	LR: 0.025000
Training Epoch: 79 [20608/50000]	Loss: 0.3686	LR: 0.025000
Training Epoch: 79 [20736/50000]	Loss: 0.4658	LR: 0.025000
Training Epoch: 79 [20864/50000]	Loss: 0.3111	LR: 0.025000
Training Epoch: 79 [20992/50000]	Loss: 0.3353	LR: 0.025000
Training Epoch: 79 [21120/50000]	Loss: 0.3521	LR: 0.025000
Training Epoch: 79 [21248/50000]	Loss: 0.4146	LR: 0.025000
Training Epoch: 79 [21376/50000]	Loss: 0.2655	LR: 0.025000
Training Epoch: 79 [21504/50000]	Loss: 0.4854	LR: 0.025000
Training Epoch: 79 [21632/50000]	Loss: 0.4678	LR: 0.025000
Training Epoch: 79 [21760/50000]	Loss: 0.5320	LR: 0.025000
Training Epoch: 79 [21888/50000]	Loss: 0.4140	LR: 0.025000
Training Epoch: 79 [22016/50000]	Loss: 0.3243	LR: 0.025000
Training Epoch: 79 [22144/50000]	Loss: 0.3478	LR: 0.025000
Training Epoch: 79 [22272/50000]	Loss: 0.4693	LR: 0.025000
Training Epoch: 79 [22400/50000]	Loss: 0.3975	LR: 0.025000
Training Epoch: 79 [22528/50000]	Loss: 0.4955	LR: 0.025000
Training Epoch: 79 [22656/50000]	Loss: 0.5302	LR: 0.025000
Training Epoch: 79 [22784/50000]	Loss: 0.3945	LR: 0.025000
Training Epoch: 79 [22912/50000]	Loss: 0.4870	LR: 0.025000
Training Epoch: 79 [23040/50000]	Loss: 0.3692	LR: 0.025000
Training Epoch: 79 [23168/50000]	Loss: 0.4363	LR: 0.025000
Training Epoch: 79 [23296/50000]	Loss: 0.3120	LR: 0.025000
Training Epoch: 79 [23424/50000]	Loss: 0.4062	LR: 0.025000
Training Epoch: 79 [23552/50000]	Loss: 0.3832	LR: 0.025000
Training Epoch: 79 [23680/50000]	Loss: 0.4075	LR: 0.025000
Training Epoch: 79 [23808/50000]	Loss: 0.3202	LR: 0.025000
Training Epoch: 79 [23936/50000]	Loss: 0.4099	LR: 0.025000
Training Epoch: 79 [24064/50000]	Loss: 0.3809	LR: 0.025000
Training Epoch: 79 [24192/50000]	Loss: 0.4874	LR: 0.025000
Training Epoch: 79 [24320/50000]	Loss: 0.4982	LR: 0.025000
Training Epoch: 79 [24448/50000]	Loss: 0.5063	LR: 0.025000
Training Epoch: 79 [24576/50000]	Loss: 0.3738	LR: 0.025000
Training Epoch: 79 [24704/50000]	Loss: 0.5276	LR: 0.025000
Training Epoch: 79 [24832/50000]	Loss: 0.4060	LR: 0.025000
Training Epoch: 79 [24960/50000]	Loss: 0.3594	LR: 0.025000
Training Epoch: 79 [25088/50000]	Loss: 0.5422	LR: 0.025000
Training Epoch: 79 [25216/50000]	Loss: 0.4644	LR: 0.025000
Training Epoch: 79 [25344/50000]	Loss: 0.4888	LR: 0.025000
Training Epoch: 79 [25472/50000]	Loss: 0.4897	LR: 0.025000
Training Epoch: 79 [25600/50000]	Loss: 0.4534	LR: 0.025000
Training Epoch: 79 [25728/50000]	Loss: 0.4301	LR: 0.025000
Training Epoch: 79 [25856/50000]	Loss: 0.5340	LR: 0.025000
Training Epoch: 79 [25984/50000]	Loss: 0.4532	LR: 0.025000
Training Epoch: 79 [26112/50000]	Loss: 0.4076	LR: 0.025000
Training Epoch: 79 [26240/50000]	Loss: 0.5541	LR: 0.025000
Training Epoch: 79 [26368/50000]	Loss: 0.5065	LR: 0.025000
Training Epoch: 79 [26496/50000]	Loss: 0.3748	LR: 0.025000
Training Epoch: 79 [26624/50000]	Loss: 0.4624	LR: 0.025000
Training Epoch: 79 [26752/50000]	Loss: 0.3956	LR: 0.025000
Training Epoch: 79 [26880/50000]	Loss: 0.4986	LR: 0.025000
Training Epoch: 79 [27008/50000]	Loss: 0.4142	LR: 0.025000
Training Epoch: 79 [27136/50000]	Loss: 0.3585	LR: 0.025000
Training Epoch: 79 [27264/50000]	Loss: 0.5108	LR: 0.025000
Training Epoch: 79 [27392/50000]	Loss: 0.4496	LR: 0.025000
Training Epoch: 79 [27520/50000]	Loss: 0.4159	LR: 0.025000
Training Epoch: 79 [27648/50000]	Loss: 0.4354	LR: 0.025000
Training Epoch: 79 [27776/50000]	Loss: 0.5033	LR: 0.025000
Training Epoch: 79 [27904/50000]	Loss: 0.5024	LR: 0.025000
Training Epoch: 79 [28032/50000]	Loss: 0.3642	LR: 0.025000
Training Epoch: 79 [28160/50000]	Loss: 0.5797	LR: 0.025000
Training Epoch: 79 [28288/50000]	Loss: 0.5247	LR: 0.025000
Training Epoch: 79 [28416/50000]	Loss: 0.4879	LR: 0.025000
Training Epoch: 79 [28544/50000]	Loss: 0.4326	LR: 0.025000
Training Epoch: 79 [28672/50000]	Loss: 0.3558	LR: 0.025000
Training Epoch: 79 [28800/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 79 [28928/50000]	Loss: 0.3546	LR: 0.025000
Training Epoch: 79 [29056/50000]	Loss: 0.5253	LR: 0.025000
Training Epoch: 79 [29184/50000]	Loss: 0.2661	LR: 0.025000
Training Epoch: 79 [29312/50000]	Loss: 0.4653	LR: 0.025000
Training Epoch: 79 [29440/50000]	Loss: 0.5305	LR: 0.025000
Training Epoch: 79 [29568/50000]	Loss: 0.4976	LR: 0.025000
Training Epoch: 79 [29696/50000]	Loss: 0.3603	LR: 0.025000
Training Epoch: 79 [29824/50000]	Loss: 0.5614	LR: 0.025000
Training Epoch: 79 [29952/50000]	Loss: 0.4749	LR: 0.025000
Training Epoch: 79 [30080/50000]	Loss: 0.6044	LR: 0.025000
Training Epoch: 79 [30208/50000]	Loss: 0.3966	LR: 0.025000
Training Epoch: 79 [30336/50000]	Loss: 0.5833	LR: 0.025000
Training Epoch: 79 [30464/50000]	Loss: 0.5969	LR: 0.025000
Training Epoch: 79 [30592/50000]	Loss: 0.4204	LR: 0.025000
Training Epoch: 79 [30720/50000]	Loss: 0.3542	LR: 0.025000
Training Epoch: 79 [30848/50000]	Loss: 0.4645	LR: 0.025000
Training Epoch: 79 [30976/50000]	Loss: 0.6826	LR: 0.025000
Training Epoch: 79 [31104/50000]	Loss: 0.5031	LR: 0.025000
Training Epoch: 79 [31232/50000]	Loss: 0.4260	LR: 0.025000
Training Epoch: 79 [31360/50000]	Loss: 0.5214	LR: 0.025000
Training Epoch: 79 [31488/50000]	Loss: 0.3996	LR: 0.025000
Training Epoch: 79 [31616/50000]	Loss: 0.4297	LR: 0.025000
Training Epoch: 79 [31744/50000]	Loss: 0.3600	LR: 0.025000
Training Epoch: 79 [31872/50000]	Loss: 0.5296	LR: 0.025000
Training Epoch: 79 [32000/50000]	Loss: 0.4987	LR: 0.025000
Training Epoch: 79 [32128/50000]	Loss: 0.5432	LR: 0.025000
Training Epoch: 79 [32256/50000]	Loss: 0.4451	LR: 0.025000
Training Epoch: 79 [32384/50000]	Loss: 0.5052	LR: 0.025000
Training Epoch: 79 [32512/50000]	Loss: 0.5923	LR: 0.025000
Training Epoch: 79 [32640/50000]	Loss: 0.4744	LR: 0.025000
Training Epoch: 79 [32768/50000]	Loss: 0.4642	LR: 0.025000
Training Epoch: 79 [32896/50000]	Loss: 0.4556	LR: 0.025000
Training Epoch: 79 [33024/50000]	Loss: 0.5358	LR: 0.025000
Training Epoch: 79 [33152/50000]	Loss: 0.5353	LR: 0.025000
Training Epoch: 79 [33280/50000]	Loss: 0.6214	LR: 0.025000
Training Epoch: 79 [33408/50000]	Loss: 0.4769	LR: 0.025000
Training Epoch: 79 [33536/50000]	Loss: 0.4333	LR: 0.025000
Training Epoch: 79 [33664/50000]	Loss: 0.5136	LR: 0.025000
Training Epoch: 79 [33792/50000]	Loss: 0.5904	LR: 0.025000
Training Epoch: 79 [33920/50000]	Loss: 0.4879	LR: 0.025000
Training Epoch: 79 [34048/50000]	Loss: 0.5476	LR: 0.025000
Training Epoch: 79 [34176/50000]	Loss: 0.5802	LR: 0.025000
Training Epoch: 79 [34304/50000]	Loss: 0.6951	LR: 0.025000
Training Epoch: 79 [34432/50000]	Loss: 0.4036	LR: 0.025000
Training Epoch: 79 [34560/50000]	Loss: 0.5446	LR: 0.025000
Training Epoch: 79 [34688/50000]	Loss: 0.4996	LR: 0.025000
Training Epoch: 79 [34816/50000]	Loss: 0.3902	LR: 0.025000
Training Epoch: 79 [34944/50000]	Loss: 0.4299	LR: 0.025000
Training Epoch: 79 [35072/50000]	Loss: 0.4977	LR: 0.025000
Training Epoch: 79 [35200/50000]	Loss: 0.6314	LR: 0.025000
Training Epoch: 79 [35328/50000]	Loss: 0.4134	LR: 0.025000
Training Epoch: 79 [35456/50000]	Loss: 0.5151	LR: 0.025000
Training Epoch: 79 [35584/50000]	Loss: 0.5126	LR: 0.025000
Training Epoch: 79 [35712/50000]	Loss: 0.6094	LR: 0.025000
Training Epoch: 79 [35840/50000]	Loss: 0.6022	LR: 0.025000
Training Epoch: 79 [35968/50000]	Loss: 0.4241	LR: 0.025000
Training Epoch: 79 [36096/50000]	Loss: 0.4428	LR: 0.025000
Training Epoch: 79 [36224/50000]	Loss: 0.4744	LR: 0.025000
Training Epoch: 79 [36352/50000]	Loss: 0.5476	LR: 0.025000
Training Epoch: 79 [36480/50000]	Loss: 0.5910	LR: 0.025000
Training Epoch: 79 [36608/50000]	Loss: 0.4654	LR: 0.025000
Training Epoch: 79 [36736/50000]	Loss: 0.7501	LR: 0.025000
Training Epoch: 79 [36864/50000]	Loss: 0.5677	LR: 0.025000
Training Epoch: 79 [36992/50000]	Loss: 0.5329	LR: 0.025000
Training Epoch: 79 [37120/50000]	Loss: 0.4255	LR: 0.025000
Training Epoch: 79 [37248/50000]	Loss: 0.5343	LR: 0.025000
Training Epoch: 79 [37376/50000]	Loss: 0.4610	LR: 0.025000
Training Epoch: 79 [37504/50000]	Loss: 0.3219	LR: 0.025000
Training Epoch: 79 [37632/50000]	Loss: 0.5082	LR: 0.025000
Training Epoch: 79 [37760/50000]	Loss: 0.5322	LR: 0.025000
Training Epoch: 79 [37888/50000]	Loss: 0.5240	LR: 0.025000
Training Epoch: 79 [38016/50000]	Loss: 0.4256	LR: 0.025000
Training Epoch: 79 [38144/50000]	Loss: 0.4959	LR: 0.025000
Training Epoch: 79 [38272/50000]	Loss: 0.4324	LR: 0.025000
Training Epoch: 79 [38400/50000]	Loss: 0.6552	LR: 0.025000
Training Epoch: 79 [38528/50000]	Loss: 0.5580	LR: 0.025000
Training Epoch: 79 [38656/50000]	Loss: 0.2785	LR: 0.025000
Training Epoch: 79 [38784/50000]	Loss: 0.6727	LR: 0.025000
Training Epoch: 79 [38912/50000]	Loss: 0.5033	LR: 0.025000
Training Epoch: 79 [39040/50000]	Loss: 0.5591	LR: 0.025000
Training Epoch: 79 [39168/50000]	Loss: 0.4335	LR: 0.025000
Training Epoch: 79 [39296/50000]	Loss: 0.5414	LR: 0.025000
Training Epoch: 79 [39424/50000]	Loss: 0.5181	LR: 0.025000
Training Epoch: 79 [39552/50000]	Loss: 0.3133	LR: 0.025000
Training Epoch: 79 [39680/50000]	Loss: 0.5025	LR: 0.025000
Training Epoch: 79 [39808/50000]	Loss: 0.4504	LR: 0.025000
Training Epoch: 79 [39936/50000]	Loss: 0.4133	LR: 0.025000
Training Epoch: 79 [40064/50000]	Loss: 0.5985	LR: 0.025000
Training Epoch: 79 [40192/50000]	Loss: 0.5818	LR: 0.025000
Training Epoch: 79 [40320/50000]	Loss: 0.4177	LR: 0.025000
Training Epoch: 79 [40448/50000]	Loss: 0.4188	LR: 0.025000
Training Epoch: 79 [40576/50000]	Loss: 0.4732	LR: 0.025000
Training Epoch: 79 [40704/50000]	Loss: 0.4219	LR: 0.025000
Training Epoch: 79 [40832/50000]	Loss: 0.5017	LR: 0.025000
Training Epoch: 79 [40960/50000]	Loss: 0.3051	LR: 0.025000
Training Epoch: 79 [41088/50000]	Loss: 0.4361	LR: 0.025000
Training Epoch: 79 [41216/50000]	Loss: 0.3477	LR: 0.025000
Training Epoch: 79 [41344/50000]	Loss: 0.5305	LR: 0.025000
Training Epoch: 79 [41472/50000]	Loss: 0.4562	LR: 0.025000
Training Epoch: 79 [41600/50000]	Loss: 0.4286	LR: 0.025000
Training Epoch: 79 [41728/50000]	Loss: 0.4894	LR: 0.025000
Training Epoch: 79 [41856/50000]	Loss: 0.4424	LR: 0.025000
Training Epoch: 79 [41984/50000]	Loss: 0.4796	LR: 0.025000
Training Epoch: 79 [42112/50000]	Loss: 0.5884	LR: 0.025000
Training Epoch: 79 [42240/50000]	Loss: 0.5419	LR: 0.025000
Training Epoch: 79 [42368/50000]	Loss: 0.3865	LR: 0.025000
Training Epoch: 79 [42496/50000]	Loss: 0.4694	LR: 0.025000
Training Epoch: 79 [42624/50000]	Loss: 0.4400	LR: 0.025000
Training Epoch: 79 [42752/50000]	Loss: 0.5325	LR: 0.025000
Training Epoch: 79 [42880/50000]	Loss: 0.5169	LR: 0.025000
Training Epoch: 79 [43008/50000]	Loss: 0.4767	LR: 0.025000
Training Epoch: 79 [43136/50000]	Loss: 0.5280	LR: 0.025000
Training Epoch: 79 [43264/50000]	Loss: 0.4117	LR: 0.025000
Training Epoch: 79 [43392/50000]	Loss: 0.7171	LR: 0.025000
Training Epoch: 79 [43520/50000]	Loss: 0.4465	LR: 0.025000
Training Epoch: 79 [43648/50000]	Loss: 0.3734	LR: 0.025000
Training Epoch: 79 [43776/50000]	Loss: 0.5890	LR: 0.025000
Training Epoch: 79 [43904/50000]	Loss: 0.4678	LR: 0.025000
Training Epoch: 79 [44032/50000]	Loss: 0.6077	LR: 0.025000
Training Epoch: 79 [44160/50000]	Loss: 0.4448	LR: 0.025000
Training Epoch: 79 [44288/50000]	Loss: 0.3979	LR: 0.025000
Training Epoch: 79 [44416/50000]	Loss: 0.6129	LR: 0.025000
Training Epoch: 79 [44544/50000]	Loss: 0.4676	LR: 0.025000
Training Epoch: 79 [44672/50000]	Loss: 0.6891	LR: 0.025000
Training Epoch: 79 [44800/50000]	Loss: 0.5484	LR: 0.025000
Training Epoch: 79 [44928/50000]	Loss: 0.4766	LR: 0.025000
Training Epoch: 79 [45056/50000]	Loss: 0.3749	LR: 0.025000
Training Epoch: 79 [45184/50000]	Loss: 0.5458	LR: 0.025000
Training Epoch: 79 [45312/50000]	Loss: 0.4700	LR: 0.025000
Training Epoch: 79 [45440/50000]	Loss: 0.4756	LR: 0.025000
Training Epoch: 79 [45568/50000]	Loss: 0.7420	LR: 0.025000
Training Epoch: 79 [45696/50000]	Loss: 0.6229	LR: 0.025000
Training Epoch: 79 [45824/50000]	Loss: 0.4182	LR: 0.025000
Training Epoch: 79 [45952/50000]	Loss: 0.5953	LR: 0.025000
Training Epoch: 79 [46080/50000]	Loss: 0.5148	LR: 0.025000
Training Epoch: 79 [46208/50000]	Loss: 0.4729	LR: 0.025000
Training Epoch: 79 [46336/50000]	Loss: 0.4775	LR: 0.025000
Training Epoch: 79 [46464/50000]	Loss: 0.5179	LR: 0.025000
Training Epoch: 79 [46592/50000]	Loss: 0.3893	LR: 0.025000
Training Epoch: 79 [46720/50000]	Loss: 0.5488	LR: 0.025000
Training Epoch: 79 [46848/50000]	Loss: 0.5289	LR: 0.025000
Training Epoch: 79 [46976/50000]	Loss: 0.5620	LR: 0.025000
Training Epoch: 79 [47104/50000]	Loss: 0.4468	LR: 0.025000
Training Epoch: 79 [47232/50000]	Loss: 0.5405	LR: 0.025000
Training Epoch: 79 [47360/50000]	Loss: 0.3435	LR: 0.025000
Training Epoch: 79 [47488/50000]	Loss: 0.5416	LR: 0.025000
Training Epoch: 79 [47616/50000]	Loss: 0.7108	LR: 0.025000
Training Epoch: 79 [47744/50000]	Loss: 0.5230	LR: 0.025000
Training Epoch: 79 [47872/50000]	Loss: 0.6221	LR: 0.025000
Training Epoch: 79 [48000/50000]	Loss: 0.4502	LR: 0.025000
Training Epoch: 79 [48128/50000]	Loss: 0.7147	LR: 0.025000
Training Epoch: 79 [48256/50000]	Loss: 0.6208	LR: 0.025000
Training Epoch: 79 [48384/50000]	Loss: 0.4881	LR: 0.025000
Training Epoch: 79 [48512/50000]	Loss: 0.4920	LR: 0.025000
Training Epoch: 79 [48640/50000]	Loss: 0.5300	LR: 0.025000
Training Epoch: 79 [48768/50000]	Loss: 0.5103	LR: 0.025000
Training Epoch: 79 [48896/50000]	Loss: 0.4198	LR: 0.025000
Training Epoch: 79 [49024/50000]	Loss: 0.6348	LR: 0.025000
Training Epoch: 79 [49152/50000]	Loss: 0.4874	LR: 0.025000
Training Epoch: 79 [49280/50000]	Loss: 0.5025	LR: 0.025000
Training Epoch: 79 [49408/50000]	Loss: 0.4063	LR: 0.025000
Training Epoch: 79 [49536/50000]	Loss: 0.5349	LR: 0.025000
Training Epoch: 79 [49664/50000]	Loss: 0.5752	LR: 0.025000
Training Epoch: 79 [49792/50000]	Loss: 0.4789	LR: 0.025000
Training Epoch: 79 [49920/50000]	Loss: 0.4810	LR: 0.025000
Training Epoch: 79 [50000/50000]	Loss: 0.6440	LR: 0.025000
epoch 79 training time consumed: 53.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  289934 GB |  289934 GB |
|       from large pool |  123392 KB |    1034 MB |  289649 GB |  289649 GB |
|       from small pool |   10798 KB |      13 MB |     285 GB |     285 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  289934 GB |  289934 GB |
|       from large pool |  123392 KB |    1034 MB |  289649 GB |  289649 GB |
|       from small pool |   10798 KB |      13 MB |     285 GB |     285 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  127588 GB |  127588 GB |
|       from large pool |  155136 KB |  433088 KB |  127272 GB |  127272 GB |
|       from small pool |    1490 KB |    3494 KB |     315 GB |     315 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   11187 K  |   11187 K  |
|       from large pool |      24    |      65    |    5839 K  |    5839 K  |
|       from small pool |     231    |     274    |    5347 K  |    5347 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   11187 K  |   11187 K  |
|       from large pool |      24    |      65    |    5839 K  |    5839 K  |
|       from small pool |     231    |     274    |    5347 K  |    5347 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5527 K  |    5527 K  |
|       from large pool |       9    |      14    |    2826 K  |    2826 K  |
|       from small pool |      12    |      16    |    2700 K  |    2700 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 79, Average loss: 0.0115, Accuracy: 0.6544, Time consumed:3.46s

Training Epoch: 80 [128/50000]	Loss: 0.3498	LR: 0.012500
Training Epoch: 80 [256/50000]	Loss: 0.3661	LR: 0.012500
Training Epoch: 80 [384/50000]	Loss: 0.3225	LR: 0.012500
Training Epoch: 80 [512/50000]	Loss: 0.5400	LR: 0.012500
Training Epoch: 80 [640/50000]	Loss: 0.3450	LR: 0.012500
Training Epoch: 80 [768/50000]	Loss: 0.4166	LR: 0.012500
Training Epoch: 80 [896/50000]	Loss: 0.3785	LR: 0.012500
Training Epoch: 80 [1024/50000]	Loss: 0.4387	LR: 0.012500
Training Epoch: 80 [1152/50000]	Loss: 0.3235	LR: 0.012500
Training Epoch: 80 [1280/50000]	Loss: 0.3613	LR: 0.012500
Training Epoch: 80 [1408/50000]	Loss: 0.4357	LR: 0.012500
Training Epoch: 80 [1536/50000]	Loss: 0.3100	LR: 0.012500
Training Epoch: 80 [1664/50000]	Loss: 0.3852	LR: 0.012500
Training Epoch: 80 [1792/50000]	Loss: 0.4222	LR: 0.012500
Training Epoch: 80 [1920/50000]	Loss: 0.3765	LR: 0.012500
Training Epoch: 80 [2048/50000]	Loss: 0.4537	LR: 0.012500
Training Epoch: 80 [2176/50000]	Loss: 0.3360	LR: 0.012500
Training Epoch: 80 [2304/50000]	Loss: 0.3395	LR: 0.012500
Training Epoch: 80 [2432/50000]	Loss: 0.3275	LR: 0.012500
Training Epoch: 80 [2560/50000]	Loss: 0.3839	LR: 0.012500
Training Epoch: 80 [2688/50000]	Loss: 0.3045	LR: 0.012500
Training Epoch: 80 [2816/50000]	Loss: 0.3392	LR: 0.012500
Training Epoch: 80 [2944/50000]	Loss: 0.3686	LR: 0.012500
Training Epoch: 80 [3072/50000]	Loss: 0.2765	LR: 0.012500
Training Epoch: 80 [3200/50000]	Loss: 0.3549	LR: 0.012500
Training Epoch: 80 [3328/50000]	Loss: 0.2736	LR: 0.012500
Training Epoch: 80 [3456/50000]	Loss: 0.2543	LR: 0.012500
Training Epoch: 80 [3584/50000]	Loss: 0.2865	LR: 0.012500
Training Epoch: 80 [3712/50000]	Loss: 0.3775	LR: 0.012500
Training Epoch: 80 [3840/50000]	Loss: 0.3727	LR: 0.012500
Training Epoch: 80 [3968/50000]	Loss: 0.2878	LR: 0.012500
Training Epoch: 80 [4096/50000]	Loss: 0.3226	LR: 0.012500
Training Epoch: 80 [4224/50000]	Loss: 0.2620	LR: 0.012500
Training Epoch: 80 [4352/50000]	Loss: 0.2696	LR: 0.012500
Training Epoch: 80 [4480/50000]	Loss: 0.2316	LR: 0.012500
Training Epoch: 80 [4608/50000]	Loss: 0.3445	LR: 0.012500
Training Epoch: 80 [4736/50000]	Loss: 0.2550	LR: 0.012500
Training Epoch: 80 [4864/50000]	Loss: 0.2187	LR: 0.012500
Training Epoch: 80 [4992/50000]	Loss: 0.2668	LR: 0.012500
Training Epoch: 80 [5120/50000]	Loss: 0.3389	LR: 0.012500
Training Epoch: 80 [5248/50000]	Loss: 0.3699	LR: 0.012500
Training Epoch: 80 [5376/50000]	Loss: 0.2571	LR: 0.012500
Training Epoch: 80 [5504/50000]	Loss: 0.3386	LR: 0.012500
Training Epoch: 80 [5632/50000]	Loss: 0.2314	LR: 0.012500
Training Epoch: 80 [5760/50000]	Loss: 0.2066	LR: 0.012500
Training Epoch: 80 [5888/50000]	Loss: 0.2235	LR: 0.012500
Training Epoch: 80 [6016/50000]	Loss: 0.2977	LR: 0.012500
Training Epoch: 80 [6144/50000]	Loss: 0.2764	LR: 0.012500
Training Epoch: 80 [6272/50000]	Loss: 0.2200	LR: 0.012500
Training Epoch: 80 [6400/50000]	Loss: 0.2243	LR: 0.012500
Training Epoch: 80 [6528/50000]	Loss: 0.3439	LR: 0.012500
Training Epoch: 80 [6656/50000]	Loss: 0.2877	LR: 0.012500
Training Epoch: 80 [6784/50000]	Loss: 0.4230	LR: 0.012500
Training Epoch: 80 [6912/50000]	Loss: 0.3153	LR: 0.012500
Training Epoch: 80 [7040/50000]	Loss: 0.1939	LR: 0.012500
Training Epoch: 80 [7168/50000]	Loss: 0.1685	LR: 0.012500
Training Epoch: 80 [7296/50000]	Loss: 0.2688	LR: 0.012500
Training Epoch: 80 [7424/50000]	Loss: 0.3130	LR: 0.012500
Training Epoch: 80 [7552/50000]	Loss: 0.3771	LR: 0.012500
Training Epoch: 80 [7680/50000]	Loss: 0.2929	LR: 0.012500
Training Epoch: 80 [7808/50000]	Loss: 0.4411	LR: 0.012500
Training Epoch: 80 [7936/50000]	Loss: 0.2712	LR: 0.012500
Training Epoch: 80 [8064/50000]	Loss: 0.3338	LR: 0.012500
Training Epoch: 80 [8192/50000]	Loss: 0.3125	LR: 0.012500
Training Epoch: 80 [8320/50000]	Loss: 0.3140	LR: 0.012500
Training Epoch: 80 [8448/50000]	Loss: 0.2281	LR: 0.012500
Training Epoch: 80 [8576/50000]	Loss: 0.2179	LR: 0.012500
Training Epoch: 80 [8704/50000]	Loss: 0.2590	LR: 0.012500
Training Epoch: 80 [8832/50000]	Loss: 0.3007	LR: 0.012500
Training Epoch: 80 [8960/50000]	Loss: 0.1850	LR: 0.012500
Training Epoch: 80 [9088/50000]	Loss: 0.2457	LR: 0.012500
Training Epoch: 80 [9216/50000]	Loss: 0.1981	LR: 0.012500
Training Epoch: 80 [9344/50000]	Loss: 0.2056	LR: 0.012500
Training Epoch: 80 [9472/50000]	Loss: 0.2506	LR: 0.012500
Training Epoch: 80 [9600/50000]	Loss: 0.2270	LR: 0.012500
Training Epoch: 80 [9728/50000]	Loss: 0.2006	LR: 0.012500
Training Epoch: 80 [9856/50000]	Loss: 0.2218	LR: 0.012500
Training Epoch: 80 [9984/50000]	Loss: 0.1711	LR: 0.012500
Training Epoch: 80 [10112/50000]	Loss: 0.2908	LR: 0.012500
Training Epoch: 80 [10240/50000]	Loss: 0.2365	LR: 0.012500
Training Epoch: 80 [10368/50000]	Loss: 0.2089	LR: 0.012500
Training Epoch: 80 [10496/50000]	Loss: 0.2108	LR: 0.012500
Training Epoch: 80 [10624/50000]	Loss: 0.2626	LR: 0.012500
Training Epoch: 80 [10752/50000]	Loss: 0.2295	LR: 0.012500
Training Epoch: 80 [10880/50000]	Loss: 0.1808	LR: 0.012500
Training Epoch: 80 [11008/50000]	Loss: 0.2298	LR: 0.012500
Training Epoch: 80 [11136/50000]	Loss: 0.2047	LR: 0.012500
Training Epoch: 80 [11264/50000]	Loss: 0.3110	LR: 0.012500
Training Epoch: 80 [11392/50000]	Loss: 0.2306	LR: 0.012500
Training Epoch: 80 [11520/50000]	Loss: 0.2475	LR: 0.012500
Training Epoch: 80 [11648/50000]	Loss: 0.2188	LR: 0.012500
Training Epoch: 80 [11776/50000]	Loss: 0.2638	LR: 0.012500
Training Epoch: 80 [11904/50000]	Loss: 0.2332	LR: 0.012500
Training Epoch: 80 [12032/50000]	Loss: 0.1956	LR: 0.012500
Training Epoch: 80 [12160/50000]	Loss: 0.3345	LR: 0.012500
Training Epoch: 80 [12288/50000]	Loss: 0.2080	LR: 0.012500
Training Epoch: 80 [12416/50000]	Loss: 0.2525	LR: 0.012500
Training Epoch: 80 [12544/50000]	Loss: 0.2661	LR: 0.012500
Training Epoch: 80 [12672/50000]	Loss: 0.2819	LR: 0.012500
Training Epoch: 80 [12800/50000]	Loss: 0.1862	LR: 0.012500
Training Epoch: 80 [12928/50000]	Loss: 0.2324	LR: 0.012500
Training Epoch: 80 [13056/50000]	Loss: 0.1883	LR: 0.012500
Training Epoch: 80 [13184/50000]	Loss: 0.1673	LR: 0.012500
Training Epoch: 80 [13312/50000]	Loss: 0.2112	LR: 0.012500
Training Epoch: 80 [13440/50000]	Loss: 0.2571	LR: 0.012500
Training Epoch: 80 [13568/50000]	Loss: 0.3505	LR: 0.012500
Training Epoch: 80 [13696/50000]	Loss: 0.2192	LR: 0.012500
Training Epoch: 80 [13824/50000]	Loss: 0.2147	LR: 0.012500
Training Epoch: 80 [13952/50000]	Loss: 0.1851	LR: 0.012500
Training Epoch: 80 [14080/50000]	Loss: 0.2351	LR: 0.012500
Training Epoch: 80 [14208/50000]	Loss: 0.3180	LR: 0.012500
Training Epoch: 80 [14336/50000]	Loss: 0.2552	LR: 0.012500
Training Epoch: 80 [14464/50000]	Loss: 0.2512	LR: 0.012500
Training Epoch: 80 [14592/50000]	Loss: 0.1791	LR: 0.012500
Training Epoch: 80 [14720/50000]	Loss: 0.1955	LR: 0.012500
Training Epoch: 80 [14848/50000]	Loss: 0.3250	LR: 0.012500
Training Epoch: 80 [14976/50000]	Loss: 0.2656	LR: 0.012500
Training Epoch: 80 [15104/50000]	Loss: 0.1809	LR: 0.012500
Training Epoch: 80 [15232/50000]	Loss: 0.2773	LR: 0.012500
Training Epoch: 80 [15360/50000]	Loss: 0.2321	LR: 0.012500
Training Epoch: 80 [15488/50000]	Loss: 0.2106	LR: 0.012500
Training Epoch: 80 [15616/50000]	Loss: 0.2438	LR: 0.012500
Training Epoch: 80 [15744/50000]	Loss: 0.2819	LR: 0.012500
Training Epoch: 80 [15872/50000]	Loss: 0.2483	LR: 0.012500
Training Epoch: 80 [16000/50000]	Loss: 0.2578	LR: 0.012500
Training Epoch: 80 [16128/50000]	Loss: 0.2506	LR: 0.012500
Training Epoch: 80 [16256/50000]	Loss: 0.1665	LR: 0.012500
Training Epoch: 80 [16384/50000]	Loss: 0.2412	LR: 0.012500
Training Epoch: 80 [16512/50000]	Loss: 0.2199	LR: 0.012500
Training Epoch: 80 [16640/50000]	Loss: 0.2016	LR: 0.012500
Training Epoch: 80 [16768/50000]	Loss: 0.2353	LR: 0.012500
Training Epoch: 80 [16896/50000]	Loss: 0.2763	LR: 0.012500
Training Epoch: 80 [17024/50000]	Loss: 0.2053	LR: 0.012500
Training Epoch: 80 [17152/50000]	Loss: 0.2332	LR: 0.012500
Training Epoch: 80 [17280/50000]	Loss: 0.2104	LR: 0.012500
Training Epoch: 80 [17408/50000]	Loss: 0.1446	LR: 0.012500
Training Epoch: 80 [17536/50000]	Loss: 0.2994	LR: 0.012500
Training Epoch: 80 [17664/50000]	Loss: 0.3249	LR: 0.012500
Training Epoch: 80 [17792/50000]	Loss: 0.1975	LR: 0.012500
Training Epoch: 80 [17920/50000]	Loss: 0.1737	LR: 0.012500
Training Epoch: 80 [18048/50000]	Loss: 0.2652	LR: 0.012500
Training Epoch: 80 [18176/50000]	Loss: 0.2760	LR: 0.012500
Training Epoch: 80 [18304/50000]	Loss: 0.3481	LR: 0.012500
Training Epoch: 80 [18432/50000]	Loss: 0.2314	LR: 0.012500
Training Epoch: 80 [18560/50000]	Loss: 0.2809	LR: 0.012500
Training Epoch: 80 [18688/50000]	Loss: 0.2425	LR: 0.012500
Training Epoch: 80 [18816/50000]	Loss: 0.1881	LR: 0.012500
Training Epoch: 80 [18944/50000]	Loss: 0.1944	LR: 0.012500
Training Epoch: 80 [19072/50000]	Loss: 0.2033	LR: 0.012500
Training Epoch: 80 [19200/50000]	Loss: 0.2452	LR: 0.012500
Training Epoch: 80 [19328/50000]	Loss: 0.1848	LR: 0.012500
Training Epoch: 80 [19456/50000]	Loss: 0.2216	LR: 0.012500
Training Epoch: 80 [19584/50000]	Loss: 0.2474	LR: 0.012500
Training Epoch: 80 [19712/50000]	Loss: 0.2308	LR: 0.012500
Training Epoch: 80 [19840/50000]	Loss: 0.2549	LR: 0.012500
Training Epoch: 80 [19968/50000]	Loss: 0.2060	LR: 0.012500
Training Epoch: 80 [20096/50000]	Loss: 0.2568	LR: 0.012500
Training Epoch: 80 [20224/50000]	Loss: 0.1959	LR: 0.012500
Training Epoch: 80 [20352/50000]	Loss: 0.2507	LR: 0.012500
Training Epoch: 80 [20480/50000]	Loss: 0.2560	LR: 0.012500
Training Epoch: 80 [20608/50000]	Loss: 0.1555	LR: 0.012500
Training Epoch: 80 [20736/50000]	Loss: 0.1986	LR: 0.012500
Training Epoch: 80 [20864/50000]	Loss: 0.2623	LR: 0.012500
Training Epoch: 80 [20992/50000]	Loss: 0.2227	LR: 0.012500
Training Epoch: 80 [21120/50000]	Loss: 0.2145	LR: 0.012500
Training Epoch: 80 [21248/50000]	Loss: 0.2499	LR: 0.012500
Training Epoch: 80 [21376/50000]	Loss: 0.1899	LR: 0.012500
Training Epoch: 80 [21504/50000]	Loss: 0.1416	LR: 0.012500
Training Epoch: 80 [21632/50000]	Loss: 0.2161	LR: 0.012500
Training Epoch: 80 [21760/50000]	Loss: 0.2629	LR: 0.012500
Training Epoch: 80 [21888/50000]	Loss: 0.2018	LR: 0.012500
Training Epoch: 80 [22016/50000]	Loss: 0.3145	LR: 0.012500
Training Epoch: 80 [22144/50000]	Loss: 0.1533	LR: 0.012500
Training Epoch: 80 [22272/50000]	Loss: 0.2626	LR: 0.012500
Training Epoch: 80 [22400/50000]	Loss: 0.2160	LR: 0.012500
Training Epoch: 80 [22528/50000]	Loss: 0.1522	LR: 0.012500
Training Epoch: 80 [22656/50000]	Loss: 0.2184	LR: 0.012500
Training Epoch: 80 [22784/50000]	Loss: 0.1533	LR: 0.012500
Training Epoch: 80 [22912/50000]	Loss: 0.1557	LR: 0.012500
Training Epoch: 80 [23040/50000]	Loss: 0.2000	LR: 0.012500
Training Epoch: 80 [23168/50000]	Loss: 0.1849	LR: 0.012500
Training Epoch: 80 [23296/50000]	Loss: 0.2182	LR: 0.012500
Training Epoch: 80 [23424/50000]	Loss: 0.1172	LR: 0.012500
Training Epoch: 80 [23552/50000]	Loss: 0.2326	LR: 0.012500
Training Epoch: 80 [23680/50000]	Loss: 0.2116	LR: 0.012500
Training Epoch: 80 [23808/50000]	Loss: 0.1880	LR: 0.012500
Training Epoch: 80 [23936/50000]	Loss: 0.1873	LR: 0.012500
Training Epoch: 80 [24064/50000]	Loss: 0.2017	LR: 0.012500
Training Epoch: 80 [24192/50000]	Loss: 0.2841	LR: 0.012500
Training Epoch: 80 [24320/50000]	Loss: 0.1177	LR: 0.012500
Training Epoch: 80 [24448/50000]	Loss: 0.1753	LR: 0.012500
Training Epoch: 80 [24576/50000]	Loss: 0.1801	LR: 0.012500
Training Epoch: 80 [24704/50000]	Loss: 0.2365	LR: 0.012500
Training Epoch: 80 [24832/50000]	Loss: 0.1798	LR: 0.012500
Training Epoch: 80 [24960/50000]	Loss: 0.1522	LR: 0.012500
Training Epoch: 80 [25088/50000]	Loss: 0.1891	LR: 0.012500
Training Epoch: 80 [25216/50000]	Loss: 0.2369	LR: 0.012500
Training Epoch: 80 [25344/50000]	Loss: 0.2222	LR: 0.012500
Training Epoch: 80 [25472/50000]	Loss: 0.2464	LR: 0.012500
Training Epoch: 80 [25600/50000]	Loss: 0.1162	LR: 0.012500
Training Epoch: 80 [25728/50000]	Loss: 0.2461	LR: 0.012500
Training Epoch: 80 [25856/50000]	Loss: 0.2001	LR: 0.012500
Training Epoch: 80 [25984/50000]	Loss: 0.1083	LR: 0.012500
Training Epoch: 80 [26112/50000]	Loss: 0.1646	LR: 0.012500
Training Epoch: 80 [26240/50000]	Loss: 0.2113	LR: 0.012500
Training Epoch: 80 [26368/50000]	Loss: 0.2027	LR: 0.012500
Training Epoch: 80 [26496/50000]	Loss: 0.1654	LR: 0.012500
Training Epoch: 80 [26624/50000]	Loss: 0.2220	LR: 0.012500
Training Epoch: 80 [26752/50000]	Loss: 0.1415	LR: 0.012500
Training Epoch: 80 [26880/50000]	Loss: 0.1828	LR: 0.012500
Training Epoch: 80 [27008/50000]	Loss: 0.2677	LR: 0.012500
Training Epoch: 80 [27136/50000]	Loss: 0.2295	LR: 0.012500
Training Epoch: 80 [27264/50000]	Loss: 0.2178	LR: 0.012500
Training Epoch: 80 [27392/50000]	Loss: 0.1739	LR: 0.012500
Training Epoch: 80 [27520/50000]	Loss: 0.1694	LR: 0.012500
Training Epoch: 80 [27648/50000]	Loss: 0.1850	LR: 0.012500
Training Epoch: 80 [27776/50000]	Loss: 0.1959	LR: 0.012500
Training Epoch: 80 [27904/50000]	Loss: 0.2028	LR: 0.012500
Training Epoch: 80 [28032/50000]	Loss: 0.1259	LR: 0.012500
Training Epoch: 80 [28160/50000]	Loss: 0.1892	LR: 0.012500
Training Epoch: 80 [28288/50000]	Loss: 0.2872	LR: 0.012500
Training Epoch: 80 [28416/50000]	Loss: 0.1768	LR: 0.012500
Training Epoch: 80 [28544/50000]	Loss: 0.1397	LR: 0.012500
Training Epoch: 80 [28672/50000]	Loss: 0.1782	LR: 0.012500
Training Epoch: 80 [28800/50000]	Loss: 0.2529	LR: 0.012500
Training Epoch: 80 [28928/50000]	Loss: 0.2431	LR: 0.012500
Training Epoch: 80 [29056/50000]	Loss: 0.1671	LR: 0.012500
Training Epoch: 80 [29184/50000]	Loss: 0.1645	LR: 0.012500
Training Epoch: 80 [29312/50000]	Loss: 0.1444	LR: 0.012500
Training Epoch: 80 [29440/50000]	Loss: 0.1809	LR: 0.012500
Training Epoch: 80 [29568/50000]	Loss: 0.2315	LR: 0.012500
Training Epoch: 80 [29696/50000]	Loss: 0.1496	LR: 0.012500
Training Epoch: 80 [29824/50000]	Loss: 0.2161	LR: 0.012500
Training Epoch: 80 [29952/50000]	Loss: 0.2199	LR: 0.012500
Training Epoch: 80 [30080/50000]	Loss: 0.2539	LR: 0.012500
Training Epoch: 80 [30208/50000]	Loss: 0.1692	LR: 0.012500
Training Epoch: 80 [30336/50000]	Loss: 0.1881	LR: 0.012500
Training Epoch: 80 [30464/50000]	Loss: 0.1435	LR: 0.012500
Training Epoch: 80 [30592/50000]	Loss: 0.2754	LR: 0.012500
Training Epoch: 80 [30720/50000]	Loss: 0.1502	LR: 0.012500
Training Epoch: 80 [30848/50000]	Loss: 0.1273	LR: 0.012500
Training Epoch: 80 [30976/50000]	Loss: 0.2456	LR: 0.012500
Training Epoch: 80 [31104/50000]	Loss: 0.2413	LR: 0.012500
Training Epoch: 80 [31232/50000]	Loss: 0.1594	LR: 0.012500
Training Epoch: 80 [31360/50000]	Loss: 0.2667	LR: 0.012500
Training Epoch: 80 [31488/50000]	Loss: 0.2941	LR: 0.012500
Training Epoch: 80 [31616/50000]	Loss: 0.1592	LR: 0.012500
Training Epoch: 80 [31744/50000]	Loss: 0.1282	LR: 0.012500
Training Epoch: 80 [31872/50000]	Loss: 0.2726	LR: 0.012500
Training Epoch: 80 [32000/50000]	Loss: 0.1752	LR: 0.012500
Training Epoch: 80 [32128/50000]	Loss: 0.1850	LR: 0.012500
Training Epoch: 80 [32256/50000]	Loss: 0.2394	LR: 0.012500
Training Epoch: 80 [32384/50000]	Loss: 0.2833	LR: 0.012500
Training Epoch: 80 [32512/50000]	Loss: 0.2207	LR: 0.012500
Training Epoch: 80 [32640/50000]	Loss: 0.2690	LR: 0.012500
Training Epoch: 80 [32768/50000]	Loss: 0.1797	LR: 0.012500
Training Epoch: 80 [32896/50000]	Loss: 0.2316	LR: 0.012500
Training Epoch: 80 [33024/50000]	Loss: 0.2616	LR: 0.012500
Training Epoch: 80 [33152/50000]	Loss: 0.1846	LR: 0.012500
Training Epoch: 80 [33280/50000]	Loss: 0.1602	LR: 0.012500
Training Epoch: 80 [33408/50000]	Loss: 0.2010	LR: 0.012500
Training Epoch: 80 [33536/50000]	Loss: 0.2257	LR: 0.012500
Training Epoch: 80 [33664/50000]	Loss: 0.2713	LR: 0.012500
Training Epoch: 80 [33792/50000]	Loss: 0.2247	LR: 0.012500
Training Epoch: 80 [33920/50000]	Loss: 0.1889	LR: 0.012500
Training Epoch: 80 [34048/50000]	Loss: 0.2219	LR: 0.012500
Training Epoch: 80 [34176/50000]	Loss: 0.1876	LR: 0.012500
Training Epoch: 80 [34304/50000]	Loss: 0.2079	LR: 0.012500
Training Epoch: 80 [34432/50000]	Loss: 0.1940	LR: 0.012500
Training Epoch: 80 [34560/50000]	Loss: 0.2305	LR: 0.012500
Training Epoch: 80 [34688/50000]	Loss: 0.2142	LR: 0.012500
Training Epoch: 80 [34816/50000]	Loss: 0.2013	LR: 0.012500
Training Epoch: 80 [34944/50000]	Loss: 0.1371	LR: 0.012500
Training Epoch: 80 [35072/50000]	Loss: 0.1382	LR: 0.012500
Training Epoch: 80 [35200/50000]	Loss: 0.2066	LR: 0.012500
Training Epoch: 80 [35328/50000]	Loss: 0.2064	LR: 0.012500
Training Epoch: 80 [35456/50000]	Loss: 0.2300	LR: 0.012500
Training Epoch: 80 [35584/50000]	Loss: 0.2841	LR: 0.012500
Training Epoch: 80 [35712/50000]	Loss: 0.1838	LR: 0.012500
Training Epoch: 80 [35840/50000]	Loss: 0.1821	LR: 0.012500
Training Epoch: 80 [35968/50000]	Loss: 0.1867	LR: 0.012500
Training Epoch: 80 [36096/50000]	Loss: 0.1706	LR: 0.012500
Training Epoch: 80 [36224/50000]	Loss: 0.1476	LR: 0.012500
Training Epoch: 80 [36352/50000]	Loss: 0.1828	LR: 0.012500
Training Epoch: 80 [36480/50000]	Loss: 0.1462	LR: 0.012500
Training Epoch: 80 [36608/50000]	Loss: 0.2064	LR: 0.012500
Training Epoch: 80 [36736/50000]	Loss: 0.1857	LR: 0.012500
Training Epoch: 80 [36864/50000]	Loss: 0.1666	LR: 0.012500
Training Epoch: 80 [36992/50000]	Loss: 0.1722	LR: 0.012500
Training Epoch: 80 [37120/50000]	Loss: 0.1696	LR: 0.012500
Training Epoch: 80 [37248/50000]	Loss: 0.1760	LR: 0.012500
Training Epoch: 80 [37376/50000]	Loss: 0.2081	LR: 0.012500
Training Epoch: 80 [37504/50000]	Loss: 0.2177	LR: 0.012500
Training Epoch: 80 [37632/50000]	Loss: 0.1866	LR: 0.012500
Training Epoch: 80 [37760/50000]	Loss: 0.1132	LR: 0.012500
Training Epoch: 80 [37888/50000]	Loss: 0.2449	LR: 0.012500
Training Epoch: 80 [38016/50000]	Loss: 0.1105	LR: 0.012500
Training Epoch: 80 [38144/50000]	Loss: 0.2266	LR: 0.012500
Training Epoch: 80 [38272/50000]	Loss: 0.1339	LR: 0.012500
Training Epoch: 80 [38400/50000]	Loss: 0.1241	LR: 0.012500
Training Epoch: 80 [38528/50000]	Loss: 0.2121	LR: 0.012500
Training Epoch: 80 [38656/50000]	Loss: 0.2103	LR: 0.012500
Training Epoch: 80 [38784/50000]	Loss: 0.2272	LR: 0.012500
Training Epoch: 80 [38912/50000]	Loss: 0.2835	LR: 0.012500
Training Epoch: 80 [39040/50000]	Loss: 0.2395	LR: 0.012500
Training Epoch: 80 [39168/50000]	Loss: 0.2429	LR: 0.012500
Training Epoch: 80 [39296/50000]	Loss: 0.2249	LR: 0.012500
Training Epoch: 80 [39424/50000]	Loss: 0.2056	LR: 0.012500
Training Epoch: 80 [39552/50000]	Loss: 0.1158	LR: 0.012500
Training Epoch: 80 [39680/50000]	Loss: 0.1468	LR: 0.012500
Training Epoch: 80 [39808/50000]	Loss: 0.1378	LR: 0.012500
Training Epoch: 80 [39936/50000]	Loss: 0.1218	LR: 0.012500
Training Epoch: 80 [40064/50000]	Loss: 0.1672	LR: 0.012500
Training Epoch: 80 [40192/50000]	Loss: 0.2565	LR: 0.012500
Training Epoch: 80 [40320/50000]	Loss: 0.1580	LR: 0.012500
Training Epoch: 80 [40448/50000]	Loss: 0.1895	LR: 0.012500
Training Epoch: 80 [40576/50000]	Loss: 0.2820	LR: 0.012500
Training Epoch: 80 [40704/50000]	Loss: 0.1529	LR: 0.012500
Training Epoch: 80 [40832/50000]	Loss: 0.2882	LR: 0.012500
Training Epoch: 80 [40960/50000]	Loss: 0.2014	LR: 0.012500
Training Epoch: 80 [41088/50000]	Loss: 0.1777	LR: 0.012500
Training Epoch: 80 [41216/50000]	Loss: 0.1928	LR: 0.012500
Training Epoch: 80 [41344/50000]	Loss: 0.1880	LR: 0.012500
Training Epoch: 80 [41472/50000]	Loss: 0.2854	LR: 0.012500
Training Epoch: 80 [41600/50000]	Loss: 0.2382	LR: 0.012500
Training Epoch: 80 [41728/50000]	Loss: 0.1812	LR: 0.012500
Training Epoch: 80 [41856/50000]	Loss: 0.1820	LR: 0.012500
Training Epoch: 80 [41984/50000]	Loss: 0.3492	LR: 0.012500
Training Epoch: 80 [42112/50000]	Loss: 0.2269	LR: 0.012500
Training Epoch: 80 [42240/50000]	Loss: 0.2567	LR: 0.012500
Training Epoch: 80 [42368/50000]	Loss: 0.2840	LR: 0.012500
Training Epoch: 80 [42496/50000]	Loss: 0.2479	LR: 0.012500
Training Epoch: 80 [42624/50000]	Loss: 0.2155	LR: 0.012500
Training Epoch: 80 [42752/50000]	Loss: 0.1870	LR: 0.012500
Training Epoch: 80 [42880/50000]	Loss: 0.1494	LR: 0.012500
Training Epoch: 80 [43008/50000]	Loss: 0.2805	LR: 0.012500
Training Epoch: 80 [43136/50000]	Loss: 0.1356	LR: 0.012500
Training Epoch: 80 [43264/50000]	Loss: 0.1519	LR: 0.012500
Training Epoch: 80 [43392/50000]	Loss: 0.2012	LR: 0.012500
Training Epoch: 80 [43520/50000]	Loss: 0.1960	LR: 0.012500
Training Epoch: 80 [43648/50000]	Loss: 0.1623	LR: 0.012500
Training Epoch: 80 [43776/50000]	Loss: 0.2261	LR: 0.012500
Training Epoch: 80 [43904/50000]	Loss: 0.1956	LR: 0.012500
Training Epoch: 80 [44032/50000]	Loss: 0.1750	LR: 0.012500
Training Epoch: 80 [44160/50000]	Loss: 0.1719	LR: 0.012500
Training Epoch: 80 [44288/50000]	Loss: 0.2282	LR: 0.012500
Training Epoch: 80 [44416/50000]	Loss: 0.2073	LR: 0.012500
Training Epoch: 80 [44544/50000]	Loss: 0.1945	LR: 0.012500
Training Epoch: 80 [44672/50000]	Loss: 0.1880	LR: 0.012500
Training Epoch: 80 [44800/50000]	Loss: 0.1517	LR: 0.012500
Training Epoch: 80 [44928/50000]	Loss: 0.2311	LR: 0.012500
Training Epoch: 80 [45056/50000]	Loss: 0.2477	LR: 0.012500
Training Epoch: 80 [45184/50000]	Loss: 0.1518	LR: 0.012500
Training Epoch: 80 [45312/50000]	Loss: 0.1778	LR: 0.012500
Training Epoch: 80 [45440/50000]	Loss: 0.2451	LR: 0.012500
Training Epoch: 80 [45568/50000]	Loss: 0.2273	LR: 0.012500
Training Epoch: 80 [45696/50000]	Loss: 0.2002	LR: 0.012500
Training Epoch: 80 [45824/50000]	Loss: 0.1639	LR: 0.012500
Training Epoch: 80 [45952/50000]	Loss: 0.2299	LR: 0.012500
Training Epoch: 80 [46080/50000]	Loss: 0.1910	LR: 0.012500
Training Epoch: 80 [46208/50000]	Loss: 0.1970	LR: 0.012500
Training Epoch: 80 [46336/50000]	Loss: 0.2216	LR: 0.012500
Training Epoch: 80 [46464/50000]	Loss: 0.1821	LR: 0.012500
Training Epoch: 80 [46592/50000]	Loss: 0.2056	LR: 0.012500
Training Epoch: 80 [46720/50000]	Loss: 0.1919	LR: 0.012500
Training Epoch: 80 [46848/50000]	Loss: 0.2359	LR: 0.012500
Training Epoch: 80 [46976/50000]	Loss: 0.2338	LR: 0.012500
Training Epoch: 80 [47104/50000]	Loss: 0.1868	LR: 0.012500
Training Epoch: 80 [47232/50000]	Loss: 0.1514	LR: 0.012500
Training Epoch: 80 [47360/50000]	Loss: 0.2028	LR: 0.012500
Training Epoch: 80 [47488/50000]	Loss: 0.3110	LR: 0.012500
Training Epoch: 80 [47616/50000]	Loss: 0.1841	LR: 0.012500
Training Epoch: 80 [47744/50000]	Loss: 0.2083	LR: 0.012500
Training Epoch: 80 [47872/50000]	Loss: 0.2202	LR: 0.012500
Training Epoch: 80 [48000/50000]	Loss: 0.2397	LR: 0.012500
Training Epoch: 80 [48128/50000]	Loss: 0.2365	LR: 0.012500
Training Epoch: 80 [48256/50000]	Loss: 0.1978	LR: 0.012500
Training Epoch: 80 [48384/50000]	Loss: 0.1800	LR: 0.012500
Training Epoch: 80 [48512/50000]	Loss: 0.1736	LR: 0.012500
Training Epoch: 80 [48640/50000]	Loss: 0.2678	LR: 0.012500
Training Epoch: 80 [48768/50000]	Loss: 0.2508	LR: 0.012500
Training Epoch: 80 [48896/50000]	Loss: 0.1675	LR: 0.012500
Training Epoch: 80 [49024/50000]	Loss: 0.1316	LR: 0.012500
Training Epoch: 80 [49152/50000]	Loss: 0.1614	LR: 0.012500
Training Epoch: 80 [49280/50000]	Loss: 0.2385	LR: 0.012500
Training Epoch: 80 [49408/50000]	Loss: 0.1984	LR: 0.012500
Training Epoch: 80 [49536/50000]	Loss: 0.3224	LR: 0.012500
Training Epoch: 80 [49664/50000]	Loss: 0.1889	LR: 0.012500
Training Epoch: 80 [49792/50000]	Loss: 0.2711	LR: 0.012500
Training Epoch: 80 [49920/50000]	Loss: 0.2520	LR: 0.012500
Training Epoch: 80 [50000/50000]	Loss: 0.2688	LR: 0.012500
epoch 80 training time consumed: 53.89s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  293604 GB |  293604 GB |
|       from large pool |  123392 KB |    1034 MB |  293315 GB |  293315 GB |
|       from small pool |   10798 KB |      13 MB |     289 GB |     289 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  293604 GB |  293604 GB |
|       from large pool |  123392 KB |    1034 MB |  293315 GB |  293315 GB |
|       from small pool |   10798 KB |      13 MB |     289 GB |     289 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  129203 GB |  129203 GB |
|       from large pool |  155136 KB |  433088 KB |  128883 GB |  128883 GB |
|       from small pool |    1490 KB |    3494 KB |     319 GB |     319 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   11329 K  |   11329 K  |
|       from large pool |      24    |      65    |    5913 K  |    5913 K  |
|       from small pool |     231    |     274    |    5415 K  |    5415 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   11329 K  |   11329 K  |
|       from large pool |      24    |      65    |    5913 K  |    5913 K  |
|       from small pool |     231    |     274    |    5415 K  |    5415 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5597 K  |    5597 K  |
|       from large pool |       9    |      14    |    2862 K  |    2862 K  |
|       from small pool |      12    |      16    |    2735 K  |    2735 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 80, Average loss: 0.0087, Accuracy: 0.7244, Time consumed:3.46s

Training Epoch: 81 [128/50000]	Loss: 0.2088	LR: 0.012500
Training Epoch: 81 [256/50000]	Loss: 0.0792	LR: 0.012500
Training Epoch: 81 [384/50000]	Loss: 0.1549	LR: 0.012500
Training Epoch: 81 [512/50000]	Loss: 0.1180	LR: 0.012500
Training Epoch: 81 [640/50000]	Loss: 0.1481	LR: 0.012500
Training Epoch: 81 [768/50000]	Loss: 0.1073	LR: 0.012500
Training Epoch: 81 [896/50000]	Loss: 0.1037	LR: 0.012500
Training Epoch: 81 [1024/50000]	Loss: 0.1595	LR: 0.012500
Training Epoch: 81 [1152/50000]	Loss: 0.1722	LR: 0.012500
Training Epoch: 81 [1280/50000]	Loss: 0.1515	LR: 0.012500
Training Epoch: 81 [1408/50000]	Loss: 0.1265	LR: 0.012500
Training Epoch: 81 [1536/50000]	Loss: 0.1673	LR: 0.012500
Training Epoch: 81 [1664/50000]	Loss: 0.1801	LR: 0.012500
Training Epoch: 81 [1792/50000]	Loss: 0.1946	LR: 0.012500
Training Epoch: 81 [1920/50000]	Loss: 0.2215	LR: 0.012500
Training Epoch: 81 [2048/50000]	Loss: 0.1752	LR: 0.012500
Training Epoch: 81 [2176/50000]	Loss: 0.1559	LR: 0.012500
Training Epoch: 81 [2304/50000]	Loss: 0.1875	LR: 0.012500
Training Epoch: 81 [2432/50000]	Loss: 0.1559	LR: 0.012500
Training Epoch: 81 [2560/50000]	Loss: 0.1594	LR: 0.012500
Training Epoch: 81 [2688/50000]	Loss: 0.1732	LR: 0.012500
Training Epoch: 81 [2816/50000]	Loss: 0.1361	LR: 0.012500
Training Epoch: 81 [2944/50000]	Loss: 0.1162	LR: 0.012500
Training Epoch: 81 [3072/50000]	Loss: 0.0929	LR: 0.012500
Training Epoch: 81 [3200/50000]	Loss: 0.1513	LR: 0.012500
Training Epoch: 81 [3328/50000]	Loss: 0.1066	LR: 0.012500
Training Epoch: 81 [3456/50000]	Loss: 0.1790	LR: 0.012500
Training Epoch: 81 [3584/50000]	Loss: 0.1827	LR: 0.012500
Training Epoch: 81 [3712/50000]	Loss: 0.1217	LR: 0.012500
Training Epoch: 81 [3840/50000]	Loss: 0.1808	LR: 0.012500
Training Epoch: 81 [3968/50000]	Loss: 0.2478	LR: 0.012500
Training Epoch: 81 [4096/50000]	Loss: 0.1492	LR: 0.012500
Training Epoch: 81 [4224/50000]	Loss: 0.1264	LR: 0.012500
Training Epoch: 81 [4352/50000]	Loss: 0.1629	LR: 0.012500
Training Epoch: 81 [4480/50000]	Loss: 0.1509	LR: 0.012500
Training Epoch: 81 [4608/50000]	Loss: 0.1198	LR: 0.012500
Training Epoch: 81 [4736/50000]	Loss: 0.1224	LR: 0.012500
Training Epoch: 81 [4864/50000]	Loss: 0.1649	LR: 0.012500
Training Epoch: 81 [4992/50000]	Loss: 0.1709	LR: 0.012500
Training Epoch: 81 [5120/50000]	Loss: 0.1328	LR: 0.012500
Training Epoch: 81 [5248/50000]	Loss: 0.0981	LR: 0.012500
Training Epoch: 81 [5376/50000]	Loss: 0.2366	LR: 0.012500
Training Epoch: 81 [5504/50000]	Loss: 0.1244	LR: 0.012500
Training Epoch: 81 [5632/50000]	Loss: 0.1280	LR: 0.012500
Training Epoch: 81 [5760/50000]	Loss: 0.1558	LR: 0.012500
Training Epoch: 81 [5888/50000]	Loss: 0.1615	LR: 0.012500
Training Epoch: 81 [6016/50000]	Loss: 0.1471	LR: 0.012500
Training Epoch: 81 [6144/50000]	Loss: 0.0974	LR: 0.012500
Training Epoch: 81 [6272/50000]	Loss: 0.1391	LR: 0.012500
Training Epoch: 81 [6400/50000]	Loss: 0.1266	LR: 0.012500
Training Epoch: 81 [6528/50000]	Loss: 0.1743	LR: 0.012500
Training Epoch: 81 [6656/50000]	Loss: 0.1328	LR: 0.012500
Training Epoch: 81 [6784/50000]	Loss: 0.0999	LR: 0.012500
Training Epoch: 81 [6912/50000]	Loss: 0.1713	LR: 0.012500
Training Epoch: 81 [7040/50000]	Loss: 0.1777	LR: 0.012500
Training Epoch: 81 [7168/50000]	Loss: 0.1456	LR: 0.012500
Training Epoch: 81 [7296/50000]	Loss: 0.1075	LR: 0.012500
Training Epoch: 81 [7424/50000]	Loss: 0.2267	LR: 0.012500
Training Epoch: 81 [7552/50000]	Loss: 0.1715	LR: 0.012500
Training Epoch: 81 [7680/50000]	Loss: 0.1902	LR: 0.012500
Training Epoch: 81 [7808/50000]	Loss: 0.0822	LR: 0.012500
Training Epoch: 81 [7936/50000]	Loss: 0.1612	LR: 0.012500
Training Epoch: 81 [8064/50000]	Loss: 0.1734	LR: 0.012500
Training Epoch: 81 [8192/50000]	Loss: 0.1753	LR: 0.012500
Training Epoch: 81 [8320/50000]	Loss: 0.1523	LR: 0.012500
Training Epoch: 81 [8448/50000]	Loss: 0.2084	LR: 0.012500
Training Epoch: 81 [8576/50000]	Loss: 0.1962	LR: 0.012500
Training Epoch: 81 [8704/50000]	Loss: 0.1166	LR: 0.012500
Training Epoch: 81 [8832/50000]	Loss: 0.1999	LR: 0.012500
Training Epoch: 81 [8960/50000]	Loss: 0.1569	LR: 0.012500
Training Epoch: 81 [9088/50000]	Loss: 0.1401	LR: 0.012500
Training Epoch: 81 [9216/50000]	Loss: 0.1816	LR: 0.012500
Training Epoch: 81 [9344/50000]	Loss: 0.1205	LR: 0.012500
Training Epoch: 81 [9472/50000]	Loss: 0.1255	LR: 0.012500
Training Epoch: 81 [9600/50000]	Loss: 0.1473	LR: 0.012500
Training Epoch: 81 [9728/50000]	Loss: 0.1334	LR: 0.012500
Training Epoch: 81 [9856/50000]	Loss: 0.2265	LR: 0.012500
Training Epoch: 81 [9984/50000]	Loss: 0.1935	LR: 0.012500
Training Epoch: 81 [10112/50000]	Loss: 0.1266	LR: 0.012500
Training Epoch: 81 [10240/50000]	Loss: 0.1703	LR: 0.012500
Training Epoch: 81 [10368/50000]	Loss: 0.1556	LR: 0.012500
Training Epoch: 81 [10496/50000]	Loss: 0.2718	LR: 0.012500
Training Epoch: 81 [10624/50000]	Loss: 0.1761	LR: 0.012500
Training Epoch: 81 [10752/50000]	Loss: 0.1862	LR: 0.012500
Training Epoch: 81 [10880/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 81 [11008/50000]	Loss: 0.1020	LR: 0.012500
Training Epoch: 81 [11136/50000]	Loss: 0.1046	LR: 0.012500
Training Epoch: 81 [11264/50000]	Loss: 0.1561	LR: 0.012500
Training Epoch: 81 [11392/50000]	Loss: 0.1370	LR: 0.012500
Training Epoch: 81 [11520/50000]	Loss: 0.1252	LR: 0.012500
Training Epoch: 81 [11648/50000]	Loss: 0.1117	LR: 0.012500
Training Epoch: 81 [11776/50000]	Loss: 0.1286	LR: 0.012500
Training Epoch: 81 [11904/50000]	Loss: 0.1867	LR: 0.012500
Training Epoch: 81 [12032/50000]	Loss: 0.0807	LR: 0.012500
Training Epoch: 81 [12160/50000]	Loss: 0.1506	LR: 0.012500
Training Epoch: 81 [12288/50000]	Loss: 0.1589	LR: 0.012500
Training Epoch: 81 [12416/50000]	Loss: 0.1248	LR: 0.012500
Training Epoch: 81 [12544/50000]	Loss: 0.1660	LR: 0.012500
Training Epoch: 81 [12672/50000]	Loss: 0.1970	LR: 0.012500
Training Epoch: 81 [12800/50000]	Loss: 0.1577	LR: 0.012500
Training Epoch: 81 [12928/50000]	Loss: 0.1289	LR: 0.012500
Training Epoch: 81 [13056/50000]	Loss: 0.1545	LR: 0.012500
Training Epoch: 81 [13184/50000]	Loss: 0.1874	LR: 0.012500
Training Epoch: 81 [13312/50000]	Loss: 0.1673	LR: 0.012500
Training Epoch: 81 [13440/50000]	Loss: 0.1308	LR: 0.012500
Training Epoch: 81 [13568/50000]	Loss: 0.1367	LR: 0.012500
Training Epoch: 81 [13696/50000]	Loss: 0.1661	LR: 0.012500
Training Epoch: 81 [13824/50000]	Loss: 0.2122	LR: 0.012500
Training Epoch: 81 [13952/50000]	Loss: 0.1261	LR: 0.012500
Training Epoch: 81 [14080/50000]	Loss: 0.1503	LR: 0.012500
Training Epoch: 81 [14208/50000]	Loss: 0.1129	LR: 0.012500
Training Epoch: 81 [14336/50000]	Loss: 0.1761	LR: 0.012500
Training Epoch: 81 [14464/50000]	Loss: 0.1528	LR: 0.012500
Training Epoch: 81 [14592/50000]	Loss: 0.1541	LR: 0.012500
Training Epoch: 81 [14720/50000]	Loss: 0.2263	LR: 0.012500
Training Epoch: 81 [14848/50000]	Loss: 0.1918	LR: 0.012500
Training Epoch: 81 [14976/50000]	Loss: 0.1894	LR: 0.012500
Training Epoch: 81 [15104/50000]	Loss: 0.1474	LR: 0.012500
Training Epoch: 81 [15232/50000]	Loss: 0.0771	LR: 0.012500
Training Epoch: 81 [15360/50000]	Loss: 0.0961	LR: 0.012500
Training Epoch: 81 [15488/50000]	Loss: 0.1216	LR: 0.012500
Training Epoch: 81 [15616/50000]	Loss: 0.1616	LR: 0.012500
Training Epoch: 81 [15744/50000]	Loss: 0.1121	LR: 0.012500
Training Epoch: 81 [15872/50000]	Loss: 0.1286	LR: 0.012500
Training Epoch: 81 [16000/50000]	Loss: 0.1713	LR: 0.012500
Training Epoch: 81 [16128/50000]	Loss: 0.1797	LR: 0.012500
Training Epoch: 81 [16256/50000]	Loss: 0.1425	LR: 0.012500
Training Epoch: 81 [16384/50000]	Loss: 0.1551	LR: 0.012500
Training Epoch: 81 [16512/50000]	Loss: 0.1295	LR: 0.012500
Training Epoch: 81 [16640/50000]	Loss: 0.1910	LR: 0.012500
Training Epoch: 81 [16768/50000]	Loss: 0.1434	LR: 0.012500
Training Epoch: 81 [16896/50000]	Loss: 0.1311	LR: 0.012500
Training Epoch: 81 [17024/50000]	Loss: 0.1699	LR: 0.012500
Training Epoch: 81 [17152/50000]	Loss: 0.1888	LR: 0.012500
Training Epoch: 81 [17280/50000]	Loss: 0.2091	LR: 0.012500
Training Epoch: 81 [17408/50000]	Loss: 0.2244	LR: 0.012500
Training Epoch: 81 [17536/50000]	Loss: 0.1569	LR: 0.012500
Training Epoch: 81 [17664/50000]	Loss: 0.0729	LR: 0.012500
Training Epoch: 81 [17792/50000]	Loss: 0.1661	LR: 0.012500
Training Epoch: 81 [17920/50000]	Loss: 0.1775	LR: 0.012500
Training Epoch: 81 [18048/50000]	Loss: 0.1020	LR: 0.012500
Training Epoch: 81 [18176/50000]	Loss: 0.1262	LR: 0.012500
Training Epoch: 81 [18304/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 81 [18432/50000]	Loss: 0.2165	LR: 0.012500
Training Epoch: 81 [18560/50000]	Loss: 0.1141	LR: 0.012500
Training Epoch: 81 [18688/50000]	Loss: 0.1339	LR: 0.012500
Training Epoch: 81 [18816/50000]	Loss: 0.2633	LR: 0.012500
Training Epoch: 81 [18944/50000]	Loss: 0.1513	LR: 0.012500
Training Epoch: 81 [19072/50000]	Loss: 0.2085	LR: 0.012500
Training Epoch: 81 [19200/50000]	Loss: 0.1375	LR: 0.012500
Training Epoch: 81 [19328/50000]	Loss: 0.2139	LR: 0.012500
Training Epoch: 81 [19456/50000]	Loss: 0.1174	LR: 0.012500
Training Epoch: 81 [19584/50000]	Loss: 0.1169	LR: 0.012500
Training Epoch: 81 [19712/50000]	Loss: 0.1820	LR: 0.012500
Training Epoch: 81 [19840/50000]	Loss: 0.1073	LR: 0.012500
Training Epoch: 81 [19968/50000]	Loss: 0.2200	LR: 0.012500
Training Epoch: 81 [20096/50000]	Loss: 0.1517	LR: 0.012500
Training Epoch: 81 [20224/50000]	Loss: 0.1702	LR: 0.012500
Training Epoch: 81 [20352/50000]	Loss: 0.1742	LR: 0.012500
Training Epoch: 81 [20480/50000]	Loss: 0.1559	LR: 0.012500
Training Epoch: 81 [20608/50000]	Loss: 0.1547	LR: 0.012500
Training Epoch: 81 [20736/50000]	Loss: 0.1498	LR: 0.012500
Training Epoch: 81 [20864/50000]	Loss: 0.0766	LR: 0.012500
Training Epoch: 81 [20992/50000]	Loss: 0.1736	LR: 0.012500
Training Epoch: 81 [21120/50000]	Loss: 0.1443	LR: 0.012500
Training Epoch: 81 [21248/50000]	Loss: 0.1506	LR: 0.012500
Training Epoch: 81 [21376/50000]	Loss: 0.1531	LR: 0.012500
Training Epoch: 81 [21504/50000]	Loss: 0.1825	LR: 0.012500
Training Epoch: 81 [21632/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 81 [21760/50000]	Loss: 0.0728	LR: 0.012500
Training Epoch: 81 [21888/50000]	Loss: 0.1692	LR: 0.012500
Training Epoch: 81 [22016/50000]	Loss: 0.1018	LR: 0.012500
Training Epoch: 81 [22144/50000]	Loss: 0.1881	LR: 0.012500
Training Epoch: 81 [22272/50000]	Loss: 0.1418	LR: 0.012500
Training Epoch: 81 [22400/50000]	Loss: 0.1232	LR: 0.012500
Training Epoch: 81 [22528/50000]	Loss: 0.2020	LR: 0.012500
Training Epoch: 81 [22656/50000]	Loss: 0.1407	LR: 0.012500
Training Epoch: 81 [22784/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 81 [22912/50000]	Loss: 0.1307	LR: 0.012500
Training Epoch: 81 [23040/50000]	Loss: 0.2302	LR: 0.012500
Training Epoch: 81 [23168/50000]	Loss: 0.0915	LR: 0.012500
Training Epoch: 81 [23296/50000]	Loss: 0.1772	LR: 0.012500
Training Epoch: 81 [23424/50000]	Loss: 0.2033	LR: 0.012500
Training Epoch: 81 [23552/50000]	Loss: 0.1776	LR: 0.012500
Training Epoch: 81 [23680/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 81 [23808/50000]	Loss: 0.1569	LR: 0.012500
Training Epoch: 81 [23936/50000]	Loss: 0.2643	LR: 0.012500
Training Epoch: 81 [24064/50000]	Loss: 0.1861	LR: 0.012500
Training Epoch: 81 [24192/50000]	Loss: 0.1763	LR: 0.012500
Training Epoch: 81 [24320/50000]	Loss: 0.2040	LR: 0.012500
Training Epoch: 81 [24448/50000]	Loss: 0.0936	LR: 0.012500
Training Epoch: 81 [24576/50000]	Loss: 0.1773	LR: 0.012500
Training Epoch: 81 [24704/50000]	Loss: 0.1188	LR: 0.012500
Training Epoch: 81 [24832/50000]	Loss: 0.1318	LR: 0.012500
Training Epoch: 81 [24960/50000]	Loss: 0.1309	LR: 0.012500
Training Epoch: 81 [25088/50000]	Loss: 0.1582	LR: 0.012500
Training Epoch: 81 [25216/50000]	Loss: 0.1410	LR: 0.012500
Training Epoch: 81 [25344/50000]	Loss: 0.1609	LR: 0.012500
Training Epoch: 81 [25472/50000]	Loss: 0.1558	LR: 0.012500
Training Epoch: 81 [25600/50000]	Loss: 0.1319	LR: 0.012500
Training Epoch: 81 [25728/50000]	Loss: 0.1347	LR: 0.012500
Training Epoch: 81 [25856/50000]	Loss: 0.1852	LR: 0.012500
Training Epoch: 81 [25984/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 81 [26112/50000]	Loss: 0.1576	LR: 0.012500
Training Epoch: 81 [26240/50000]	Loss: 0.1056	LR: 0.012500
Training Epoch: 81 [26368/50000]	Loss: 0.1815	LR: 0.012500
Training Epoch: 81 [26496/50000]	Loss: 0.0910	LR: 0.012500
Training Epoch: 81 [26624/50000]	Loss: 0.0891	LR: 0.012500
Training Epoch: 81 [26752/50000]	Loss: 0.1440	LR: 0.012500
Training Epoch: 81 [26880/50000]	Loss: 0.1015	LR: 0.012500
Training Epoch: 81 [27008/50000]	Loss: 0.1626	LR: 0.012500
Training Epoch: 81 [27136/50000]	Loss: 0.1388	LR: 0.012500
Training Epoch: 81 [27264/50000]	Loss: 0.1036	LR: 0.012500
Training Epoch: 81 [27392/50000]	Loss: 0.1120	LR: 0.012500
Training Epoch: 81 [27520/50000]	Loss: 0.2200	LR: 0.012500
Training Epoch: 81 [27648/50000]	Loss: 0.2584	LR: 0.012500
Training Epoch: 81 [27776/50000]	Loss: 0.1177	LR: 0.012500
Training Epoch: 81 [27904/50000]	Loss: 0.1595	LR: 0.012500
Training Epoch: 81 [28032/50000]	Loss: 0.2268	LR: 0.012500
Training Epoch: 81 [28160/50000]	Loss: 0.1441	LR: 0.012500
Training Epoch: 81 [28288/50000]	Loss: 0.1633	LR: 0.012500
Training Epoch: 81 [28416/50000]	Loss: 0.1154	LR: 0.012500
Training Epoch: 81 [28544/50000]	Loss: 0.1583	LR: 0.012500
Training Epoch: 81 [28672/50000]	Loss: 0.2096	LR: 0.012500
Training Epoch: 81 [28800/50000]	Loss: 0.1869	LR: 0.012500
Training Epoch: 81 [28928/50000]	Loss: 0.1146	LR: 0.012500
Training Epoch: 81 [29056/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 81 [29184/50000]	Loss: 0.1696	LR: 0.012500
Training Epoch: 81 [29312/50000]	Loss: 0.1651	LR: 0.012500
Training Epoch: 81 [29440/50000]	Loss: 0.1267	LR: 0.012500
Training Epoch: 81 [29568/50000]	Loss: 0.1132	LR: 0.012500
Training Epoch: 81 [29696/50000]	Loss: 0.2419	LR: 0.012500
Training Epoch: 81 [29824/50000]	Loss: 0.0897	LR: 0.012500
Training Epoch: 81 [29952/50000]	Loss: 0.1971	LR: 0.012500
Training Epoch: 81 [30080/50000]	Loss: 0.1183	LR: 0.012500
Training Epoch: 81 [30208/50000]	Loss: 0.1794	LR: 0.012500
Training Epoch: 81 [30336/50000]	Loss: 0.1911	LR: 0.012500
Training Epoch: 81 [30464/50000]	Loss: 0.1964	LR: 0.012500
Training Epoch: 81 [30592/50000]	Loss: 0.1232	LR: 0.012500
Training Epoch: 81 [30720/50000]	Loss: 0.1171	LR: 0.012500
Training Epoch: 81 [30848/50000]	Loss: 0.2263	LR: 0.012500
Training Epoch: 81 [30976/50000]	Loss: 0.1973	LR: 0.012500
Training Epoch: 81 [31104/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 81 [31232/50000]	Loss: 0.1551	LR: 0.012500
Training Epoch: 81 [31360/50000]	Loss: 0.1047	LR: 0.012500
Training Epoch: 81 [31488/50000]	Loss: 0.1952	LR: 0.012500
Training Epoch: 81 [31616/50000]	Loss: 0.1617	LR: 0.012500
Training Epoch: 81 [31744/50000]	Loss: 0.1490	LR: 0.012500
Training Epoch: 81 [31872/50000]	Loss: 0.1293	LR: 0.012500
Training Epoch: 81 [32000/50000]	Loss: 0.1889	LR: 0.012500
Training Epoch: 81 [32128/50000]	Loss: 0.2311	LR: 0.012500
Training Epoch: 81 [32256/50000]	Loss: 0.1740	LR: 0.012500
Training Epoch: 81 [32384/50000]	Loss: 0.1288	LR: 0.012500
Training Epoch: 81 [32512/50000]	Loss: 0.1641	LR: 0.012500
Training Epoch: 81 [32640/50000]	Loss: 0.1077	LR: 0.012500
Training Epoch: 81 [32768/50000]	Loss: 0.1924	LR: 0.012500
Training Epoch: 81 [32896/50000]	Loss: 0.2030	LR: 0.012500
Training Epoch: 81 [33024/50000]	Loss: 0.1655	LR: 0.012500
Training Epoch: 81 [33152/50000]	Loss: 0.2369	LR: 0.012500
Training Epoch: 81 [33280/50000]	Loss: 0.1086	LR: 0.012500
Training Epoch: 81 [33408/50000]	Loss: 0.1891	LR: 0.012500
Training Epoch: 81 [33536/50000]	Loss: 0.1871	LR: 0.012500
Training Epoch: 81 [33664/50000]	Loss: 0.1262	LR: 0.012500
Training Epoch: 81 [33792/50000]	Loss: 0.1868	LR: 0.012500
Training Epoch: 81 [33920/50000]	Loss: 0.1795	LR: 0.012500
Training Epoch: 81 [34048/50000]	Loss: 0.1119	LR: 0.012500
Training Epoch: 81 [34176/50000]	Loss: 0.1946	LR: 0.012500
Training Epoch: 81 [34304/50000]	Loss: 0.1769	LR: 0.012500
Training Epoch: 81 [34432/50000]	Loss: 0.1944	LR: 0.012500
Training Epoch: 81 [34560/50000]	Loss: 0.1139	LR: 0.012500
Training Epoch: 81 [34688/50000]	Loss: 0.1922	LR: 0.012500
Training Epoch: 81 [34816/50000]	Loss: 0.1093	LR: 0.012500
Training Epoch: 81 [34944/50000]	Loss: 0.1784	LR: 0.012500
Training Epoch: 81 [35072/50000]	Loss: 0.1368	LR: 0.012500
Training Epoch: 81 [35200/50000]	Loss: 0.0944	LR: 0.012500
Training Epoch: 81 [35328/50000]	Loss: 0.1684	LR: 0.012500
Training Epoch: 81 [35456/50000]	Loss: 0.1470	LR: 0.012500
Training Epoch: 81 [35584/50000]	Loss: 0.1478	LR: 0.012500
Training Epoch: 81 [35712/50000]	Loss: 0.1679	LR: 0.012500
Training Epoch: 81 [35840/50000]	Loss: 0.1837	LR: 0.012500
Training Epoch: 81 [35968/50000]	Loss: 0.1449	LR: 0.012500
Training Epoch: 81 [36096/50000]	Loss: 0.1938	LR: 0.012500
Training Epoch: 81 [36224/50000]	Loss: 0.0970	LR: 0.012500
Training Epoch: 81 [36352/50000]	Loss: 0.1690	LR: 0.012500
Training Epoch: 81 [36480/50000]	Loss: 0.1336	LR: 0.012500
Training Epoch: 81 [36608/50000]	Loss: 0.1549	LR: 0.012500
Training Epoch: 81 [36736/50000]	Loss: 0.1603	LR: 0.012500
Training Epoch: 81 [36864/50000]	Loss: 0.1728	LR: 0.012500
Training Epoch: 81 [36992/50000]	Loss: 0.2140	LR: 0.012500
Training Epoch: 81 [37120/50000]	Loss: 0.2534	LR: 0.012500
Training Epoch: 81 [37248/50000]	Loss: 0.1234	LR: 0.012500
Training Epoch: 81 [37376/50000]	Loss: 0.1428	LR: 0.012500
Training Epoch: 81 [37504/50000]	Loss: 0.1724	LR: 0.012500
Training Epoch: 81 [37632/50000]	Loss: 0.2200	LR: 0.012500
Training Epoch: 81 [37760/50000]	Loss: 0.1501	LR: 0.012500
Training Epoch: 81 [37888/50000]	Loss: 0.1793	LR: 0.012500
Training Epoch: 81 [38016/50000]	Loss: 0.1118	LR: 0.012500
Training Epoch: 81 [38144/50000]	Loss: 0.1396	LR: 0.012500
Training Epoch: 81 [38272/50000]	Loss: 0.2321	LR: 0.012500
Training Epoch: 81 [38400/50000]	Loss: 0.1781	LR: 0.012500
Training Epoch: 81 [38528/50000]	Loss: 0.1824	LR: 0.012500
Training Epoch: 81 [38656/50000]	Loss: 0.1313	LR: 0.012500
Training Epoch: 81 [38784/50000]	Loss: 0.0995	LR: 0.012500
Training Epoch: 81 [38912/50000]	Loss: 0.1536	LR: 0.012500
Training Epoch: 81 [39040/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 81 [39168/50000]	Loss: 0.0903	LR: 0.012500
Training Epoch: 81 [39296/50000]	Loss: 0.1334	LR: 0.012500
Training Epoch: 81 [39424/50000]	Loss: 0.1498	LR: 0.012500
Training Epoch: 81 [39552/50000]	Loss: 0.1953	LR: 0.012500
Training Epoch: 81 [39680/50000]	Loss: 0.1659	LR: 0.012500
Training Epoch: 81 [39808/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 81 [39936/50000]	Loss: 0.1144	LR: 0.012500
Training Epoch: 81 [40064/50000]	Loss: 0.1263	LR: 0.012500
Training Epoch: 81 [40192/50000]	Loss: 0.1943	LR: 0.012500
Training Epoch: 81 [40320/50000]	Loss: 0.1561	LR: 0.012500
Training Epoch: 81 [40448/50000]	Loss: 0.1801	LR: 0.012500
Training Epoch: 81 [40576/50000]	Loss: 0.1754	LR: 0.012500
Training Epoch: 81 [40704/50000]	Loss: 0.1512	LR: 0.012500
Training Epoch: 81 [40832/50000]	Loss: 0.1878	LR: 0.012500
Training Epoch: 81 [40960/50000]	Loss: 0.1165	LR: 0.012500
Training Epoch: 81 [41088/50000]	Loss: 0.2262	LR: 0.012500
Training Epoch: 81 [41216/50000]	Loss: 0.1677	LR: 0.012500
Training Epoch: 81 [41344/50000]	Loss: 0.1616	LR: 0.012500
Training Epoch: 81 [41472/50000]	Loss: 0.2012	LR: 0.012500
Training Epoch: 81 [41600/50000]	Loss: 0.0595	LR: 0.012500
Training Epoch: 81 [41728/50000]	Loss: 0.1016	LR: 0.012500
Training Epoch: 81 [41856/50000]	Loss: 0.1648	LR: 0.012500
Training Epoch: 81 [41984/50000]	Loss: 0.2134	LR: 0.012500
Training Epoch: 81 [42112/50000]	Loss: 0.2259	LR: 0.012500
Training Epoch: 81 [42240/50000]	Loss: 0.1605	LR: 0.012500
Training Epoch: 81 [42368/50000]	Loss: 0.1403	LR: 0.012500
Training Epoch: 81 [42496/50000]	Loss: 0.1791	LR: 0.012500
Training Epoch: 81 [42624/50000]	Loss: 0.1354	LR: 0.012500
Training Epoch: 81 [42752/50000]	Loss: 0.1652	LR: 0.012500
Training Epoch: 81 [42880/50000]	Loss: 0.2364	LR: 0.012500
Training Epoch: 81 [43008/50000]	Loss: 0.1473	LR: 0.012500
Training Epoch: 81 [43136/50000]	Loss: 0.0934	LR: 0.012500
Training Epoch: 81 [43264/50000]	Loss: 0.2009	LR: 0.012500
Training Epoch: 81 [43392/50000]	Loss: 0.1657	LR: 0.012500
Training Epoch: 81 [43520/50000]	Loss: 0.1239	LR: 0.012500
Training Epoch: 81 [43648/50000]	Loss: 0.1856	LR: 0.012500
Training Epoch: 81 [43776/50000]	Loss: 0.0868	LR: 0.012500
Training Epoch: 81 [43904/50000]	Loss: 0.1377	LR: 0.012500
Training Epoch: 81 [44032/50000]	Loss: 0.1659	LR: 0.012500
Training Epoch: 81 [44160/50000]	Loss: 0.2087	LR: 0.012500
Training Epoch: 81 [44288/50000]	Loss: 0.1455	LR: 0.012500
Training Epoch: 81 [44416/50000]	Loss: 0.1841	LR: 0.012500
Training Epoch: 81 [44544/50000]	Loss: 0.1323	LR: 0.012500
Training Epoch: 81 [44672/50000]	Loss: 0.1560	LR: 0.012500
Training Epoch: 81 [44800/50000]	Loss: 0.1757	LR: 0.012500
Training Epoch: 81 [44928/50000]	Loss: 0.1072	LR: 0.012500
Training Epoch: 81 [45056/50000]	Loss: 0.1871	LR: 0.012500
Training Epoch: 81 [45184/50000]	Loss: 0.1499	LR: 0.012500
Training Epoch: 81 [45312/50000]	Loss: 0.2385	LR: 0.012500
Training Epoch: 81 [45440/50000]	Loss: 0.1892	LR: 0.012500
Training Epoch: 81 [45568/50000]	Loss: 0.1384	LR: 0.012500
Training Epoch: 81 [45696/50000]	Loss: 0.0975	LR: 0.012500
Training Epoch: 81 [45824/50000]	Loss: 0.1924	LR: 0.012500
Training Epoch: 81 [45952/50000]	Loss: 0.1528	LR: 0.012500
Training Epoch: 81 [46080/50000]	Loss: 0.1689	LR: 0.012500
Training Epoch: 81 [46208/50000]	Loss: 0.1991	LR: 0.012500
Training Epoch: 81 [46336/50000]	Loss: 0.0987	LR: 0.012500
Training Epoch: 81 [46464/50000]	Loss: 0.1691	LR: 0.012500
Training Epoch: 81 [46592/50000]	Loss: 0.1447	LR: 0.012500
Training Epoch: 81 [46720/50000]	Loss: 0.1641	LR: 0.012500
Training Epoch: 81 [46848/50000]	Loss: 0.2130	LR: 0.012500
Training Epoch: 81 [46976/50000]	Loss: 0.1250	LR: 0.012500
Training Epoch: 81 [47104/50000]	Loss: 0.2492	LR: 0.012500
Training Epoch: 81 [47232/50000]	Loss: 0.1600	LR: 0.012500
Training Epoch: 81 [47360/50000]	Loss: 0.1091	LR: 0.012500
Training Epoch: 81 [47488/50000]	Loss: 0.1777	LR: 0.012500
Training Epoch: 81 [47616/50000]	Loss: 0.1698	LR: 0.012500
Training Epoch: 81 [47744/50000]	Loss: 0.1900	LR: 0.012500
Training Epoch: 81 [47872/50000]	Loss: 0.1641	LR: 0.012500
Training Epoch: 81 [48000/50000]	Loss: 0.1381	LR: 0.012500
Training Epoch: 81 [48128/50000]	Loss: 0.1237	LR: 0.012500
Training Epoch: 81 [48256/50000]	Loss: 0.1684	LR: 0.012500
Training Epoch: 81 [48384/50000]	Loss: 0.1443	LR: 0.012500
Training Epoch: 81 [48512/50000]	Loss: 0.0920	LR: 0.012500
Training Epoch: 81 [48640/50000]	Loss: 0.1734	LR: 0.012500
Training Epoch: 81 [48768/50000]	Loss: 0.1606	LR: 0.012500
Training Epoch: 81 [48896/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 81 [49024/50000]	Loss: 0.1509	LR: 0.012500
Training Epoch: 81 [49152/50000]	Loss: 0.1515	LR: 0.012500
Training Epoch: 81 [49280/50000]	Loss: 0.1718	LR: 0.012500
Training Epoch: 81 [49408/50000]	Loss: 0.2597	LR: 0.012500
Training Epoch: 81 [49536/50000]	Loss: 0.0757	LR: 0.012500
Training Epoch: 81 [49664/50000]	Loss: 0.2267	LR: 0.012500
Training Epoch: 81 [49792/50000]	Loss: 0.1733	LR: 0.012500
Training Epoch: 81 [49920/50000]	Loss: 0.0884	LR: 0.012500
Training Epoch: 81 [50000/50000]	Loss: 0.1626	LR: 0.012500
epoch 81 training time consumed: 54.12s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  297274 GB |  297274 GB |
|       from large pool |  123392 KB |    1034 MB |  296982 GB |  296981 GB |
|       from small pool |   10798 KB |      13 MB |     292 GB |     292 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  297274 GB |  297274 GB |
|       from large pool |  123392 KB |    1034 MB |  296982 GB |  296981 GB |
|       from small pool |   10798 KB |      13 MB |     292 GB |     292 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  130818 GB |  130818 GB |
|       from large pool |  155136 KB |  433088 KB |  130494 GB |  130494 GB |
|       from small pool |    1490 KB |    3494 KB |     323 GB |     323 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   11470 K  |   11470 K  |
|       from large pool |      24    |      65    |    5987 K  |    5987 K  |
|       from small pool |     231    |     274    |    5483 K  |    5483 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   11470 K  |   11470 K  |
|       from large pool |      24    |      65    |    5987 K  |    5987 K  |
|       from small pool |     231    |     274    |    5483 K  |    5483 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5667 K  |    5667 K  |
|       from large pool |       9    |      14    |    2898 K  |    2898 K  |
|       from small pool |      12    |      16    |    2769 K  |    2769 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 81, Average loss: 0.0089, Accuracy: 0.7228, Time consumed:3.45s

saving weights file to checkpoint/resnet18/Saturday_27_May_2023_16h_28m_13s_resnet18_cutout_8_cifar100/resnet18-81-best.pth
Training Epoch: 82 [128/50000]	Loss: 0.1151	LR: 0.012500
Training Epoch: 82 [256/50000]	Loss: 0.0848	LR: 0.012500
Training Epoch: 82 [384/50000]	Loss: 0.1298	LR: 0.012500
Training Epoch: 82 [512/50000]	Loss: 0.0685	LR: 0.012500
Training Epoch: 82 [640/50000]	Loss: 0.1560	LR: 0.012500
Training Epoch: 82 [768/50000]	Loss: 0.1454	LR: 0.012500
Training Epoch: 82 [896/50000]	Loss: 0.1094	LR: 0.012500
Training Epoch: 82 [1024/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 82 [1152/50000]	Loss: 0.1458	LR: 0.012500
Training Epoch: 82 [1280/50000]	Loss: 0.0995	LR: 0.012500
Training Epoch: 82 [1408/50000]	Loss: 0.1020	LR: 0.012500
Training Epoch: 82 [1536/50000]	Loss: 0.1566	LR: 0.012500
Training Epoch: 82 [1664/50000]	Loss: 0.1190	LR: 0.012500
Training Epoch: 82 [1792/50000]	Loss: 0.1145	LR: 0.012500
Training Epoch: 82 [1920/50000]	Loss: 0.1571	LR: 0.012500
Training Epoch: 82 [2048/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 82 [2176/50000]	Loss: 0.1339	LR: 0.012500
Training Epoch: 82 [2304/50000]	Loss: 0.1240	LR: 0.012500
Training Epoch: 82 [2432/50000]	Loss: 0.1386	LR: 0.012500
Training Epoch: 82 [2560/50000]	Loss: 0.1171	LR: 0.012500
Training Epoch: 82 [2688/50000]	Loss: 0.1644	LR: 0.012500
Training Epoch: 82 [2816/50000]	Loss: 0.1639	LR: 0.012500
Training Epoch: 82 [2944/50000]	Loss: 0.1643	LR: 0.012500
Training Epoch: 82 [3072/50000]	Loss: 0.1776	LR: 0.012500
Training Epoch: 82 [3200/50000]	Loss: 0.0622	LR: 0.012500
Training Epoch: 82 [3328/50000]	Loss: 0.1487	LR: 0.012500
Training Epoch: 82 [3456/50000]	Loss: 0.0840	LR: 0.012500
Training Epoch: 82 [3584/50000]	Loss: 0.1042	LR: 0.012500
Training Epoch: 82 [3712/50000]	Loss: 0.1132	LR: 0.012500
Training Epoch: 82 [3840/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 82 [3968/50000]	Loss: 0.0712	LR: 0.012500
Training Epoch: 82 [4096/50000]	Loss: 0.2071	LR: 0.012500
Training Epoch: 82 [4224/50000]	Loss: 0.1157	LR: 0.012500
Training Epoch: 82 [4352/50000]	Loss: 0.2261	LR: 0.012500
Training Epoch: 82 [4480/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 82 [4608/50000]	Loss: 0.1538	LR: 0.012500
Training Epoch: 82 [4736/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 82 [4864/50000]	Loss: 0.1660	LR: 0.012500
Training Epoch: 82 [4992/50000]	Loss: 0.1375	LR: 0.012500
Training Epoch: 82 [5120/50000]	Loss: 0.1129	LR: 0.012500
Training Epoch: 82 [5248/50000]	Loss: 0.1370	LR: 0.012500
Training Epoch: 82 [5376/50000]	Loss: 0.1423	LR: 0.012500
Training Epoch: 82 [5504/50000]	Loss: 0.1500	LR: 0.012500
Training Epoch: 82 [5632/50000]	Loss: 0.1154	LR: 0.012500
Training Epoch: 82 [5760/50000]	Loss: 0.1556	LR: 0.012500
Training Epoch: 82 [5888/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 82 [6016/50000]	Loss: 0.1346	LR: 0.012500
Training Epoch: 82 [6144/50000]	Loss: 0.1098	LR: 0.012500
Training Epoch: 82 [6272/50000]	Loss: 0.1135	LR: 0.012500
Training Epoch: 82 [6400/50000]	Loss: 0.1812	LR: 0.012500
Training Epoch: 82 [6528/50000]	Loss: 0.1218	LR: 0.012500
Training Epoch: 82 [6656/50000]	Loss: 0.1252	LR: 0.012500
Training Epoch: 82 [6784/50000]	Loss: 0.1339	LR: 0.012500
Training Epoch: 82 [6912/50000]	Loss: 0.1851	LR: 0.012500
Training Epoch: 82 [7040/50000]	Loss: 0.1738	LR: 0.012500
Training Epoch: 82 [7168/50000]	Loss: 0.0977	LR: 0.012500
Training Epoch: 82 [7296/50000]	Loss: 0.0642	LR: 0.012500
Training Epoch: 82 [7424/50000]	Loss: 0.1052	LR: 0.012500
Training Epoch: 82 [7552/50000]	Loss: 0.0535	LR: 0.012500
Training Epoch: 82 [7680/50000]	Loss: 0.1790	LR: 0.012500
Training Epoch: 82 [7808/50000]	Loss: 0.1120	LR: 0.012500
Training Epoch: 82 [7936/50000]	Loss: 0.1619	LR: 0.012500
Training Epoch: 82 [8064/50000]	Loss: 0.0798	LR: 0.012500
Training Epoch: 82 [8192/50000]	Loss: 0.1333	LR: 0.012500
Training Epoch: 82 [8320/50000]	Loss: 0.1817	LR: 0.012500
Training Epoch: 82 [8448/50000]	Loss: 0.1313	LR: 0.012500
Training Epoch: 82 [8576/50000]	Loss: 0.1715	LR: 0.012500
Training Epoch: 82 [8704/50000]	Loss: 0.1101	LR: 0.012500
Training Epoch: 82 [8832/50000]	Loss: 0.1317	LR: 0.012500
Training Epoch: 82 [8960/50000]	Loss: 0.1546	LR: 0.012500
Training Epoch: 82 [9088/50000]	Loss: 0.1045	LR: 0.012500
Training Epoch: 82 [9216/50000]	Loss: 0.1460	LR: 0.012500
Training Epoch: 82 [9344/50000]	Loss: 0.0837	LR: 0.012500
Training Epoch: 82 [9472/50000]	Loss: 0.1746	LR: 0.012500
Training Epoch: 82 [9600/50000]	Loss: 0.1550	LR: 0.012500
Training Epoch: 82 [9728/50000]	Loss: 0.1535	LR: 0.012500
Training Epoch: 82 [9856/50000]	Loss: 0.1432	LR: 0.012500
Training Epoch: 82 [9984/50000]	Loss: 0.0836	LR: 0.012500
Training Epoch: 82 [10112/50000]	Loss: 0.1045	LR: 0.012500
Training Epoch: 82 [10240/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 82 [10368/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 82 [10496/50000]	Loss: 0.1435	LR: 0.012500
Training Epoch: 82 [10624/50000]	Loss: 0.1508	LR: 0.012500
Training Epoch: 82 [10752/50000]	Loss: 0.1188	LR: 0.012500
Training Epoch: 82 [10880/50000]	Loss: 0.2120	LR: 0.012500
Training Epoch: 82 [11008/50000]	Loss: 0.1134	LR: 0.012500
Training Epoch: 82 [11136/50000]	Loss: 0.0846	LR: 0.012500
Training Epoch: 82 [11264/50000]	Loss: 0.1151	LR: 0.012500
Training Epoch: 82 [11392/50000]	Loss: 0.1951	LR: 0.012500
Training Epoch: 82 [11520/50000]	Loss: 0.0756	LR: 0.012500
Training Epoch: 82 [11648/50000]	Loss: 0.1061	LR: 0.012500
Training Epoch: 82 [11776/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 82 [11904/50000]	Loss: 0.1428	LR: 0.012500
Training Epoch: 82 [12032/50000]	Loss: 0.1261	LR: 0.012500
Training Epoch: 82 [12160/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 82 [12288/50000]	Loss: 0.0981	LR: 0.012500
Training Epoch: 82 [12416/50000]	Loss: 0.1058	LR: 0.012500
Training Epoch: 82 [12544/50000]	Loss: 0.1385	LR: 0.012500
Training Epoch: 82 [12672/50000]	Loss: 0.1864	LR: 0.012500
Training Epoch: 82 [12800/50000]	Loss: 0.1545	LR: 0.012500
Training Epoch: 82 [12928/50000]	Loss: 0.0839	LR: 0.012500
Training Epoch: 82 [13056/50000]	Loss: 0.1777	LR: 0.012500
Training Epoch: 82 [13184/50000]	Loss: 0.1656	LR: 0.012500
Training Epoch: 82 [13312/50000]	Loss: 0.1014	LR: 0.012500
Training Epoch: 82 [13440/50000]	Loss: 0.1395	LR: 0.012500
Training Epoch: 82 [13568/50000]	Loss: 0.1318	LR: 0.012500
Training Epoch: 82 [13696/50000]	Loss: 0.1650	LR: 0.012500
Training Epoch: 82 [13824/50000]	Loss: 0.1686	LR: 0.012500
Training Epoch: 82 [13952/50000]	Loss: 0.1468	LR: 0.012500
Training Epoch: 82 [14080/50000]	Loss: 0.1384	LR: 0.012500
Training Epoch: 82 [14208/50000]	Loss: 0.1342	LR: 0.012500
Training Epoch: 82 [14336/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 82 [14464/50000]	Loss: 0.0999	LR: 0.012500
Training Epoch: 82 [14592/50000]	Loss: 0.1005	LR: 0.012500
Training Epoch: 82 [14720/50000]	Loss: 0.1518	LR: 0.012500
Training Epoch: 82 [14848/50000]	Loss: 0.1570	LR: 0.012500
Training Epoch: 82 [14976/50000]	Loss: 0.1329	LR: 0.012500
Training Epoch: 82 [15104/50000]	Loss: 0.0941	LR: 0.012500
Training Epoch: 82 [15232/50000]	Loss: 0.1512	LR: 0.012500
Training Epoch: 82 [15360/50000]	Loss: 0.1328	LR: 0.012500
Training Epoch: 82 [15488/50000]	Loss: 0.1416	LR: 0.012500
Training Epoch: 82 [15616/50000]	Loss: 0.0727	LR: 0.012500
Training Epoch: 82 [15744/50000]	Loss: 0.1978	LR: 0.012500
Training Epoch: 82 [15872/50000]	Loss: 0.1232	LR: 0.012500
Training Epoch: 82 [16000/50000]	Loss: 0.1240	LR: 0.012500
Training Epoch: 82 [16128/50000]	Loss: 0.1980	LR: 0.012500
Training Epoch: 82 [16256/50000]	Loss: 0.1118	LR: 0.012500
Training Epoch: 82 [16384/50000]	Loss: 0.0979	LR: 0.012500
Training Epoch: 82 [16512/50000]	Loss: 0.1619	LR: 0.012500
Training Epoch: 82 [16640/50000]	Loss: 0.1969	LR: 0.012500
Training Epoch: 82 [16768/50000]	Loss: 0.1109	LR: 0.012500
Training Epoch: 82 [16896/50000]	Loss: 0.1042	LR: 0.012500
Training Epoch: 82 [17024/50000]	Loss: 0.0606	LR: 0.012500
Training Epoch: 82 [17152/50000]	Loss: 0.0935	LR: 0.012500
Training Epoch: 82 [17280/50000]	Loss: 0.0958	LR: 0.012500
Training Epoch: 82 [17408/50000]	Loss: 0.1385	LR: 0.012500
Training Epoch: 82 [17536/50000]	Loss: 0.1755	LR: 0.012500
Training Epoch: 82 [17664/50000]	Loss: 0.1065	LR: 0.012500
Training Epoch: 82 [17792/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 82 [17920/50000]	Loss: 0.1571	LR: 0.012500
Training Epoch: 82 [18048/50000]	Loss: 0.0957	LR: 0.012500
Training Epoch: 82 [18176/50000]	Loss: 0.1315	LR: 0.012500
Training Epoch: 82 [18304/50000]	Loss: 0.0938	LR: 0.012500
Training Epoch: 82 [18432/50000]	Loss: 0.1370	LR: 0.012500
Training Epoch: 82 [18560/50000]	Loss: 0.1876	LR: 0.012500
Training Epoch: 82 [18688/50000]	Loss: 0.1805	LR: 0.012500
Training Epoch: 82 [18816/50000]	Loss: 0.1314	LR: 0.012500
Training Epoch: 82 [18944/50000]	Loss: 0.1240	LR: 0.012500
Training Epoch: 82 [19072/50000]	Loss: 0.0946	LR: 0.012500
Training Epoch: 82 [19200/50000]	Loss: 0.1265	LR: 0.012500
Training Epoch: 82 [19328/50000]	Loss: 0.0888	LR: 0.012500
Training Epoch: 82 [19456/50000]	Loss: 0.1196	LR: 0.012500
Training Epoch: 82 [19584/50000]	Loss: 0.0787	LR: 0.012500
Training Epoch: 82 [19712/50000]	Loss: 0.1495	LR: 0.012500
Training Epoch: 82 [19840/50000]	Loss: 0.1383	LR: 0.012500
Training Epoch: 82 [19968/50000]	Loss: 0.1138	LR: 0.012500
Training Epoch: 82 [20096/50000]	Loss: 0.1903	LR: 0.012500
Training Epoch: 82 [20224/50000]	Loss: 0.1295	LR: 0.012500
Training Epoch: 82 [20352/50000]	Loss: 0.1388	LR: 0.012500
Training Epoch: 82 [20480/50000]	Loss: 0.1892	LR: 0.012500
Training Epoch: 82 [20608/50000]	Loss: 0.1066	LR: 0.012500
Training Epoch: 82 [20736/50000]	Loss: 0.1335	LR: 0.012500
Training Epoch: 82 [20864/50000]	Loss: 0.1301	LR: 0.012500
Training Epoch: 82 [20992/50000]	Loss: 0.1556	LR: 0.012500
Training Epoch: 82 [21120/50000]	Loss: 0.1356	LR: 0.012500
Training Epoch: 82 [21248/50000]	Loss: 0.2008	LR: 0.012500
Training Epoch: 82 [21376/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 82 [21504/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 82 [21632/50000]	Loss: 0.1462	LR: 0.012500
Training Epoch: 82 [21760/50000]	Loss: 0.0835	LR: 0.012500
Training Epoch: 82 [21888/50000]	Loss: 0.1797	LR: 0.012500
Training Epoch: 82 [22016/50000]	Loss: 0.1153	LR: 0.012500
Training Epoch: 82 [22144/50000]	Loss: 0.1613	LR: 0.012500
Training Epoch: 82 [22272/50000]	Loss: 0.1626	LR: 0.012500
Training Epoch: 82 [22400/50000]	Loss: 0.1616	LR: 0.012500
Training Epoch: 82 [22528/50000]	Loss: 0.1496	LR: 0.012500
Training Epoch: 82 [22656/50000]	Loss: 0.0788	LR: 0.012500
Training Epoch: 82 [22784/50000]	Loss: 0.1257	LR: 0.012500
Training Epoch: 82 [22912/50000]	Loss: 0.0991	LR: 0.012500
Training Epoch: 82 [23040/50000]	Loss: 0.1251	LR: 0.012500
Training Epoch: 82 [23168/50000]	Loss: 0.0983	LR: 0.012500
Training Epoch: 82 [23296/50000]	Loss: 0.1010	LR: 0.012500
Training Epoch: 82 [23424/50000]	Loss: 0.1427	LR: 0.012500
Training Epoch: 82 [23552/50000]	Loss: 0.0942	LR: 0.012500
Training Epoch: 82 [23680/50000]	Loss: 0.1563	LR: 0.012500
Training Epoch: 82 [23808/50000]	Loss: 0.1222	LR: 0.012500
Training Epoch: 82 [23936/50000]	Loss: 0.1135	LR: 0.012500
Training Epoch: 82 [24064/50000]	Loss: 0.1193	LR: 0.012500
Training Epoch: 82 [24192/50000]	Loss: 0.1174	LR: 0.012500
Training Epoch: 82 [24320/50000]	Loss: 0.0985	LR: 0.012500
Training Epoch: 82 [24448/50000]	Loss: 0.1327	LR: 0.012500
Training Epoch: 82 [24576/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 82 [24704/50000]	Loss: 0.1640	LR: 0.012500
Training Epoch: 82 [24832/50000]	Loss: 0.0844	LR: 0.012500
Training Epoch: 82 [24960/50000]	Loss: 0.1073	LR: 0.012500
Training Epoch: 82 [25088/50000]	Loss: 0.1236	LR: 0.012500
Training Epoch: 82 [25216/50000]	Loss: 0.1299	LR: 0.012500
Training Epoch: 82 [25344/50000]	Loss: 0.0929	LR: 0.012500
Training Epoch: 82 [25472/50000]	Loss: 0.1016	LR: 0.012500
Training Epoch: 82 [25600/50000]	Loss: 0.1127	LR: 0.012500
Training Epoch: 82 [25728/50000]	Loss: 0.1784	LR: 0.012500
Training Epoch: 82 [25856/50000]	Loss: 0.1845	LR: 0.012500
Training Epoch: 82 [25984/50000]	Loss: 0.1479	LR: 0.012500
Training Epoch: 82 [26112/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 82 [26240/50000]	Loss: 0.0806	LR: 0.012500
Training Epoch: 82 [26368/50000]	Loss: 0.1188	LR: 0.012500
Training Epoch: 82 [26496/50000]	Loss: 0.2190	LR: 0.012500
Training Epoch: 82 [26624/50000]	Loss: 0.0847	LR: 0.012500
Training Epoch: 82 [26752/50000]	Loss: 0.0889	LR: 0.012500
Training Epoch: 82 [26880/50000]	Loss: 0.1546	LR: 0.012500
Training Epoch: 82 [27008/50000]	Loss: 0.0835	LR: 0.012500
Training Epoch: 82 [27136/50000]	Loss: 0.1646	LR: 0.012500
Training Epoch: 82 [27264/50000]	Loss: 0.1463	LR: 0.012500
Training Epoch: 82 [27392/50000]	Loss: 0.1653	LR: 0.012500
Training Epoch: 82 [27520/50000]	Loss: 0.0905	LR: 0.012500
Training Epoch: 82 [27648/50000]	Loss: 0.0792	LR: 0.012500
Training Epoch: 82 [27776/50000]	Loss: 0.1704	LR: 0.012500
Training Epoch: 82 [27904/50000]	Loss: 0.1309	LR: 0.012500
Training Epoch: 82 [28032/50000]	Loss: 0.0812	LR: 0.012500
Training Epoch: 82 [28160/50000]	Loss: 0.0989	LR: 0.012500
Training Epoch: 82 [28288/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 82 [28416/50000]	Loss: 0.1812	LR: 0.012500
Training Epoch: 82 [28544/50000]	Loss: 0.1156	LR: 0.012500
Training Epoch: 82 [28672/50000]	Loss: 0.1741	LR: 0.012500
Training Epoch: 82 [28800/50000]	Loss: 0.1961	LR: 0.012500
Training Epoch: 82 [28928/50000]	Loss: 0.1506	LR: 0.012500
Training Epoch: 82 [29056/50000]	Loss: 0.1500	LR: 0.012500
Training Epoch: 82 [29184/50000]	Loss: 0.1046	LR: 0.012500
Training Epoch: 82 [29312/50000]	Loss: 0.1397	LR: 0.012500
Training Epoch: 82 [29440/50000]	Loss: 0.1486	LR: 0.012500
Training Epoch: 82 [29568/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 82 [29696/50000]	Loss: 0.1601	LR: 0.012500
Training Epoch: 82 [29824/50000]	Loss: 0.1191	LR: 0.012500
Training Epoch: 82 [29952/50000]	Loss: 0.0626	LR: 0.012500
Training Epoch: 82 [30080/50000]	Loss: 0.1256	LR: 0.012500
Training Epoch: 82 [30208/50000]	Loss: 0.0920	LR: 0.012500
Training Epoch: 82 [30336/50000]	Loss: 0.1710	LR: 0.012500
Training Epoch: 82 [30464/50000]	Loss: 0.1051	LR: 0.012500
Training Epoch: 82 [30592/50000]	Loss: 0.1886	LR: 0.012500
Training Epoch: 82 [30720/50000]	Loss: 0.1391	LR: 0.012500
Training Epoch: 82 [30848/50000]	Loss: 0.1520	LR: 0.012500
Training Epoch: 82 [30976/50000]	Loss: 0.1011	LR: 0.012500
Training Epoch: 82 [31104/50000]	Loss: 0.1818	LR: 0.012500
Training Epoch: 82 [31232/50000]	Loss: 0.1622	LR: 0.012500
Training Epoch: 82 [31360/50000]	Loss: 0.1418	LR: 0.012500
Training Epoch: 82 [31488/50000]	Loss: 0.1778	LR: 0.012500
Training Epoch: 82 [31616/50000]	Loss: 0.3000	LR: 0.012500
Training Epoch: 82 [31744/50000]	Loss: 0.0971	LR: 0.012500
Training Epoch: 82 [31872/50000]	Loss: 0.1105	LR: 0.012500
Training Epoch: 82 [32000/50000]	Loss: 0.1389	LR: 0.012500
Training Epoch: 82 [32128/50000]	Loss: 0.0820	LR: 0.012500
Training Epoch: 82 [32256/50000]	Loss: 0.1672	LR: 0.012500
Training Epoch: 82 [32384/50000]	Loss: 0.2053	LR: 0.012500
Training Epoch: 82 [32512/50000]	Loss: 0.1295	LR: 0.012500
Training Epoch: 82 [32640/50000]	Loss: 0.0951	LR: 0.012500
Training Epoch: 82 [32768/50000]	Loss: 0.1241	LR: 0.012500
Training Epoch: 82 [32896/50000]	Loss: 0.1494	LR: 0.012500
Training Epoch: 82 [33024/50000]	Loss: 0.2102	LR: 0.012500
Training Epoch: 82 [33152/50000]	Loss: 0.1962	LR: 0.012500
Training Epoch: 82 [33280/50000]	Loss: 0.1733	LR: 0.012500
Training Epoch: 82 [33408/50000]	Loss: 0.0839	LR: 0.012500
Training Epoch: 82 [33536/50000]	Loss: 0.1355	LR: 0.012500
Training Epoch: 82 [33664/50000]	Loss: 0.1379	LR: 0.012500
Training Epoch: 82 [33792/50000]	Loss: 0.1146	LR: 0.012500
Training Epoch: 82 [33920/50000]	Loss: 0.1295	LR: 0.012500
Training Epoch: 82 [34048/50000]	Loss: 0.1469	LR: 0.012500
Training Epoch: 82 [34176/50000]	Loss: 0.1238	LR: 0.012500
Training Epoch: 82 [34304/50000]	Loss: 0.1556	LR: 0.012500
Training Epoch: 82 [34432/50000]	Loss: 0.1715	LR: 0.012500
Training Epoch: 82 [34560/50000]	Loss: 0.1673	LR: 0.012500
Training Epoch: 82 [34688/50000]	Loss: 0.1321	LR: 0.012500
Training Epoch: 82 [34816/50000]	Loss: 0.0857	LR: 0.012500
Training Epoch: 82 [34944/50000]	Loss: 0.1156	LR: 0.012500
Training Epoch: 82 [35072/50000]	Loss: 0.1352	LR: 0.012500
Training Epoch: 82 [35200/50000]	Loss: 0.1367	LR: 0.012500
Training Epoch: 82 [35328/50000]	Loss: 0.1234	LR: 0.012500
Training Epoch: 82 [35456/50000]	Loss: 0.1569	LR: 0.012500
Training Epoch: 82 [35584/50000]	Loss: 0.0897	LR: 0.012500
Training Epoch: 82 [35712/50000]	Loss: 0.2245	LR: 0.012500
Training Epoch: 82 [35840/50000]	Loss: 0.1528	LR: 0.012500
Training Epoch: 82 [35968/50000]	Loss: 0.1767	LR: 0.012500
Training Epoch: 82 [36096/50000]	Loss: 0.1287	LR: 0.012500
Training Epoch: 82 [36224/50000]	Loss: 0.0877	LR: 0.012500
Training Epoch: 82 [36352/50000]	Loss: 0.1219	LR: 0.012500
Training Epoch: 82 [36480/50000]	Loss: 0.1178	LR: 0.012500
Training Epoch: 82 [36608/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 82 [36736/50000]	Loss: 0.1302	LR: 0.012500
Training Epoch: 82 [36864/50000]	Loss: 0.1123	LR: 0.012500
Training Epoch: 82 [36992/50000]	Loss: 0.1587	LR: 0.012500
Training Epoch: 82 [37120/50000]	Loss: 0.1433	LR: 0.012500
Training Epoch: 82 [37248/50000]	Loss: 0.2071	LR: 0.012500
Training Epoch: 82 [37376/50000]	Loss: 0.1411	LR: 0.012500
Training Epoch: 82 [37504/50000]	Loss: 0.0928	LR: 0.012500
Training Epoch: 82 [37632/50000]	Loss: 0.1417	LR: 0.012500
Training Epoch: 82 [37760/50000]	Loss: 0.2634	LR: 0.012500
Training Epoch: 82 [37888/50000]	Loss: 0.1019	LR: 0.012500
Training Epoch: 82 [38016/50000]	Loss: 0.1646	LR: 0.012500
Training Epoch: 82 [38144/50000]	Loss: 0.1815	LR: 0.012500
Training Epoch: 82 [38272/50000]	Loss: 0.1334	LR: 0.012500
Training Epoch: 82 [38400/50000]	Loss: 0.1392	LR: 0.012500
Training Epoch: 82 [38528/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 82 [38656/50000]	Loss: 0.1863	LR: 0.012500
Training Epoch: 82 [38784/50000]	Loss: 0.0985	LR: 0.012500
Training Epoch: 82 [38912/50000]	Loss: 0.1602	LR: 0.012500
Training Epoch: 82 [39040/50000]	Loss: 0.1802	LR: 0.012500
Training Epoch: 82 [39168/50000]	Loss: 0.1134	LR: 0.012500
Training Epoch: 82 [39296/50000]	Loss: 0.1613	LR: 0.012500
Training Epoch: 82 [39424/50000]	Loss: 0.1792	LR: 0.012500
Training Epoch: 82 [39552/50000]	Loss: 0.1839	LR: 0.012500
Training Epoch: 82 [39680/50000]	Loss: 0.1287	LR: 0.012500
Training Epoch: 82 [39808/50000]	Loss: 0.1726	LR: 0.012500
Training Epoch: 82 [39936/50000]	Loss: 0.1152	LR: 0.012500
Training Epoch: 82 [40064/50000]	Loss: 0.2396	LR: 0.012500
Training Epoch: 82 [40192/50000]	Loss: 0.1968	LR: 0.012500
Training Epoch: 82 [40320/50000]	Loss: 0.1137	LR: 0.012500
Training Epoch: 82 [40448/50000]	Loss: 0.1767	LR: 0.012500
Training Epoch: 82 [40576/50000]	Loss: 0.0843	LR: 0.012500
Training Epoch: 82 [40704/50000]	Loss: 0.1605	LR: 0.012500
Training Epoch: 82 [40832/50000]	Loss: 0.1181	LR: 0.012500
Training Epoch: 82 [40960/50000]	Loss: 0.1334	LR: 0.012500
Training Epoch: 82 [41088/50000]	Loss: 0.1217	LR: 0.012500
Training Epoch: 82 [41216/50000]	Loss: 0.1234	LR: 0.012500
Training Epoch: 82 [41344/50000]	Loss: 0.0934	LR: 0.012500
Training Epoch: 82 [41472/50000]	Loss: 0.1530	LR: 0.012500
Training Epoch: 82 [41600/50000]	Loss: 0.1491	LR: 0.012500
Training Epoch: 82 [41728/50000]	Loss: 0.1187	LR: 0.012500
Training Epoch: 82 [41856/50000]	Loss: 0.2262	LR: 0.012500
Training Epoch: 82 [41984/50000]	Loss: 0.1324	LR: 0.012500
Training Epoch: 82 [42112/50000]	Loss: 0.1434	LR: 0.012500
Training Epoch: 82 [42240/50000]	Loss: 0.1626	LR: 0.012500
Training Epoch: 82 [42368/50000]	Loss: 0.1552	LR: 0.012500
Training Epoch: 82 [42496/50000]	Loss: 0.1048	LR: 0.012500
Training Epoch: 82 [42624/50000]	Loss: 0.2014	LR: 0.012500
Training Epoch: 82 [42752/50000]	Loss: 0.1780	LR: 0.012500
Training Epoch: 82 [42880/50000]	Loss: 0.1349	LR: 0.012500
Training Epoch: 82 [43008/50000]	Loss: 0.1950	LR: 0.012500
Training Epoch: 82 [43136/50000]	Loss: 0.1136	LR: 0.012500
Training Epoch: 82 [43264/50000]	Loss: 0.1647	LR: 0.012500
Training Epoch: 82 [43392/50000]	Loss: 0.1213	LR: 0.012500
Training Epoch: 82 [43520/50000]	Loss: 0.0907	LR: 0.012500
Training Epoch: 82 [43648/50000]	Loss: 0.1593	LR: 0.012500
Training Epoch: 82 [43776/50000]	Loss: 0.1218	LR: 0.012500
Training Epoch: 82 [43904/50000]	Loss: 0.2310	LR: 0.012500
Training Epoch: 82 [44032/50000]	Loss: 0.1983	LR: 0.012500
Training Epoch: 82 [44160/50000]	Loss: 0.2030	LR: 0.012500
Training Epoch: 82 [44288/50000]	Loss: 0.1389	LR: 0.012500
Training Epoch: 82 [44416/50000]	Loss: 0.0816	LR: 0.012500
Training Epoch: 82 [44544/50000]	Loss: 0.1416	LR: 0.012500
Training Epoch: 82 [44672/50000]	Loss: 0.1779	LR: 0.012500
Training Epoch: 82 [44800/50000]	Loss: 0.1413	LR: 0.012500
Training Epoch: 82 [44928/50000]	Loss: 0.1695	LR: 0.012500
Training Epoch: 82 [45056/50000]	Loss: 0.1704	LR: 0.012500
Training Epoch: 82 [45184/50000]	Loss: 0.1293	LR: 0.012500
Training Epoch: 82 [45312/50000]	Loss: 0.1546	LR: 0.012500
Training Epoch: 82 [45440/50000]	Loss: 0.1664	LR: 0.012500
Training Epoch: 82 [45568/50000]	Loss: 0.2257	LR: 0.012500
Training Epoch: 82 [45696/50000]	Loss: 0.1504	LR: 0.012500
Training Epoch: 82 [45824/50000]	Loss: 0.1148	LR: 0.012500
Training Epoch: 82 [45952/50000]	Loss: 0.1627	LR: 0.012500
Training Epoch: 82 [46080/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 82 [46208/50000]	Loss: 0.1602	LR: 0.012500
Training Epoch: 82 [46336/50000]	Loss: 0.1910	LR: 0.012500
Training Epoch: 82 [46464/50000]	Loss: 0.2282	LR: 0.012500
Training Epoch: 82 [46592/50000]	Loss: 0.1421	LR: 0.012500
Training Epoch: 82 [46720/50000]	Loss: 0.1758	LR: 0.012500
Training Epoch: 82 [46848/50000]	Loss: 0.1581	LR: 0.012500
Training Epoch: 82 [46976/50000]	Loss: 0.1273	LR: 0.012500
Training Epoch: 82 [47104/50000]	Loss: 0.2283	LR: 0.012500
Training Epoch: 82 [47232/50000]	Loss: 0.1400	LR: 0.012500
Training Epoch: 82 [47360/50000]	Loss: 0.1552	LR: 0.012500
Training Epoch: 82 [47488/50000]	Loss: 0.1584	LR: 0.012500
Training Epoch: 82 [47616/50000]	Loss: 0.1243	LR: 0.012500
Training Epoch: 82 [47744/50000]	Loss: 0.1310	LR: 0.012500
Training Epoch: 82 [47872/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 82 [48000/50000]	Loss: 0.1468	LR: 0.012500
Training Epoch: 82 [48128/50000]	Loss: 0.0735	LR: 0.012500
Training Epoch: 82 [48256/50000]	Loss: 0.0849	LR: 0.012500
Training Epoch: 82 [48384/50000]	Loss: 0.2009	LR: 0.012500
Training Epoch: 82 [48512/50000]	Loss: 0.2680	LR: 0.012500
Training Epoch: 82 [48640/50000]	Loss: 0.1199	LR: 0.012500
Training Epoch: 82 [48768/50000]	Loss: 0.0645	LR: 0.012500
Training Epoch: 82 [48896/50000]	Loss: 0.2285	LR: 0.012500
Training Epoch: 82 [49024/50000]	Loss: 0.1511	LR: 0.012500
Training Epoch: 82 [49152/50000]	Loss: 0.1825	LR: 0.012500
Training Epoch: 82 [49280/50000]	Loss: 0.1811	LR: 0.012500
Training Epoch: 82 [49408/50000]	Loss: 0.1743	LR: 0.012500
Training Epoch: 82 [49536/50000]	Loss: 0.0981	LR: 0.012500
Training Epoch: 82 [49664/50000]	Loss: 0.2093	LR: 0.012500
Training Epoch: 82 [49792/50000]	Loss: 0.1190	LR: 0.012500
Training Epoch: 82 [49920/50000]	Loss: 0.1286	LR: 0.012500
Training Epoch: 82 [50000/50000]	Loss: 0.0956	LR: 0.012500
epoch 82 training time consumed: 53.87s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  300945 GB |  300944 GB |
|       from large pool |  123392 KB |    1034 MB |  300648 GB |  300648 GB |
|       from small pool |   10798 KB |      13 MB |     296 GB |     296 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  300945 GB |  300944 GB |
|       from large pool |  123392 KB |    1034 MB |  300648 GB |  300648 GB |
|       from small pool |   10798 KB |      13 MB |     296 GB |     296 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  132433 GB |  132433 GB |
|       from large pool |  155136 KB |  433088 KB |  132105 GB |  132105 GB |
|       from small pool |    1490 KB |    3494 KB |     327 GB |     327 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   11612 K  |   11612 K  |
|       from large pool |      24    |      65    |    6061 K  |    6061 K  |
|       from small pool |     231    |     274    |    5550 K  |    5550 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   11612 K  |   11612 K  |
|       from large pool |      24    |      65    |    6061 K  |    6061 K  |
|       from small pool |     231    |     274    |    5550 K  |    5550 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5737 K  |    5737 K  |
|       from large pool |       9    |      14    |    2933 K  |    2933 K  |
|       from small pool |      12    |      16    |    2803 K  |    2803 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 82, Average loss: 0.0090, Accuracy: 0.7275, Time consumed:3.46s

saving weights file to checkpoint/resnet18/Saturday_27_May_2023_16h_28m_13s_resnet18_cutout_8_cifar100/resnet18-82-best.pth
Training Epoch: 83 [128/50000]	Loss: 0.2375	LR: 0.012500
Training Epoch: 83 [256/50000]	Loss: 0.1179	LR: 0.012500
Training Epoch: 83 [384/50000]	Loss: 0.0873	LR: 0.012500
Training Epoch: 83 [512/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 83 [640/50000]	Loss: 0.1418	LR: 0.012500
Training Epoch: 83 [768/50000]	Loss: 0.0919	LR: 0.012500
Training Epoch: 83 [896/50000]	Loss: 0.1196	LR: 0.012500
Training Epoch: 83 [1024/50000]	Loss: 0.1026	LR: 0.012500
Training Epoch: 83 [1152/50000]	Loss: 0.1316	LR: 0.012500
Training Epoch: 83 [1280/50000]	Loss: 0.1048	LR: 0.012500
Training Epoch: 83 [1408/50000]	Loss: 0.1612	LR: 0.012500
Training Epoch: 83 [1536/50000]	Loss: 0.0820	LR: 0.012500
Training Epoch: 83 [1664/50000]	Loss: 0.1225	LR: 0.012500
Training Epoch: 83 [1792/50000]	Loss: 0.0515	LR: 0.012500
Training Epoch: 83 [1920/50000]	Loss: 0.1706	LR: 0.012500
Training Epoch: 83 [2048/50000]	Loss: 0.1286	LR: 0.012500
Training Epoch: 83 [2176/50000]	Loss: 0.0811	LR: 0.012500
Training Epoch: 83 [2304/50000]	Loss: 0.0954	LR: 0.012500
Training Epoch: 83 [2432/50000]	Loss: 0.1039	LR: 0.012500
Training Epoch: 83 [2560/50000]	Loss: 0.1393	LR: 0.012500
Training Epoch: 83 [2688/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 83 [2816/50000]	Loss: 0.2040	LR: 0.012500
Training Epoch: 83 [2944/50000]	Loss: 0.1312	LR: 0.012500
Training Epoch: 83 [3072/50000]	Loss: 0.1039	LR: 0.012500
Training Epoch: 83 [3200/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 83 [3328/50000]	Loss: 0.0978	LR: 0.012500
Training Epoch: 83 [3456/50000]	Loss: 0.0945	LR: 0.012500
Training Epoch: 83 [3584/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 83 [3712/50000]	Loss: 0.1433	LR: 0.012500
Training Epoch: 83 [3840/50000]	Loss: 0.1221	LR: 0.012500
Training Epoch: 83 [3968/50000]	Loss: 0.1301	LR: 0.012500
Training Epoch: 83 [4096/50000]	Loss: 0.1109	LR: 0.012500
Training Epoch: 83 [4224/50000]	Loss: 0.1266	LR: 0.012500
Training Epoch: 83 [4352/50000]	Loss: 0.1353	LR: 0.012500
Training Epoch: 83 [4480/50000]	Loss: 0.1385	LR: 0.012500
Training Epoch: 83 [4608/50000]	Loss: 0.0989	LR: 0.012500
Training Epoch: 83 [4736/50000]	Loss: 0.0879	LR: 0.012500
Training Epoch: 83 [4864/50000]	Loss: 0.1196	LR: 0.012500
Training Epoch: 83 [4992/50000]	Loss: 0.1192	LR: 0.012500
Training Epoch: 83 [5120/50000]	Loss: 0.1201	LR: 0.012500
Training Epoch: 83 [5248/50000]	Loss: 0.1017	LR: 0.012500
Training Epoch: 83 [5376/50000]	Loss: 0.1171	LR: 0.012500
Training Epoch: 83 [5504/50000]	Loss: 0.1591	LR: 0.012500
Training Epoch: 83 [5632/50000]	Loss: 0.1129	LR: 0.012500
Training Epoch: 83 [5760/50000]	Loss: 0.1236	LR: 0.012500
Training Epoch: 83 [5888/50000]	Loss: 0.1266	LR: 0.012500
Training Epoch: 83 [6016/50000]	Loss: 0.1010	LR: 0.012500
Training Epoch: 83 [6144/50000]	Loss: 0.0987	LR: 0.012500
Training Epoch: 83 [6272/50000]	Loss: 0.1252	LR: 0.012500
Training Epoch: 83 [6400/50000]	Loss: 0.1494	LR: 0.012500
Training Epoch: 83 [6528/50000]	Loss: 0.1203	LR: 0.012500
Training Epoch: 83 [6656/50000]	Loss: 0.1067	LR: 0.012500
Training Epoch: 83 [6784/50000]	Loss: 0.1455	LR: 0.012500
Training Epoch: 83 [6912/50000]	Loss: 0.1062	LR: 0.012500
Training Epoch: 83 [7040/50000]	Loss: 0.1060	LR: 0.012500
Training Epoch: 83 [7168/50000]	Loss: 0.1774	LR: 0.012500
Training Epoch: 83 [7296/50000]	Loss: 0.1248	LR: 0.012500
Training Epoch: 83 [7424/50000]	Loss: 0.1841	LR: 0.012500
Training Epoch: 83 [7552/50000]	Loss: 0.1010	LR: 0.012500
Training Epoch: 83 [7680/50000]	Loss: 0.1514	LR: 0.012500
Training Epoch: 83 [7808/50000]	Loss: 0.0988	LR: 0.012500
Training Epoch: 83 [7936/50000]	Loss: 0.1299	LR: 0.012500
Training Epoch: 83 [8064/50000]	Loss: 0.0780	LR: 0.012500
Training Epoch: 83 [8192/50000]	Loss: 0.1135	LR: 0.012500
Training Epoch: 83 [8320/50000]	Loss: 0.0935	LR: 0.012500
Training Epoch: 83 [8448/50000]	Loss: 0.1254	LR: 0.012500
Training Epoch: 83 [8576/50000]	Loss: 0.0770	LR: 0.012500
Training Epoch: 83 [8704/50000]	Loss: 0.1456	LR: 0.012500
Training Epoch: 83 [8832/50000]	Loss: 0.1250	LR: 0.012500
Training Epoch: 83 [8960/50000]	Loss: 0.0947	LR: 0.012500
Training Epoch: 83 [9088/50000]	Loss: 0.0937	LR: 0.012500
Training Epoch: 83 [9216/50000]	Loss: 0.0728	LR: 0.012500
Training Epoch: 83 [9344/50000]	Loss: 0.0992	LR: 0.012500
Training Epoch: 83 [9472/50000]	Loss: 0.0945	LR: 0.012500
Training Epoch: 83 [9600/50000]	Loss: 0.1018	LR: 0.012500
Training Epoch: 83 [9728/50000]	Loss: 0.1408	LR: 0.012500
Training Epoch: 83 [9856/50000]	Loss: 0.1497	LR: 0.012500
Training Epoch: 83 [9984/50000]	Loss: 0.0970	LR: 0.012500
Training Epoch: 83 [10112/50000]	Loss: 0.1009	LR: 0.012500
Training Epoch: 83 [10240/50000]	Loss: 0.1492	LR: 0.012500
Training Epoch: 83 [10368/50000]	Loss: 0.1974	LR: 0.012500
Training Epoch: 83 [10496/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 83 [10624/50000]	Loss: 0.1049	LR: 0.012500
Training Epoch: 83 [10752/50000]	Loss: 0.0959	LR: 0.012500
Training Epoch: 83 [10880/50000]	Loss: 0.1065	LR: 0.012500
Training Epoch: 83 [11008/50000]	Loss: 0.1295	LR: 0.012500
Training Epoch: 83 [11136/50000]	Loss: 0.0841	LR: 0.012500
Training Epoch: 83 [11264/50000]	Loss: 0.1354	LR: 0.012500
Training Epoch: 83 [11392/50000]	Loss: 0.1062	LR: 0.012500
Training Epoch: 83 [11520/50000]	Loss: 0.0828	LR: 0.012500
Training Epoch: 83 [11648/50000]	Loss: 0.0846	LR: 0.012500
Training Epoch: 83 [11776/50000]	Loss: 0.0875	LR: 0.012500
Training Epoch: 83 [11904/50000]	Loss: 0.0676	LR: 0.012500
Training Epoch: 83 [12032/50000]	Loss: 0.1646	LR: 0.012500
Training Epoch: 83 [12160/50000]	Loss: 0.1129	LR: 0.012500
Training Epoch: 83 [12288/50000]	Loss: 0.1870	LR: 0.012500
Training Epoch: 83 [12416/50000]	Loss: 0.1012	LR: 0.012500
Training Epoch: 83 [12544/50000]	Loss: 0.2107	LR: 0.012500
Training Epoch: 83 [12672/50000]	Loss: 0.1581	LR: 0.012500
Training Epoch: 83 [12800/50000]	Loss: 0.1464	LR: 0.012500
Training Epoch: 83 [12928/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 83 [13056/50000]	Loss: 0.1321	LR: 0.012500
Training Epoch: 83 [13184/50000]	Loss: 0.1388	LR: 0.012500
Training Epoch: 83 [13312/50000]	Loss: 0.1434	LR: 0.012500
Training Epoch: 83 [13440/50000]	Loss: 0.1373	LR: 0.012500
Training Epoch: 83 [13568/50000]	Loss: 0.1891	LR: 0.012500
Training Epoch: 83 [13696/50000]	Loss: 0.1043	LR: 0.012500
Training Epoch: 83 [13824/50000]	Loss: 0.1308	LR: 0.012500
Training Epoch: 83 [13952/50000]	Loss: 0.1084	LR: 0.012500
Training Epoch: 83 [14080/50000]	Loss: 0.1277	LR: 0.012500
Training Epoch: 83 [14208/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 83 [14336/50000]	Loss: 0.1672	LR: 0.012500
Training Epoch: 83 [14464/50000]	Loss: 0.1211	LR: 0.012500
Training Epoch: 83 [14592/50000]	Loss: 0.1245	LR: 0.012500
Training Epoch: 83 [14720/50000]	Loss: 0.0721	LR: 0.012500
Training Epoch: 83 [14848/50000]	Loss: 0.1817	LR: 0.012500
Training Epoch: 83 [14976/50000]	Loss: 0.0993	LR: 0.012500
Training Epoch: 83 [15104/50000]	Loss: 0.0977	LR: 0.012500
Training Epoch: 83 [15232/50000]	Loss: 0.0967	LR: 0.012500
Training Epoch: 83 [15360/50000]	Loss: 0.0911	LR: 0.012500
Training Epoch: 83 [15488/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 83 [15616/50000]	Loss: 0.1448	LR: 0.012500
Training Epoch: 83 [15744/50000]	Loss: 0.1057	LR: 0.012500
Training Epoch: 83 [15872/50000]	Loss: 0.0710	LR: 0.012500
Training Epoch: 83 [16000/50000]	Loss: 0.1091	LR: 0.012500
Training Epoch: 83 [16128/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 83 [16256/50000]	Loss: 0.1504	LR: 0.012500
Training Epoch: 83 [16384/50000]	Loss: 0.1169	LR: 0.012500
Training Epoch: 83 [16512/50000]	Loss: 0.1242	LR: 0.012500
Training Epoch: 83 [16640/50000]	Loss: 0.0954	LR: 0.012500
Training Epoch: 83 [16768/50000]	Loss: 0.1728	LR: 0.012500
Training Epoch: 83 [16896/50000]	Loss: 0.0732	LR: 0.012500
Training Epoch: 83 [17024/50000]	Loss: 0.1116	LR: 0.012500
Training Epoch: 83 [17152/50000]	Loss: 0.1571	LR: 0.012500
Training Epoch: 83 [17280/50000]	Loss: 0.1252	LR: 0.012500
Training Epoch: 83 [17408/50000]	Loss: 0.0973	LR: 0.012500
Training Epoch: 83 [17536/50000]	Loss: 0.1310	LR: 0.012500
Training Epoch: 83 [17664/50000]	Loss: 0.0977	LR: 0.012500
Training Epoch: 83 [17792/50000]	Loss: 0.0822	LR: 0.012500
Training Epoch: 83 [17920/50000]	Loss: 0.1258	LR: 0.012500
Training Epoch: 83 [18048/50000]	Loss: 0.1462	LR: 0.012500
Training Epoch: 83 [18176/50000]	Loss: 0.0783	LR: 0.012500
Training Epoch: 83 [18304/50000]	Loss: 0.1420	LR: 0.012500
Training Epoch: 83 [18432/50000]	Loss: 0.1703	LR: 0.012500
Training Epoch: 83 [18560/50000]	Loss: 0.1634	LR: 0.012500
Training Epoch: 83 [18688/50000]	Loss: 0.1008	LR: 0.012500
Training Epoch: 83 [18816/50000]	Loss: 0.1304	LR: 0.012500
Training Epoch: 83 [18944/50000]	Loss: 0.1485	LR: 0.012500
Training Epoch: 83 [19072/50000]	Loss: 0.0555	LR: 0.012500
Training Epoch: 83 [19200/50000]	Loss: 0.1202	LR: 0.012500
Training Epoch: 83 [19328/50000]	Loss: 0.1472	LR: 0.012500
Training Epoch: 83 [19456/50000]	Loss: 0.1625	LR: 0.012500
Training Epoch: 83 [19584/50000]	Loss: 0.1650	LR: 0.012500
Training Epoch: 83 [19712/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 83 [19840/50000]	Loss: 0.1569	LR: 0.012500
Training Epoch: 83 [19968/50000]	Loss: 0.1799	LR: 0.012500
Training Epoch: 83 [20096/50000]	Loss: 0.1183	LR: 0.012500
Training Epoch: 83 [20224/50000]	Loss: 0.1516	LR: 0.012500
Training Epoch: 83 [20352/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 83 [20480/50000]	Loss: 0.1163	LR: 0.012500
Training Epoch: 83 [20608/50000]	Loss: 0.1334	LR: 0.012500
Training Epoch: 83 [20736/50000]	Loss: 0.1213	LR: 0.012500
Training Epoch: 83 [20864/50000]	Loss: 0.1195	LR: 0.012500
Training Epoch: 83 [20992/50000]	Loss: 0.1218	LR: 0.012500
Training Epoch: 83 [21120/50000]	Loss: 0.1064	LR: 0.012500
Training Epoch: 83 [21248/50000]	Loss: 0.0777	LR: 0.012500
Training Epoch: 83 [21376/50000]	Loss: 0.1245	LR: 0.012500
Training Epoch: 83 [21504/50000]	Loss: 0.0695	LR: 0.012500
Training Epoch: 83 [21632/50000]	Loss: 0.1439	LR: 0.012500
Training Epoch: 83 [21760/50000]	Loss: 0.1003	LR: 0.012500
Training Epoch: 83 [21888/50000]	Loss: 0.1504	LR: 0.012500
Training Epoch: 83 [22016/50000]	Loss: 0.1175	LR: 0.012500
Training Epoch: 83 [22144/50000]	Loss: 0.1279	LR: 0.012500
Training Epoch: 83 [22272/50000]	Loss: 0.0948	LR: 0.012500
Training Epoch: 83 [22400/50000]	Loss: 0.1161	LR: 0.012500
Training Epoch: 83 [22528/50000]	Loss: 0.1450	LR: 0.012500
Training Epoch: 83 [22656/50000]	Loss: 0.0861	LR: 0.012500
Training Epoch: 83 [22784/50000]	Loss: 0.1071	LR: 0.012500
Training Epoch: 83 [22912/50000]	Loss: 0.0774	LR: 0.012500
Training Epoch: 83 [23040/50000]	Loss: 0.1123	LR: 0.012500
Training Epoch: 83 [23168/50000]	Loss: 0.0997	LR: 0.012500
Training Epoch: 83 [23296/50000]	Loss: 0.1377	LR: 0.012500
Training Epoch: 83 [23424/50000]	Loss: 0.0838	LR: 0.012500
Training Epoch: 83 [23552/50000]	Loss: 0.1581	LR: 0.012500
Training Epoch: 83 [23680/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 83 [23808/50000]	Loss: 0.1640	LR: 0.012500
Training Epoch: 83 [23936/50000]	Loss: 0.1093	LR: 0.012500
Training Epoch: 83 [24064/50000]	Loss: 0.0754	LR: 0.012500
Training Epoch: 83 [24192/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 83 [24320/50000]	Loss: 0.1245	LR: 0.012500
Training Epoch: 83 [24448/50000]	Loss: 0.0986	LR: 0.012500
Training Epoch: 83 [24576/50000]	Loss: 0.0789	LR: 0.012500
Training Epoch: 83 [24704/50000]	Loss: 0.1167	LR: 0.012500
Training Epoch: 83 [24832/50000]	Loss: 0.1861	LR: 0.012500
Training Epoch: 83 [24960/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 83 [25088/50000]	Loss: 0.1227	LR: 0.012500
Training Epoch: 83 [25216/50000]	Loss: 0.1836	LR: 0.012500
Training Epoch: 83 [25344/50000]	Loss: 0.1430	LR: 0.012500
Training Epoch: 83 [25472/50000]	Loss: 0.1368	LR: 0.012500
Training Epoch: 83 [25600/50000]	Loss: 0.0711	LR: 0.012500
Training Epoch: 83 [25728/50000]	Loss: 0.1087	LR: 0.012500
Training Epoch: 83 [25856/50000]	Loss: 0.1087	LR: 0.012500
Training Epoch: 83 [25984/50000]	Loss: 0.1160	LR: 0.012500
Training Epoch: 83 [26112/50000]	Loss: 0.1288	LR: 0.012500
Training Epoch: 83 [26240/50000]	Loss: 0.1352	LR: 0.012500
Training Epoch: 83 [26368/50000]	Loss: 0.0994	LR: 0.012500
Training Epoch: 83 [26496/50000]	Loss: 0.1251	LR: 0.012500
Training Epoch: 83 [26624/50000]	Loss: 0.1519	LR: 0.012500
Training Epoch: 83 [26752/50000]	Loss: 0.1255	LR: 0.012500
Training Epoch: 83 [26880/50000]	Loss: 0.1695	LR: 0.012500
Training Epoch: 83 [27008/50000]	Loss: 0.1468	LR: 0.012500
Training Epoch: 83 [27136/50000]	Loss: 0.1470	LR: 0.012500
Training Epoch: 83 [27264/50000]	Loss: 0.1278	LR: 0.012500
Training Epoch: 83 [27392/50000]	Loss: 0.1028	LR: 0.012500
Training Epoch: 83 [27520/50000]	Loss: 0.1153	LR: 0.012500
Training Epoch: 83 [27648/50000]	Loss: 0.0995	LR: 0.012500
Training Epoch: 83 [27776/50000]	Loss: 0.1527	LR: 0.012500
Training Epoch: 83 [27904/50000]	Loss: 0.1205	LR: 0.012500
Training Epoch: 83 [28032/50000]	Loss: 0.1731	LR: 0.012500
Training Epoch: 83 [28160/50000]	Loss: 0.0991	LR: 0.012500
Training Epoch: 83 [28288/50000]	Loss: 0.1112	LR: 0.012500
Training Epoch: 83 [28416/50000]	Loss: 0.1403	LR: 0.012500
Training Epoch: 83 [28544/50000]	Loss: 0.1325	LR: 0.012500
Training Epoch: 83 [28672/50000]	Loss: 0.1449	LR: 0.012500
Training Epoch: 83 [28800/50000]	Loss: 0.1117	LR: 0.012500
Training Epoch: 83 [28928/50000]	Loss: 0.0863	LR: 0.012500
Training Epoch: 83 [29056/50000]	Loss: 0.1478	LR: 0.012500
Training Epoch: 83 [29184/50000]	Loss: 0.1371	LR: 0.012500
Training Epoch: 83 [29312/50000]	Loss: 0.1574	LR: 0.012500
Training Epoch: 83 [29440/50000]	Loss: 0.1763	LR: 0.012500
Training Epoch: 83 [29568/50000]	Loss: 0.0771	LR: 0.012500
Training Epoch: 83 [29696/50000]	Loss: 0.0854	LR: 0.012500
Training Epoch: 83 [29824/50000]	Loss: 0.1612	LR: 0.012500
Training Epoch: 83 [29952/50000]	Loss: 0.0718	LR: 0.012500
Training Epoch: 83 [30080/50000]	Loss: 0.0751	LR: 0.012500
Training Epoch: 83 [30208/50000]	Loss: 0.1571	LR: 0.012500
Training Epoch: 83 [30336/50000]	Loss: 0.1086	LR: 0.012500
Training Epoch: 83 [30464/50000]	Loss: 0.1534	LR: 0.012500
Training Epoch: 83 [30592/50000]	Loss: 0.1701	LR: 0.012500
Training Epoch: 83 [30720/50000]	Loss: 0.1415	LR: 0.012500
Training Epoch: 83 [30848/50000]	Loss: 0.1207	LR: 0.012500
Training Epoch: 83 [30976/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 83 [31104/50000]	Loss: 0.1031	LR: 0.012500
Training Epoch: 83 [31232/50000]	Loss: 0.1583	LR: 0.012500
Training Epoch: 83 [31360/50000]	Loss: 0.1058	LR: 0.012500
Training Epoch: 83 [31488/50000]	Loss: 0.0870	LR: 0.012500
Training Epoch: 83 [31616/50000]	Loss: 0.1282	LR: 0.012500
Training Epoch: 83 [31744/50000]	Loss: 0.1426	LR: 0.012500
Training Epoch: 83 [31872/50000]	Loss: 0.1000	LR: 0.012500
Training Epoch: 83 [32000/50000]	Loss: 0.1638	LR: 0.012500
Training Epoch: 83 [32128/50000]	Loss: 0.1613	LR: 0.012500
Training Epoch: 83 [32256/50000]	Loss: 0.1466	LR: 0.012500
Training Epoch: 83 [32384/50000]	Loss: 0.1487	LR: 0.012500
Training Epoch: 83 [32512/50000]	Loss: 0.0952	LR: 0.012500
Training Epoch: 83 [32640/50000]	Loss: 0.1585	LR: 0.012500
Training Epoch: 83 [32768/50000]	Loss: 0.1156	LR: 0.012500
Training Epoch: 83 [32896/50000]	Loss: 0.1486	LR: 0.012500
Training Epoch: 83 [33024/50000]	Loss: 0.1356	LR: 0.012500
Training Epoch: 83 [33152/50000]	Loss: 0.1629	LR: 0.012500
Training Epoch: 83 [33280/50000]	Loss: 0.1695	LR: 0.012500
Training Epoch: 83 [33408/50000]	Loss: 0.2001	LR: 0.012500
Training Epoch: 83 [33536/50000]	Loss: 0.1459	LR: 0.012500
Training Epoch: 83 [33664/50000]	Loss: 0.0628	LR: 0.012500
Training Epoch: 83 [33792/50000]	Loss: 0.1355	LR: 0.012500
Training Epoch: 83 [33920/50000]	Loss: 0.1707	LR: 0.012500
Training Epoch: 83 [34048/50000]	Loss: 0.1109	LR: 0.012500
Training Epoch: 83 [34176/50000]	Loss: 0.1586	LR: 0.012500
Training Epoch: 83 [34304/50000]	Loss: 0.1130	LR: 0.012500
Training Epoch: 83 [34432/50000]	Loss: 0.1601	LR: 0.012500
Training Epoch: 83 [34560/50000]	Loss: 0.1483	LR: 0.012500
Training Epoch: 83 [34688/50000]	Loss: 0.1080	LR: 0.012500
Training Epoch: 83 [34816/50000]	Loss: 0.1117	LR: 0.012500
Training Epoch: 83 [34944/50000]	Loss: 0.1100	LR: 0.012500
Training Epoch: 83 [35072/50000]	Loss: 0.1664	LR: 0.012500
Training Epoch: 83 [35200/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 83 [35328/50000]	Loss: 0.0900	LR: 0.012500
Training Epoch: 83 [35456/50000]	Loss: 0.1295	LR: 0.012500
Training Epoch: 83 [35584/50000]	Loss: 0.0983	LR: 0.012500
Training Epoch: 83 [35712/50000]	Loss: 0.1012	LR: 0.012500
Training Epoch: 83 [35840/50000]	Loss: 0.2953	LR: 0.012500
Training Epoch: 83 [35968/50000]	Loss: 0.1603	LR: 0.012500
Training Epoch: 83 [36096/50000]	Loss: 0.0781	LR: 0.012500
Training Epoch: 83 [36224/50000]	Loss: 0.1555	LR: 0.012500
Training Epoch: 83 [36352/50000]	Loss: 0.1321	LR: 0.012500
Training Epoch: 83 [36480/50000]	Loss: 0.1125	LR: 0.012500
Training Epoch: 83 [36608/50000]	Loss: 0.2012	LR: 0.012500
Training Epoch: 83 [36736/50000]	Loss: 0.1291	LR: 0.012500
Training Epoch: 83 [36864/50000]	Loss: 0.1037	LR: 0.012500
Training Epoch: 83 [36992/50000]	Loss: 0.2043	LR: 0.012500
Training Epoch: 83 [37120/50000]	Loss: 0.1223	LR: 0.012500
Training Epoch: 83 [37248/50000]	Loss: 0.1100	LR: 0.012500
Training Epoch: 83 [37376/50000]	Loss: 0.1331	LR: 0.012500
Training Epoch: 83 [37504/50000]	Loss: 0.1340	LR: 0.012500
Training Epoch: 83 [37632/50000]	Loss: 0.1563	LR: 0.012500
Training Epoch: 83 [37760/50000]	Loss: 0.2065	LR: 0.012500
Training Epoch: 83 [37888/50000]	Loss: 0.1121	LR: 0.012500
Training Epoch: 83 [38016/50000]	Loss: 0.1508	LR: 0.012500
Training Epoch: 83 [38144/50000]	Loss: 0.1545	LR: 0.012500
Training Epoch: 83 [38272/50000]	Loss: 0.1134	LR: 0.012500
Training Epoch: 83 [38400/50000]	Loss: 0.1863	LR: 0.012500
Training Epoch: 83 [38528/50000]	Loss: 0.2353	LR: 0.012500
Training Epoch: 83 [38656/50000]	Loss: 0.1728	LR: 0.012500
Training Epoch: 83 [38784/50000]	Loss: 0.1111	LR: 0.012500
Training Epoch: 83 [38912/50000]	Loss: 0.1449	LR: 0.012500
Training Epoch: 83 [39040/50000]	Loss: 0.1383	LR: 0.012500
Training Epoch: 83 [39168/50000]	Loss: 0.1005	LR: 0.012500
Training Epoch: 83 [39296/50000]	Loss: 0.0942	LR: 0.012500
Training Epoch: 83 [39424/50000]	Loss: 0.1009	LR: 0.012500
Training Epoch: 83 [39552/50000]	Loss: 0.1914	LR: 0.012500
Training Epoch: 83 [39680/50000]	Loss: 0.1304	LR: 0.012500
Training Epoch: 83 [39808/50000]	Loss: 0.1728	LR: 0.012500
Training Epoch: 83 [39936/50000]	Loss: 0.1674	LR: 0.012500
Training Epoch: 83 [40064/50000]	Loss: 0.1215	LR: 0.012500
Training Epoch: 83 [40192/50000]	Loss: 0.1582	LR: 0.012500
Training Epoch: 83 [40320/50000]	Loss: 0.1302	LR: 0.012500
Training Epoch: 83 [40448/50000]	Loss: 0.1342	LR: 0.012500
Training Epoch: 83 [40576/50000]	Loss: 0.0712	LR: 0.012500
Training Epoch: 83 [40704/50000]	Loss: 0.0960	LR: 0.012500
Training Epoch: 83 [40832/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 83 [40960/50000]	Loss: 0.1755	LR: 0.012500
Training Epoch: 83 [41088/50000]	Loss: 0.1282	LR: 0.012500
Training Epoch: 83 [41216/50000]	Loss: 0.1457	LR: 0.012500
Training Epoch: 83 [41344/50000]	Loss: 0.0577	LR: 0.012500
Training Epoch: 83 [41472/50000]	Loss: 0.1708	LR: 0.012500
Training Epoch: 83 [41600/50000]	Loss: 0.0807	LR: 0.012500
Training Epoch: 83 [41728/50000]	Loss: 0.1374	LR: 0.012500
Training Epoch: 83 [41856/50000]	Loss: 0.1002	LR: 0.012500
Training Epoch: 83 [41984/50000]	Loss: 0.1236	LR: 0.012500
Training Epoch: 83 [42112/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 83 [42240/50000]	Loss: 0.1168	LR: 0.012500
Training Epoch: 83 [42368/50000]	Loss: 0.0812	LR: 0.012500
Training Epoch: 83 [42496/50000]	Loss: 0.1170	LR: 0.012500
Training Epoch: 83 [42624/50000]	Loss: 0.1404	LR: 0.012500
Training Epoch: 83 [42752/50000]	Loss: 0.1664	LR: 0.012500
Training Epoch: 83 [42880/50000]	Loss: 0.2292	LR: 0.012500
Training Epoch: 83 [43008/50000]	Loss: 0.1416	LR: 0.012500
Training Epoch: 83 [43136/50000]	Loss: 0.1702	LR: 0.012500
Training Epoch: 83 [43264/50000]	Loss: 0.1417	LR: 0.012500
Training Epoch: 83 [43392/50000]	Loss: 0.1445	LR: 0.012500
Training Epoch: 83 [43520/50000]	Loss: 0.0868	LR: 0.012500
Training Epoch: 83 [43648/50000]	Loss: 0.1160	LR: 0.012500
Training Epoch: 83 [43776/50000]	Loss: 0.1186	LR: 0.012500
Training Epoch: 83 [43904/50000]	Loss: 0.1573	LR: 0.012500
Training Epoch: 83 [44032/50000]	Loss: 0.1160	LR: 0.012500
Training Epoch: 83 [44160/50000]	Loss: 0.1242	LR: 0.012500
Training Epoch: 83 [44288/50000]	Loss: 0.1563	LR: 0.012500
Training Epoch: 83 [44416/50000]	Loss: 0.1446	LR: 0.012500
Training Epoch: 83 [44544/50000]	Loss: 0.1815	LR: 0.012500
Training Epoch: 83 [44672/50000]	Loss: 0.1517	LR: 0.012500
Training Epoch: 83 [44800/50000]	Loss: 0.1867	LR: 0.012500
Training Epoch: 83 [44928/50000]	Loss: 0.0938	LR: 0.012500
Training Epoch: 83 [45056/50000]	Loss: 0.1450	LR: 0.012500
Training Epoch: 83 [45184/50000]	Loss: 0.1668	LR: 0.012500
Training Epoch: 83 [45312/50000]	Loss: 0.1290	LR: 0.012500
Training Epoch: 83 [45440/50000]	Loss: 0.1200	LR: 0.012500
Training Epoch: 83 [45568/50000]	Loss: 0.1706	LR: 0.012500
Training Epoch: 83 [45696/50000]	Loss: 0.1095	LR: 0.012500
Training Epoch: 83 [45824/50000]	Loss: 0.1500	LR: 0.012500
Training Epoch: 83 [45952/50000]	Loss: 0.1997	LR: 0.012500
Training Epoch: 83 [46080/50000]	Loss: 0.2125	LR: 0.012500
Training Epoch: 83 [46208/50000]	Loss: 0.0916	LR: 0.012500
Training Epoch: 83 [46336/50000]	Loss: 0.0761	LR: 0.012500
Training Epoch: 83 [46464/50000]	Loss: 0.2634	LR: 0.012500
Training Epoch: 83 [46592/50000]	Loss: 0.1788	LR: 0.012500
Training Epoch: 83 [46720/50000]	Loss: 0.1280	LR: 0.012500
Training Epoch: 83 [46848/50000]	Loss: 0.1612	LR: 0.012500
Training Epoch: 83 [46976/50000]	Loss: 0.1610	LR: 0.012500
Training Epoch: 83 [47104/50000]	Loss: 0.1863	LR: 0.012500
Training Epoch: 83 [47232/50000]	Loss: 0.0818	LR: 0.012500
Training Epoch: 83 [47360/50000]	Loss: 0.1945	LR: 0.012500
Training Epoch: 83 [47488/50000]	Loss: 0.1791	LR: 0.012500
Training Epoch: 83 [47616/50000]	Loss: 0.1715	LR: 0.012500
Training Epoch: 83 [47744/50000]	Loss: 0.1298	LR: 0.012500
Training Epoch: 83 [47872/50000]	Loss: 0.1414	LR: 0.012500
Training Epoch: 83 [48000/50000]	Loss: 0.1157	LR: 0.012500
Training Epoch: 83 [48128/50000]	Loss: 0.2054	LR: 0.012500
Training Epoch: 83 [48256/50000]	Loss: 0.1651	LR: 0.012500
Training Epoch: 83 [48384/50000]	Loss: 0.1618	LR: 0.012500
Training Epoch: 83 [48512/50000]	Loss: 0.0960	LR: 0.012500
Training Epoch: 83 [48640/50000]	Loss: 0.1017	LR: 0.012500
Training Epoch: 83 [48768/50000]	Loss: 0.1340	LR: 0.012500
Training Epoch: 83 [48896/50000]	Loss: 0.1426	LR: 0.012500
Training Epoch: 83 [49024/50000]	Loss: 0.1908	LR: 0.012500
Training Epoch: 83 [49152/50000]	Loss: 0.1242	LR: 0.012500
Training Epoch: 83 [49280/50000]	Loss: 0.1745	LR: 0.012500
Training Epoch: 83 [49408/50000]	Loss: 0.1022	LR: 0.012500
Training Epoch: 83 [49536/50000]	Loss: 0.1480	LR: 0.012500
Training Epoch: 83 [49664/50000]	Loss: 0.1401	LR: 0.012500
Training Epoch: 83 [49792/50000]	Loss: 0.1691	LR: 0.012500
Training Epoch: 83 [49920/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 83 [50000/50000]	Loss: 0.1411	LR: 0.012500
epoch 83 training time consumed: 53.96s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  304615 GB |  304614 GB |
|       from large pool |  123392 KB |    1034 MB |  304314 GB |  304314 GB |
|       from small pool |   10798 KB |      13 MB |     300 GB |     300 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  304615 GB |  304614 GB |
|       from large pool |  123392 KB |    1034 MB |  304314 GB |  304314 GB |
|       from small pool |   10798 KB |      13 MB |     300 GB |     300 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156626 KB |  434719 KB |  134048 GB |  134048 GB |
|       from large pool |  155136 KB |  433088 KB |  133716 GB |  133716 GB |
|       from small pool |    1490 KB |    3494 KB |     331 GB |     331 GB |
|---------------------------------------------------------------------------|
| Allocations           |     255    |     335    |   11754 K  |   11753 K  |
|       from large pool |      24    |      65    |    6135 K  |    6135 K  |
|       from small pool |     231    |     274    |    5618 K  |    5618 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     255    |     335    |   11754 K  |   11753 K  |
|       from large pool |      24    |      65    |    6135 K  |    6135 K  |
|       from small pool |     231    |     274    |    5618 K  |    5618 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      26    |    5807 K  |    5807 K  |
|       from large pool |       9    |      14    |    2969 K  |    2969 K  |
|       from small pool |      12    |      16    |    2837 K  |    2837 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 83, Average loss: 0.0093, Accuracy: 0.7244, Time consumed:3.46s

Training Epoch: 84 [128/50000]	Loss: 0.1017	LR: 0.012500
Training Epoch: 84 [256/50000]	Loss: 0.1553	LR: 0.012500
Training Epoch: 84 [384/50000]	Loss: 0.0709	LR: 0.012500
Training Epoch: 84 [512/50000]	Loss: 0.1427	LR: 0.012500
Training Epoch: 84 [640/50000]	Loss: 0.1316	LR: 0.012500
Training Epoch: 84 [768/50000]	Loss: 0.0818	LR: 0.012500
Training Epoch: 84 [896/50000]	Loss: 0.1063	LR: 0.012500
Training Epoch: 84 [1024/50000]	Loss: 0.0851	LR: 0.012500
Training Epoch: 84 [1152/50000]	Loss: 0.1026	LR: 0.012500
Training Epoch: 84 [1280/50000]	Loss: 0.1174	LR: 0.012500
Training Epoch: 84 [1408/50000]	Loss: 0.1308	LR: 0.012500
Training Epoch: 84 [1536/50000]	Loss: 0.0876	LR: 0.012500
Training Epoch: 84 [1664/50000]	Loss: 0.1372	LR: 0.012500
Training Epoch: 84 [1792/50000]	Loss: 0.0724	LR: 0.012500
Training Epoch: 84 [1920/50000]	Loss: 0.1496	LR: 0.012500
Training Epoch: 84 [2048/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 84 [2176/50000]	Loss: 0.0993	LR: 0.012500
Training Epoch: 84 [2304/50000]	Loss: 0.0780	LR: 0.012500
Training Epoch: 84 [2432/50000]	Loss: 0.0610	LR: 0.012500
Training Epoch: 84 [2560/50000]	Loss: 0.1423	LR: 0.012500
Training Epoch: 84 [2688/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 84 [2816/50000]	Loss: 0.1116	LR: 0.012500
Training Epoch: 84 [2944/50000]	Loss: 0.0866	LR: 0.012500
Training Epoch: 84 [3072/50000]	Loss: 0.0586	LR: 0.012500
Training Epoch: 84 [3200/50000]	Loss: 0.1106	LR: 0.012500
Training Epoch: 84 [3328/50000]	Loss: 0.1144	LR: 0.012500
Training Epoch: 84 [3456/50000]	Loss: 0.2538	LR: 0.012500
Training Epoch: 84 [3584/50000]	Loss: 0.1317	LR: 0.012500
Training Epoch: 84 [3712/50000]	Loss: 0.0824	LR: 0.012500
Training Epoch: 84 [3840/50000]	Loss: 0.0892	LR: 0.012500
Training Epoch: 84 [3968/50000]	Loss: 0.1056	LR: 0.012500
Training Epoch: 84 [4096/50000]	Loss: 0.0993	LR: 0.012500
Training Epoch: 84 [4224/50000]	Loss: 0.1927	LR: 0.012500
Training Epoch: 84 [4352/50000]	Loss: 0.1089	LR: 0.012500
Training Epoch: 84 [4480/50000]	Loss: 0.1545	LR: 0.012500
Training Epoch: 84 [4608/50000]	Loss: 0.1064	LR: 0.012500
Training Epoch: 84 [4736/50000]	Loss: 0.0796	LR: 0.012500
Training Epoch: 84 [4864/50000]	Loss: 0.1145	LR: 0.012500
Training Epoch: 84 [4992/50000]	Loss: 0.1174	LR: 0.012500
Training Epoch: 84 [5120/50000]	Loss: 0.1487	LR: 0.012500
Training Epoch: 84 [5248/50000]	Loss: 0.1052	LR: 0.012500
Training Epoch: 84 [5376/50000]	Loss: 0.1479	LR: 0.012500
Training Epoch: 84 [5504/50000]	Loss: 0.0676	LR: 0.012500
Training Epoch: 84 [5632/50000]	Loss: 0.0819	LR: 0.012500
Training Epoch: 84 [5760/50000]	Loss: 0.1295	LR: 0.012500
Training Epoch: 84 [5888/50000]	Loss: 0.0698	LR: 0.012500
Training Epoch: 84 [6016/50000]	Loss: 0.0838	LR: 0.012500
Training Epoch: 84 [6144/50000]	Loss: 0.0565	LR: 0.012500
Training Epoch: 84 [6272/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 84 [6400/50000]	Loss: 0.0876	LR: 0.012500
Training Epoch: 84 [6528/50000]	Loss: 0.1045	LR: 0.012500
Training Epoch: 84 [6656/50000]	Loss: 0.1267	LR: 0.012500
Training Epoch: 84 [6784/50000]	Loss: 0.0854	LR: 0.012500
Training Epoch: 84 [6912/50000]	Loss: 0.0755	LR: 0.012500
Training Epoch: 84 [7040/50000]	Loss: 0.0822	LR: 0.012500
Training Epoch: 84 [7168/50000]	Loss: 0.1768	LR: 0.012500
Training Epoch: 84 [7296/50000]	Loss: 0.1022	LR: 0.012500
Training Epoch: 84 [7424/50000]	Loss: 0.1047	LR: 0.012500
Training Epoch: 84 [7552/50000]	Loss: 0.1335	LR: 0.012500
Training Epoch: 84 [7680/50000]	Loss: 0.0868	LR: 0.012500
Training Epoch: 84 [7808/50000]	Loss: 0.1025	LR: 0.012500
Training Epoch: 84 [7936/50000]	Loss: 0.0903	LR: 0.012500
Training Epoch: 84 [8064/50000]	Loss: 0.0873	LR: 0.012500
Training Epoch: 84 [8192/50000]	Loss: 0.1445	LR: 0.012500
Training Epoch: 84 [8320/50000]	Loss: 0.0823	LR: 0.012500
Training Epoch: 84 [8448/50000]	Loss: 0.0821	LR: 0.012500
Training Epoch: 84 [8576/50000]	Loss: 0.0685	LR: 0.012500
Training Epoch: 84 [8704/50000]	Loss: 0.0747	LR: 0.012500
Training Epoch: 84 [8832/50000]	Loss: 0.0749	LR: 0.012500
Training Epoch: 84 [8960/50000]	Loss: 0.0776	LR: 0.012500
Training Epoch: 84 [9088/50000]	Loss: 0.0753	LR: 0.012500
Training Epoch: 84 [9216/50000]	Loss: 0.1079	LR: 0.012500
Training Epoch: 84 [9344/50000]	Loss: 0.1538	LR: 0.012500
Training Epoch: 84 [9472/50000]	Loss: 0.1565	LR: 0.012500
Training Epoch: 84 [9600/50000]	Loss: 0.0764	LR: 0.012500
Training Epoch: 84 [9728/50000]	Loss: 0.0943	LR: 0.012500
Training Epoch: 84 [9856/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 84 [9984/50000]	Loss: 0.0707	LR: 0.012500
Training Epoch: 84 [10112/50000]	Loss: 0.1271	LR: 0.012500
Training Epoch: 84 [10240/50000]	Loss: 0.1261	LR: 0.012500
Training Epoch: 84 [10368/50000]	Loss: 0.1288	LR: 0.012500
Training Epoch: 84 [10496/50000]	Loss: 0.1257	LR: 0.012500
Training Epoch: 84 [10624/50000]	Loss: 0.0750	LR: 0.012500
Training Epoch: 84 [10752/50000]	Loss: 0.0663	LR: 0.012500
Training Epoch: 84 [10880/50000]	Loss: 0.1279	LR: 0.012500
Training Epoch: 84 [11008/50000]	Loss: 0.0850	LR: 0.012500
Training Epoch: 84 [11136/50000]	Loss: 0.0973	LR: 0.012500
Training Epoch: 84 [11264/50000]	Loss: 0.1095	LR: 0.012500
Training Epoch: 84 [11392/50000]	Loss: 0.1243	LR: 0.012500
Training Epoch: 84 [11520/50000]	Loss: 0.0748	LR: 0.012500
Training Epoch: 84 [11648/50000]	Loss: 0.1316	LR: 0.012500
Training Epoch: 84 [11776/50000]	Loss: 0.1329	LR: 0.012500
Training Epoch: 84 [11904/50000]	Loss: 0.1018	LR: 0.012500
Training Epoch: 84 [12032/50000]	Loss: 0.1606	LR: 0.012500
Training Epoch: 84 [12160/50000]	Loss: 0.2177	LR: 0.012500
Training Epoch: 84 [12288/50000]	Loss: 0.0855	LR: 0.012500
Training Epoch: 84 [12416/50000]	Loss: 0.0784	LR: 0.012500
Training Epoch: 84 [12544/50000]	Loss: 0.0868	LR: 0.012500
Training Epoch: 84 [12672/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 84 [12800/50000]	Loss: 0.1222	LR: 0.012500
Training Epoch: 84 [12928/50000]	Loss: 0.0711	LR: 0.012500
Training Epoch: 84 [13056/50000]	Loss: 0.0821	LR: 0.012500
Training Epoch: 84 [13184/50000]	Loss: 0.1459	LR: 0.012500
Training Epoch: 84 [13312/50000]	Loss: 0.1924	LR: 0.012500
Training Epoch: 84 [13440/50000]	Loss: 0.0976	LR: 0.012500
Training Epoch: 84 [13568/50000]	Loss: 0.0682	LR: 0.012500
Training Epoch: 84 [13696/50000]	Loss: 0.1417	LR: 0.012500
Training Epoch: 84 [13824/50000]	Loss: 0.0644	LR: 0.012500
Training Epoch: 84 [13952/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 84 [14080/50000]	Loss: 0.0922	LR: 0.012500
Training Epoch: 84 [14208/50000]	Loss: 0.1008	LR: 0.012500
Training Epoch: 84 [14336/50000]	Loss: 0.1358	LR: 0.012500
Training Epoch: 84 [14464/50000]	Loss: 0.0822	LR: 0.012500
Training Epoch: 84 [14592/50000]	Loss: 0.1043	LR: 0.012500
Training Epoch: 84 [14720/50000]	Loss: 0.1090	LR: 0.012500
Training Epoch: 84 [14848/50000]	Loss: 0.1075	LR: 0.012500
Training Epoch: 84 [14976/50000]	Loss: 0.1389	LR: 0.012500
Training Epoch: 84 [15104/50000]	Loss: 0.1312	LR: 0.012500
Training Epoch: 84 [15232/50000]	Loss: 0.1485	LR: 0.012500
Training Epoch: 84 [15360/50000]	Loss: 0.1045	LR: 0.012500
Training Epoch: 84 [15488/50000]	Loss: 0.1365	LR: 0.012500
Training Epoch: 84 [15616/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 84 [15744/50000]	Loss: 0.1379	LR: 0.012500
Training Epoch: 84 [15872/50000]	Loss: 0.1121	LR: 0.012500
Training Epoch: 84 [16000/50000]	Loss: 0.0755	LR: 0.012500
Training Epoch: 84 [16128/50000]	Loss: 0.1312	LR: 0.012500
Training Epoch: 84 [16256/50000]	Loss: 0.1652	LR: 0.012500
Training Epoch: 84 [16384/50000]	Loss: 0.1360	LR: 0.012500
Training Epoch: 84 [16512/50000]	Loss: 0.1125	LR: 0.012500
Training Epoch: 84 [16640/50000]	Loss: 0.0688	LR: 0.012500
Training Epoch: 84 [16768/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 84 [16896/50000]	Loss: 0.0677	LR: 0.012500
Training Epoch: 84 [17024/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 84 [17152/50000]	Loss: 0.1511	LR: 0.012500
Training Epoch: 84 [17280/50000]	Loss: 0.1645	LR: 0.012500
Training Epoch: 84 [17408/50000]	Loss: 0.1331	LR: 0.012500
Training Epoch: 84 [17536/50000]	Loss: 0.1034	LR: 0.012500
Training Epoch: 84 [17664/50000]	Loss: 0.1086	LR: 0.012500
Training Epoch: 84 [17792/50000]	Loss: 0.1511	LR: 0.012500
Training Epoch: 84 [17920/50000]	Loss: 0.1082	LR: 0.012500
Training Epoch: 84 [18048/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 84 [18176/50000]	Loss: 0.1071	LR: 0.012500
Training Epoch: 84 [18304/50000]	Loss: 0.0901	LR: 0.012500
Training Epoch: 84 [18432/50000]	Loss: 0.0593	LR: 0.012500
Training Epoch: 84 [18560/50000]	Loss: 0.0874	LR: 0.012500
Training Epoch: 84 [18688/50000]	Loss: 0.0704	LR: 0.012500
Training Epoch: 84 [18816/50000]	Loss: 0.1212	LR: 0.012500
Training Epoch: 84 [18944/50000]	Loss: 0.0570	LR: 0.012500
Training Epoch: 84 [19072/50000]	Loss: 0.1502	LR: 0.012500
Training Epoch: 84 [19200/50000]	Loss: 0.0970	LR: 0.012500
Training Epoch: 84 [19328/50000]	Loss: 0.1314	LR: 0.012500
Training Epoch: 84 [19456/50000]	Loss: 0.1209	LR: 0.012500
Training Epoch: 84 [19584/50000]	Loss: 0.0636	LR: 0.012500
Training Epoch: 84 [19712/50000]	Loss: 0.0809	LR: 0.012500
Training Epoch: 84 [19840/50000]	Loss: 0.1125	LR: 0.012500
Training Epoch: 84 [19968/50000]	Loss: 0.1107	LR: 0.012500
Training Epoch: 84 [20096/50000]	Loss: 0.0805	LR: 0.012500
Training Epoch: 84 [20224/50000]	Loss: 0.0999	LR: 0.012500
Training Epoch: 84 [20352/50000]	Loss: 0.1052	LR: 0.012500
Training Epoch: 84 [20480/50000]	Loss: 0.1285	LR: 0.012500
Training Epoch: 84 [20608/50000]	Loss: 0.1390	LR: 0.012500
Training Epoch: 84 [20736/50000]	Loss: 0.1013	LR: 0.012500
Training Epoch: 84 [20864/50000]	Loss: 0.0954	LR: 0.012500
Training Epoch: 84 [20992/50000]	Loss: 0.1173	LR: 0.012500
Training Epoch: 84 [21120/50000]	Loss: 0.1501	LR: 0.012500
Training Epoch: 84 [21248/50000]	Loss: 0.0772	LR: 0.012500
Training Epoch: 84 [21376/50000]	Loss: 0.0782	LR: 0.012500
Training Epoch: 84 [21504/50000]	Loss: 0.1185	LR: 0.012500
Training Epoch: 84 [21632/50000]	Loss: 0.0696	LR: 0.012500
Training Epoch: 84 [21760/50000]	Loss: 0.1232	LR: 0.012500
Training Epoch: 84 [21888/50000]	Loss: 0.1023	LR: 0.012500
Training Epoch: 84 [22016/50000]	Loss: 0.1260	LR: 0.012500
Training Epoch: 84 [22144/50000]	Loss: 0.1275	LR: 0.012500
Training Epoch: 84 [22272/50000]	Loss: 0.1251	LR: 0.012500
Training Epoch: 84 [22400/50000]	Loss: 0.0997	LR: 0.012500
Training Epoch: 84 [22528/50000]	Loss: 0.1119	LR: 0.012500
Training Epoch: 84 [22656/50000]	Loss: 0.1191	LR: 0.012500
Training Epoch: 84 [22784/50000]	Loss: 0.1300	LR: 0.012500
Training Epoch: 84 [22912/50000]	Loss: 0.1040	LR: 0.012500
Training Epoch: 84 [23040/50000]	Loss: 0.0809	LR: 0.012500
Training Epoch: 84 [23168/50000]	Loss: 0.0856	LR: 0.012500
Training Epoch: 84 [23296/50000]	Loss: 0.0702	LR: 0.012500
Training Epoch: 84 [23424/50000]	Loss: 0.1170	LR: 0.012500
Training Epoch: 84 [23552/50000]	Loss: 0.0818	LR: 0.012500
Training Epoch: 84 [23680/50000]	Loss: 0.1174	LR: 0.012500
Training Epoch: 84 [23808/50000]	Loss: 0.1405	LR: 0.012500
Training Epoch: 84 [23936/50000]	Loss: 0.1271	LR: 0.012500
Training Epoch: 84 [24064/50000]	Loss: 0.0724	LR: 0.012500
Training Epoch: 84 [24192/50000]	Loss: 0.1530	LR: 0.012500
Training Epoch: 84 [24320/50000]	Loss: 0.0951	LR: 0.012500
Training Epoch: 84 [24448/50000]	Loss: 0.0625	LR: 0.012500
Training Epoch: 84 [24576/50000]	Loss: 0.0873	LR: 0.012500
Training Epoch: 84 [24704/50000]	Loss: 0.1518	LR: 0.012500
Training Epoch: 84 [24832/50000]	Loss: 0.0824	LR: 0.012500
Training Epoch: 84 [24960/50000]	Loss: 0.0924	LR: 0.012500
Training Epoch: 84 [25088/50000]	Loss: 0.1564	LR: 0.012500
Training Epoch: 84 [25216/50000]	Loss: 0.1961	LR: 0.012500
Training Epoch: 84 [25344/50000]	Loss: 0.1303	LR: 0.012500
Training Epoch: 84 [25472/50000]	Loss: 0.0882	LR: 0.012500
Training Epoch: 84 [25600/50000]	Loss: 0.1095	LR: 0.012500
Training Epoch: 84 [25728/50000]	Loss: 0.0818	LR: 0.012500
Training Epoch: 84 [25856/50000]	Loss: 0.0529	LR: 0.012500
Training Epoch: 84 [25984/50000]	Loss: 0.0954	LR: 0.012500
Training Epoch: 84 [26112/50000]	Loss: 0.0666	LR: 0.012500
Training Epoch: 84 [26240/50000]	Loss: 0.1076	LR: 0.012500
Training Epoch: 84 [26368/50000]	Loss: 0.1334	LR: 0.012500
Training Epoch: 84 [26496/50000]	Loss: 0.0937	LR: 0.012500
Training Epoch: 84 [26624/50000]	Loss: 0.1669	LR: 0.012500
Training Epoch: 84 [26752/50000]	Loss: 0.0773	LR: 0.012500
Training Epoch: 84 [26880/50000]	Loss: 0.1173	LR: 0.012500
Training Epoch: 84 [27008/50000]	Loss: 0.1600	LR: 0.012500
Training Epoch: 84 [27136/50000]	Loss: 0.0505	LR: 0.012500
Training Epoch: 84 [27264/50000]	Loss: 0.2026	LR: 0.012500
Training Epoch: 84 [27392/50000]	Loss: 0.1852	LR: 0.012500
Training Epoch: 84 [27520/50000]	Loss: 0.0742	LR: 0.012500
Training Epoch: 84 [27648/50000]	Loss: 0.1002	LR: 0.012500
Training Epoch: 84 [27776/50000]	Loss: 0.1004	LR: 0.012500
Training Epoch: 84 [27904/50000]	Loss: 0.1185	LR: 0.012500
Training Epoch: 84 [28032/50000]	Loss: 0.0853	LR: 0.012500
Training Epoch: 84 [28160/50000]	Loss: 0.1390	LR: 0.012500
Training Epoch: 84 [28288/50000]	Loss: 0.1125	LR: 0.012500
Training Epoch: 84 [28416/50000]	Loss: 0.1829	LR: 0.012500
Training Epoch: 84 [28544/50000]	Loss: 0.1410	LR: 0.012500
Training Epoch: 84 [28672/50000]	Loss: 0.1068	LR: 0.012500
Training Epoch: 84 [28800/50000]	Loss: 0.1228	LR: 0.012500
Training Epoch: 84 [28928/50000]	Loss: 0.1461	LR: 0.012500
Training Epoch: 84 [29056/50000]	Loss: 0.1093	LR: 0.012500
Training Epoch: 84 [29184/50000]	Loss: 0.1014	LR: 0.012500
Training Epoch: 84 [29312/50000]	Loss: 0.0588	LR: 0.012500
Training Epoch: 84 [29440/50000]	Loss: 0.1142	LR: 0.012500
Training Epoch: 84 [29568/50000]	Loss: 0.0626	LR: 0.012500
Training Epoch: 84 [29696/50000]	Loss: 0.0942	LR: 0.012500
Training Epoch: 84 [29824/50000]	Loss: 0.1182	LR: 0.012500
Training Epoch: 84 [29952/50000]	Loss: 0.1792	LR: 0.012500
Training Epoch: 84 [30080/50000]	Loss: 0.1403	LR: 0.012500
Training Epoch: 84 [30208/50000]	Loss: 0.1032	LR: 0.012500
Training Epoch: 84 [30336/50000]	Loss: 0.1560	LR: 0.012500
Training Epoch: 84 [30464/50000]	Loss: 0.1081	LR: 0.012500
Training Epoch: 84 [30592/50000]	Loss: 0.1137	LR: 0.012500
Training Epoch: 84 [30720/50000]	Loss: 0.1378	LR: 0.012500
Training Epoch: 84 [30848/50000]	Loss: 0.1782	LR: 0.012500
Training Epoch: 84 [30976/50000]	Loss: 0.1412	LR: 0.012500
Training Epoch: 84 [31104/50000]	Loss: 0.1649	LR: 0.012500
Training Epoch: 84 [31232/50000]	Loss: 0.1634	LR: 0.012500
Training Epoch: 84 [31360/50000]	Loss: 0.1920	LR: 0.012500
Training Epoch: 84 [31488/50000]	Loss: 0.1557	LR: 0.012500
Training Epoch: 84 [31616/50000]	Loss: 0.1096	LR: 0.012500
Training Epoch: 84 [31744/50000]	Loss: 0.1580	LR: 0.012500
Training Epoch: 84 [31872/50000]	Loss: 0.0802	LR: 0.012500
Training Epoch: 84 [32000/50000]	Loss: 0.0875	LR: 0.012500
Training Epoch: 84 [32128/50000]	Loss: 0.0990	LR: 0.012500
Training Epoch: 84 [32256/50000]	Loss: 0.1190	LR: 0.012500
Training Epoch: 84 [32384/50000]	Loss: 0.1377	LR: 0.012500
Training Epoch: 84 [32512/50000]	Loss: 0.0692	LR: 0.012500
Training Epoch: 84 [32640/50000]	Loss: 0.0961	LR: 0.012500
Training Epoch: 84 [32768/50000]	Loss: 0.0962	LR: 0.012500
Training Epoch: 84 [32896/50000]	Loss: 0.1374	LR: 0.012500
Training Epoch: 84 [33024/50000]	Loss: 0.0856	LR: 0.012500
Training Epoch: 84 [33152/50000]	Loss: 0.0946	LR: 0.012500
Training Epoch: 84 [33280/50000]	Loss: 0.1638	LR: 0.012500
Training Epoch: 84 [33408/50000]	Loss: 0.1587	LR: 0.012500
Training Epoch: 84 [33536/50000]	Loss: 0.0873	LR: 0.012500
Training Epoch: 84 [33664/50000]	Loss: 0.0899	LR: 0.012500
Training Epoch: 84 [33792/50000]	Loss: 0.1389	LR: 0.012500
Training Epoch: 84 [33920/50000]	Loss: 0.1064	LR: 0.012500
Training Epoch: 84 [34048/50000]	Loss: 0.1602	LR: 0.012500
Training Epoch: 84 [34176/50000]	Loss: 0.0790	LR: 0.012500
Training Epoch: 84 [34304/50000]	Loss: 0.1230	LR: 0.012500
Training Epoch: 84 [34432/50000]	Loss: 0.2029	LR: 0.012500
Training Epoch: 84 [34560/50000]	Loss: 0.0883	LR: 0.012500
Training Epoch: 84 [34688/50000]	Loss: 0.0833	LR: 0.012500
Training Epoch: 84 [34816/50000]	Loss: 0.1598	LR: 0.012500
Training Epoch: 84 [34944/50000]	Loss: 0.2003	LR: 0.012500
Training Epoch: 84 [35072/50000]	Loss: 0.1555	LR: 0.012500
Training Epoch: 84 [35200/50000]	Loss: 0.1469	LR: 0.012500
Training Epoch: 84 [35328/50000]	Loss: 0.1203	LR: 0.012500
Training Epoch: 84 [35456/50000]	Loss: 0.1184	LR: 0.012500
Training Epoch: 84 [35584/50000]	Loss: 0.1157	LR: 0.012500
Training Epoch: 84 [35712/50000]	Loss: 0.1527	LR: 0.012500
Training Epoch: 84 [35840/50000]	Loss: 0.1796	LR: 0.012500
Training Epoch: 84 [35968/50000]	Loss: 0.1600	LR: 0.012500
Training Epoch: 84 [36096/50000]	Loss: 0.1671	LR: 0.012500
Training Epoch: 84 [36224/50000]	Loss: 0.1057	LR: 0.012500
Training Epoch: 84 [36352/50000]	Loss: 0.0920	LR: 0.012500
Training Epoch: 84 [36480/50000]	Loss: 0.1891	LR: 0.012500
Training Epoch: 84 [36608/50000]	Loss: 0.0848	LR: 0.012500
Training Epoch: 84 [36736/50000]	Loss: 0.0461	LR: 0.012500
Training Epoch: 84 [36864/50000]	Loss: 0.1720	LR: 0.012500
Training Epoch: 84 [36992/50000]	Loss: 0.2010	LR: 0.012500
Training Epoch: 84 [37120/50000]	Loss: 0.1377	LR: 0.012500
Training Epoch: 84 [37248/50000]	Loss: 0.1163	LR: 0.012500
Training Epoch: 84 [37376/50000]	Loss: 0.0735	LR: 0.012500
Training Epoch: 84 [37504/50000]	Loss: 0.1083	LR: 0.012500
Training Epoch: 84 [37632/50000]	Loss: 0.1234	LR: 0.012500
Training Epoch: 84 [37760/50000]	Loss: 0.1106	LR: 0.012500
Training Epoch: 84 [37888/50000]	Loss: 0.1310	LR: 0.012500
Training Epoch: 84 [38016/50000]	Loss: 0.0785	LR: 0.012500
Training Epoch: 84 [38144/50000]	Loss: 0.0989	LR: 0.012500
Training Epoch: 84 [38272/50000]	Loss: 0.1407	LR: 0.012500
Training Epoch: 84 [38400/50000]	Loss: 0.1316	LR: 0.012500
Training Epoch: 84 [38528/50000]	Loss: 0.1581	LR: 0.012500
Training Epoch: 84 [38656/50000]	Loss: 0.1148	LR: 0.012500
Training Epoch: 84 [38784/50000]	Loss: 0.1481	LR: 0.012500
Training Epoch: 84 [38912/50000]	Loss: 0.1421	LR: 0.012500
Training Epoch: 84 [39040/50000]	Loss: 0.1602	LR: 0.012500
Training Epoch: 84 [39168/50000]	Loss: 0.1790	LR: 0.012500
Training Epoch: 84 [39296/50000]	Loss: 0.1672	LR: 0.012500
Training Epoch: 84 [39424/50000]	Loss: 0.1056	LR: 0.012500
Training Epoch: 84 [39552/50000]	Loss: 0.1719	LR: 0.012500
Training Epoch: 84 [39680/50000]	Loss: 0.1495	LR: 0.012500
Training Epoch: 84 [39808/50000]	Loss: 0.1115	LR: 0.012500
Training Epoch: 84 [39936/50000]	Loss: 0.0809	LR: 0.012500
Training Epoch: 84 [40064/50000]	Loss: 0.1788	LR: 0.012500
Training Epoch: 84 [40192/50000]	Loss: 0.0984	LR: 0.012500
Training Epoch: 84 [40320/50000]	Loss: 0.1415	LR: 0.012500
Training Epoch: 84 [40448/50000]	Loss: 0.1673	LR: 0.012500
Training Epoch: 84 [40576/50000]	Loss: 0.0897	LR: 0.012500
Training Epoch: 84 [40704/50000]	Loss: 0.1417	LR: 0.012500
Training Epoch: 84 [40832/50000]	Loss: 0.1039	LR: 0.012500
Training Epoch: 84 [40960/50000]	Loss: 0.1058	LR: 0.012500
Training Epoch: 84 [41088/50000]	Loss: 0.1217	LR: 0.012500
Training Epoch: 84 [41216/50000]	Loss: 0.1273	LR: 0.012500
Training Epoch: 84 [41344/50000]	Loss: 0.1287	LR: 0.012500
Training Epoch: 84 [41472/50000]	Loss: 0.2027	LR: 0.012500
Training Epoch: 84 [41600/50000]	Loss: 0.1465	LR: 0.012500
Training Epoch: 84 [41728/50000]	Loss: 0.1405	LR: 0.012500
Training Epoch: 84 [41856/50000]	Loss: 0.2670	LR: 0.012500
Training Epoch: 84 [41984/50000]	Loss: 0.1150	LR: 0.012500
Training Epoch: 84 [42112/50000]	Loss: 0.0875	LR: 0.012500
Training Epoch: 84 [42240/50000]	Loss: 0.1146	LR: 0.012500
Training Epoch: 84 [42368/50000]	Loss: 0.1583	LR: 0.012500
Training Epoch: 84 [42496/50000]	Loss: 0.1381	LR: 0.012500
Training Epoch: 84 [42624/50000]	Loss: 0.1763	LR: 0.012500
Training Epoch: 84 [42752/50000]	Loss: 0.1492	LR: 0.012500
Training Epoch: 84 [42880/50000]	Loss: 0.1725	LR: 0.012500
Training Epoch: 84 [43008/50000]	Loss: 0.0893	LR: 0.012500
Training Epoch: 84 [43136/50000]	Loss: 0.1360	LR: 0.012500
Training Epoch: 84 [43264/50000]	Loss: 0.0948	LR: 0.012500
Training Epoch: 84 [43392/50000]	Loss: 0.1333	LR: 0.012500
Training Epoch: 84 [43520/50000]	Loss: 0.1243	LR: 0.012500
Training Epoch: 84 [43648/50000]	Loss: 0.1101	LR: 0.012500
Training Epoch: 84 [43776/50000]	Loss: 0.1593	LR: 0.012500
Training Epoch: 84 [43904/50000]	Loss: 0.1339	LR: 0.012500
Training Epoch: 84 [44032/50000]	Loss: 0.1398	LR: 0.012500
Training Epoch: 84 [44160/50000]	Loss: 0.1229	LR: 0.012500
Training Epoch: 84 [44288/50000]	Loss: 0.1204	LR: 0.012500
Training Epoch: 84 [44416/50000]	Loss: 0.1065	LR: 0.012500
Training Epoch: 84 [44544/50000]	Loss: 0.1161	LR: 0.012500
Training Epoch: 84 [44672/50000]	Loss: 0.1631	LR: 0.012500
Training Epoch: 84 [44800/50000]	Loss: 0.0879	LR: 0.012500
Training Epoch: 84 [44928/50000]	Loss: 0.2176	LR: 0.012500
Training Epoch: 84 [45056/50000]	Loss: 0.1258	LR: 0.012500
Training Epoch: 84 [45184/50000]	Loss: 0.1441	LR: 0.012500
Training Epoch: 84 [45312/50000]	Loss: 0.1686	LR: 0.012500
Training Epoch: 84 [45440/50000]	Loss: 0.1299	LR: 0.012500
Training Epoch: 84 [45568/50000]	Loss: 0.1596	LR: 0.012500
Training Epoch: 84 [45696/50000]	Loss: 0.1652	LR: 0.012500
Training Epoch: 84 [45824/50000]	Loss: 0.2154	LR: 0.012500
Training Epoch: 84 [45952/50000]	Loss: 0.0613	LR: 0.012500
Training Epoch: 84 [46080/50000]	Loss: 0.1826	LR: 0.012500
Training Epoch: 84 [46208/50000]	Loss: 0.1387	LR: 0.012500
Training Epoch: 84 [46336/50000]	Loss: 0.2027	LR: 0.012500
Training Epoch: 84 [46464/50000]	Loss: 0.1179	LR: 0.012500
Training Epoch: 84 [46592/50000]	Loss: 0.0783	LR: 0.012500
Training Epoch: 84 [46720/50000]	Loss: 0.1161	LR: 0.012500
Training Epoch: 84 [46848/50000]	Loss: 0.0731	LR: 0.012500
Training Epoch: 84 [46976/50000]	Loss: 0.1922	LR: 0.012500
Training Epoch: 84 [47104/50000]	Loss: 0.2180	LR: 0.012500
Training Epoch: 84 [47232/50000]	Loss: 0.1362	LR: 0.012500
Training Epoch: 84 [47360/50000]	Loss: 0.1204	LR: 0.012500
Training Epoch: 84 [47488/50000]	Loss: 0.1414	LR: 0.012500
Training Epoch: 84 [47616/50000]	Loss: 0.0740	LR: 0.012500
Training Epoch: 84 [47744/50000]	Loss: 0.1644	LR: 0.012500
Training Epoch: 84 [47872/50000]	Loss: 0.1839	LR: 0.012500
Training Epoch: 84 [48000/50000]	Loss: 0.1630	LR: 0.012500
Training Epoch: 84 [48128/50000]	Loss: 0.1101	LR: 0.012500
Training Epoch: 84 [48256/50000]	Loss: 0.1604	LR: 0.012500
Training Epoch: 84 [48384/50000]	Loss: 0.1169	LR: 0.012500
Training Epoch: 84 [48512/50000]	Loss: 0.1259	LR: 0.012500
Training Epoch: 84 [48640/50000]	Loss: 0.1444	LR: 0.012500
Training Epoch: 84 [48768/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 84 [48896/50000]	Loss: 0.0556	LR: 0.012500
Training Epoch: 84 [49024/50000]	Loss: 0.2171	LR: 0.012500
Training Epoch: 84 [49152/50000]	Loss: 0.1760	LR: 0.012500
Training Epoch: 84 [49280/50000]	Loss: 0.0780	LR: 0.012500
Training Epoch: 84 [49408/50000]	Loss: 0.1205	LR: 0.012500
Training Epoch: 84 [49536/50000]	Loss: 0.1047	LR: 0.012500
Training Epoch: 84 [49664/50000]	Loss: 0.1318	LR: 0.012500
Training Epoch: 84 [49792/50000]	Loss: 0.1258	LR: 0.012500
Training Epoch: 84 [49920/50000]	Loss: 0.0549	LR: 0.012500
Training Epoch: 84 [50000/50000]	Loss: 0.1901	LR: 0.012500
epoch 84 training time consumed: 53.93s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  308285 GB |  308284 GB |
|       from large pool |  123392 KB |    1034 MB |  307981 GB |  307981 GB |
|       from small pool |   10798 KB |      13 MB |     303 GB |     303 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  308285 GB |  308284 GB |
|       from large pool |  123392 KB |    1034 MB |  307981 GB |  307981 GB |
|       from small pool |   10798 KB |      13 MB |     303 GB |     303 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  135663 GB |  135663 GB |
|       from large pool |  155136 KB |  433088 KB |  135327 GB |  135327 GB |
|       from small pool |    1489 KB |    3494 KB |     335 GB |     335 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   11895 K  |   11895 K  |
|       from large pool |      24    |      65    |    6209 K  |    6209 K  |
|       from small pool |     232    |     275    |    5686 K  |    5686 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   11895 K  |   11895 K  |
|       from large pool |      24    |      65    |    6209 K  |    6209 K  |
|       from small pool |     232    |     275    |    5686 K  |    5686 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    5880 K  |    5880 K  |
|       from large pool |       9    |      14    |    3005 K  |    3005 K  |
|       from small pool |      12    |      17    |    2874 K  |    2874 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 84, Average loss: 0.0088, Accuracy: 0.7228, Time consumed:3.46s

Training Epoch: 85 [128/50000]	Loss: 0.1001	LR: 0.012500
Training Epoch: 85 [256/50000]	Loss: 0.1312	LR: 0.012500
Training Epoch: 85 [384/50000]	Loss: 0.0906	LR: 0.012500
Training Epoch: 85 [512/50000]	Loss: 0.1574	LR: 0.012500
Training Epoch: 85 [640/50000]	Loss: 0.0951	LR: 0.012500
Training Epoch: 85 [768/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 85 [896/50000]	Loss: 0.1037	LR: 0.012500
Training Epoch: 85 [1024/50000]	Loss: 0.1139	LR: 0.012500
Training Epoch: 85 [1152/50000]	Loss: 0.0992	LR: 0.012500
Training Epoch: 85 [1280/50000]	Loss: 0.1147	LR: 0.012500
Training Epoch: 85 [1408/50000]	Loss: 0.1016	LR: 0.012500
Training Epoch: 85 [1536/50000]	Loss: 0.1269	LR: 0.012500
Training Epoch: 85 [1664/50000]	Loss: 0.1005	LR: 0.012500
Training Epoch: 85 [1792/50000]	Loss: 0.1104	LR: 0.012500
Training Epoch: 85 [1920/50000]	Loss: 0.1672	LR: 0.012500
Training Epoch: 85 [2048/50000]	Loss: 0.0862	LR: 0.012500
Training Epoch: 85 [2176/50000]	Loss: 0.1135	LR: 0.012500
Training Epoch: 85 [2304/50000]	Loss: 0.0793	LR: 0.012500
Training Epoch: 85 [2432/50000]	Loss: 0.1590	LR: 0.012500
Training Epoch: 85 [2560/50000]	Loss: 0.0918	LR: 0.012500
Training Epoch: 85 [2688/50000]	Loss: 0.1072	LR: 0.012500
Training Epoch: 85 [2816/50000]	Loss: 0.1455	LR: 0.012500
Training Epoch: 85 [2944/50000]	Loss: 0.1100	LR: 0.012500
Training Epoch: 85 [3072/50000]	Loss: 0.1501	LR: 0.012500
Training Epoch: 85 [3200/50000]	Loss: 0.1051	LR: 0.012500
Training Epoch: 85 [3328/50000]	Loss: 0.0867	LR: 0.012500
Training Epoch: 85 [3456/50000]	Loss: 0.1554	LR: 0.012500
Training Epoch: 85 [3584/50000]	Loss: 0.1653	LR: 0.012500
Training Epoch: 85 [3712/50000]	Loss: 0.0847	LR: 0.012500
Training Epoch: 85 [3840/50000]	Loss: 0.1105	LR: 0.012500
Training Epoch: 85 [3968/50000]	Loss: 0.1140	LR: 0.012500
Training Epoch: 85 [4096/50000]	Loss: 0.1030	LR: 0.012500
Training Epoch: 85 [4224/50000]	Loss: 0.1433	LR: 0.012500
Training Epoch: 85 [4352/50000]	Loss: 0.0836	LR: 0.012500
Training Epoch: 85 [4480/50000]	Loss: 0.1031	LR: 0.012500
Training Epoch: 85 [4608/50000]	Loss: 0.1015	LR: 0.012500
Training Epoch: 85 [4736/50000]	Loss: 0.1493	LR: 0.012500
Training Epoch: 85 [4864/50000]	Loss: 0.1944	LR: 0.012500
Training Epoch: 85 [4992/50000]	Loss: 0.0822	LR: 0.012500
Training Epoch: 85 [5120/50000]	Loss: 0.1483	LR: 0.012500
Training Epoch: 85 [5248/50000]	Loss: 0.1343	LR: 0.012500
Training Epoch: 85 [5376/50000]	Loss: 0.1189	LR: 0.012500
Training Epoch: 85 [5504/50000]	Loss: 0.0729	LR: 0.012500
Training Epoch: 85 [5632/50000]	Loss: 0.0807	LR: 0.012500
Training Epoch: 85 [5760/50000]	Loss: 0.1173	LR: 0.012500
Training Epoch: 85 [5888/50000]	Loss: 0.1113	LR: 0.012500
Training Epoch: 85 [6016/50000]	Loss: 0.1368	LR: 0.012500
Training Epoch: 85 [6144/50000]	Loss: 0.1092	LR: 0.012500
Training Epoch: 85 [6272/50000]	Loss: 0.0951	LR: 0.012500
Training Epoch: 85 [6400/50000]	Loss: 0.1288	LR: 0.012500
Training Epoch: 85 [6528/50000]	Loss: 0.0888	LR: 0.012500
Training Epoch: 85 [6656/50000]	Loss: 0.1327	LR: 0.012500
Training Epoch: 85 [6784/50000]	Loss: 0.1116	LR: 0.012500
Training Epoch: 85 [6912/50000]	Loss: 0.0835	LR: 0.012500
Training Epoch: 85 [7040/50000]	Loss: 0.1455	LR: 0.012500
Training Epoch: 85 [7168/50000]	Loss: 0.0629	LR: 0.012500
Training Epoch: 85 [7296/50000]	Loss: 0.1098	LR: 0.012500
Training Epoch: 85 [7424/50000]	Loss: 0.0952	LR: 0.012500
Training Epoch: 85 [7552/50000]	Loss: 0.0769	LR: 0.012500
Training Epoch: 85 [7680/50000]	Loss: 0.0863	LR: 0.012500
Training Epoch: 85 [7808/50000]	Loss: 0.0917	LR: 0.012500
Training Epoch: 85 [7936/50000]	Loss: 0.0844	LR: 0.012500
Training Epoch: 85 [8064/50000]	Loss: 0.1328	LR: 0.012500
Training Epoch: 85 [8192/50000]	Loss: 0.1696	LR: 0.012500
Training Epoch: 85 [8320/50000]	Loss: 0.1129	LR: 0.012500
Training Epoch: 85 [8448/50000]	Loss: 0.0874	LR: 0.012500
Training Epoch: 85 [8576/50000]	Loss: 0.1109	LR: 0.012500
Training Epoch: 85 [8704/50000]	Loss: 0.0660	LR: 0.012500
Training Epoch: 85 [8832/50000]	Loss: 0.0996	LR: 0.012500
Training Epoch: 85 [8960/50000]	Loss: 0.1300	LR: 0.012500
Training Epoch: 85 [9088/50000]	Loss: 0.1261	LR: 0.012500
Training Epoch: 85 [9216/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 85 [9344/50000]	Loss: 0.0767	LR: 0.012500
Training Epoch: 85 [9472/50000]	Loss: 0.0667	LR: 0.012500
Training Epoch: 85 [9600/50000]	Loss: 0.1031	LR: 0.012500
Training Epoch: 85 [9728/50000]	Loss: 0.1142	LR: 0.012500
Training Epoch: 85 [9856/50000]	Loss: 0.1310	LR: 0.012500
Training Epoch: 85 [9984/50000]	Loss: 0.0708	LR: 0.012500
Training Epoch: 85 [10112/50000]	Loss: 0.1053	LR: 0.012500
Training Epoch: 85 [10240/50000]	Loss: 0.0989	LR: 0.012500
Training Epoch: 85 [10368/50000]	Loss: 0.0913	LR: 0.012500
Training Epoch: 85 [10496/50000]	Loss: 0.0828	LR: 0.012500
Training Epoch: 85 [10624/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 85 [10752/50000]	Loss: 0.1027	LR: 0.012500
Training Epoch: 85 [10880/50000]	Loss: 0.0609	LR: 0.012500
Training Epoch: 85 [11008/50000]	Loss: 0.0844	LR: 0.012500
Training Epoch: 85 [11136/50000]	Loss: 0.1514	LR: 0.012500
Training Epoch: 85 [11264/50000]	Loss: 0.1610	LR: 0.012500
Training Epoch: 85 [11392/50000]	Loss: 0.1237	LR: 0.012500
Training Epoch: 85 [11520/50000]	Loss: 0.0999	LR: 0.012500
Training Epoch: 85 [11648/50000]	Loss: 0.1459	LR: 0.012500
Training Epoch: 85 [11776/50000]	Loss: 0.0724	LR: 0.012500
Training Epoch: 85 [11904/50000]	Loss: 0.0764	LR: 0.012500
Training Epoch: 85 [12032/50000]	Loss: 0.1151	LR: 0.012500
Training Epoch: 85 [12160/50000]	Loss: 0.1362	LR: 0.012500
Training Epoch: 85 [12288/50000]	Loss: 0.1104	LR: 0.012500
Training Epoch: 85 [12416/50000]	Loss: 0.0959	LR: 0.012500
Training Epoch: 85 [12544/50000]	Loss: 0.0816	LR: 0.012500
Training Epoch: 85 [12672/50000]	Loss: 0.1077	LR: 0.012500
Training Epoch: 85 [12800/50000]	Loss: 0.0976	LR: 0.012500
Training Epoch: 85 [12928/50000]	Loss: 0.0705	LR: 0.012500
Training Epoch: 85 [13056/50000]	Loss: 0.1087	LR: 0.012500
Training Epoch: 85 [13184/50000]	Loss: 0.1488	LR: 0.012500
Training Epoch: 85 [13312/50000]	Loss: 0.0904	LR: 0.012500
Training Epoch: 85 [13440/50000]	Loss: 0.0723	LR: 0.012500
Training Epoch: 85 [13568/50000]	Loss: 0.1017	LR: 0.012500
Training Epoch: 85 [13696/50000]	Loss: 0.1224	LR: 0.012500
Training Epoch: 85 [13824/50000]	Loss: 0.0781	LR: 0.012500
Training Epoch: 85 [13952/50000]	Loss: 0.0753	LR: 0.012500
Training Epoch: 85 [14080/50000]	Loss: 0.0880	LR: 0.012500
Training Epoch: 85 [14208/50000]	Loss: 0.1081	LR: 0.012500
Training Epoch: 85 [14336/50000]	Loss: 0.1057	LR: 0.012500
Training Epoch: 85 [14464/50000]	Loss: 0.0857	LR: 0.012500
Training Epoch: 85 [14592/50000]	Loss: 0.0719	LR: 0.012500
Training Epoch: 85 [14720/50000]	Loss: 0.1113	LR: 0.012500
Training Epoch: 85 [14848/50000]	Loss: 0.0944	LR: 0.012500
Training Epoch: 85 [14976/50000]	Loss: 0.1325	LR: 0.012500
Training Epoch: 85 [15104/50000]	Loss: 0.0969	LR: 0.012500
Training Epoch: 85 [15232/50000]	Loss: 0.0766	LR: 0.012500
Training Epoch: 85 [15360/50000]	Loss: 0.1104	LR: 0.012500
Training Epoch: 85 [15488/50000]	Loss: 0.0920	LR: 0.012500
Training Epoch: 85 [15616/50000]	Loss: 0.1223	LR: 0.012500
Training Epoch: 85 [15744/50000]	Loss: 0.1562	LR: 0.012500
Training Epoch: 85 [15872/50000]	Loss: 0.1449	LR: 0.012500
Training Epoch: 85 [16000/50000]	Loss: 0.0735	LR: 0.012500
Training Epoch: 85 [16128/50000]	Loss: 0.0572	LR: 0.012500
Training Epoch: 85 [16256/50000]	Loss: 0.1198	LR: 0.012500
Training Epoch: 85 [16384/50000]	Loss: 0.1502	LR: 0.012500
Training Epoch: 85 [16512/50000]	Loss: 0.1044	LR: 0.012500
Training Epoch: 85 [16640/50000]	Loss: 0.1707	LR: 0.012500
Training Epoch: 85 [16768/50000]	Loss: 0.1424	LR: 0.012500
Training Epoch: 85 [16896/50000]	Loss: 0.1523	LR: 0.012500
Training Epoch: 85 [17024/50000]	Loss: 0.1091	LR: 0.012500
Training Epoch: 85 [17152/50000]	Loss: 0.1104	LR: 0.012500
Training Epoch: 85 [17280/50000]	Loss: 0.1006	LR: 0.012500
Training Epoch: 85 [17408/50000]	Loss: 0.1439	LR: 0.012500
Training Epoch: 85 [17536/50000]	Loss: 0.1227	LR: 0.012500
Training Epoch: 85 [17664/50000]	Loss: 0.1428	LR: 0.012500
Training Epoch: 85 [17792/50000]	Loss: 0.0497	LR: 0.012500
Training Epoch: 85 [17920/50000]	Loss: 0.0985	LR: 0.012500
Training Epoch: 85 [18048/50000]	Loss: 0.1354	LR: 0.012500
Training Epoch: 85 [18176/50000]	Loss: 0.1576	LR: 0.012500
Training Epoch: 85 [18304/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 85 [18432/50000]	Loss: 0.1646	LR: 0.012500
Training Epoch: 85 [18560/50000]	Loss: 0.1249	LR: 0.012500
Training Epoch: 85 [18688/50000]	Loss: 0.0776	LR: 0.012500
Training Epoch: 85 [18816/50000]	Loss: 0.1097	LR: 0.012500
Training Epoch: 85 [18944/50000]	Loss: 0.1000	LR: 0.012500
Training Epoch: 85 [19072/50000]	Loss: 0.1973	LR: 0.012500
Training Epoch: 85 [19200/50000]	Loss: 0.1630	LR: 0.012500
Training Epoch: 85 [19328/50000]	Loss: 0.1629	LR: 0.012500
Training Epoch: 85 [19456/50000]	Loss: 0.1148	LR: 0.012500
Training Epoch: 85 [19584/50000]	Loss: 0.2149	LR: 0.012500
Training Epoch: 85 [19712/50000]	Loss: 0.1415	LR: 0.012500
Training Epoch: 85 [19840/50000]	Loss: 0.1576	LR: 0.012500
Training Epoch: 85 [19968/50000]	Loss: 0.1003	LR: 0.012500
Training Epoch: 85 [20096/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 85 [20224/50000]	Loss: 0.1273	LR: 0.012500
Training Epoch: 85 [20352/50000]	Loss: 0.0701	LR: 0.012500
Training Epoch: 85 [20480/50000]	Loss: 0.0778	LR: 0.012500
Training Epoch: 85 [20608/50000]	Loss: 0.0625	LR: 0.012500
Training Epoch: 85 [20736/50000]	Loss: 0.1250	LR: 0.012500
Training Epoch: 85 [20864/50000]	Loss: 0.1420	LR: 0.012500
Training Epoch: 85 [20992/50000]	Loss: 0.1166	LR: 0.012500
Training Epoch: 85 [21120/50000]	Loss: 0.1402	LR: 0.012500
Training Epoch: 85 [21248/50000]	Loss: 0.1171	LR: 0.012500
Training Epoch: 85 [21376/50000]	Loss: 0.1893	LR: 0.012500
Training Epoch: 85 [21504/50000]	Loss: 0.1210	LR: 0.012500
Training Epoch: 85 [21632/50000]	Loss: 0.1161	LR: 0.012500
Training Epoch: 85 [21760/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 85 [21888/50000]	Loss: 0.0865	LR: 0.012500
Training Epoch: 85 [22016/50000]	Loss: 0.1051	LR: 0.012500
Training Epoch: 85 [22144/50000]	Loss: 0.1015	LR: 0.012500
Training Epoch: 85 [22272/50000]	Loss: 0.0806	LR: 0.012500
Training Epoch: 85 [22400/50000]	Loss: 0.1330	LR: 0.012500
Training Epoch: 85 [22528/50000]	Loss: 0.1993	LR: 0.012500
Training Epoch: 85 [22656/50000]	Loss: 0.0896	LR: 0.012500
Training Epoch: 85 [22784/50000]	Loss: 0.1305	LR: 0.012500
Training Epoch: 85 [22912/50000]	Loss: 0.0875	LR: 0.012500
Training Epoch: 85 [23040/50000]	Loss: 0.0958	LR: 0.012500
Training Epoch: 85 [23168/50000]	Loss: 0.1056	LR: 0.012500
Training Epoch: 85 [23296/50000]	Loss: 0.0955	LR: 0.012500
Training Epoch: 85 [23424/50000]	Loss: 0.1339	LR: 0.012500
Training Epoch: 85 [23552/50000]	Loss: 0.1315	LR: 0.012500
Training Epoch: 85 [23680/50000]	Loss: 0.0902	LR: 0.012500
Training Epoch: 85 [23808/50000]	Loss: 0.1865	LR: 0.012500
Training Epoch: 85 [23936/50000]	Loss: 0.1527	LR: 0.012500
Training Epoch: 85 [24064/50000]	Loss: 0.0833	LR: 0.012500
Training Epoch: 85 [24192/50000]	Loss: 0.1829	LR: 0.012500
Training Epoch: 85 [24320/50000]	Loss: 0.1656	LR: 0.012500
Training Epoch: 85 [24448/50000]	Loss: 0.1047	LR: 0.012500
Training Epoch: 85 [24576/50000]	Loss: 0.1319	LR: 0.012500
Training Epoch: 85 [24704/50000]	Loss: 0.1127	LR: 0.012500
Training Epoch: 85 [24832/50000]	Loss: 0.0657	LR: 0.012500
Training Epoch: 85 [24960/50000]	Loss: 0.0962	LR: 0.012500
Training Epoch: 85 [25088/50000]	Loss: 0.1558	LR: 0.012500
Training Epoch: 85 [25216/50000]	Loss: 0.1236	LR: 0.012500
Training Epoch: 85 [25344/50000]	Loss: 0.0981	LR: 0.012500
Training Epoch: 85 [25472/50000]	Loss: 0.1057	LR: 0.012500
Training Epoch: 85 [25600/50000]	Loss: 0.0955	LR: 0.012500
Training Epoch: 85 [25728/50000]	Loss: 0.1062	LR: 0.012500
Training Epoch: 85 [25856/50000]	Loss: 0.1380	LR: 0.012500
Training Epoch: 85 [25984/50000]	Loss: 0.1387	LR: 0.012500
Training Epoch: 85 [26112/50000]	Loss: 0.1428	LR: 0.012500
Training Epoch: 85 [26240/50000]	Loss: 0.1520	LR: 0.012500
Training Epoch: 85 [26368/50000]	Loss: 0.0899	LR: 0.012500
Training Epoch: 85 [26496/50000]	Loss: 0.1351	LR: 0.012500
Training Epoch: 85 [26624/50000]	Loss: 0.0869	LR: 0.012500
Training Epoch: 85 [26752/50000]	Loss: 0.0856	LR: 0.012500
Training Epoch: 85 [26880/50000]	Loss: 0.0834	LR: 0.012500
Training Epoch: 85 [27008/50000]	Loss: 0.2161	LR: 0.012500
Training Epoch: 85 [27136/50000]	Loss: 0.1538	LR: 0.012500
Training Epoch: 85 [27264/50000]	Loss: 0.1313	LR: 0.012500
Training Epoch: 85 [27392/50000]	Loss: 0.1245	LR: 0.012500
Training Epoch: 85 [27520/50000]	Loss: 0.1015	LR: 0.012500
Training Epoch: 85 [27648/50000]	Loss: 0.1473	LR: 0.012500
Training Epoch: 85 [27776/50000]	Loss: 0.1217	LR: 0.012500
Training Epoch: 85 [27904/50000]	Loss: 0.1063	LR: 0.012500
Training Epoch: 85 [28032/50000]	Loss: 0.1463	LR: 0.012500
Training Epoch: 85 [28160/50000]	Loss: 0.1017	LR: 0.012500
Training Epoch: 85 [28288/50000]	Loss: 0.1158	LR: 0.012500
Training Epoch: 85 [28416/50000]	Loss: 0.1053	LR: 0.012500
Training Epoch: 85 [28544/50000]	Loss: 0.1415	LR: 0.012500
Training Epoch: 85 [28672/50000]	Loss: 0.0837	LR: 0.012500
Training Epoch: 85 [28800/50000]	Loss: 0.1093	LR: 0.012500
Training Epoch: 85 [28928/50000]	Loss: 0.0733	LR: 0.012500
Training Epoch: 85 [29056/50000]	Loss: 0.1020	LR: 0.012500
Training Epoch: 85 [29184/50000]	Loss: 0.1629	LR: 0.012500
Training Epoch: 85 [29312/50000]	Loss: 0.1263	LR: 0.012500
Training Epoch: 85 [29440/50000]	Loss: 0.0878	LR: 0.012500
Training Epoch: 85 [29568/50000]	Loss: 0.1094	LR: 0.012500
Training Epoch: 85 [29696/50000]	Loss: 0.1757	LR: 0.012500
Training Epoch: 85 [29824/50000]	Loss: 0.1377	LR: 0.012500
Training Epoch: 85 [29952/50000]	Loss: 0.1138	LR: 0.012500
Training Epoch: 85 [30080/50000]	Loss: 0.1313	LR: 0.012500
Training Epoch: 85 [30208/50000]	Loss: 0.0709	LR: 0.012500
Training Epoch: 85 [30336/50000]	Loss: 0.1098	LR: 0.012500
Training Epoch: 85 [30464/50000]	Loss: 0.1117	LR: 0.012500
Training Epoch: 85 [30592/50000]	Loss: 0.0781	LR: 0.012500
Training Epoch: 85 [30720/50000]	Loss: 0.1002	LR: 0.012500
Training Epoch: 85 [30848/50000]	Loss: 0.1700	LR: 0.012500
Training Epoch: 85 [30976/50000]	Loss: 0.1493	LR: 0.012500
Training Epoch: 85 [31104/50000]	Loss: 0.1068	LR: 0.012500
Training Epoch: 85 [31232/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 85 [31360/50000]	Loss: 0.0932	LR: 0.012500
Training Epoch: 85 [31488/50000]	Loss: 0.1022	LR: 0.012500
Training Epoch: 85 [31616/50000]	Loss: 0.1534	LR: 0.012500
Training Epoch: 85 [31744/50000]	Loss: 0.1439	LR: 0.012500
Training Epoch: 85 [31872/50000]	Loss: 0.1241	LR: 0.012500
Training Epoch: 85 [32000/50000]	Loss: 0.0920	LR: 0.012500
Training Epoch: 85 [32128/50000]	Loss: 0.1136	LR: 0.012500
Training Epoch: 85 [32256/50000]	Loss: 0.1018	LR: 0.012500
Training Epoch: 85 [32384/50000]	Loss: 0.1689	LR: 0.012500
Training Epoch: 85 [32512/50000]	Loss: 0.0989	LR: 0.012500
Training Epoch: 85 [32640/50000]	Loss: 0.0936	LR: 0.012500
Training Epoch: 85 [32768/50000]	Loss: 0.1395	LR: 0.012500
Training Epoch: 85 [32896/50000]	Loss: 0.0984	LR: 0.012500
Training Epoch: 85 [33024/50000]	Loss: 0.1123	LR: 0.012500
Training Epoch: 85 [33152/50000]	Loss: 0.0904	LR: 0.012500
Training Epoch: 85 [33280/50000]	Loss: 0.0925	LR: 0.012500
Training Epoch: 85 [33408/50000]	Loss: 0.1277	LR: 0.012500
Training Epoch: 85 [33536/50000]	Loss: 0.0714	LR: 0.012500
Training Epoch: 85 [33664/50000]	Loss: 0.1086	LR: 0.012500
Training Epoch: 85 [33792/50000]	Loss: 0.0798	LR: 0.012500
Training Epoch: 85 [33920/50000]	Loss: 0.1248	LR: 0.012500
Training Epoch: 85 [34048/50000]	Loss: 0.0816	LR: 0.012500
Training Epoch: 85 [34176/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 85 [34304/50000]	Loss: 0.1023	LR: 0.012500
Training Epoch: 85 [34432/50000]	Loss: 0.0886	LR: 0.012500
Training Epoch: 85 [34560/50000]	Loss: 0.1966	LR: 0.012500
Training Epoch: 85 [34688/50000]	Loss: 0.1143	LR: 0.012500
Training Epoch: 85 [34816/50000]	Loss: 0.1414	LR: 0.012500
Training Epoch: 85 [34944/50000]	Loss: 0.0706	LR: 0.012500
Training Epoch: 85 [35072/50000]	Loss: 0.1551	LR: 0.012500
Training Epoch: 85 [35200/50000]	Loss: 0.1536	LR: 0.012500
Training Epoch: 85 [35328/50000]	Loss: 0.1026	LR: 0.012500
Training Epoch: 85 [35456/50000]	Loss: 0.1131	LR: 0.012500
Training Epoch: 85 [35584/50000]	Loss: 0.1151	LR: 0.012500
Training Epoch: 85 [35712/50000]	Loss: 0.0954	LR: 0.012500
Training Epoch: 85 [35840/50000]	Loss: 0.1225	LR: 0.012500
Training Epoch: 85 [35968/50000]	Loss: 0.0593	LR: 0.012500
Training Epoch: 85 [36096/50000]	Loss: 0.1465	LR: 0.012500
Training Epoch: 85 [36224/50000]	Loss: 0.1589	LR: 0.012500
Training Epoch: 85 [36352/50000]	Loss: 0.0831	LR: 0.012500
Training Epoch: 85 [36480/50000]	Loss: 0.1442	LR: 0.012500
Training Epoch: 85 [36608/50000]	Loss: 0.1541	LR: 0.012500
Training Epoch: 85 [36736/50000]	Loss: 0.1152	LR: 0.012500
Training Epoch: 85 [36864/50000]	Loss: 0.1150	LR: 0.012500
Training Epoch: 85 [36992/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 85 [37120/50000]	Loss: 0.1767	LR: 0.012500
Training Epoch: 85 [37248/50000]	Loss: 0.1036	LR: 0.012500
Training Epoch: 85 [37376/50000]	Loss: 0.0918	LR: 0.012500
Training Epoch: 85 [37504/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 85 [37632/50000]	Loss: 0.0717	LR: 0.012500
Training Epoch: 85 [37760/50000]	Loss: 0.1529	LR: 0.012500
Training Epoch: 85 [37888/50000]	Loss: 0.2034	LR: 0.012500
Training Epoch: 85 [38016/50000]	Loss: 0.1038	LR: 0.012500
Training Epoch: 85 [38144/50000]	Loss: 0.1104	LR: 0.012500
Training Epoch: 85 [38272/50000]	Loss: 0.1336	LR: 0.012500
Training Epoch: 85 [38400/50000]	Loss: 0.1354	LR: 0.012500
Training Epoch: 85 [38528/50000]	Loss: 0.1159	LR: 0.012500
Training Epoch: 85 [38656/50000]	Loss: 0.1667	LR: 0.012500
Training Epoch: 85 [38784/50000]	Loss: 0.1136	LR: 0.012500
Training Epoch: 85 [38912/50000]	Loss: 0.1070	LR: 0.012500
Training Epoch: 85 [39040/50000]	Loss: 0.0858	LR: 0.012500
Training Epoch: 85 [39168/50000]	Loss: 0.1255	LR: 0.012500
Training Epoch: 85 [39296/50000]	Loss: 0.1167	LR: 0.012500
Training Epoch: 85 [39424/50000]	Loss: 0.1172	LR: 0.012500
Training Epoch: 85 [39552/50000]	Loss: 0.1766	LR: 0.012500
Training Epoch: 85 [39680/50000]	Loss: 0.1179	LR: 0.012500
Training Epoch: 85 [39808/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 85 [39936/50000]	Loss: 0.1650	LR: 0.012500
Training Epoch: 85 [40064/50000]	Loss: 0.1734	LR: 0.012500
Training Epoch: 85 [40192/50000]	Loss: 0.1062	LR: 0.012500
Training Epoch: 85 [40320/50000]	Loss: 0.1407	LR: 0.012500
Training Epoch: 85 [40448/50000]	Loss: 0.1440	LR: 0.012500
Training Epoch: 85 [40576/50000]	Loss: 0.1863	LR: 0.012500
Training Epoch: 85 [40704/50000]	Loss: 0.1037	LR: 0.012500
Training Epoch: 85 [40832/50000]	Loss: 0.1589	LR: 0.012500
Training Epoch: 85 [40960/50000]	Loss: 0.1127	LR: 0.012500
Training Epoch: 85 [41088/50000]	Loss: 0.1823	LR: 0.012500
Training Epoch: 85 [41216/50000]	Loss: 0.1741	LR: 0.012500
Training Epoch: 85 [41344/50000]	Loss: 0.1375	LR: 0.012500
Training Epoch: 85 [41472/50000]	Loss: 0.1071	LR: 0.012500
Training Epoch: 85 [41600/50000]	Loss: 0.1193	LR: 0.012500
Training Epoch: 85 [41728/50000]	Loss: 0.1417	LR: 0.012500
Training Epoch: 85 [41856/50000]	Loss: 0.0904	LR: 0.012500
Training Epoch: 85 [41984/50000]	Loss: 0.1300	LR: 0.012500
Training Epoch: 85 [42112/50000]	Loss: 0.0745	LR: 0.012500
Training Epoch: 85 [42240/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 85 [42368/50000]	Loss: 0.1072	LR: 0.012500
Training Epoch: 85 [42496/50000]	Loss: 0.1444	LR: 0.012500
Training Epoch: 85 [42624/50000]	Loss: 0.1995	LR: 0.012500
Training Epoch: 85 [42752/50000]	Loss: 0.1782	LR: 0.012500
Training Epoch: 85 [42880/50000]	Loss: 0.1295	LR: 0.012500
Training Epoch: 85 [43008/50000]	Loss: 0.1368	LR: 0.012500
Training Epoch: 85 [43136/50000]	Loss: 0.0966	LR: 0.012500
Training Epoch: 85 [43264/50000]	Loss: 0.1157	LR: 0.012500
Training Epoch: 85 [43392/50000]	Loss: 0.1877	LR: 0.012500
Training Epoch: 85 [43520/50000]	Loss: 0.1238	LR: 0.012500
Training Epoch: 85 [43648/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 85 [43776/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 85 [43904/50000]	Loss: 0.1876	LR: 0.012500
Training Epoch: 85 [44032/50000]	Loss: 0.0621	LR: 0.012500
Training Epoch: 85 [44160/50000]	Loss: 0.1312	LR: 0.012500
Training Epoch: 85 [44288/50000]	Loss: 0.1106	LR: 0.012500
Training Epoch: 85 [44416/50000]	Loss: 0.1781	LR: 0.012500
Training Epoch: 85 [44544/50000]	Loss: 0.1764	LR: 0.012500
Training Epoch: 85 [44672/50000]	Loss: 0.0522	LR: 0.012500
Training Epoch: 85 [44800/50000]	Loss: 0.1652	LR: 0.012500
Training Epoch: 85 [44928/50000]	Loss: 0.1397	LR: 0.012500
Training Epoch: 85 [45056/50000]	Loss: 0.1251	LR: 0.012500
Training Epoch: 85 [45184/50000]	Loss: 0.1006	LR: 0.012500
Training Epoch: 85 [45312/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 85 [45440/50000]	Loss: 0.1211	LR: 0.012500
Training Epoch: 85 [45568/50000]	Loss: 0.1808	LR: 0.012500
Training Epoch: 85 [45696/50000]	Loss: 0.1345	LR: 0.012500
Training Epoch: 85 [45824/50000]	Loss: 0.1430	LR: 0.012500
Training Epoch: 85 [45952/50000]	Loss: 0.0753	LR: 0.012500
Training Epoch: 85 [46080/50000]	Loss: 0.1227	LR: 0.012500
Training Epoch: 85 [46208/50000]	Loss: 0.1202	LR: 0.012500
Training Epoch: 85 [46336/50000]	Loss: 0.0902	LR: 0.012500
Training Epoch: 85 [46464/50000]	Loss: 0.0988	LR: 0.012500
Training Epoch: 85 [46592/50000]	Loss: 0.1391	LR: 0.012500
Training Epoch: 85 [46720/50000]	Loss: 0.1185	LR: 0.012500
Training Epoch: 85 [46848/50000]	Loss: 0.1249	LR: 0.012500
Training Epoch: 85 [46976/50000]	Loss: 0.0816	LR: 0.012500
Training Epoch: 85 [47104/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 85 [47232/50000]	Loss: 0.1274	LR: 0.012500
Training Epoch: 85 [47360/50000]	Loss: 0.1052	LR: 0.012500
Training Epoch: 85 [47488/50000]	Loss: 0.1185	LR: 0.012500
Training Epoch: 85 [47616/50000]	Loss: 0.1540	LR: 0.012500
Training Epoch: 85 [47744/50000]	Loss: 0.1561	LR: 0.012500
Training Epoch: 85 [47872/50000]	Loss: 0.1566	LR: 0.012500
Training Epoch: 85 [48000/50000]	Loss: 0.1459	LR: 0.012500
Training Epoch: 85 [48128/50000]	Loss: 0.1379	LR: 0.012500
Training Epoch: 85 [48256/50000]	Loss: 0.1013	LR: 0.012500
Training Epoch: 85 [48384/50000]	Loss: 0.1541	LR: 0.012500
Training Epoch: 85 [48512/50000]	Loss: 0.1265	LR: 0.012500
Training Epoch: 85 [48640/50000]	Loss: 0.1816	LR: 0.012500
Training Epoch: 85 [48768/50000]	Loss: 0.0945	LR: 0.012500
Training Epoch: 85 [48896/50000]	Loss: 0.1154	LR: 0.012500
Training Epoch: 85 [49024/50000]	Loss: 0.1260	LR: 0.012500
Training Epoch: 85 [49152/50000]	Loss: 0.0907	LR: 0.012500
Training Epoch: 85 [49280/50000]	Loss: 0.1908	LR: 0.012500
Training Epoch: 85 [49408/50000]	Loss: 0.1178	LR: 0.012500
Training Epoch: 85 [49536/50000]	Loss: 0.1051	LR: 0.012500
Training Epoch: 85 [49664/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 85 [49792/50000]	Loss: 0.1569	LR: 0.012500
Training Epoch: 85 [49920/50000]	Loss: 0.1454	LR: 0.012500
Training Epoch: 85 [50000/50000]	Loss: 0.2393	LR: 0.012500
epoch 85 training time consumed: 53.80s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  311955 GB |  311955 GB |
|       from large pool |  123392 KB |    1034 MB |  311647 GB |  311647 GB |
|       from small pool |   10798 KB |      13 MB |     307 GB |     307 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  311955 GB |  311955 GB |
|       from large pool |  123392 KB |    1034 MB |  311647 GB |  311647 GB |
|       from small pool |   10798 KB |      13 MB |     307 GB |     307 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  137278 GB |  137278 GB |
|       from large pool |  155136 KB |  433088 KB |  136938 GB |  136938 GB |
|       from small pool |    1489 KB |    3494 KB |     339 GB |     339 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   12037 K  |   12037 K  |
|       from large pool |      24    |      65    |    6283 K  |    6283 K  |
|       from small pool |     232    |     275    |    5754 K  |    5753 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   12037 K  |   12037 K  |
|       from large pool |      24    |      65    |    6283 K  |    6283 K  |
|       from small pool |     232    |     275    |    5754 K  |    5753 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    5952 K  |    5952 K  |
|       from large pool |       9    |      14    |    3041 K  |    3041 K  |
|       from small pool |      12    |      17    |    2911 K  |    2911 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 85, Average loss: 0.0093, Accuracy: 0.7158, Time consumed:3.45s

Training Epoch: 86 [128/50000]	Loss: 0.1491	LR: 0.012500
Training Epoch: 86 [256/50000]	Loss: 0.0895	LR: 0.012500
Training Epoch: 86 [384/50000]	Loss: 0.1071	LR: 0.012500
Training Epoch: 86 [512/50000]	Loss: 0.1404	LR: 0.012500
Training Epoch: 86 [640/50000]	Loss: 0.0835	LR: 0.012500
Training Epoch: 86 [768/50000]	Loss: 0.0966	LR: 0.012500
Training Epoch: 86 [896/50000]	Loss: 0.0795	LR: 0.012500
Training Epoch: 86 [1024/50000]	Loss: 0.0908	LR: 0.012500
Training Epoch: 86 [1152/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 86 [1280/50000]	Loss: 0.1936	LR: 0.012500
Training Epoch: 86 [1408/50000]	Loss: 0.1402	LR: 0.012500
Training Epoch: 86 [1536/50000]	Loss: 0.0926	LR: 0.012500
Training Epoch: 86 [1664/50000]	Loss: 0.1157	LR: 0.012500
Training Epoch: 86 [1792/50000]	Loss: 0.1130	LR: 0.012500
Training Epoch: 86 [1920/50000]	Loss: 0.1525	LR: 0.012500
Training Epoch: 86 [2048/50000]	Loss: 0.0787	LR: 0.012500
Training Epoch: 86 [2176/50000]	Loss: 0.1158	LR: 0.012500
Training Epoch: 86 [2304/50000]	Loss: 0.1526	LR: 0.012500
Training Epoch: 86 [2432/50000]	Loss: 0.1952	LR: 0.012500
Training Epoch: 86 [2560/50000]	Loss: 0.1094	LR: 0.012500
Training Epoch: 86 [2688/50000]	Loss: 0.1585	LR: 0.012500
Training Epoch: 86 [2816/50000]	Loss: 0.1034	LR: 0.012500
Training Epoch: 86 [2944/50000]	Loss: 0.0861	LR: 0.012500
Training Epoch: 86 [3072/50000]	Loss: 0.0794	LR: 0.012500
Training Epoch: 86 [3200/50000]	Loss: 0.1324	LR: 0.012500
Training Epoch: 86 [3328/50000]	Loss: 0.1382	LR: 0.012500
Training Epoch: 86 [3456/50000]	Loss: 0.1029	LR: 0.012500
Training Epoch: 86 [3584/50000]	Loss: 0.1004	LR: 0.012500
Training Epoch: 86 [3712/50000]	Loss: 0.1124	LR: 0.012500
Training Epoch: 86 [3840/50000]	Loss: 0.1510	LR: 0.012500
Training Epoch: 86 [3968/50000]	Loss: 0.1423	LR: 0.012500
Training Epoch: 86 [4096/50000]	Loss: 0.1605	LR: 0.012500
Training Epoch: 86 [4224/50000]	Loss: 0.0609	LR: 0.012500
Training Epoch: 86 [4352/50000]	Loss: 0.0935	LR: 0.012500
Training Epoch: 86 [4480/50000]	Loss: 0.0903	LR: 0.012500
Training Epoch: 86 [4608/50000]	Loss: 0.0966	LR: 0.012500
Training Epoch: 86 [4736/50000]	Loss: 0.0636	LR: 0.012500
Training Epoch: 86 [4864/50000]	Loss: 0.0829	LR: 0.012500
Training Epoch: 86 [4992/50000]	Loss: 0.1005	LR: 0.012500
Training Epoch: 86 [5120/50000]	Loss: 0.1234	LR: 0.012500
Training Epoch: 86 [5248/50000]	Loss: 0.1504	LR: 0.012500
Training Epoch: 86 [5376/50000]	Loss: 0.0928	LR: 0.012500
Training Epoch: 86 [5504/50000]	Loss: 0.0854	LR: 0.012500
Training Epoch: 86 [5632/50000]	Loss: 0.0790	LR: 0.012500
Training Epoch: 86 [5760/50000]	Loss: 0.1479	LR: 0.012500
Training Epoch: 86 [5888/50000]	Loss: 0.1120	LR: 0.012500
Training Epoch: 86 [6016/50000]	Loss: 0.1467	LR: 0.012500
Training Epoch: 86 [6144/50000]	Loss: 0.0748	LR: 0.012500
Training Epoch: 86 [6272/50000]	Loss: 0.0748	LR: 0.012500
Training Epoch: 86 [6400/50000]	Loss: 0.1231	LR: 0.012500
Training Epoch: 86 [6528/50000]	Loss: 0.1128	LR: 0.012500
Training Epoch: 86 [6656/50000]	Loss: 0.1094	LR: 0.012500
Training Epoch: 86 [6784/50000]	Loss: 0.1595	LR: 0.012500
Training Epoch: 86 [6912/50000]	Loss: 0.0648	LR: 0.012500
Training Epoch: 86 [7040/50000]	Loss: 0.0576	LR: 0.012500
Training Epoch: 86 [7168/50000]	Loss: 0.1009	LR: 0.012500
Training Epoch: 86 [7296/50000]	Loss: 0.0795	LR: 0.012500
Training Epoch: 86 [7424/50000]	Loss: 0.1263	LR: 0.012500
Training Epoch: 86 [7552/50000]	Loss: 0.1005	LR: 0.012500
Training Epoch: 86 [7680/50000]	Loss: 0.0616	LR: 0.012500
Training Epoch: 86 [7808/50000]	Loss: 0.0837	LR: 0.012500
Training Epoch: 86 [7936/50000]	Loss: 0.0776	LR: 0.012500
Training Epoch: 86 [8064/50000]	Loss: 0.1155	LR: 0.012500
Training Epoch: 86 [8192/50000]	Loss: 0.0819	LR: 0.012500
Training Epoch: 86 [8320/50000]	Loss: 0.0514	LR: 0.012500
Training Epoch: 86 [8448/50000]	Loss: 0.1401	LR: 0.012500
Training Epoch: 86 [8576/50000]	Loss: 0.1304	LR: 0.012500
Training Epoch: 86 [8704/50000]	Loss: 0.0992	LR: 0.012500
Training Epoch: 86 [8832/50000]	Loss: 0.0753	LR: 0.012500
Training Epoch: 86 [8960/50000]	Loss: 0.0868	LR: 0.012500
Training Epoch: 86 [9088/50000]	Loss: 0.1780	LR: 0.012500
Training Epoch: 86 [9216/50000]	Loss: 0.1197	LR: 0.012500
Training Epoch: 86 [9344/50000]	Loss: 0.0752	LR: 0.012500
Training Epoch: 86 [9472/50000]	Loss: 0.1676	LR: 0.012500
Training Epoch: 86 [9600/50000]	Loss: 0.1107	LR: 0.012500
Training Epoch: 86 [9728/50000]	Loss: 0.1178	LR: 0.012500
Training Epoch: 86 [9856/50000]	Loss: 0.0723	LR: 0.012500
Training Epoch: 86 [9984/50000]	Loss: 0.1562	LR: 0.012500
Training Epoch: 86 [10112/50000]	Loss: 0.1131	LR: 0.012500
Training Epoch: 86 [10240/50000]	Loss: 0.1045	LR: 0.012500
Training Epoch: 86 [10368/50000]	Loss: 0.0850	LR: 0.012500
Training Epoch: 86 [10496/50000]	Loss: 0.0860	LR: 0.012500
Training Epoch: 86 [10624/50000]	Loss: 0.0749	LR: 0.012500
Training Epoch: 86 [10752/50000]	Loss: 0.1162	LR: 0.012500
Training Epoch: 86 [10880/50000]	Loss: 0.1353	LR: 0.012500
Training Epoch: 86 [11008/50000]	Loss: 0.0942	LR: 0.012500
Training Epoch: 86 [11136/50000]	Loss: 0.1016	LR: 0.012500
Training Epoch: 86 [11264/50000]	Loss: 0.0953	LR: 0.012500
Training Epoch: 86 [11392/50000]	Loss: 0.1905	LR: 0.012500
Training Epoch: 86 [11520/50000]	Loss: 0.0934	LR: 0.012500
Training Epoch: 86 [11648/50000]	Loss: 0.0798	LR: 0.012500
Training Epoch: 86 [11776/50000]	Loss: 0.0684	LR: 0.012500
Training Epoch: 86 [11904/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 86 [12032/50000]	Loss: 0.2175	LR: 0.012500
Training Epoch: 86 [12160/50000]	Loss: 0.0951	LR: 0.012500
Training Epoch: 86 [12288/50000]	Loss: 0.1161	LR: 0.012500
Training Epoch: 86 [12416/50000]	Loss: 0.1446	LR: 0.012500
Training Epoch: 86 [12544/50000]	Loss: 0.1151	LR: 0.012500
Training Epoch: 86 [12672/50000]	Loss: 0.0990	LR: 0.012500
Training Epoch: 86 [12800/50000]	Loss: 0.0913	LR: 0.012500
Training Epoch: 86 [12928/50000]	Loss: 0.0954	LR: 0.012500
Training Epoch: 86 [13056/50000]	Loss: 0.1176	LR: 0.012500
Training Epoch: 86 [13184/50000]	Loss: 0.0778	LR: 0.012500
Training Epoch: 86 [13312/50000]	Loss: 0.1902	LR: 0.012500
Training Epoch: 86 [13440/50000]	Loss: 0.1262	LR: 0.012500
Training Epoch: 86 [13568/50000]	Loss: 0.0883	LR: 0.012500
Training Epoch: 86 [13696/50000]	Loss: 0.1239	LR: 0.012500
Training Epoch: 86 [13824/50000]	Loss: 0.1347	LR: 0.012500
Training Epoch: 86 [13952/50000]	Loss: 0.0784	LR: 0.012500
Training Epoch: 86 [14080/50000]	Loss: 0.0903	LR: 0.012500
Training Epoch: 86 [14208/50000]	Loss: 0.1242	LR: 0.012500
Training Epoch: 86 [14336/50000]	Loss: 0.1189	LR: 0.012500
Training Epoch: 86 [14464/50000]	Loss: 0.1229	LR: 0.012500
Training Epoch: 86 [14592/50000]	Loss: 0.0706	LR: 0.012500
Training Epoch: 86 [14720/50000]	Loss: 0.0822	LR: 0.012500
Training Epoch: 86 [14848/50000]	Loss: 0.1254	LR: 0.012500
Training Epoch: 86 [14976/50000]	Loss: 0.0678	LR: 0.012500
Training Epoch: 86 [15104/50000]	Loss: 0.1209	LR: 0.012500
Training Epoch: 86 [15232/50000]	Loss: 0.1284	LR: 0.012500
Training Epoch: 86 [15360/50000]	Loss: 0.1574	LR: 0.012500
Training Epoch: 86 [15488/50000]	Loss: 0.0943	LR: 0.012500
Training Epoch: 86 [15616/50000]	Loss: 0.1291	LR: 0.012500
Training Epoch: 86 [15744/50000]	Loss: 0.0790	LR: 0.012500
Training Epoch: 86 [15872/50000]	Loss: 0.1595	LR: 0.012500
Training Epoch: 86 [16000/50000]	Loss: 0.1136	LR: 0.012500
Training Epoch: 86 [16128/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 86 [16256/50000]	Loss: 0.1454	LR: 0.012500
Training Epoch: 86 [16384/50000]	Loss: 0.0896	LR: 0.012500
Training Epoch: 86 [16512/50000]	Loss: 0.0891	LR: 0.012500
Training Epoch: 86 [16640/50000]	Loss: 0.0964	LR: 0.012500
Training Epoch: 86 [16768/50000]	Loss: 0.1318	LR: 0.012500
Training Epoch: 86 [16896/50000]	Loss: 0.0896	LR: 0.012500
Training Epoch: 86 [17024/50000]	Loss: 0.1745	LR: 0.012500
Training Epoch: 86 [17152/50000]	Loss: 0.1230	LR: 0.012500
Training Epoch: 86 [17280/50000]	Loss: 0.1339	LR: 0.012500
Training Epoch: 86 [17408/50000]	Loss: 0.0860	LR: 0.012500
Training Epoch: 86 [17536/50000]	Loss: 0.1190	LR: 0.012500
Training Epoch: 86 [17664/50000]	Loss: 0.1509	LR: 0.012500
Training Epoch: 86 [17792/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 86 [17920/50000]	Loss: 0.1677	LR: 0.012500
Training Epoch: 86 [18048/50000]	Loss: 0.0765	LR: 0.012500
Training Epoch: 86 [18176/50000]	Loss: 0.1226	LR: 0.012500
Training Epoch: 86 [18304/50000]	Loss: 0.1025	LR: 0.012500
Training Epoch: 86 [18432/50000]	Loss: 0.1034	LR: 0.012500
Training Epoch: 86 [18560/50000]	Loss: 0.1026	LR: 0.012500
Training Epoch: 86 [18688/50000]	Loss: 0.1049	LR: 0.012500
Training Epoch: 86 [18816/50000]	Loss: 0.0829	LR: 0.012500
Training Epoch: 86 [18944/50000]	Loss: 0.0917	LR: 0.012500
Training Epoch: 86 [19072/50000]	Loss: 0.1521	LR: 0.012500
Training Epoch: 86 [19200/50000]	Loss: 0.0658	LR: 0.012500
Training Epoch: 86 [19328/50000]	Loss: 0.1757	LR: 0.012500
Training Epoch: 86 [19456/50000]	Loss: 0.1455	LR: 0.012500
Training Epoch: 86 [19584/50000]	Loss: 0.1273	LR: 0.012500
Training Epoch: 86 [19712/50000]	Loss: 0.0992	LR: 0.012500
Training Epoch: 86 [19840/50000]	Loss: 0.0958	LR: 0.012500
Training Epoch: 86 [19968/50000]	Loss: 0.0892	LR: 0.012500
Training Epoch: 86 [20096/50000]	Loss: 0.1174	LR: 0.012500
Training Epoch: 86 [20224/50000]	Loss: 0.1776	LR: 0.012500
Training Epoch: 86 [20352/50000]	Loss: 0.0968	LR: 0.012500
Training Epoch: 86 [20480/50000]	Loss: 0.0634	LR: 0.012500
Training Epoch: 86 [20608/50000]	Loss: 0.1343	LR: 0.012500
Training Epoch: 86 [20736/50000]	Loss: 0.2127	LR: 0.012500
Training Epoch: 86 [20864/50000]	Loss: 0.1078	LR: 0.012500
Training Epoch: 86 [20992/50000]	Loss: 0.1845	LR: 0.012500
Training Epoch: 86 [21120/50000]	Loss: 0.0971	LR: 0.012500
Training Epoch: 86 [21248/50000]	Loss: 0.0952	LR: 0.012500
Training Epoch: 86 [21376/50000]	Loss: 0.1325	LR: 0.012500
Training Epoch: 86 [21504/50000]	Loss: 0.1217	LR: 0.012500
Training Epoch: 86 [21632/50000]	Loss: 0.1700	LR: 0.012500
Training Epoch: 86 [21760/50000]	Loss: 0.1870	LR: 0.012500
Training Epoch: 86 [21888/50000]	Loss: 0.1262	LR: 0.012500
Training Epoch: 86 [22016/50000]	Loss: 0.1047	LR: 0.012500
Training Epoch: 86 [22144/50000]	Loss: 0.1358	LR: 0.012500
Training Epoch: 86 [22272/50000]	Loss: 0.1092	LR: 0.012500
Training Epoch: 86 [22400/50000]	Loss: 0.1368	LR: 0.012500
Training Epoch: 86 [22528/50000]	Loss: 0.1135	LR: 0.012500
Training Epoch: 86 [22656/50000]	Loss: 0.1189	LR: 0.012500
Training Epoch: 86 [22784/50000]	Loss: 0.1245	LR: 0.012500
Training Epoch: 86 [22912/50000]	Loss: 0.1418	LR: 0.012500
Training Epoch: 86 [23040/50000]	Loss: 0.2378	LR: 0.012500
Training Epoch: 86 [23168/50000]	Loss: 0.1716	LR: 0.012500
Training Epoch: 86 [23296/50000]	Loss: 0.2234	LR: 0.012500
Training Epoch: 86 [23424/50000]	Loss: 0.1144	LR: 0.012500
Training Epoch: 86 [23552/50000]	Loss: 0.0872	LR: 0.012500
Training Epoch: 86 [23680/50000]	Loss: 0.1298	LR: 0.012500
Training Epoch: 86 [23808/50000]	Loss: 0.0938	LR: 0.012500
Training Epoch: 86 [23936/50000]	Loss: 0.1253	LR: 0.012500
Training Epoch: 86 [24064/50000]	Loss: 0.1018	LR: 0.012500
Training Epoch: 86 [24192/50000]	Loss: 0.1097	LR: 0.012500
Training Epoch: 86 [24320/50000]	Loss: 0.1185	LR: 0.012500
Training Epoch: 86 [24448/50000]	Loss: 0.0960	LR: 0.012500
Training Epoch: 86 [24576/50000]	Loss: 0.0857	LR: 0.012500
Training Epoch: 86 [24704/50000]	Loss: 0.0686	LR: 0.012500
Training Epoch: 86 [24832/50000]	Loss: 0.1034	LR: 0.012500
Training Epoch: 86 [24960/50000]	Loss: 0.1213	LR: 0.012500
Training Epoch: 86 [25088/50000]	Loss: 0.1276	LR: 0.012500
Training Epoch: 86 [25216/50000]	Loss: 0.0846	LR: 0.012500
Training Epoch: 86 [25344/50000]	Loss: 0.1132	LR: 0.012500
Training Epoch: 86 [25472/50000]	Loss: 0.1317	LR: 0.012500
Training Epoch: 86 [25600/50000]	Loss: 0.0713	LR: 0.012500
Training Epoch: 86 [25728/50000]	Loss: 0.1664	LR: 0.012500
Training Epoch: 86 [25856/50000]	Loss: 0.0925	LR: 0.012500
Training Epoch: 86 [25984/50000]	Loss: 0.0968	LR: 0.012500
Training Epoch: 86 [26112/50000]	Loss: 0.0780	LR: 0.012500
Training Epoch: 86 [26240/50000]	Loss: 0.0959	LR: 0.012500
Training Epoch: 86 [26368/50000]	Loss: 0.1516	LR: 0.012500
Training Epoch: 86 [26496/50000]	Loss: 0.1012	LR: 0.012500
Training Epoch: 86 [26624/50000]	Loss: 0.0976	LR: 0.012500
Training Epoch: 86 [26752/50000]	Loss: 0.1297	LR: 0.012500
Training Epoch: 86 [26880/50000]	Loss: 0.1002	LR: 0.012500
Training Epoch: 86 [27008/50000]	Loss: 0.0793	LR: 0.012500
Training Epoch: 86 [27136/50000]	Loss: 0.1253	LR: 0.012500
Training Epoch: 86 [27264/50000]	Loss: 0.0806	LR: 0.012500
Training Epoch: 86 [27392/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 86 [27520/50000]	Loss: 0.1004	LR: 0.012500
Training Epoch: 86 [27648/50000]	Loss: 0.1546	LR: 0.012500
Training Epoch: 86 [27776/50000]	Loss: 0.0794	LR: 0.012500
Training Epoch: 86 [27904/50000]	Loss: 0.0842	LR: 0.012500
Training Epoch: 86 [28032/50000]	Loss: 0.1705	LR: 0.012500
Training Epoch: 86 [28160/50000]	Loss: 0.0923	LR: 0.012500
Training Epoch: 86 [28288/50000]	Loss: 0.1653	LR: 0.012500
Training Epoch: 86 [28416/50000]	Loss: 0.1507	LR: 0.012500
Training Epoch: 86 [28544/50000]	Loss: 0.1889	LR: 0.012500
Training Epoch: 86 [28672/50000]	Loss: 0.1236	LR: 0.012500
Training Epoch: 86 [28800/50000]	Loss: 0.1267	LR: 0.012500
Training Epoch: 86 [28928/50000]	Loss: 0.0942	LR: 0.012500
Training Epoch: 86 [29056/50000]	Loss: 0.1003	LR: 0.012500
Training Epoch: 86 [29184/50000]	Loss: 0.1317	LR: 0.012500
Training Epoch: 86 [29312/50000]	Loss: 0.1072	LR: 0.012500
Training Epoch: 86 [29440/50000]	Loss: 0.0877	LR: 0.012500
Training Epoch: 86 [29568/50000]	Loss: 0.1412	LR: 0.012500
Training Epoch: 86 [29696/50000]	Loss: 0.1891	LR: 0.012500
Training Epoch: 86 [29824/50000]	Loss: 0.0956	LR: 0.012500
Training Epoch: 86 [29952/50000]	Loss: 0.0873	LR: 0.012500
Training Epoch: 86 [30080/50000]	Loss: 0.0933	LR: 0.012500
Training Epoch: 86 [30208/50000]	Loss: 0.0903	LR: 0.012500
Training Epoch: 86 [30336/50000]	Loss: 0.1091	LR: 0.012500
Training Epoch: 86 [30464/50000]	Loss: 0.1198	LR: 0.012500
Training Epoch: 86 [30592/50000]	Loss: 0.0729	LR: 0.012500
Training Epoch: 86 [30720/50000]	Loss: 0.0756	LR: 0.012500
Training Epoch: 86 [30848/50000]	Loss: 0.0990	LR: 0.012500
Training Epoch: 86 [30976/50000]	Loss: 0.1137	LR: 0.012500
Training Epoch: 86 [31104/50000]	Loss: 0.1338	LR: 0.012500
Training Epoch: 86 [31232/50000]	Loss: 0.1093	LR: 0.012500
Training Epoch: 86 [31360/50000]	Loss: 0.0824	LR: 0.012500
Training Epoch: 86 [31488/50000]	Loss: 0.1257	LR: 0.012500
Training Epoch: 86 [31616/50000]	Loss: 0.1411	LR: 0.012500
Training Epoch: 86 [31744/50000]	Loss: 0.1400	LR: 0.012500
Training Epoch: 86 [31872/50000]	Loss: 0.1686	LR: 0.012500
Training Epoch: 86 [32000/50000]	Loss: 0.0959	LR: 0.012500
Training Epoch: 86 [32128/50000]	Loss: 0.0999	LR: 0.012500
Training Epoch: 86 [32256/50000]	Loss: 0.0427	LR: 0.012500
Training Epoch: 86 [32384/50000]	Loss: 0.1340	LR: 0.012500
Training Epoch: 86 [32512/50000]	Loss: 0.1142	LR: 0.012500
Training Epoch: 86 [32640/50000]	Loss: 0.0960	LR: 0.012500
Training Epoch: 86 [32768/50000]	Loss: 0.0777	LR: 0.012500
Training Epoch: 86 [32896/50000]	Loss: 0.0996	LR: 0.012500
Training Epoch: 86 [33024/50000]	Loss: 0.0924	LR: 0.012500
Training Epoch: 86 [33152/50000]	Loss: 0.1096	LR: 0.012500
Training Epoch: 86 [33280/50000]	Loss: 0.0845	LR: 0.012500
Training Epoch: 86 [33408/50000]	Loss: 0.0958	LR: 0.012500
Training Epoch: 86 [33536/50000]	Loss: 0.1787	LR: 0.012500
Training Epoch: 86 [33664/50000]	Loss: 0.0795	LR: 0.012500
Training Epoch: 86 [33792/50000]	Loss: 0.1121	LR: 0.012500
Training Epoch: 86 [33920/50000]	Loss: 0.1533	LR: 0.012500
Training Epoch: 86 [34048/50000]	Loss: 0.0758	LR: 0.012500
Training Epoch: 86 [34176/50000]	Loss: 0.0602	LR: 0.012500
Training Epoch: 86 [34304/50000]	Loss: 0.1550	LR: 0.012500
Training Epoch: 86 [34432/50000]	Loss: 0.0885	LR: 0.012500
Training Epoch: 86 [34560/50000]	Loss: 0.0557	LR: 0.012500
Training Epoch: 86 [34688/50000]	Loss: 0.1985	LR: 0.012500
Training Epoch: 86 [34816/50000]	Loss: 0.1394	LR: 0.012500
Training Epoch: 86 [34944/50000]	Loss: 0.1005	LR: 0.012500
Training Epoch: 86 [35072/50000]	Loss: 0.1250	LR: 0.012500
Training Epoch: 86 [35200/50000]	Loss: 0.1543	LR: 0.012500
Training Epoch: 86 [35328/50000]	Loss: 0.1662	LR: 0.012500
Training Epoch: 86 [35456/50000]	Loss: 0.1136	LR: 0.012500
Training Epoch: 86 [35584/50000]	Loss: 0.0866	LR: 0.012500
Training Epoch: 86 [35712/50000]	Loss: 0.1715	LR: 0.012500
Training Epoch: 86 [35840/50000]	Loss: 0.1581	LR: 0.012500
Training Epoch: 86 [35968/50000]	Loss: 0.1405	LR: 0.012500
Training Epoch: 86 [36096/50000]	Loss: 0.1011	LR: 0.012500
Training Epoch: 86 [36224/50000]	Loss: 0.1318	LR: 0.012500
Training Epoch: 86 [36352/50000]	Loss: 0.1005	LR: 0.012500
Training Epoch: 86 [36480/50000]	Loss: 0.1229	LR: 0.012500
Training Epoch: 86 [36608/50000]	Loss: 0.1089	LR: 0.012500
Training Epoch: 86 [36736/50000]	Loss: 0.1512	LR: 0.012500
Training Epoch: 86 [36864/50000]	Loss: 0.1416	LR: 0.012500
Training Epoch: 86 [36992/50000]	Loss: 0.1754	LR: 0.012500
Training Epoch: 86 [37120/50000]	Loss: 0.0892	LR: 0.012500
Training Epoch: 86 [37248/50000]	Loss: 0.0707	LR: 0.012500
Training Epoch: 86 [37376/50000]	Loss: 0.1242	LR: 0.012500
Training Epoch: 86 [37504/50000]	Loss: 0.1324	LR: 0.012500
Training Epoch: 86 [37632/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 86 [37760/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 86 [37888/50000]	Loss: 0.0934	LR: 0.012500
Training Epoch: 86 [38016/50000]	Loss: 0.1678	LR: 0.012500
Training Epoch: 86 [38144/50000]	Loss: 0.1479	LR: 0.012500
Training Epoch: 86 [38272/50000]	Loss: 0.1817	LR: 0.012500
Training Epoch: 86 [38400/50000]	Loss: 0.0744	LR: 0.012500
Training Epoch: 86 [38528/50000]	Loss: 0.1014	LR: 0.012500
Training Epoch: 86 [38656/50000]	Loss: 0.1697	LR: 0.012500
Training Epoch: 86 [38784/50000]	Loss: 0.0889	LR: 0.012500
Training Epoch: 86 [38912/50000]	Loss: 0.1448	LR: 0.012500
Training Epoch: 86 [39040/50000]	Loss: 0.1568	LR: 0.012500
Training Epoch: 86 [39168/50000]	Loss: 0.1433	LR: 0.012500
Training Epoch: 86 [39296/50000]	Loss: 0.1903	LR: 0.012500
Training Epoch: 86 [39424/50000]	Loss: 0.0986	LR: 0.012500
Training Epoch: 86 [39552/50000]	Loss: 0.0885	LR: 0.012500
Training Epoch: 86 [39680/50000]	Loss: 0.1086	LR: 0.012500
Training Epoch: 86 [39808/50000]	Loss: 0.1307	LR: 0.012500
Training Epoch: 86 [39936/50000]	Loss: 0.1597	LR: 0.012500
Training Epoch: 86 [40064/50000]	Loss: 0.1371	LR: 0.012500
Training Epoch: 86 [40192/50000]	Loss: 0.1815	LR: 0.012500
Training Epoch: 86 [40320/50000]	Loss: 0.0976	LR: 0.012500
Training Epoch: 86 [40448/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 86 [40576/50000]	Loss: 0.1375	LR: 0.012500
Training Epoch: 86 [40704/50000]	Loss: 0.1604	LR: 0.012500
Training Epoch: 86 [40832/50000]	Loss: 0.1395	LR: 0.012500
Training Epoch: 86 [40960/50000]	Loss: 0.1252	LR: 0.012500
Training Epoch: 86 [41088/50000]	Loss: 0.0953	LR: 0.012500
Training Epoch: 86 [41216/50000]	Loss: 0.1035	LR: 0.012500
Training Epoch: 86 [41344/50000]	Loss: 0.0998	LR: 0.012500
Training Epoch: 86 [41472/50000]	Loss: 0.1093	LR: 0.012500
Training Epoch: 86 [41600/50000]	Loss: 0.0834	LR: 0.012500
Training Epoch: 86 [41728/50000]	Loss: 0.0794	LR: 0.012500
Training Epoch: 86 [41856/50000]	Loss: 0.1157	LR: 0.012500
Training Epoch: 86 [41984/50000]	Loss: 0.1795	LR: 0.012500
Training Epoch: 86 [42112/50000]	Loss: 0.0986	LR: 0.012500
Training Epoch: 86 [42240/50000]	Loss: 0.0898	LR: 0.012500
Training Epoch: 86 [42368/50000]	Loss: 0.1010	LR: 0.012500
Training Epoch: 86 [42496/50000]	Loss: 0.0844	LR: 0.012500
Training Epoch: 86 [42624/50000]	Loss: 0.0984	LR: 0.012500
Training Epoch: 86 [42752/50000]	Loss: 0.1083	LR: 0.012500
Training Epoch: 86 [42880/50000]	Loss: 0.1278	LR: 0.012500
Training Epoch: 86 [43008/50000]	Loss: 0.0955	LR: 0.012500
Training Epoch: 86 [43136/50000]	Loss: 0.1414	LR: 0.012500
Training Epoch: 86 [43264/50000]	Loss: 0.1251	LR: 0.012500
Training Epoch: 86 [43392/50000]	Loss: 0.1307	LR: 0.012500
Training Epoch: 86 [43520/50000]	Loss: 0.1345	LR: 0.012500
Training Epoch: 86 [43648/50000]	Loss: 0.0858	LR: 0.012500
Training Epoch: 86 [43776/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 86 [43904/50000]	Loss: 0.0812	LR: 0.012500
Training Epoch: 86 [44032/50000]	Loss: 0.1379	LR: 0.012500
Training Epoch: 86 [44160/50000]	Loss: 0.1325	LR: 0.012500
Training Epoch: 86 [44288/50000]	Loss: 0.1192	LR: 0.012500
Training Epoch: 86 [44416/50000]	Loss: 0.1943	LR: 0.012500
Training Epoch: 86 [44544/50000]	Loss: 0.1498	LR: 0.012500
Training Epoch: 86 [44672/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 86 [44800/50000]	Loss: 0.0639	LR: 0.012500
Training Epoch: 86 [44928/50000]	Loss: 0.1942	LR: 0.012500
Training Epoch: 86 [45056/50000]	Loss: 0.1180	LR: 0.012500
Training Epoch: 86 [45184/50000]	Loss: 0.1131	LR: 0.012500
Training Epoch: 86 [45312/50000]	Loss: 0.1517	LR: 0.012500
Training Epoch: 86 [45440/50000]	Loss: 0.1323	LR: 0.012500
Training Epoch: 86 [45568/50000]	Loss: 0.1676	LR: 0.012500
Training Epoch: 86 [45696/50000]	Loss: 0.1621	LR: 0.012500
Training Epoch: 86 [45824/50000]	Loss: 0.1013	LR: 0.012500
Training Epoch: 86 [45952/50000]	Loss: 0.1137	LR: 0.012500
Training Epoch: 86 [46080/50000]	Loss: 0.0926	LR: 0.012500
Training Epoch: 86 [46208/50000]	Loss: 0.2171	LR: 0.012500
Training Epoch: 86 [46336/50000]	Loss: 0.1062	LR: 0.012500
Training Epoch: 86 [46464/50000]	Loss: 0.1201	LR: 0.012500
Training Epoch: 86 [46592/50000]	Loss: 0.0582	LR: 0.012500
Training Epoch: 86 [46720/50000]	Loss: 0.1241	LR: 0.012500
Training Epoch: 86 [46848/50000]	Loss: 0.0840	LR: 0.012500
Training Epoch: 86 [46976/50000]	Loss: 0.1602	LR: 0.012500
Training Epoch: 86 [47104/50000]	Loss: 0.1193	LR: 0.012500
Training Epoch: 86 [47232/50000]	Loss: 0.1313	LR: 0.012500
Training Epoch: 86 [47360/50000]	Loss: 0.1579	LR: 0.012500
Training Epoch: 86 [47488/50000]	Loss: 0.1137	LR: 0.012500
Training Epoch: 86 [47616/50000]	Loss: 0.0907	LR: 0.012500
Training Epoch: 86 [47744/50000]	Loss: 0.1896	LR: 0.012500
Training Epoch: 86 [47872/50000]	Loss: 0.1322	LR: 0.012500
Training Epoch: 86 [48000/50000]	Loss: 0.1533	LR: 0.012500
Training Epoch: 86 [48128/50000]	Loss: 0.0877	LR: 0.012500
Training Epoch: 86 [48256/50000]	Loss: 0.1370	LR: 0.012500
Training Epoch: 86 [48384/50000]	Loss: 0.1234	LR: 0.012500
Training Epoch: 86 [48512/50000]	Loss: 0.1387	LR: 0.012500
Training Epoch: 86 [48640/50000]	Loss: 0.1025	LR: 0.012500
Training Epoch: 86 [48768/50000]	Loss: 0.1836	LR: 0.012500
Training Epoch: 86 [48896/50000]	Loss: 0.1511	LR: 0.012500
Training Epoch: 86 [49024/50000]	Loss: 0.1849	LR: 0.012500
Training Epoch: 86 [49152/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 86 [49280/50000]	Loss: 0.1092	LR: 0.012500
Training Epoch: 86 [49408/50000]	Loss: 0.0685	LR: 0.012500
Training Epoch: 86 [49536/50000]	Loss: 0.0899	LR: 0.012500
Training Epoch: 86 [49664/50000]	Loss: 0.0963	LR: 0.012500
Training Epoch: 86 [49792/50000]	Loss: 0.1211	LR: 0.012500
Training Epoch: 86 [49920/50000]	Loss: 0.1779	LR: 0.012500
Training Epoch: 86 [50000/50000]	Loss: 0.1673	LR: 0.012500
epoch 86 training time consumed: 53.86s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  315625 GB |  315625 GB |
|       from large pool |  123392 KB |    1034 MB |  315314 GB |  315314 GB |
|       from small pool |   10798 KB |      13 MB |     310 GB |     310 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  315625 GB |  315625 GB |
|       from large pool |  123392 KB |    1034 MB |  315314 GB |  315314 GB |
|       from small pool |   10798 KB |      13 MB |     310 GB |     310 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  138893 GB |  138893 GB |
|       from large pool |  155136 KB |  433088 KB |  138549 GB |  138549 GB |
|       from small pool |    1489 KB |    3494 KB |     343 GB |     343 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   12178 K  |   12178 K  |
|       from large pool |      24    |      65    |    6357 K  |    6357 K  |
|       from small pool |     232    |     275    |    5821 K  |    5821 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   12178 K  |   12178 K  |
|       from large pool |      24    |      65    |    6357 K  |    6357 K  |
|       from small pool |     232    |     275    |    5821 K  |    5821 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6025 K  |    6025 K  |
|       from large pool |       9    |      14    |    3076 K  |    3076 K  |
|       from small pool |      12    |      17    |    2948 K  |    2948 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 86, Average loss: 0.0092, Accuracy: 0.7159, Time consumed:3.47s

Training Epoch: 87 [128/50000]	Loss: 0.1430	LR: 0.012500
Training Epoch: 87 [256/50000]	Loss: 0.1131	LR: 0.012500
Training Epoch: 87 [384/50000]	Loss: 0.0797	LR: 0.012500
Training Epoch: 87 [512/50000]	Loss: 0.1142	LR: 0.012500
Training Epoch: 87 [640/50000]	Loss: 0.1140	LR: 0.012500
Training Epoch: 87 [768/50000]	Loss: 0.1283	LR: 0.012500
Training Epoch: 87 [896/50000]	Loss: 0.0766	LR: 0.012500
Training Epoch: 87 [1024/50000]	Loss: 0.1182	LR: 0.012500
Training Epoch: 87 [1152/50000]	Loss: 0.1218	LR: 0.012500
Training Epoch: 87 [1280/50000]	Loss: 0.0684	LR: 0.012500
Training Epoch: 87 [1408/50000]	Loss: 0.0821	LR: 0.012500
Training Epoch: 87 [1536/50000]	Loss: 0.0753	LR: 0.012500
Training Epoch: 87 [1664/50000]	Loss: 0.1678	LR: 0.012500
Training Epoch: 87 [1792/50000]	Loss: 0.1584	LR: 0.012500
Training Epoch: 87 [1920/50000]	Loss: 0.1216	LR: 0.012500
Training Epoch: 87 [2048/50000]	Loss: 0.1024	LR: 0.012500
Training Epoch: 87 [2176/50000]	Loss: 0.1125	LR: 0.012500
Training Epoch: 87 [2304/50000]	Loss: 0.1841	LR: 0.012500
Training Epoch: 87 [2432/50000]	Loss: 0.1081	LR: 0.012500
Training Epoch: 87 [2560/50000]	Loss: 0.0751	LR: 0.012500
Training Epoch: 87 [2688/50000]	Loss: 0.1831	LR: 0.012500
Training Epoch: 87 [2816/50000]	Loss: 0.1283	LR: 0.012500
Training Epoch: 87 [2944/50000]	Loss: 0.1574	LR: 0.012500
Training Epoch: 87 [3072/50000]	Loss: 0.0840	LR: 0.012500
Training Epoch: 87 [3200/50000]	Loss: 0.1183	LR: 0.012500
Training Epoch: 87 [3328/50000]	Loss: 0.0596	LR: 0.012500
Training Epoch: 87 [3456/50000]	Loss: 0.0776	LR: 0.012500
Training Epoch: 87 [3584/50000]	Loss: 0.0879	LR: 0.012500
Training Epoch: 87 [3712/50000]	Loss: 0.0949	LR: 0.012500
Training Epoch: 87 [3840/50000]	Loss: 0.0806	LR: 0.012500
Training Epoch: 87 [3968/50000]	Loss: 0.1581	LR: 0.012500
Training Epoch: 87 [4096/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 87 [4224/50000]	Loss: 0.1150	LR: 0.012500
Training Epoch: 87 [4352/50000]	Loss: 0.0968	LR: 0.012500
Training Epoch: 87 [4480/50000]	Loss: 0.1301	LR: 0.012500
Training Epoch: 87 [4608/50000]	Loss: 0.1107	LR: 0.012500
Training Epoch: 87 [4736/50000]	Loss: 0.1026	LR: 0.012500
Training Epoch: 87 [4864/50000]	Loss: 0.1030	LR: 0.012500
Training Epoch: 87 [4992/50000]	Loss: 0.1431	LR: 0.012500
Training Epoch: 87 [5120/50000]	Loss: 0.0959	LR: 0.012500
Training Epoch: 87 [5248/50000]	Loss: 0.1392	LR: 0.012500
Training Epoch: 87 [5376/50000]	Loss: 0.0824	LR: 0.012500
Training Epoch: 87 [5504/50000]	Loss: 0.1001	LR: 0.012500
Training Epoch: 87 [5632/50000]	Loss: 0.1740	LR: 0.012500
Training Epoch: 87 [5760/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 87 [5888/50000]	Loss: 0.1130	LR: 0.012500
Training Epoch: 87 [6016/50000]	Loss: 0.2193	LR: 0.012500
Training Epoch: 87 [6144/50000]	Loss: 0.1919	LR: 0.012500
Training Epoch: 87 [6272/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 87 [6400/50000]	Loss: 0.0534	LR: 0.012500
Training Epoch: 87 [6528/50000]	Loss: 0.2274	LR: 0.012500
Training Epoch: 87 [6656/50000]	Loss: 0.1213	LR: 0.012500
Training Epoch: 87 [6784/50000]	Loss: 0.1080	LR: 0.012500
Training Epoch: 87 [6912/50000]	Loss: 0.0799	LR: 0.012500
Training Epoch: 87 [7040/50000]	Loss: 0.2056	LR: 0.012500
Training Epoch: 87 [7168/50000]	Loss: 0.1000	LR: 0.012500
Training Epoch: 87 [7296/50000]	Loss: 0.1134	LR: 0.012500
Training Epoch: 87 [7424/50000]	Loss: 0.1373	LR: 0.012500
Training Epoch: 87 [7552/50000]	Loss: 0.1080	LR: 0.012500
Training Epoch: 87 [7680/50000]	Loss: 0.1425	LR: 0.012500
Training Epoch: 87 [7808/50000]	Loss: 0.1019	LR: 0.012500
Training Epoch: 87 [7936/50000]	Loss: 0.1366	LR: 0.012500
Training Epoch: 87 [8064/50000]	Loss: 0.1298	LR: 0.012500
Training Epoch: 87 [8192/50000]	Loss: 0.1295	LR: 0.012500
Training Epoch: 87 [8320/50000]	Loss: 0.0809	LR: 0.012500
Training Epoch: 87 [8448/50000]	Loss: 0.0957	LR: 0.012500
Training Epoch: 87 [8576/50000]	Loss: 0.0851	LR: 0.012500
Training Epoch: 87 [8704/50000]	Loss: 0.0818	LR: 0.012500
Training Epoch: 87 [8832/50000]	Loss: 0.1624	LR: 0.012500
Training Epoch: 87 [8960/50000]	Loss: 0.1344	LR: 0.012500
Training Epoch: 87 [9088/50000]	Loss: 0.1202	LR: 0.012500
Training Epoch: 87 [9216/50000]	Loss: 0.0522	LR: 0.012500
Training Epoch: 87 [9344/50000]	Loss: 0.1446	LR: 0.012500
Training Epoch: 87 [9472/50000]	Loss: 0.0600	LR: 0.012500
Training Epoch: 87 [9600/50000]	Loss: 0.1251	LR: 0.012500
Training Epoch: 87 [9728/50000]	Loss: 0.1002	LR: 0.012500
Training Epoch: 87 [9856/50000]	Loss: 0.1194	LR: 0.012500
Training Epoch: 87 [9984/50000]	Loss: 0.1362	LR: 0.012500
Training Epoch: 87 [10112/50000]	Loss: 0.1830	LR: 0.012500
Training Epoch: 87 [10240/50000]	Loss: 0.0792	LR: 0.012500
Training Epoch: 87 [10368/50000]	Loss: 0.1388	LR: 0.012500
Training Epoch: 87 [10496/50000]	Loss: 0.1516	LR: 0.012500
Training Epoch: 87 [10624/50000]	Loss: 0.1189	LR: 0.012500
Training Epoch: 87 [10752/50000]	Loss: 0.0734	LR: 0.012500
Training Epoch: 87 [10880/50000]	Loss: 0.0735	LR: 0.012500
Training Epoch: 87 [11008/50000]	Loss: 0.1598	LR: 0.012500
Training Epoch: 87 [11136/50000]	Loss: 0.0627	LR: 0.012500
Training Epoch: 87 [11264/50000]	Loss: 0.0998	LR: 0.012500
Training Epoch: 87 [11392/50000]	Loss: 0.0958	LR: 0.012500
Training Epoch: 87 [11520/50000]	Loss: 0.1597	LR: 0.012500
Training Epoch: 87 [11648/50000]	Loss: 0.1828	LR: 0.012500
Training Epoch: 87 [11776/50000]	Loss: 0.0952	LR: 0.012500
Training Epoch: 87 [11904/50000]	Loss: 0.0551	LR: 0.012500
Training Epoch: 87 [12032/50000]	Loss: 0.1537	LR: 0.012500
Training Epoch: 87 [12160/50000]	Loss: 0.1391	LR: 0.012500
Training Epoch: 87 [12288/50000]	Loss: 0.1501	LR: 0.012500
Training Epoch: 87 [12416/50000]	Loss: 0.0777	LR: 0.012500
Training Epoch: 87 [12544/50000]	Loss: 0.1165	LR: 0.012500
Training Epoch: 87 [12672/50000]	Loss: 0.1569	LR: 0.012500
Training Epoch: 87 [12800/50000]	Loss: 0.1228	LR: 0.012500
Training Epoch: 87 [12928/50000]	Loss: 0.1238	LR: 0.012500
Training Epoch: 87 [13056/50000]	Loss: 0.1149	LR: 0.012500
Training Epoch: 87 [13184/50000]	Loss: 0.1165	LR: 0.012500
Training Epoch: 87 [13312/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 87 [13440/50000]	Loss: 0.1045	LR: 0.012500
Training Epoch: 87 [13568/50000]	Loss: 0.1290	LR: 0.012500
Training Epoch: 87 [13696/50000]	Loss: 0.0786	LR: 0.012500
Training Epoch: 87 [13824/50000]	Loss: 0.0977	LR: 0.012500
Training Epoch: 87 [13952/50000]	Loss: 0.1044	LR: 0.012500
Training Epoch: 87 [14080/50000]	Loss: 0.0566	LR: 0.012500
Training Epoch: 87 [14208/50000]	Loss: 0.1030	LR: 0.012500
Training Epoch: 87 [14336/50000]	Loss: 0.0671	LR: 0.012500
Training Epoch: 87 [14464/50000]	Loss: 0.1067	LR: 0.012500
Training Epoch: 87 [14592/50000]	Loss: 0.1795	LR: 0.012500
Training Epoch: 87 [14720/50000]	Loss: 0.1928	LR: 0.012500
Training Epoch: 87 [14848/50000]	Loss: 0.0746	LR: 0.012500
Training Epoch: 87 [14976/50000]	Loss: 0.1021	LR: 0.012500
Training Epoch: 87 [15104/50000]	Loss: 0.0886	LR: 0.012500
Training Epoch: 87 [15232/50000]	Loss: 0.0828	LR: 0.012500
Training Epoch: 87 [15360/50000]	Loss: 0.1216	LR: 0.012500
Training Epoch: 87 [15488/50000]	Loss: 0.1383	LR: 0.012500
Training Epoch: 87 [15616/50000]	Loss: 0.1057	LR: 0.012500
Training Epoch: 87 [15744/50000]	Loss: 0.1912	LR: 0.012500
Training Epoch: 87 [15872/50000]	Loss: 0.0882	LR: 0.012500
Training Epoch: 87 [16000/50000]	Loss: 0.0673	LR: 0.012500
Training Epoch: 87 [16128/50000]	Loss: 0.1272	LR: 0.012500
Training Epoch: 87 [16256/50000]	Loss: 0.0493	LR: 0.012500
Training Epoch: 87 [16384/50000]	Loss: 0.0684	LR: 0.012500
Training Epoch: 87 [16512/50000]	Loss: 0.0613	LR: 0.012500
Training Epoch: 87 [16640/50000]	Loss: 0.1244	LR: 0.012500
Training Epoch: 87 [16768/50000]	Loss: 0.1052	LR: 0.012500
Training Epoch: 87 [16896/50000]	Loss: 0.1665	LR: 0.012500
Training Epoch: 87 [17024/50000]	Loss: 0.1065	LR: 0.012500
Training Epoch: 87 [17152/50000]	Loss: 0.1077	LR: 0.012500
Training Epoch: 87 [17280/50000]	Loss: 0.1300	LR: 0.012500
Training Epoch: 87 [17408/50000]	Loss: 0.0914	LR: 0.012500
Training Epoch: 87 [17536/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 87 [17664/50000]	Loss: 0.0919	LR: 0.012500
Training Epoch: 87 [17792/50000]	Loss: 0.1167	LR: 0.012500
Training Epoch: 87 [17920/50000]	Loss: 0.1419	LR: 0.012500
Training Epoch: 87 [18048/50000]	Loss: 0.0665	LR: 0.012500
Training Epoch: 87 [18176/50000]	Loss: 0.0995	LR: 0.012500
Training Epoch: 87 [18304/50000]	Loss: 0.1351	LR: 0.012500
Training Epoch: 87 [18432/50000]	Loss: 0.0875	LR: 0.012500
Training Epoch: 87 [18560/50000]	Loss: 0.1451	LR: 0.012500
Training Epoch: 87 [18688/50000]	Loss: 0.0969	LR: 0.012500
Training Epoch: 87 [18816/50000]	Loss: 0.0657	LR: 0.012500
Training Epoch: 87 [18944/50000]	Loss: 0.1639	LR: 0.012500
Training Epoch: 87 [19072/50000]	Loss: 0.1253	LR: 0.012500
Training Epoch: 87 [19200/50000]	Loss: 0.1146	LR: 0.012500
Training Epoch: 87 [19328/50000]	Loss: 0.0731	LR: 0.012500
Training Epoch: 87 [19456/50000]	Loss: 0.1750	LR: 0.012500
Training Epoch: 87 [19584/50000]	Loss: 0.1022	LR: 0.012500
Training Epoch: 87 [19712/50000]	Loss: 0.0921	LR: 0.012500
Training Epoch: 87 [19840/50000]	Loss: 0.1321	LR: 0.012500
Training Epoch: 87 [19968/50000]	Loss: 0.2220	LR: 0.012500
Training Epoch: 87 [20096/50000]	Loss: 0.1103	LR: 0.012500
Training Epoch: 87 [20224/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 87 [20352/50000]	Loss: 0.1280	LR: 0.012500
Training Epoch: 87 [20480/50000]	Loss: 0.1584	LR: 0.012500
Training Epoch: 87 [20608/50000]	Loss: 0.1661	LR: 0.012500
Training Epoch: 87 [20736/50000]	Loss: 0.1223	LR: 0.012500
Training Epoch: 87 [20864/50000]	Loss: 0.1075	LR: 0.012500
Training Epoch: 87 [20992/50000]	Loss: 0.1078	LR: 0.012500
Training Epoch: 87 [21120/50000]	Loss: 0.1202	LR: 0.012500
Training Epoch: 87 [21248/50000]	Loss: 0.1531	LR: 0.012500
Training Epoch: 87 [21376/50000]	Loss: 0.1251	LR: 0.012500
Training Epoch: 87 [21504/50000]	Loss: 0.1596	LR: 0.012500
Training Epoch: 87 [21632/50000]	Loss: 0.0826	LR: 0.012500
Training Epoch: 87 [21760/50000]	Loss: 0.1091	LR: 0.012500
Training Epoch: 87 [21888/50000]	Loss: 0.1312	LR: 0.012500
Training Epoch: 87 [22016/50000]	Loss: 0.0941	LR: 0.012500
Training Epoch: 87 [22144/50000]	Loss: 0.1161	LR: 0.012500
Training Epoch: 87 [22272/50000]	Loss: 0.0707	LR: 0.012500
Training Epoch: 87 [22400/50000]	Loss: 0.1330	LR: 0.012500
Training Epoch: 87 [22528/50000]	Loss: 0.1915	LR: 0.012500
Training Epoch: 87 [22656/50000]	Loss: 0.1255	LR: 0.012500
Training Epoch: 87 [22784/50000]	Loss: 0.1172	LR: 0.012500
Training Epoch: 87 [22912/50000]	Loss: 0.1903	LR: 0.012500
Training Epoch: 87 [23040/50000]	Loss: 0.0702	LR: 0.012500
Training Epoch: 87 [23168/50000]	Loss: 0.0884	LR: 0.012500
Training Epoch: 87 [23296/50000]	Loss: 0.1571	LR: 0.012500
Training Epoch: 87 [23424/50000]	Loss: 0.1286	LR: 0.012500
Training Epoch: 87 [23552/50000]	Loss: 0.1322	LR: 0.012500
Training Epoch: 87 [23680/50000]	Loss: 0.0918	LR: 0.012500
Training Epoch: 87 [23808/50000]	Loss: 0.1611	LR: 0.012500
Training Epoch: 87 [23936/50000]	Loss: 0.1018	LR: 0.012500
Training Epoch: 87 [24064/50000]	Loss: 0.0746	LR: 0.012500
Training Epoch: 87 [24192/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 87 [24320/50000]	Loss: 0.0868	LR: 0.012500
Training Epoch: 87 [24448/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 87 [24576/50000]	Loss: 0.1005	LR: 0.012500
Training Epoch: 87 [24704/50000]	Loss: 0.1635	LR: 0.012500
Training Epoch: 87 [24832/50000]	Loss: 0.1141	LR: 0.012500
Training Epoch: 87 [24960/50000]	Loss: 0.0788	LR: 0.012500
Training Epoch: 87 [25088/50000]	Loss: 0.1374	LR: 0.012500
Training Epoch: 87 [25216/50000]	Loss: 0.1205	LR: 0.012500
Training Epoch: 87 [25344/50000]	Loss: 0.1382	LR: 0.012500
Training Epoch: 87 [25472/50000]	Loss: 0.1158	LR: 0.012500
Training Epoch: 87 [25600/50000]	Loss: 0.1000	LR: 0.012500
Training Epoch: 87 [25728/50000]	Loss: 0.1179	LR: 0.012500
Training Epoch: 87 [25856/50000]	Loss: 0.1340	LR: 0.012500
Training Epoch: 87 [25984/50000]	Loss: 0.1265	LR: 0.012500
Training Epoch: 87 [26112/50000]	Loss: 0.0946	LR: 0.012500
Training Epoch: 87 [26240/50000]	Loss: 0.0613	LR: 0.012500
Training Epoch: 87 [26368/50000]	Loss: 0.0883	LR: 0.012500
Training Epoch: 87 [26496/50000]	Loss: 0.1199	LR: 0.012500
Training Epoch: 87 [26624/50000]	Loss: 0.1016	LR: 0.012500
Training Epoch: 87 [26752/50000]	Loss: 0.1244	LR: 0.012500
Training Epoch: 87 [26880/50000]	Loss: 0.1828	LR: 0.012500
Training Epoch: 87 [27008/50000]	Loss: 0.1608	LR: 0.012500
Training Epoch: 87 [27136/50000]	Loss: 0.0819	LR: 0.012500
Training Epoch: 87 [27264/50000]	Loss: 0.0792	LR: 0.012500
Training Epoch: 87 [27392/50000]	Loss: 0.0853	LR: 0.012500
Training Epoch: 87 [27520/50000]	Loss: 0.1419	LR: 0.012500
Training Epoch: 87 [27648/50000]	Loss: 0.0820	LR: 0.012500
Training Epoch: 87 [27776/50000]	Loss: 0.0934	LR: 0.012500
Training Epoch: 87 [27904/50000]	Loss: 0.0988	LR: 0.012500
Training Epoch: 87 [28032/50000]	Loss: 0.1238	LR: 0.012500
Training Epoch: 87 [28160/50000]	Loss: 0.0860	LR: 0.012500
Training Epoch: 87 [28288/50000]	Loss: 0.0613	LR: 0.012500
Training Epoch: 87 [28416/50000]	Loss: 0.0970	LR: 0.012500
Training Epoch: 87 [28544/50000]	Loss: 0.1271	LR: 0.012500
Training Epoch: 87 [28672/50000]	Loss: 0.0859	LR: 0.012500
Training Epoch: 87 [28800/50000]	Loss: 0.1263	LR: 0.012500
Training Epoch: 87 [28928/50000]	Loss: 0.1091	LR: 0.012500
Training Epoch: 87 [29056/50000]	Loss: 0.1042	LR: 0.012500
Training Epoch: 87 [29184/50000]	Loss: 0.1705	LR: 0.012500
Training Epoch: 87 [29312/50000]	Loss: 0.0927	LR: 0.012500
Training Epoch: 87 [29440/50000]	Loss: 0.0789	LR: 0.012500
Training Epoch: 87 [29568/50000]	Loss: 0.1626	LR: 0.012500
Training Epoch: 87 [29696/50000]	Loss: 0.1461	LR: 0.012500
Training Epoch: 87 [29824/50000]	Loss: 0.1523	LR: 0.012500
Training Epoch: 87 [29952/50000]	Loss: 0.2183	LR: 0.012500
Training Epoch: 87 [30080/50000]	Loss: 0.0977	LR: 0.012500
Training Epoch: 87 [30208/50000]	Loss: 0.0981	LR: 0.012500
Training Epoch: 87 [30336/50000]	Loss: 0.0959	LR: 0.012500
Training Epoch: 87 [30464/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 87 [30592/50000]	Loss: 0.1433	LR: 0.012500
Training Epoch: 87 [30720/50000]	Loss: 0.1352	LR: 0.012500
Training Epoch: 87 [30848/50000]	Loss: 0.1326	LR: 0.012500
Training Epoch: 87 [30976/50000]	Loss: 0.1817	LR: 0.012500
Training Epoch: 87 [31104/50000]	Loss: 0.1449	LR: 0.012500
Training Epoch: 87 [31232/50000]	Loss: 0.1491	LR: 0.012500
Training Epoch: 87 [31360/50000]	Loss: 0.1193	LR: 0.012500
Training Epoch: 87 [31488/50000]	Loss: 0.1548	LR: 0.012500
Training Epoch: 87 [31616/50000]	Loss: 0.1238	LR: 0.012500
Training Epoch: 87 [31744/50000]	Loss: 0.0811	LR: 0.012500
Training Epoch: 87 [31872/50000]	Loss: 0.0681	LR: 0.012500
Training Epoch: 87 [32000/50000]	Loss: 0.1406	LR: 0.012500
Training Epoch: 87 [32128/50000]	Loss: 0.1475	LR: 0.012500
Training Epoch: 87 [32256/50000]	Loss: 0.1332	LR: 0.012500
Training Epoch: 87 [32384/50000]	Loss: 0.1117	LR: 0.012500
Training Epoch: 87 [32512/50000]	Loss: 0.0622	LR: 0.012500
Training Epoch: 87 [32640/50000]	Loss: 0.0985	LR: 0.012500
Training Epoch: 87 [32768/50000]	Loss: 0.0997	LR: 0.012500
Training Epoch: 87 [32896/50000]	Loss: 0.1723	LR: 0.012500
Training Epoch: 87 [33024/50000]	Loss: 0.1254	LR: 0.012500
Training Epoch: 87 [33152/50000]	Loss: 0.0858	LR: 0.012500
Training Epoch: 87 [33280/50000]	Loss: 0.1370	LR: 0.012500
Training Epoch: 87 [33408/50000]	Loss: 0.1249	LR: 0.012500
Training Epoch: 87 [33536/50000]	Loss: 0.0960	LR: 0.012500
Training Epoch: 87 [33664/50000]	Loss: 0.1977	LR: 0.012500
Training Epoch: 87 [33792/50000]	Loss: 0.1363	LR: 0.012500
Training Epoch: 87 [33920/50000]	Loss: 0.1142	LR: 0.012500
Training Epoch: 87 [34048/50000]	Loss: 0.0885	LR: 0.012500
Training Epoch: 87 [34176/50000]	Loss: 0.1496	LR: 0.012500
Training Epoch: 87 [34304/50000]	Loss: 0.1564	LR: 0.012500
Training Epoch: 87 [34432/50000]	Loss: 0.1409	LR: 0.012500
Training Epoch: 87 [34560/50000]	Loss: 0.1311	LR: 0.012500
Training Epoch: 87 [34688/50000]	Loss: 0.0747	LR: 0.012500
Training Epoch: 87 [34816/50000]	Loss: 0.1388	LR: 0.012500
Training Epoch: 87 [34944/50000]	Loss: 0.1801	LR: 0.012500
Training Epoch: 87 [35072/50000]	Loss: 0.1627	LR: 0.012500
Training Epoch: 87 [35200/50000]	Loss: 0.0753	LR: 0.012500
Training Epoch: 87 [35328/50000]	Loss: 0.1052	LR: 0.012500
Training Epoch: 87 [35456/50000]	Loss: 0.1265	LR: 0.012500
Training Epoch: 87 [35584/50000]	Loss: 0.1792	LR: 0.012500
Training Epoch: 87 [35712/50000]	Loss: 0.2291	LR: 0.012500
Training Epoch: 87 [35840/50000]	Loss: 0.1428	LR: 0.012500
Training Epoch: 87 [35968/50000]	Loss: 0.1823	LR: 0.012500
Training Epoch: 87 [36096/50000]	Loss: 0.1416	LR: 0.012500
Training Epoch: 87 [36224/50000]	Loss: 0.1253	LR: 0.012500
Training Epoch: 87 [36352/50000]	Loss: 0.1215	LR: 0.012500
Training Epoch: 87 [36480/50000]	Loss: 0.1206	LR: 0.012500
Training Epoch: 87 [36608/50000]	Loss: 0.0924	LR: 0.012500
Training Epoch: 87 [36736/50000]	Loss: 0.1419	LR: 0.012500
Training Epoch: 87 [36864/50000]	Loss: 0.1311	LR: 0.012500
Training Epoch: 87 [36992/50000]	Loss: 0.1105	LR: 0.012500
Training Epoch: 87 [37120/50000]	Loss: 0.1325	LR: 0.012500
Training Epoch: 87 [37248/50000]	Loss: 0.1225	LR: 0.012500
Training Epoch: 87 [37376/50000]	Loss: 0.1656	LR: 0.012500
Training Epoch: 87 [37504/50000]	Loss: 0.0962	LR: 0.012500
Training Epoch: 87 [37632/50000]	Loss: 0.1550	LR: 0.012500
Training Epoch: 87 [37760/50000]	Loss: 0.1241	LR: 0.012500
Training Epoch: 87 [37888/50000]	Loss: 0.0560	LR: 0.012500
Training Epoch: 87 [38016/50000]	Loss: 0.1715	LR: 0.012500
Training Epoch: 87 [38144/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 87 [38272/50000]	Loss: 0.1666	LR: 0.012500
Training Epoch: 87 [38400/50000]	Loss: 0.0780	LR: 0.012500
Training Epoch: 87 [38528/50000]	Loss: 0.1048	LR: 0.012500
Training Epoch: 87 [38656/50000]	Loss: 0.1230	LR: 0.012500
Training Epoch: 87 [38784/50000]	Loss: 0.1876	LR: 0.012500
Training Epoch: 87 [38912/50000]	Loss: 0.1357	LR: 0.012500
Training Epoch: 87 [39040/50000]	Loss: 0.0925	LR: 0.012500
Training Epoch: 87 [39168/50000]	Loss: 0.1156	LR: 0.012500
Training Epoch: 87 [39296/50000]	Loss: 0.1574	LR: 0.012500
Training Epoch: 87 [39424/50000]	Loss: 0.1668	LR: 0.012500
Training Epoch: 87 [39552/50000]	Loss: 0.1062	LR: 0.012500
Training Epoch: 87 [39680/50000]	Loss: 0.0800	LR: 0.012500
Training Epoch: 87 [39808/50000]	Loss: 0.0966	LR: 0.012500
Training Epoch: 87 [39936/50000]	Loss: 0.1571	LR: 0.012500
Training Epoch: 87 [40064/50000]	Loss: 0.1558	LR: 0.012500
Training Epoch: 87 [40192/50000]	Loss: 0.1357	LR: 0.012500
Training Epoch: 87 [40320/50000]	Loss: 0.1070	LR: 0.012500
Training Epoch: 87 [40448/50000]	Loss: 0.1710	LR: 0.012500
Training Epoch: 87 [40576/50000]	Loss: 0.1799	LR: 0.012500
Training Epoch: 87 [40704/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 87 [40832/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 87 [40960/50000]	Loss: 0.1878	LR: 0.012500
Training Epoch: 87 [41088/50000]	Loss: 0.1103	LR: 0.012500
Training Epoch: 87 [41216/50000]	Loss: 0.1939	LR: 0.012500
Training Epoch: 87 [41344/50000]	Loss: 0.1049	LR: 0.012500
Training Epoch: 87 [41472/50000]	Loss: 0.0679	LR: 0.012500
Training Epoch: 87 [41600/50000]	Loss: 0.0998	LR: 0.012500
Training Epoch: 87 [41728/50000]	Loss: 0.0932	LR: 0.012500
Training Epoch: 87 [41856/50000]	Loss: 0.2059	LR: 0.012500
Training Epoch: 87 [41984/50000]	Loss: 0.1102	LR: 0.012500
Training Epoch: 87 [42112/50000]	Loss: 0.1128	LR: 0.012500
Training Epoch: 87 [42240/50000]	Loss: 0.1371	LR: 0.012500
Training Epoch: 87 [42368/50000]	Loss: 0.1263	LR: 0.012500
Training Epoch: 87 [42496/50000]	Loss: 0.1399	LR: 0.012500
Training Epoch: 87 [42624/50000]	Loss: 0.1244	LR: 0.012500
Training Epoch: 87 [42752/50000]	Loss: 0.2531	LR: 0.012500
Training Epoch: 87 [42880/50000]	Loss: 0.1018	LR: 0.012500
Training Epoch: 87 [43008/50000]	Loss: 0.1426	LR: 0.012500
Training Epoch: 87 [43136/50000]	Loss: 0.1777	LR: 0.012500
Training Epoch: 87 [43264/50000]	Loss: 0.1262	LR: 0.012500
Training Epoch: 87 [43392/50000]	Loss: 0.0911	LR: 0.012500
Training Epoch: 87 [43520/50000]	Loss: 0.1252	LR: 0.012500
Training Epoch: 87 [43648/50000]	Loss: 0.1114	LR: 0.012500
Training Epoch: 87 [43776/50000]	Loss: 0.0997	LR: 0.012500
Training Epoch: 87 [43904/50000]	Loss: 0.1947	LR: 0.012500
Training Epoch: 87 [44032/50000]	Loss: 0.1163	LR: 0.012500
Training Epoch: 87 [44160/50000]	Loss: 0.0742	LR: 0.012500
Training Epoch: 87 [44288/50000]	Loss: 0.1526	LR: 0.012500
Training Epoch: 87 [44416/50000]	Loss: 0.1215	LR: 0.012500
Training Epoch: 87 [44544/50000]	Loss: 0.0685	LR: 0.012500
Training Epoch: 87 [44672/50000]	Loss: 0.1141	LR: 0.012500
Training Epoch: 87 [44800/50000]	Loss: 0.1438	LR: 0.012500
Training Epoch: 87 [44928/50000]	Loss: 0.0964	LR: 0.012500
Training Epoch: 87 [45056/50000]	Loss: 0.1297	LR: 0.012500
Training Epoch: 87 [45184/50000]	Loss: 0.1443	LR: 0.012500
Training Epoch: 87 [45312/50000]	Loss: 0.1196	LR: 0.012500
Training Epoch: 87 [45440/50000]	Loss: 0.1645	LR: 0.012500
Training Epoch: 87 [45568/50000]	Loss: 0.1460	LR: 0.012500
Training Epoch: 87 [45696/50000]	Loss: 0.0904	LR: 0.012500
Training Epoch: 87 [45824/50000]	Loss: 0.1496	LR: 0.012500
Training Epoch: 87 [45952/50000]	Loss: 0.1396	LR: 0.012500
Training Epoch: 87 [46080/50000]	Loss: 0.1071	LR: 0.012500
Training Epoch: 87 [46208/50000]	Loss: 0.1555	LR: 0.012500
Training Epoch: 87 [46336/50000]	Loss: 0.1505	LR: 0.012500
Training Epoch: 87 [46464/50000]	Loss: 0.1173	LR: 0.012500
Training Epoch: 87 [46592/50000]	Loss: 0.1358	LR: 0.012500
Training Epoch: 87 [46720/50000]	Loss: 0.0864	LR: 0.012500
Training Epoch: 87 [46848/50000]	Loss: 0.1132	LR: 0.012500
Training Epoch: 87 [46976/50000]	Loss: 0.1577	LR: 0.012500
Training Epoch: 87 [47104/50000]	Loss: 0.1409	LR: 0.012500
Training Epoch: 87 [47232/50000]	Loss: 0.1696	LR: 0.012500
Training Epoch: 87 [47360/50000]	Loss: 0.1728	LR: 0.012500
Training Epoch: 87 [47488/50000]	Loss: 0.0795	LR: 0.012500
Training Epoch: 87 [47616/50000]	Loss: 0.1507	LR: 0.012500
Training Epoch: 87 [47744/50000]	Loss: 0.0980	LR: 0.012500
Training Epoch: 87 [47872/50000]	Loss: 0.0721	LR: 0.012500
Training Epoch: 87 [48000/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 87 [48128/50000]	Loss: 0.1322	LR: 0.012500
Training Epoch: 87 [48256/50000]	Loss: 0.1047	LR: 0.012500
Training Epoch: 87 [48384/50000]	Loss: 0.1218	LR: 0.012500
Training Epoch: 87 [48512/50000]	Loss: 0.1180	LR: 0.012500
Training Epoch: 87 [48640/50000]	Loss: 0.0725	LR: 0.012500
Training Epoch: 87 [48768/50000]	Loss: 0.0899	LR: 0.012500
Training Epoch: 87 [48896/50000]	Loss: 0.1007	LR: 0.012500
Training Epoch: 87 [49024/50000]	Loss: 0.1186	LR: 0.012500
Training Epoch: 87 [49152/50000]	Loss: 0.1179	LR: 0.012500
Training Epoch: 87 [49280/50000]	Loss: 0.1254	LR: 0.012500
Training Epoch: 87 [49408/50000]	Loss: 0.1878	LR: 0.012500
Training Epoch: 87 [49536/50000]	Loss: 0.1149	LR: 0.012500
Training Epoch: 87 [49664/50000]	Loss: 0.0944	LR: 0.012500
Training Epoch: 87 [49792/50000]	Loss: 0.1397	LR: 0.012500
Training Epoch: 87 [49920/50000]	Loss: 0.1500	LR: 0.012500
Training Epoch: 87 [50000/50000]	Loss: 0.1349	LR: 0.012500
epoch 87 training time consumed: 54.08s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  319295 GB |  319295 GB |
|       from large pool |  123392 KB |    1034 MB |  318980 GB |  318980 GB |
|       from small pool |   10798 KB |      13 MB |     314 GB |     314 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  319295 GB |  319295 GB |
|       from large pool |  123392 KB |    1034 MB |  318980 GB |  318980 GB |
|       from small pool |   10798 KB |      13 MB |     314 GB |     314 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  140508 GB |  140508 GB |
|       from large pool |  155136 KB |  433088 KB |  140160 GB |  140160 GB |
|       from small pool |    1489 KB |    3494 KB |     347 GB |     347 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   12320 K  |   12320 K  |
|       from large pool |      24    |      65    |    6431 K  |    6431 K  |
|       from small pool |     232    |     275    |    5889 K  |    5889 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   12320 K  |   12320 K  |
|       from large pool |      24    |      65    |    6431 K  |    6431 K  |
|       from small pool |     232    |     275    |    5889 K  |    5889 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6097 K  |    6097 K  |
|       from large pool |       9    |      14    |    3112 K  |    3112 K  |
|       from small pool |      12    |      17    |    2984 K  |    2984 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 87, Average loss: 0.0092, Accuracy: 0.7195, Time consumed:3.48s

Training Epoch: 88 [128/50000]	Loss: 0.0830	LR: 0.012500
Training Epoch: 88 [256/50000]	Loss: 0.0747	LR: 0.012500
Training Epoch: 88 [384/50000]	Loss: 0.0882	LR: 0.012500
Training Epoch: 88 [512/50000]	Loss: 0.1159	LR: 0.012500
Training Epoch: 88 [640/50000]	Loss: 0.1237	LR: 0.012500
Training Epoch: 88 [768/50000]	Loss: 0.0938	LR: 0.012500
Training Epoch: 88 [896/50000]	Loss: 0.0780	LR: 0.012500
Training Epoch: 88 [1024/50000]	Loss: 0.1509	LR: 0.012500
Training Epoch: 88 [1152/50000]	Loss: 0.0868	LR: 0.012500
Training Epoch: 88 [1280/50000]	Loss: 0.0957	LR: 0.012500
Training Epoch: 88 [1408/50000]	Loss: 0.0769	LR: 0.012500
Training Epoch: 88 [1536/50000]	Loss: 0.1226	LR: 0.012500
Training Epoch: 88 [1664/50000]	Loss: 0.0847	LR: 0.012500
Training Epoch: 88 [1792/50000]	Loss: 0.0955	LR: 0.012500
Training Epoch: 88 [1920/50000]	Loss: 0.1183	LR: 0.012500
Training Epoch: 88 [2048/50000]	Loss: 0.1079	LR: 0.012500
Training Epoch: 88 [2176/50000]	Loss: 0.1465	LR: 0.012500
Training Epoch: 88 [2304/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 88 [2432/50000]	Loss: 0.1048	LR: 0.012500
Training Epoch: 88 [2560/50000]	Loss: 0.0796	LR: 0.012500
Training Epoch: 88 [2688/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 88 [2816/50000]	Loss: 0.1124	LR: 0.012500
Training Epoch: 88 [2944/50000]	Loss: 0.0864	LR: 0.012500
Training Epoch: 88 [3072/50000]	Loss: 0.1460	LR: 0.012500
Training Epoch: 88 [3200/50000]	Loss: 0.0967	LR: 0.012500
Training Epoch: 88 [3328/50000]	Loss: 0.1139	LR: 0.012500
Training Epoch: 88 [3456/50000]	Loss: 0.1011	LR: 0.012500
Training Epoch: 88 [3584/50000]	Loss: 0.1345	LR: 0.012500
Training Epoch: 88 [3712/50000]	Loss: 0.1308	LR: 0.012500
Training Epoch: 88 [3840/50000]	Loss: 0.1121	LR: 0.012500
Training Epoch: 88 [3968/50000]	Loss: 0.1045	LR: 0.012500
Training Epoch: 88 [4096/50000]	Loss: 0.0507	LR: 0.012500
Training Epoch: 88 [4224/50000]	Loss: 0.0888	LR: 0.012500
Training Epoch: 88 [4352/50000]	Loss: 0.1555	LR: 0.012500
Training Epoch: 88 [4480/50000]	Loss: 0.1038	LR: 0.012500
Training Epoch: 88 [4608/50000]	Loss: 0.0741	LR: 0.012500
Training Epoch: 88 [4736/50000]	Loss: 0.0602	LR: 0.012500
Training Epoch: 88 [4864/50000]	Loss: 0.1568	LR: 0.012500
Training Epoch: 88 [4992/50000]	Loss: 0.0924	LR: 0.012500
Training Epoch: 88 [5120/50000]	Loss: 0.1102	LR: 0.012500
Training Epoch: 88 [5248/50000]	Loss: 0.0976	LR: 0.012500
Training Epoch: 88 [5376/50000]	Loss: 0.0898	LR: 0.012500
Training Epoch: 88 [5504/50000]	Loss: 0.0981	LR: 0.012500
Training Epoch: 88 [5632/50000]	Loss: 0.0604	LR: 0.012500
Training Epoch: 88 [5760/50000]	Loss: 0.0652	LR: 0.012500
Training Epoch: 88 [5888/50000]	Loss: 0.1111	LR: 0.012500
Training Epoch: 88 [6016/50000]	Loss: 0.0633	LR: 0.012500
Training Epoch: 88 [6144/50000]	Loss: 0.1120	LR: 0.012500
Training Epoch: 88 [6272/50000]	Loss: 0.0812	LR: 0.012500
Training Epoch: 88 [6400/50000]	Loss: 0.1130	LR: 0.012500
Training Epoch: 88 [6528/50000]	Loss: 0.0657	LR: 0.012500
Training Epoch: 88 [6656/50000]	Loss: 0.0864	LR: 0.012500
Training Epoch: 88 [6784/50000]	Loss: 0.0897	LR: 0.012500
Training Epoch: 88 [6912/50000]	Loss: 0.1190	LR: 0.012500
Training Epoch: 88 [7040/50000]	Loss: 0.0897	LR: 0.012500
Training Epoch: 88 [7168/50000]	Loss: 0.1071	LR: 0.012500
Training Epoch: 88 [7296/50000]	Loss: 0.0923	LR: 0.012500
Training Epoch: 88 [7424/50000]	Loss: 0.0799	LR: 0.012500
Training Epoch: 88 [7552/50000]	Loss: 0.0970	LR: 0.012500
Training Epoch: 88 [7680/50000]	Loss: 0.1385	LR: 0.012500
Training Epoch: 88 [7808/50000]	Loss: 0.0924	LR: 0.012500
Training Epoch: 88 [7936/50000]	Loss: 0.0935	LR: 0.012500
Training Epoch: 88 [8064/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 88 [8192/50000]	Loss: 0.1097	LR: 0.012500
Training Epoch: 88 [8320/50000]	Loss: 0.0618	LR: 0.012500
Training Epoch: 88 [8448/50000]	Loss: 0.0618	LR: 0.012500
Training Epoch: 88 [8576/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 88 [8704/50000]	Loss: 0.1335	LR: 0.012500
Training Epoch: 88 [8832/50000]	Loss: 0.0758	LR: 0.012500
Training Epoch: 88 [8960/50000]	Loss: 0.1085	LR: 0.012500
Training Epoch: 88 [9088/50000]	Loss: 0.0979	LR: 0.012500
Training Epoch: 88 [9216/50000]	Loss: 0.1196	LR: 0.012500
Training Epoch: 88 [9344/50000]	Loss: 0.1379	LR: 0.012500
Training Epoch: 88 [9472/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 88 [9600/50000]	Loss: 0.0846	LR: 0.012500
Training Epoch: 88 [9728/50000]	Loss: 0.1336	LR: 0.012500
Training Epoch: 88 [9856/50000]	Loss: 0.0702	LR: 0.012500
Training Epoch: 88 [9984/50000]	Loss: 0.1075	LR: 0.012500
Training Epoch: 88 [10112/50000]	Loss: 0.0723	LR: 0.012500
Training Epoch: 88 [10240/50000]	Loss: 0.1032	LR: 0.012500
Training Epoch: 88 [10368/50000]	Loss: 0.0959	LR: 0.012500
Training Epoch: 88 [10496/50000]	Loss: 0.1581	LR: 0.012500
Training Epoch: 88 [10624/50000]	Loss: 0.1077	LR: 0.012500
Training Epoch: 88 [10752/50000]	Loss: 0.0985	LR: 0.012500
Training Epoch: 88 [10880/50000]	Loss: 0.1234	LR: 0.012500
Training Epoch: 88 [11008/50000]	Loss: 0.0975	LR: 0.012500
Training Epoch: 88 [11136/50000]	Loss: 0.0991	LR: 0.012500
Training Epoch: 88 [11264/50000]	Loss: 0.0962	LR: 0.012500
Training Epoch: 88 [11392/50000]	Loss: 0.1196	LR: 0.012500
Training Epoch: 88 [11520/50000]	Loss: 0.1297	LR: 0.012500
Training Epoch: 88 [11648/50000]	Loss: 0.0954	LR: 0.012500
Training Epoch: 88 [11776/50000]	Loss: 0.1482	LR: 0.012500
Training Epoch: 88 [11904/50000]	Loss: 0.1072	LR: 0.012500
Training Epoch: 88 [12032/50000]	Loss: 0.1704	LR: 0.012500
Training Epoch: 88 [12160/50000]	Loss: 0.0744	LR: 0.012500
Training Epoch: 88 [12288/50000]	Loss: 0.0799	LR: 0.012500
Training Epoch: 88 [12416/50000]	Loss: 0.1387	LR: 0.012500
Training Epoch: 88 [12544/50000]	Loss: 0.0901	LR: 0.012500
Training Epoch: 88 [12672/50000]	Loss: 0.1053	LR: 0.012500
Training Epoch: 88 [12800/50000]	Loss: 0.1623	LR: 0.012500
Training Epoch: 88 [12928/50000]	Loss: 0.0956	LR: 0.012500
Training Epoch: 88 [13056/50000]	Loss: 0.0753	LR: 0.012500
Training Epoch: 88 [13184/50000]	Loss: 0.0627	LR: 0.012500
Training Epoch: 88 [13312/50000]	Loss: 0.1359	LR: 0.012500
Training Epoch: 88 [13440/50000]	Loss: 0.0959	LR: 0.012500
Training Epoch: 88 [13568/50000]	Loss: 0.0926	LR: 0.012500
Training Epoch: 88 [13696/50000]	Loss: 0.1747	LR: 0.012500
Training Epoch: 88 [13824/50000]	Loss: 0.1155	LR: 0.012500
Training Epoch: 88 [13952/50000]	Loss: 0.1115	LR: 0.012500
Training Epoch: 88 [14080/50000]	Loss: 0.0769	LR: 0.012500
Training Epoch: 88 [14208/50000]	Loss: 0.1127	LR: 0.012500
Training Epoch: 88 [14336/50000]	Loss: 0.0709	LR: 0.012500
Training Epoch: 88 [14464/50000]	Loss: 0.0517	LR: 0.012500
Training Epoch: 88 [14592/50000]	Loss: 0.0760	LR: 0.012500
Training Epoch: 88 [14720/50000]	Loss: 0.1588	LR: 0.012500
Training Epoch: 88 [14848/50000]	Loss: 0.1505	LR: 0.012500
Training Epoch: 88 [14976/50000]	Loss: 0.0831	LR: 0.012500
Training Epoch: 88 [15104/50000]	Loss: 0.1214	LR: 0.012500
Training Epoch: 88 [15232/50000]	Loss: 0.0970	LR: 0.012500
Training Epoch: 88 [15360/50000]	Loss: 0.0586	LR: 0.012500
Training Epoch: 88 [15488/50000]	Loss: 0.1247	LR: 0.012500
Training Epoch: 88 [15616/50000]	Loss: 0.1213	LR: 0.012500
Training Epoch: 88 [15744/50000]	Loss: 0.1783	LR: 0.012500
Training Epoch: 88 [15872/50000]	Loss: 0.0807	LR: 0.012500
Training Epoch: 88 [16000/50000]	Loss: 0.1219	LR: 0.012500
Training Epoch: 88 [16128/50000]	Loss: 0.1042	LR: 0.012500
Training Epoch: 88 [16256/50000]	Loss: 0.1276	LR: 0.012500
Training Epoch: 88 [16384/50000]	Loss: 0.1111	LR: 0.012500
Training Epoch: 88 [16512/50000]	Loss: 0.1087	LR: 0.012500
Training Epoch: 88 [16640/50000]	Loss: 0.0995	LR: 0.012500
Training Epoch: 88 [16768/50000]	Loss: 0.1094	LR: 0.012500
Training Epoch: 88 [16896/50000]	Loss: 0.1151	LR: 0.012500
Training Epoch: 88 [17024/50000]	Loss: 0.1909	LR: 0.012500
Training Epoch: 88 [17152/50000]	Loss: 0.1321	LR: 0.012500
Training Epoch: 88 [17280/50000]	Loss: 0.0880	LR: 0.012500
Training Epoch: 88 [17408/50000]	Loss: 0.1654	LR: 0.012500
Training Epoch: 88 [17536/50000]	Loss: 0.0742	LR: 0.012500
Training Epoch: 88 [17664/50000]	Loss: 0.1349	LR: 0.012500
Training Epoch: 88 [17792/50000]	Loss: 0.0885	LR: 0.012500
Training Epoch: 88 [17920/50000]	Loss: 0.1413	LR: 0.012500
Training Epoch: 88 [18048/50000]	Loss: 0.0933	LR: 0.012500
Training Epoch: 88 [18176/50000]	Loss: 0.0750	LR: 0.012500
Training Epoch: 88 [18304/50000]	Loss: 0.0842	LR: 0.012500
Training Epoch: 88 [18432/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 88 [18560/50000]	Loss: 0.1064	LR: 0.012500
Training Epoch: 88 [18688/50000]	Loss: 0.1227	LR: 0.012500
Training Epoch: 88 [18816/50000]	Loss: 0.1379	LR: 0.012500
Training Epoch: 88 [18944/50000]	Loss: 0.0848	LR: 0.012500
Training Epoch: 88 [19072/50000]	Loss: 0.1051	LR: 0.012500
Training Epoch: 88 [19200/50000]	Loss: 0.1239	LR: 0.012500
Training Epoch: 88 [19328/50000]	Loss: 0.0555	LR: 0.012500
Training Epoch: 88 [19456/50000]	Loss: 0.1719	LR: 0.012500
Training Epoch: 88 [19584/50000]	Loss: 0.1169	LR: 0.012500
Training Epoch: 88 [19712/50000]	Loss: 0.1196	LR: 0.012500
Training Epoch: 88 [19840/50000]	Loss: 0.1406	LR: 0.012500
Training Epoch: 88 [19968/50000]	Loss: 0.1391	LR: 0.012500
Training Epoch: 88 [20096/50000]	Loss: 0.1598	LR: 0.012500
Training Epoch: 88 [20224/50000]	Loss: 0.1237	LR: 0.012500
Training Epoch: 88 [20352/50000]	Loss: 0.0921	LR: 0.012500
Training Epoch: 88 [20480/50000]	Loss: 0.1638	LR: 0.012500
Training Epoch: 88 [20608/50000]	Loss: 0.0943	LR: 0.012500
Training Epoch: 88 [20736/50000]	Loss: 0.1244	LR: 0.012500
Training Epoch: 88 [20864/50000]	Loss: 0.0942	LR: 0.012500
Training Epoch: 88 [20992/50000]	Loss: 0.0892	LR: 0.012500
Training Epoch: 88 [21120/50000]	Loss: 0.0968	LR: 0.012500
Training Epoch: 88 [21248/50000]	Loss: 0.1123	LR: 0.012500
Training Epoch: 88 [21376/50000]	Loss: 0.1424	LR: 0.012500
Training Epoch: 88 [21504/50000]	Loss: 0.1092	LR: 0.012500
Training Epoch: 88 [21632/50000]	Loss: 0.1867	LR: 0.012500
Training Epoch: 88 [21760/50000]	Loss: 0.1043	LR: 0.012500
Training Epoch: 88 [21888/50000]	Loss: 0.1029	LR: 0.012500
Training Epoch: 88 [22016/50000]	Loss: 0.1515	LR: 0.012500
Training Epoch: 88 [22144/50000]	Loss: 0.1941	LR: 0.012500
Training Epoch: 88 [22272/50000]	Loss: 0.1187	LR: 0.012500
Training Epoch: 88 [22400/50000]	Loss: 0.1075	LR: 0.012500
Training Epoch: 88 [22528/50000]	Loss: 0.0784	LR: 0.012500
Training Epoch: 88 [22656/50000]	Loss: 0.1501	LR: 0.012500
Training Epoch: 88 [22784/50000]	Loss: 0.1620	LR: 0.012500
Training Epoch: 88 [22912/50000]	Loss: 0.1670	LR: 0.012500
Training Epoch: 88 [23040/50000]	Loss: 0.1324	LR: 0.012500
Training Epoch: 88 [23168/50000]	Loss: 0.1382	LR: 0.012500
Training Epoch: 88 [23296/50000]	Loss: 0.1307	LR: 0.012500
Training Epoch: 88 [23424/50000]	Loss: 0.1552	LR: 0.012500
Training Epoch: 88 [23552/50000]	Loss: 0.0769	LR: 0.012500
Training Epoch: 88 [23680/50000]	Loss: 0.1145	LR: 0.012500
Training Epoch: 88 [23808/50000]	Loss: 0.1197	LR: 0.012500
Training Epoch: 88 [23936/50000]	Loss: 0.1626	LR: 0.012500
Training Epoch: 88 [24064/50000]	Loss: 0.1530	LR: 0.012500
Training Epoch: 88 [24192/50000]	Loss: 0.1084	LR: 0.012500
Training Epoch: 88 [24320/50000]	Loss: 0.0606	LR: 0.012500
Training Epoch: 88 [24448/50000]	Loss: 0.1350	LR: 0.012500
Training Epoch: 88 [24576/50000]	Loss: 0.1171	LR: 0.012500
Training Epoch: 88 [24704/50000]	Loss: 0.1422	LR: 0.012500
Training Epoch: 88 [24832/50000]	Loss: 0.0849	LR: 0.012500
Training Epoch: 88 [24960/50000]	Loss: 0.1346	LR: 0.012500
Training Epoch: 88 [25088/50000]	Loss: 0.1110	LR: 0.012500
Training Epoch: 88 [25216/50000]	Loss: 0.1617	LR: 0.012500
Training Epoch: 88 [25344/50000]	Loss: 0.1358	LR: 0.012500
Training Epoch: 88 [25472/50000]	Loss: 0.1016	LR: 0.012500
Training Epoch: 88 [25600/50000]	Loss: 0.0896	LR: 0.012500
Training Epoch: 88 [25728/50000]	Loss: 0.1018	LR: 0.012500
Training Epoch: 88 [25856/50000]	Loss: 0.1057	LR: 0.012500
Training Epoch: 88 [25984/50000]	Loss: 0.0987	LR: 0.012500
Training Epoch: 88 [26112/50000]	Loss: 0.0913	LR: 0.012500
Training Epoch: 88 [26240/50000]	Loss: 0.0931	LR: 0.012500
Training Epoch: 88 [26368/50000]	Loss: 0.1769	LR: 0.012500
Training Epoch: 88 [26496/50000]	Loss: 0.1619	LR: 0.012500
Training Epoch: 88 [26624/50000]	Loss: 0.1521	LR: 0.012500
Training Epoch: 88 [26752/50000]	Loss: 0.1034	LR: 0.012500
Training Epoch: 88 [26880/50000]	Loss: 0.1997	LR: 0.012500
Training Epoch: 88 [27008/50000]	Loss: 0.1422	LR: 0.012500
Training Epoch: 88 [27136/50000]	Loss: 0.0837	LR: 0.012500
Training Epoch: 88 [27264/50000]	Loss: 0.1439	LR: 0.012500
Training Epoch: 88 [27392/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 88 [27520/50000]	Loss: 0.0869	LR: 0.012500
Training Epoch: 88 [27648/50000]	Loss: 0.1338	LR: 0.012500
Training Epoch: 88 [27776/50000]	Loss: 0.1007	LR: 0.012500
Training Epoch: 88 [27904/50000]	Loss: 0.1479	LR: 0.012500
Training Epoch: 88 [28032/50000]	Loss: 0.1308	LR: 0.012500
Training Epoch: 88 [28160/50000]	Loss: 0.1556	LR: 0.012500
Training Epoch: 88 [28288/50000]	Loss: 0.1111	LR: 0.012500
Training Epoch: 88 [28416/50000]	Loss: 0.2006	LR: 0.012500
Training Epoch: 88 [28544/50000]	Loss: 0.1211	LR: 0.012500
Training Epoch: 88 [28672/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 88 [28800/50000]	Loss: 0.0884	LR: 0.012500
Training Epoch: 88 [28928/50000]	Loss: 0.0897	LR: 0.012500
Training Epoch: 88 [29056/50000]	Loss: 0.1392	LR: 0.012500
Training Epoch: 88 [29184/50000]	Loss: 0.1633	LR: 0.012500
Training Epoch: 88 [29312/50000]	Loss: 0.1093	LR: 0.012500
Training Epoch: 88 [29440/50000]	Loss: 0.0845	LR: 0.012500
Training Epoch: 88 [29568/50000]	Loss: 0.1338	LR: 0.012500
Training Epoch: 88 [29696/50000]	Loss: 0.0985	LR: 0.012500
Training Epoch: 88 [29824/50000]	Loss: 0.1278	LR: 0.012500
Training Epoch: 88 [29952/50000]	Loss: 0.1857	LR: 0.012500
Training Epoch: 88 [30080/50000]	Loss: 0.1237	LR: 0.012500
Training Epoch: 88 [30208/50000]	Loss: 0.2060	LR: 0.012500
Training Epoch: 88 [30336/50000]	Loss: 0.1314	LR: 0.012500
Training Epoch: 88 [30464/50000]	Loss: 0.1459	LR: 0.012500
Training Epoch: 88 [30592/50000]	Loss: 0.1136	LR: 0.012500
Training Epoch: 88 [30720/50000]	Loss: 0.1531	LR: 0.012500
Training Epoch: 88 [30848/50000]	Loss: 0.1303	LR: 0.012500
Training Epoch: 88 [30976/50000]	Loss: 0.1758	LR: 0.012500
Training Epoch: 88 [31104/50000]	Loss: 0.1213	LR: 0.012500
Training Epoch: 88 [31232/50000]	Loss: 0.1230	LR: 0.012500
Training Epoch: 88 [31360/50000]	Loss: 0.1576	LR: 0.012500
Training Epoch: 88 [31488/50000]	Loss: 0.1656	LR: 0.012500
Training Epoch: 88 [31616/50000]	Loss: 0.1205	LR: 0.012500
Training Epoch: 88 [31744/50000]	Loss: 0.1091	LR: 0.012500
Training Epoch: 88 [31872/50000]	Loss: 0.1765	LR: 0.012500
Training Epoch: 88 [32000/50000]	Loss: 0.1015	LR: 0.012500
Training Epoch: 88 [32128/50000]	Loss: 0.0757	LR: 0.012500
Training Epoch: 88 [32256/50000]	Loss: 0.1449	LR: 0.012500
Training Epoch: 88 [32384/50000]	Loss: 0.1379	LR: 0.012500
Training Epoch: 88 [32512/50000]	Loss: 0.1000	LR: 0.012500
Training Epoch: 88 [32640/50000]	Loss: 0.1429	LR: 0.012500
Training Epoch: 88 [32768/50000]	Loss: 0.1201	LR: 0.012500
Training Epoch: 88 [32896/50000]	Loss: 0.1205	LR: 0.012500
Training Epoch: 88 [33024/50000]	Loss: 0.1112	LR: 0.012500
Training Epoch: 88 [33152/50000]	Loss: 0.0946	LR: 0.012500
Training Epoch: 88 [33280/50000]	Loss: 0.1201	LR: 0.012500
Training Epoch: 88 [33408/50000]	Loss: 0.1255	LR: 0.012500
Training Epoch: 88 [33536/50000]	Loss: 0.1242	LR: 0.012500
Training Epoch: 88 [33664/50000]	Loss: 0.1021	LR: 0.012500
Training Epoch: 88 [33792/50000]	Loss: 0.1392	LR: 0.012500
Training Epoch: 88 [33920/50000]	Loss: 0.0972	LR: 0.012500
Training Epoch: 88 [34048/50000]	Loss: 0.0941	LR: 0.012500
Training Epoch: 88 [34176/50000]	Loss: 0.1726	LR: 0.012500
Training Epoch: 88 [34304/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 88 [34432/50000]	Loss: 0.1087	LR: 0.012500
Training Epoch: 88 [34560/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 88 [34688/50000]	Loss: 0.1316	LR: 0.012500
Training Epoch: 88 [34816/50000]	Loss: 0.1452	LR: 0.012500
Training Epoch: 88 [34944/50000]	Loss: 0.1637	LR: 0.012500
Training Epoch: 88 [35072/50000]	Loss: 0.1001	LR: 0.012500
Training Epoch: 88 [35200/50000]	Loss: 0.2134	LR: 0.012500
Training Epoch: 88 [35328/50000]	Loss: 0.1218	LR: 0.012500
Training Epoch: 88 [35456/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 88 [35584/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 88 [35712/50000]	Loss: 0.1444	LR: 0.012500
Training Epoch: 88 [35840/50000]	Loss: 0.1372	LR: 0.012500
Training Epoch: 88 [35968/50000]	Loss: 0.0967	LR: 0.012500
Training Epoch: 88 [36096/50000]	Loss: 0.1395	LR: 0.012500
Training Epoch: 88 [36224/50000]	Loss: 0.0547	LR: 0.012500
Training Epoch: 88 [36352/50000]	Loss: 0.1074	LR: 0.012500
Training Epoch: 88 [36480/50000]	Loss: 0.1127	LR: 0.012500
Training Epoch: 88 [36608/50000]	Loss: 0.1074	LR: 0.012500
Training Epoch: 88 [36736/50000]	Loss: 0.0813	LR: 0.012500
Training Epoch: 88 [36864/50000]	Loss: 0.1204	LR: 0.012500
Training Epoch: 88 [36992/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 88 [37120/50000]	Loss: 0.0730	LR: 0.012500
Training Epoch: 88 [37248/50000]	Loss: 0.0911	LR: 0.012500
Training Epoch: 88 [37376/50000]	Loss: 0.0919	LR: 0.012500
Training Epoch: 88 [37504/50000]	Loss: 0.0974	LR: 0.012500
Training Epoch: 88 [37632/50000]	Loss: 0.1084	LR: 0.012500
Training Epoch: 88 [37760/50000]	Loss: 0.1949	LR: 0.012500
Training Epoch: 88 [37888/50000]	Loss: 0.0983	LR: 0.012500
Training Epoch: 88 [38016/50000]	Loss: 0.1053	LR: 0.012500
Training Epoch: 88 [38144/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 88 [38272/50000]	Loss: 0.1373	LR: 0.012500
Training Epoch: 88 [38400/50000]	Loss: 0.1460	LR: 0.012500
Training Epoch: 88 [38528/50000]	Loss: 0.0920	LR: 0.012500
Training Epoch: 88 [38656/50000]	Loss: 0.1199	LR: 0.012500
Training Epoch: 88 [38784/50000]	Loss: 0.1986	LR: 0.012500
Training Epoch: 88 [38912/50000]	Loss: 0.0813	LR: 0.012500
Training Epoch: 88 [39040/50000]	Loss: 0.1280	LR: 0.012500
Training Epoch: 88 [39168/50000]	Loss: 0.0928	LR: 0.012500
Training Epoch: 88 [39296/50000]	Loss: 0.0913	LR: 0.012500
Training Epoch: 88 [39424/50000]	Loss: 0.0954	LR: 0.012500
Training Epoch: 88 [39552/50000]	Loss: 0.1234	LR: 0.012500
Training Epoch: 88 [39680/50000]	Loss: 0.1029	LR: 0.012500
Training Epoch: 88 [39808/50000]	Loss: 0.1836	LR: 0.012500
Training Epoch: 88 [39936/50000]	Loss: 0.0993	LR: 0.012500
Training Epoch: 88 [40064/50000]	Loss: 0.1647	LR: 0.012500
Training Epoch: 88 [40192/50000]	Loss: 0.1874	LR: 0.012500
Training Epoch: 88 [40320/50000]	Loss: 0.0946	LR: 0.012500
Training Epoch: 88 [40448/50000]	Loss: 0.1103	LR: 0.012500
Training Epoch: 88 [40576/50000]	Loss: 0.1275	LR: 0.012500
Training Epoch: 88 [40704/50000]	Loss: 0.1209	LR: 0.012500
Training Epoch: 88 [40832/50000]	Loss: 0.1386	LR: 0.012500
Training Epoch: 88 [40960/50000]	Loss: 0.1906	LR: 0.012500
Training Epoch: 88 [41088/50000]	Loss: 0.0826	LR: 0.012500
Training Epoch: 88 [41216/50000]	Loss: 0.0859	LR: 0.012500
Training Epoch: 88 [41344/50000]	Loss: 0.0895	LR: 0.012500
Training Epoch: 88 [41472/50000]	Loss: 0.0924	LR: 0.012500
Training Epoch: 88 [41600/50000]	Loss: 0.0940	LR: 0.012500
Training Epoch: 88 [41728/50000]	Loss: 0.0762	LR: 0.012500
Training Epoch: 88 [41856/50000]	Loss: 0.1026	LR: 0.012500
Training Epoch: 88 [41984/50000]	Loss: 0.1607	LR: 0.012500
Training Epoch: 88 [42112/50000]	Loss: 0.1102	LR: 0.012500
Training Epoch: 88 [42240/50000]	Loss: 0.0698	LR: 0.012500
Training Epoch: 88 [42368/50000]	Loss: 0.1835	LR: 0.012500
Training Epoch: 88 [42496/50000]	Loss: 0.0544	LR: 0.012500
Training Epoch: 88 [42624/50000]	Loss: 0.0937	LR: 0.012500
Training Epoch: 88 [42752/50000]	Loss: 0.1100	LR: 0.012500
Training Epoch: 88 [42880/50000]	Loss: 0.2167	LR: 0.012500
Training Epoch: 88 [43008/50000]	Loss: 0.0832	LR: 0.012500
Training Epoch: 88 [43136/50000]	Loss: 0.1026	LR: 0.012500
Training Epoch: 88 [43264/50000]	Loss: 0.0966	LR: 0.012500
Training Epoch: 88 [43392/50000]	Loss: 0.0974	LR: 0.012500
Training Epoch: 88 [43520/50000]	Loss: 0.0992	LR: 0.012500
Training Epoch: 88 [43648/50000]	Loss: 0.1007	LR: 0.012500
Training Epoch: 88 [43776/50000]	Loss: 0.1025	LR: 0.012500
Training Epoch: 88 [43904/50000]	Loss: 0.1331	LR: 0.012500
Training Epoch: 88 [44032/50000]	Loss: 0.1441	LR: 0.012500
Training Epoch: 88 [44160/50000]	Loss: 0.1536	LR: 0.012500
Training Epoch: 88 [44288/50000]	Loss: 0.1395	LR: 0.012500
Training Epoch: 88 [44416/50000]	Loss: 0.1018	LR: 0.012500
Training Epoch: 88 [44544/50000]	Loss: 0.1335	LR: 0.012500
Training Epoch: 88 [44672/50000]	Loss: 0.1653	LR: 0.012500
Training Epoch: 88 [44800/50000]	Loss: 0.1216	LR: 0.012500
Training Epoch: 88 [44928/50000]	Loss: 0.1400	LR: 0.012500
Training Epoch: 88 [45056/50000]	Loss: 0.1731	LR: 0.012500
Training Epoch: 88 [45184/50000]	Loss: 0.1959	LR: 0.012500
Training Epoch: 88 [45312/50000]	Loss: 0.1415	LR: 0.012500
Training Epoch: 88 [45440/50000]	Loss: 0.1043	LR: 0.012500
Training Epoch: 88 [45568/50000]	Loss: 0.0948	LR: 0.012500
Training Epoch: 88 [45696/50000]	Loss: 0.0780	LR: 0.012500
Training Epoch: 88 [45824/50000]	Loss: 0.1446	LR: 0.012500
Training Epoch: 88 [45952/50000]	Loss: 0.1232	LR: 0.012500
Training Epoch: 88 [46080/50000]	Loss: 0.1048	LR: 0.012500
Training Epoch: 88 [46208/50000]	Loss: 0.1712	LR: 0.012500
Training Epoch: 88 [46336/50000]	Loss: 0.0837	LR: 0.012500
Training Epoch: 88 [46464/50000]	Loss: 0.1335	LR: 0.012500
Training Epoch: 88 [46592/50000]	Loss: 0.1688	LR: 0.012500
Training Epoch: 88 [46720/50000]	Loss: 0.1594	LR: 0.012500
Training Epoch: 88 [46848/50000]	Loss: 0.1098	LR: 0.012500
Training Epoch: 88 [46976/50000]	Loss: 0.0920	LR: 0.012500
Training Epoch: 88 [47104/50000]	Loss: 0.0832	LR: 0.012500
Training Epoch: 88 [47232/50000]	Loss: 0.1017	LR: 0.012500
Training Epoch: 88 [47360/50000]	Loss: 0.0810	LR: 0.012500
Training Epoch: 88 [47488/50000]	Loss: 0.1985	LR: 0.012500
Training Epoch: 88 [47616/50000]	Loss: 0.1230	LR: 0.012500
Training Epoch: 88 [47744/50000]	Loss: 0.1647	LR: 0.012500
Training Epoch: 88 [47872/50000]	Loss: 0.2029	LR: 0.012500
Training Epoch: 88 [48000/50000]	Loss: 0.1283	LR: 0.012500
Training Epoch: 88 [48128/50000]	Loss: 0.0970	LR: 0.012500
Training Epoch: 88 [48256/50000]	Loss: 0.1707	LR: 0.012500
Training Epoch: 88 [48384/50000]	Loss: 0.1578	LR: 0.012500
Training Epoch: 88 [48512/50000]	Loss: 0.1559	LR: 0.012500
Training Epoch: 88 [48640/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 88 [48768/50000]	Loss: 0.0887	LR: 0.012500
Training Epoch: 88 [48896/50000]	Loss: 0.0931	LR: 0.012500
Training Epoch: 88 [49024/50000]	Loss: 0.2786	LR: 0.012500
Training Epoch: 88 [49152/50000]	Loss: 0.0924	LR: 0.012500
Training Epoch: 88 [49280/50000]	Loss: 0.1935	LR: 0.012500
Training Epoch: 88 [49408/50000]	Loss: 0.0538	LR: 0.012500
Training Epoch: 88 [49536/50000]	Loss: 0.1420	LR: 0.012500
Training Epoch: 88 [49664/50000]	Loss: 0.1625	LR: 0.012500
Training Epoch: 88 [49792/50000]	Loss: 0.1424	LR: 0.012500
Training Epoch: 88 [49920/50000]	Loss: 0.0955	LR: 0.012500
Training Epoch: 88 [50000/50000]	Loss: 0.1807	LR: 0.012500
epoch 88 training time consumed: 53.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  322965 GB |  322965 GB |
|       from large pool |  123392 KB |    1034 MB |  322647 GB |  322646 GB |
|       from small pool |   10798 KB |      13 MB |     318 GB |     318 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  322965 GB |  322965 GB |
|       from large pool |  123392 KB |    1034 MB |  322647 GB |  322646 GB |
|       from small pool |   10798 KB |      13 MB |     318 GB |     318 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  142123 GB |  142123 GB |
|       from large pool |  155136 KB |  433088 KB |  141771 GB |  141771 GB |
|       from small pool |    1489 KB |    3494 KB |     351 GB |     351 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   12462 K  |   12461 K  |
|       from large pool |      24    |      65    |    6505 K  |    6505 K  |
|       from small pool |     232    |     275    |    5957 K  |    5956 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   12462 K  |   12461 K  |
|       from large pool |      24    |      65    |    6505 K  |    6505 K  |
|       from small pool |     232    |     275    |    5957 K  |    5956 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6170 K  |    6170 K  |
|       from large pool |       9    |      14    |    3148 K  |    3148 K  |
|       from small pool |      12    |      17    |    3021 K  |    3021 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 88, Average loss: 0.0093, Accuracy: 0.7167, Time consumed:3.45s

Training Epoch: 89 [128/50000]	Loss: 0.1507	LR: 0.012500
Training Epoch: 89 [256/50000]	Loss: 0.1151	LR: 0.012500
Training Epoch: 89 [384/50000]	Loss: 0.1619	LR: 0.012500
Training Epoch: 89 [512/50000]	Loss: 0.0949	LR: 0.012500
Training Epoch: 89 [640/50000]	Loss: 0.1423	LR: 0.012500
Training Epoch: 89 [768/50000]	Loss: 0.0811	LR: 0.012500
Training Epoch: 89 [896/50000]	Loss: 0.1462	LR: 0.012500
Training Epoch: 89 [1024/50000]	Loss: 0.1048	LR: 0.012500
Training Epoch: 89 [1152/50000]	Loss: 0.1456	LR: 0.012500
Training Epoch: 89 [1280/50000]	Loss: 0.1623	LR: 0.012500
Training Epoch: 89 [1408/50000]	Loss: 0.1005	LR: 0.012500
Training Epoch: 89 [1536/50000]	Loss: 0.1499	LR: 0.012500
Training Epoch: 89 [1664/50000]	Loss: 0.1231	LR: 0.012500
Training Epoch: 89 [1792/50000]	Loss: 0.0650	LR: 0.012500
Training Epoch: 89 [1920/50000]	Loss: 0.1919	LR: 0.012500
Training Epoch: 89 [2048/50000]	Loss: 0.1591	LR: 0.012500
Training Epoch: 89 [2176/50000]	Loss: 0.0935	LR: 0.012500
Training Epoch: 89 [2304/50000]	Loss: 0.0929	LR: 0.012500
Training Epoch: 89 [2432/50000]	Loss: 0.1231	LR: 0.012500
Training Epoch: 89 [2560/50000]	Loss: 0.0983	LR: 0.012500
Training Epoch: 89 [2688/50000]	Loss: 0.1162	LR: 0.012500
Training Epoch: 89 [2816/50000]	Loss: 0.1614	LR: 0.012500
Training Epoch: 89 [2944/50000]	Loss: 0.1189	LR: 0.012500
Training Epoch: 89 [3072/50000]	Loss: 0.1148	LR: 0.012500
Training Epoch: 89 [3200/50000]	Loss: 0.1628	LR: 0.012500
Training Epoch: 89 [3328/50000]	Loss: 0.1444	LR: 0.012500
Training Epoch: 89 [3456/50000]	Loss: 0.1321	LR: 0.012500
Training Epoch: 89 [3584/50000]	Loss: 0.0875	LR: 0.012500
Training Epoch: 89 [3712/50000]	Loss: 0.1497	LR: 0.012500
Training Epoch: 89 [3840/50000]	Loss: 0.1337	LR: 0.012500
Training Epoch: 89 [3968/50000]	Loss: 0.1140	LR: 0.012500
Training Epoch: 89 [4096/50000]	Loss: 0.1884	LR: 0.012500
Training Epoch: 89 [4224/50000]	Loss: 0.1051	LR: 0.012500
Training Epoch: 89 [4352/50000]	Loss: 0.1357	LR: 0.012500
Training Epoch: 89 [4480/50000]	Loss: 0.1494	LR: 0.012500
Training Epoch: 89 [4608/50000]	Loss: 0.1228	LR: 0.012500
Training Epoch: 89 [4736/50000]	Loss: 0.1154	LR: 0.012500
Training Epoch: 89 [4864/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 89 [4992/50000]	Loss: 0.1562	LR: 0.012500
Training Epoch: 89 [5120/50000]	Loss: 0.1355	LR: 0.012500
Training Epoch: 89 [5248/50000]	Loss: 0.0824	LR: 0.012500
Training Epoch: 89 [5376/50000]	Loss: 0.1770	LR: 0.012500
Training Epoch: 89 [5504/50000]	Loss: 0.1519	LR: 0.012500
Training Epoch: 89 [5632/50000]	Loss: 0.1382	LR: 0.012500
Training Epoch: 89 [5760/50000]	Loss: 0.1612	LR: 0.012500
Training Epoch: 89 [5888/50000]	Loss: 0.2513	LR: 0.012500
Training Epoch: 89 [6016/50000]	Loss: 0.1682	LR: 0.012500
Training Epoch: 89 [6144/50000]	Loss: 0.0664	LR: 0.012500
Training Epoch: 89 [6272/50000]	Loss: 0.1562	LR: 0.012500
Training Epoch: 89 [6400/50000]	Loss: 0.1328	LR: 0.012500
Training Epoch: 89 [6528/50000]	Loss: 0.1522	LR: 0.012500
Training Epoch: 89 [6656/50000]	Loss: 0.1112	LR: 0.012500
Training Epoch: 89 [6784/50000]	Loss: 0.1207	LR: 0.012500
Training Epoch: 89 [6912/50000]	Loss: 0.1370	LR: 0.012500
Training Epoch: 89 [7040/50000]	Loss: 0.0804	LR: 0.012500
Training Epoch: 89 [7168/50000]	Loss: 0.1019	LR: 0.012500
Training Epoch: 89 [7296/50000]	Loss: 0.1044	LR: 0.012500
Training Epoch: 89 [7424/50000]	Loss: 0.1010	LR: 0.012500
Training Epoch: 89 [7552/50000]	Loss: 0.0817	LR: 0.012500
Training Epoch: 89 [7680/50000]	Loss: 0.0688	LR: 0.012500
Training Epoch: 89 [7808/50000]	Loss: 0.1053	LR: 0.012500
Training Epoch: 89 [7936/50000]	Loss: 0.1378	LR: 0.012500
Training Epoch: 89 [8064/50000]	Loss: 0.0851	LR: 0.012500
Training Epoch: 89 [8192/50000]	Loss: 0.0808	LR: 0.012500
Training Epoch: 89 [8320/50000]	Loss: 0.0716	LR: 0.012500
Training Epoch: 89 [8448/50000]	Loss: 0.1367	LR: 0.012500
Training Epoch: 89 [8576/50000]	Loss: 0.1691	LR: 0.012500
Training Epoch: 89 [8704/50000]	Loss: 0.0865	LR: 0.012500
Training Epoch: 89 [8832/50000]	Loss: 0.1166	LR: 0.012500
Training Epoch: 89 [8960/50000]	Loss: 0.0503	LR: 0.012500
Training Epoch: 89 [9088/50000]	Loss: 0.0763	LR: 0.012500
Training Epoch: 89 [9216/50000]	Loss: 0.1276	LR: 0.012500
Training Epoch: 89 [9344/50000]	Loss: 0.1249	LR: 0.012500
Training Epoch: 89 [9472/50000]	Loss: 0.0826	LR: 0.012500
Training Epoch: 89 [9600/50000]	Loss: 0.1262	LR: 0.012500
Training Epoch: 89 [9728/50000]	Loss: 0.0925	LR: 0.012500
Training Epoch: 89 [9856/50000]	Loss: 0.1167	LR: 0.012500
Training Epoch: 89 [9984/50000]	Loss: 0.1432	LR: 0.012500
Training Epoch: 89 [10112/50000]	Loss: 0.1503	LR: 0.012500
Training Epoch: 89 [10240/50000]	Loss: 0.1130	LR: 0.012500
Training Epoch: 89 [10368/50000]	Loss: 0.1073	LR: 0.012500
Training Epoch: 89 [10496/50000]	Loss: 0.1454	LR: 0.012500
Training Epoch: 89 [10624/50000]	Loss: 0.1499	LR: 0.012500
Training Epoch: 89 [10752/50000]	Loss: 0.0914	LR: 0.012500
Training Epoch: 89 [10880/50000]	Loss: 0.1225	LR: 0.012500
Training Epoch: 89 [11008/50000]	Loss: 0.0988	LR: 0.012500
Training Epoch: 89 [11136/50000]	Loss: 0.1114	LR: 0.012500
Training Epoch: 89 [11264/50000]	Loss: 0.1114	LR: 0.012500
Training Epoch: 89 [11392/50000]	Loss: 0.1643	LR: 0.012500
Training Epoch: 89 [11520/50000]	Loss: 0.1865	LR: 0.012500
Training Epoch: 89 [11648/50000]	Loss: 0.0848	LR: 0.012500
Training Epoch: 89 [11776/50000]	Loss: 0.1316	LR: 0.012500
Training Epoch: 89 [11904/50000]	Loss: 0.1157	LR: 0.012500
Training Epoch: 89 [12032/50000]	Loss: 0.1036	LR: 0.012500
Training Epoch: 89 [12160/50000]	Loss: 0.1054	LR: 0.012500
Training Epoch: 89 [12288/50000]	Loss: 0.1495	LR: 0.012500
Training Epoch: 89 [12416/50000]	Loss: 0.1916	LR: 0.012500
Training Epoch: 89 [12544/50000]	Loss: 0.1063	LR: 0.012500
Training Epoch: 89 [12672/50000]	Loss: 0.1534	LR: 0.012500
Training Epoch: 89 [12800/50000]	Loss: 0.1222	LR: 0.012500
Training Epoch: 89 [12928/50000]	Loss: 0.0950	LR: 0.012500
Training Epoch: 89 [13056/50000]	Loss: 0.0844	LR: 0.012500
Training Epoch: 89 [13184/50000]	Loss: 0.1499	LR: 0.012500
Training Epoch: 89 [13312/50000]	Loss: 0.1207	LR: 0.012500
Training Epoch: 89 [13440/50000]	Loss: 0.1152	LR: 0.012500
Training Epoch: 89 [13568/50000]	Loss: 0.1200	LR: 0.012500
Training Epoch: 89 [13696/50000]	Loss: 0.0919	LR: 0.012500
Training Epoch: 89 [13824/50000]	Loss: 0.1278	LR: 0.012500
Training Epoch: 89 [13952/50000]	Loss: 0.0898	LR: 0.012500
Training Epoch: 89 [14080/50000]	Loss: 0.1910	LR: 0.012500
Training Epoch: 89 [14208/50000]	Loss: 0.1359	LR: 0.012500
Training Epoch: 89 [14336/50000]	Loss: 0.0842	LR: 0.012500
Training Epoch: 89 [14464/50000]	Loss: 0.1671	LR: 0.012500
Training Epoch: 89 [14592/50000]	Loss: 0.1159	LR: 0.012500
Training Epoch: 89 [14720/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 89 [14848/50000]	Loss: 0.0857	LR: 0.012500
Training Epoch: 89 [14976/50000]	Loss: 0.0915	LR: 0.012500
Training Epoch: 89 [15104/50000]	Loss: 0.1080	LR: 0.012500
Training Epoch: 89 [15232/50000]	Loss: 0.1210	LR: 0.012500
Training Epoch: 89 [15360/50000]	Loss: 0.1475	LR: 0.012500
Training Epoch: 89 [15488/50000]	Loss: 0.1228	LR: 0.012500
Training Epoch: 89 [15616/50000]	Loss: 0.1354	LR: 0.012500
Training Epoch: 89 [15744/50000]	Loss: 0.1656	LR: 0.012500
Training Epoch: 89 [15872/50000]	Loss: 0.1631	LR: 0.012500
Training Epoch: 89 [16000/50000]	Loss: 0.1337	LR: 0.012500
Training Epoch: 89 [16128/50000]	Loss: 0.1124	LR: 0.012500
Training Epoch: 89 [16256/50000]	Loss: 0.1049	LR: 0.012500
Training Epoch: 89 [16384/50000]	Loss: 0.0760	LR: 0.012500
Training Epoch: 89 [16512/50000]	Loss: 0.1199	LR: 0.012500
Training Epoch: 89 [16640/50000]	Loss: 0.1419	LR: 0.012500
Training Epoch: 89 [16768/50000]	Loss: 0.1051	LR: 0.012500
Training Epoch: 89 [16896/50000]	Loss: 0.0925	LR: 0.012500
Training Epoch: 89 [17024/50000]	Loss: 0.1061	LR: 0.012500
Training Epoch: 89 [17152/50000]	Loss: 0.1105	LR: 0.012500
Training Epoch: 89 [17280/50000]	Loss: 0.1951	LR: 0.012500
Training Epoch: 89 [17408/50000]	Loss: 0.1021	LR: 0.012500
Training Epoch: 89 [17536/50000]	Loss: 0.0754	LR: 0.012500
Training Epoch: 89 [17664/50000]	Loss: 0.1013	LR: 0.012500
Training Epoch: 89 [17792/50000]	Loss: 0.1302	LR: 0.012500
Training Epoch: 89 [17920/50000]	Loss: 0.0884	LR: 0.012500
Training Epoch: 89 [18048/50000]	Loss: 0.0795	LR: 0.012500
Training Epoch: 89 [18176/50000]	Loss: 0.1310	LR: 0.012500
Training Epoch: 89 [18304/50000]	Loss: 0.1380	LR: 0.012500
Training Epoch: 89 [18432/50000]	Loss: 0.1499	LR: 0.012500
Training Epoch: 89 [18560/50000]	Loss: 0.1065	LR: 0.012500
Training Epoch: 89 [18688/50000]	Loss: 0.0930	LR: 0.012500
Training Epoch: 89 [18816/50000]	Loss: 0.0864	LR: 0.012500
Training Epoch: 89 [18944/50000]	Loss: 0.1363	LR: 0.012500
Training Epoch: 89 [19072/50000]	Loss: 0.1338	LR: 0.012500
Training Epoch: 89 [19200/50000]	Loss: 0.1359	LR: 0.012500
Training Epoch: 89 [19328/50000]	Loss: 0.1717	LR: 0.012500
Training Epoch: 89 [19456/50000]	Loss: 0.2145	LR: 0.012500
Training Epoch: 89 [19584/50000]	Loss: 0.1782	LR: 0.012500
Training Epoch: 89 [19712/50000]	Loss: 0.0906	LR: 0.012500
Training Epoch: 89 [19840/50000]	Loss: 0.1091	LR: 0.012500
Training Epoch: 89 [19968/50000]	Loss: 0.1647	LR: 0.012500
Training Epoch: 89 [20096/50000]	Loss: 0.1164	LR: 0.012500
Training Epoch: 89 [20224/50000]	Loss: 0.1910	LR: 0.012500
Training Epoch: 89 [20352/50000]	Loss: 0.1494	LR: 0.012500
Training Epoch: 89 [20480/50000]	Loss: 0.1465	LR: 0.012500
Training Epoch: 89 [20608/50000]	Loss: 0.1384	LR: 0.012500
Training Epoch: 89 [20736/50000]	Loss: 0.1479	LR: 0.012500
Training Epoch: 89 [20864/50000]	Loss: 0.0907	LR: 0.012500
Training Epoch: 89 [20992/50000]	Loss: 0.1142	LR: 0.012500
Training Epoch: 89 [21120/50000]	Loss: 0.1101	LR: 0.012500
Training Epoch: 89 [21248/50000]	Loss: 0.1202	LR: 0.012500
Training Epoch: 89 [21376/50000]	Loss: 0.1148	LR: 0.012500
Training Epoch: 89 [21504/50000]	Loss: 0.1344	LR: 0.012500
Training Epoch: 89 [21632/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 89 [21760/50000]	Loss: 0.1629	LR: 0.012500
Training Epoch: 89 [21888/50000]	Loss: 0.1125	LR: 0.012500
Training Epoch: 89 [22016/50000]	Loss: 0.1431	LR: 0.012500
Training Epoch: 89 [22144/50000]	Loss: 0.1389	LR: 0.012500
Training Epoch: 89 [22272/50000]	Loss: 0.0762	LR: 0.012500
Training Epoch: 89 [22400/50000]	Loss: 0.0619	LR: 0.012500
Training Epoch: 89 [22528/50000]	Loss: 0.0826	LR: 0.012500
Training Epoch: 89 [22656/50000]	Loss: 0.1058	LR: 0.012500
Training Epoch: 89 [22784/50000]	Loss: 0.1202	LR: 0.012500
Training Epoch: 89 [22912/50000]	Loss: 0.1266	LR: 0.012500
Training Epoch: 89 [23040/50000]	Loss: 0.0989	LR: 0.012500
Training Epoch: 89 [23168/50000]	Loss: 0.0772	LR: 0.012500
Training Epoch: 89 [23296/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 89 [23424/50000]	Loss: 0.1343	LR: 0.012500
Training Epoch: 89 [23552/50000]	Loss: 0.1139	LR: 0.012500
Training Epoch: 89 [23680/50000]	Loss: 0.1311	LR: 0.012500
Training Epoch: 89 [23808/50000]	Loss: 0.1543	LR: 0.012500
Training Epoch: 89 [23936/50000]	Loss: 0.1404	LR: 0.012500
Training Epoch: 89 [24064/50000]	Loss: 0.0562	LR: 0.012500
Training Epoch: 89 [24192/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 89 [24320/50000]	Loss: 0.1093	LR: 0.012500
Training Epoch: 89 [24448/50000]	Loss: 0.1516	LR: 0.012500
Training Epoch: 89 [24576/50000]	Loss: 0.1188	LR: 0.012500
Training Epoch: 89 [24704/50000]	Loss: 0.1340	LR: 0.012500
Training Epoch: 89 [24832/50000]	Loss: 0.1033	LR: 0.012500
Training Epoch: 89 [24960/50000]	Loss: 0.0918	LR: 0.012500
Training Epoch: 89 [25088/50000]	Loss: 0.1273	LR: 0.012500
Training Epoch: 89 [25216/50000]	Loss: 0.1797	LR: 0.012500
Training Epoch: 89 [25344/50000]	Loss: 0.1204	LR: 0.012500
Training Epoch: 89 [25472/50000]	Loss: 0.1662	LR: 0.012500
Training Epoch: 89 [25600/50000]	Loss: 0.1826	LR: 0.012500
Training Epoch: 89 [25728/50000]	Loss: 0.1117	LR: 0.012500
Training Epoch: 89 [25856/50000]	Loss: 0.0789	LR: 0.012500
Training Epoch: 89 [25984/50000]	Loss: 0.1105	LR: 0.012500
Training Epoch: 89 [26112/50000]	Loss: 0.1394	LR: 0.012500
Training Epoch: 89 [26240/50000]	Loss: 0.1500	LR: 0.012500
Training Epoch: 89 [26368/50000]	Loss: 0.0875	LR: 0.012500
Training Epoch: 89 [26496/50000]	Loss: 0.1126	LR: 0.012500
Training Epoch: 89 [26624/50000]	Loss: 0.1475	LR: 0.012500
Training Epoch: 89 [26752/50000]	Loss: 0.1034	LR: 0.012500
Training Epoch: 89 [26880/50000]	Loss: 0.1227	LR: 0.012500
Training Epoch: 89 [27008/50000]	Loss: 0.1145	LR: 0.012500
Training Epoch: 89 [27136/50000]	Loss: 0.1484	LR: 0.012500
Training Epoch: 89 [27264/50000]	Loss: 0.1658	LR: 0.012500
Training Epoch: 89 [27392/50000]	Loss: 0.1226	LR: 0.012500
Training Epoch: 89 [27520/50000]	Loss: 0.1298	LR: 0.012500
Training Epoch: 89 [27648/50000]	Loss: 0.1980	LR: 0.012500
Training Epoch: 89 [27776/50000]	Loss: 0.0864	LR: 0.012500
Training Epoch: 89 [27904/50000]	Loss: 0.2481	LR: 0.012500
Training Epoch: 89 [28032/50000]	Loss: 0.0932	LR: 0.012500
Training Epoch: 89 [28160/50000]	Loss: 0.1266	LR: 0.012500
Training Epoch: 89 [28288/50000]	Loss: 0.0632	LR: 0.012500
Training Epoch: 89 [28416/50000]	Loss: 0.1231	LR: 0.012500
Training Epoch: 89 [28544/50000]	Loss: 0.1379	LR: 0.012500
Training Epoch: 89 [28672/50000]	Loss: 0.1433	LR: 0.012500
Training Epoch: 89 [28800/50000]	Loss: 0.0926	LR: 0.012500
Training Epoch: 89 [28928/50000]	Loss: 0.1780	LR: 0.012500
Training Epoch: 89 [29056/50000]	Loss: 0.2178	LR: 0.012500
Training Epoch: 89 [29184/50000]	Loss: 0.1204	LR: 0.012500
Training Epoch: 89 [29312/50000]	Loss: 0.1729	LR: 0.012500
Training Epoch: 89 [29440/50000]	Loss: 0.1162	LR: 0.012500
Training Epoch: 89 [29568/50000]	Loss: 0.1311	LR: 0.012500
Training Epoch: 89 [29696/50000]	Loss: 0.1072	LR: 0.012500
Training Epoch: 89 [29824/50000]	Loss: 0.1201	LR: 0.012500
Training Epoch: 89 [29952/50000]	Loss: 0.1228	LR: 0.012500
Training Epoch: 89 [30080/50000]	Loss: 0.1245	LR: 0.012500
Training Epoch: 89 [30208/50000]	Loss: 0.1161	LR: 0.012500
Training Epoch: 89 [30336/50000]	Loss: 0.1387	LR: 0.012500
Training Epoch: 89 [30464/50000]	Loss: 0.1477	LR: 0.012500
Training Epoch: 89 [30592/50000]	Loss: 0.1370	LR: 0.012500
Training Epoch: 89 [30720/50000]	Loss: 0.1138	LR: 0.012500
Training Epoch: 89 [30848/50000]	Loss: 0.0778	LR: 0.012500
Training Epoch: 89 [30976/50000]	Loss: 0.1260	LR: 0.012500
Training Epoch: 89 [31104/50000]	Loss: 0.1097	LR: 0.012500
Training Epoch: 89 [31232/50000]	Loss: 0.1284	LR: 0.012500
Training Epoch: 89 [31360/50000]	Loss: 0.1467	LR: 0.012500
Training Epoch: 89 [31488/50000]	Loss: 0.1026	LR: 0.012500
Training Epoch: 89 [31616/50000]	Loss: 0.0838	LR: 0.012500
Training Epoch: 89 [31744/50000]	Loss: 0.0817	LR: 0.012500
Training Epoch: 89 [31872/50000]	Loss: 0.1291	LR: 0.012500
Training Epoch: 89 [32000/50000]	Loss: 0.1074	LR: 0.012500
Training Epoch: 89 [32128/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 89 [32256/50000]	Loss: 0.1134	LR: 0.012500
Training Epoch: 89 [32384/50000]	Loss: 0.1180	LR: 0.012500
Training Epoch: 89 [32512/50000]	Loss: 0.1345	LR: 0.012500
Training Epoch: 89 [32640/50000]	Loss: 0.1455	LR: 0.012500
Training Epoch: 89 [32768/50000]	Loss: 0.1775	LR: 0.012500
Training Epoch: 89 [32896/50000]	Loss: 0.1733	LR: 0.012500
Training Epoch: 89 [33024/50000]	Loss: 0.1271	LR: 0.012500
Training Epoch: 89 [33152/50000]	Loss: 0.1080	LR: 0.012500
Training Epoch: 89 [33280/50000]	Loss: 0.1474	LR: 0.012500
Training Epoch: 89 [33408/50000]	Loss: 0.1549	LR: 0.012500
Training Epoch: 89 [33536/50000]	Loss: 0.1554	LR: 0.012500
Training Epoch: 89 [33664/50000]	Loss: 0.0969	LR: 0.012500
Training Epoch: 89 [33792/50000]	Loss: 0.0884	LR: 0.012500
Training Epoch: 89 [33920/50000]	Loss: 0.1797	LR: 0.012500
Training Epoch: 89 [34048/50000]	Loss: 0.0975	LR: 0.012500
Training Epoch: 89 [34176/50000]	Loss: 0.1618	LR: 0.012500
Training Epoch: 89 [34304/50000]	Loss: 0.1404	LR: 0.012500
Training Epoch: 89 [34432/50000]	Loss: 0.1620	LR: 0.012500
Training Epoch: 89 [34560/50000]	Loss: 0.1457	LR: 0.012500
Training Epoch: 89 [34688/50000]	Loss: 0.1528	LR: 0.012500
Training Epoch: 89 [34816/50000]	Loss: 0.1250	LR: 0.012500
Training Epoch: 89 [34944/50000]	Loss: 0.1278	LR: 0.012500
Training Epoch: 89 [35072/50000]	Loss: 0.1241	LR: 0.012500
Training Epoch: 89 [35200/50000]	Loss: 0.1462	LR: 0.012500
Training Epoch: 89 [35328/50000]	Loss: 0.2224	LR: 0.012500
Training Epoch: 89 [35456/50000]	Loss: 0.1515	LR: 0.012500
Training Epoch: 89 [35584/50000]	Loss: 0.1975	LR: 0.012500
Training Epoch: 89 [35712/50000]	Loss: 0.1373	LR: 0.012500
Training Epoch: 89 [35840/50000]	Loss: 0.1773	LR: 0.012500
Training Epoch: 89 [35968/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 89 [36096/50000]	Loss: 0.1784	LR: 0.012500
Training Epoch: 89 [36224/50000]	Loss: 0.0885	LR: 0.012500
Training Epoch: 89 [36352/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 89 [36480/50000]	Loss: 0.1424	LR: 0.012500
Training Epoch: 89 [36608/50000]	Loss: 0.1297	LR: 0.012500
Training Epoch: 89 [36736/50000]	Loss: 0.1026	LR: 0.012500
Training Epoch: 89 [36864/50000]	Loss: 0.1279	LR: 0.012500
Training Epoch: 89 [36992/50000]	Loss: 0.1693	LR: 0.012500
Training Epoch: 89 [37120/50000]	Loss: 0.1396	LR: 0.012500
Training Epoch: 89 [37248/50000]	Loss: 0.1862	LR: 0.012500
Training Epoch: 89 [37376/50000]	Loss: 0.1253	LR: 0.012500
Training Epoch: 89 [37504/50000]	Loss: 0.1672	LR: 0.012500
Training Epoch: 89 [37632/50000]	Loss: 0.1107	LR: 0.012500
Training Epoch: 89 [37760/50000]	Loss: 0.1459	LR: 0.012500
Training Epoch: 89 [37888/50000]	Loss: 0.1536	LR: 0.012500
Training Epoch: 89 [38016/50000]	Loss: 0.2206	LR: 0.012500
Training Epoch: 89 [38144/50000]	Loss: 0.1137	LR: 0.012500
Training Epoch: 89 [38272/50000]	Loss: 0.1621	LR: 0.012500
Training Epoch: 89 [38400/50000]	Loss: 0.1443	LR: 0.012500
Training Epoch: 89 [38528/50000]	Loss: 0.1376	LR: 0.012500
Training Epoch: 89 [38656/50000]	Loss: 0.0966	LR: 0.012500
Training Epoch: 89 [38784/50000]	Loss: 0.0944	LR: 0.012500
Training Epoch: 89 [38912/50000]	Loss: 0.1166	LR: 0.012500
Training Epoch: 89 [39040/50000]	Loss: 0.0991	LR: 0.012500
Training Epoch: 89 [39168/50000]	Loss: 0.0865	LR: 0.012500
Training Epoch: 89 [39296/50000]	Loss: 0.1155	LR: 0.012500
Training Epoch: 89 [39424/50000]	Loss: 0.1095	LR: 0.012500
Training Epoch: 89 [39552/50000]	Loss: 0.0982	LR: 0.012500
Training Epoch: 89 [39680/50000]	Loss: 0.1406	LR: 0.012500
Training Epoch: 89 [39808/50000]	Loss: 0.1374	LR: 0.012500
Training Epoch: 89 [39936/50000]	Loss: 0.0958	LR: 0.012500
Training Epoch: 89 [40064/50000]	Loss: 0.1056	LR: 0.012500
Training Epoch: 89 [40192/50000]	Loss: 0.1209	LR: 0.012500
Training Epoch: 89 [40320/50000]	Loss: 0.1785	LR: 0.012500
Training Epoch: 89 [40448/50000]	Loss: 0.0873	LR: 0.012500
Training Epoch: 89 [40576/50000]	Loss: 0.0912	LR: 0.012500
Training Epoch: 89 [40704/50000]	Loss: 0.1229	LR: 0.012500
Training Epoch: 89 [40832/50000]	Loss: 0.1578	LR: 0.012500
Training Epoch: 89 [40960/50000]	Loss: 0.2079	LR: 0.012500
Training Epoch: 89 [41088/50000]	Loss: 0.1435	LR: 0.012500
Training Epoch: 89 [41216/50000]	Loss: 0.1486	LR: 0.012500
Training Epoch: 89 [41344/50000]	Loss: 0.1367	LR: 0.012500
Training Epoch: 89 [41472/50000]	Loss: 0.1446	LR: 0.012500
Training Epoch: 89 [41600/50000]	Loss: 0.0970	LR: 0.012500
Training Epoch: 89 [41728/50000]	Loss: 0.1211	LR: 0.012500
Training Epoch: 89 [41856/50000]	Loss: 0.1004	LR: 0.012500
Training Epoch: 89 [41984/50000]	Loss: 0.1621	LR: 0.012500
Training Epoch: 89 [42112/50000]	Loss: 0.2106	LR: 0.012500
Training Epoch: 89 [42240/50000]	Loss: 0.1605	LR: 0.012500
Training Epoch: 89 [42368/50000]	Loss: 0.1549	LR: 0.012500
Training Epoch: 89 [42496/50000]	Loss: 0.1256	LR: 0.012500
Training Epoch: 89 [42624/50000]	Loss: 0.1954	LR: 0.012500
Training Epoch: 89 [42752/50000]	Loss: 0.1149	LR: 0.012500
Training Epoch: 89 [42880/50000]	Loss: 0.1275	LR: 0.012500
Training Epoch: 89 [43008/50000]	Loss: 0.1152	LR: 0.012500
Training Epoch: 89 [43136/50000]	Loss: 0.1567	LR: 0.012500
Training Epoch: 89 [43264/50000]	Loss: 0.1168	LR: 0.012500
Training Epoch: 89 [43392/50000]	Loss: 0.1313	LR: 0.012500
Training Epoch: 89 [43520/50000]	Loss: 0.1873	LR: 0.012500
Training Epoch: 89 [43648/50000]	Loss: 0.1401	LR: 0.012500
Training Epoch: 89 [43776/50000]	Loss: 0.1421	LR: 0.012500
Training Epoch: 89 [43904/50000]	Loss: 0.2565	LR: 0.012500
Training Epoch: 89 [44032/50000]	Loss: 0.2122	LR: 0.012500
Training Epoch: 89 [44160/50000]	Loss: 0.2136	LR: 0.012500
Training Epoch: 89 [44288/50000]	Loss: 0.1436	LR: 0.012500
Training Epoch: 89 [44416/50000]	Loss: 0.2017	LR: 0.012500
Training Epoch: 89 [44544/50000]	Loss: 0.0920	LR: 0.012500
Training Epoch: 89 [44672/50000]	Loss: 0.1323	LR: 0.012500
Training Epoch: 89 [44800/50000]	Loss: 0.1009	LR: 0.012500
Training Epoch: 89 [44928/50000]	Loss: 0.2311	LR: 0.012500
Training Epoch: 89 [45056/50000]	Loss: 0.1230	LR: 0.012500
Training Epoch: 89 [45184/50000]	Loss: 0.1728	LR: 0.012500
Training Epoch: 89 [45312/50000]	Loss: 0.1106	LR: 0.012500
Training Epoch: 89 [45440/50000]	Loss: 0.0935	LR: 0.012500
Training Epoch: 89 [45568/50000]	Loss: 0.1488	LR: 0.012500
Training Epoch: 89 [45696/50000]	Loss: 0.1123	LR: 0.012500
Training Epoch: 89 [45824/50000]	Loss: 0.1076	LR: 0.012500
Training Epoch: 89 [45952/50000]	Loss: 0.1496	LR: 0.012500
Training Epoch: 89 [46080/50000]	Loss: 0.1412	LR: 0.012500
Training Epoch: 89 [46208/50000]	Loss: 0.1073	LR: 0.012500
Training Epoch: 89 [46336/50000]	Loss: 0.1777	LR: 0.012500
Training Epoch: 89 [46464/50000]	Loss: 0.1025	LR: 0.012500
Training Epoch: 89 [46592/50000]	Loss: 0.1547	LR: 0.012500
Training Epoch: 89 [46720/50000]	Loss: 0.1544	LR: 0.012500
Training Epoch: 89 [46848/50000]	Loss: 0.1795	LR: 0.012500
Training Epoch: 89 [46976/50000]	Loss: 0.1037	LR: 0.012500
Training Epoch: 89 [47104/50000]	Loss: 0.1183	LR: 0.012500
Training Epoch: 89 [47232/50000]	Loss: 0.1109	LR: 0.012500
Training Epoch: 89 [47360/50000]	Loss: 0.2331	LR: 0.012500
Training Epoch: 89 [47488/50000]	Loss: 0.1547	LR: 0.012500
Training Epoch: 89 [47616/50000]	Loss: 0.1678	LR: 0.012500
Training Epoch: 89 [47744/50000]	Loss: 0.1613	LR: 0.012500
Training Epoch: 89 [47872/50000]	Loss: 0.1289	LR: 0.012500
Training Epoch: 89 [48000/50000]	Loss: 0.1184	LR: 0.012500
Training Epoch: 89 [48128/50000]	Loss: 0.0911	LR: 0.012500
Training Epoch: 89 [48256/50000]	Loss: 0.1327	LR: 0.012500
Training Epoch: 89 [48384/50000]	Loss: 0.1191	LR: 0.012500
Training Epoch: 89 [48512/50000]	Loss: 0.1944	LR: 0.012500
Training Epoch: 89 [48640/50000]	Loss: 0.1630	LR: 0.012500
Training Epoch: 89 [48768/50000]	Loss: 0.2316	LR: 0.012500
Training Epoch: 89 [48896/50000]	Loss: 0.1558	LR: 0.012500
Training Epoch: 89 [49024/50000]	Loss: 0.0811	LR: 0.012500
Training Epoch: 89 [49152/50000]	Loss: 0.1627	LR: 0.012500
Training Epoch: 89 [49280/50000]	Loss: 0.1572	LR: 0.012500
Training Epoch: 89 [49408/50000]	Loss: 0.1997	LR: 0.012500
Training Epoch: 89 [49536/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 89 [49664/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 89 [49792/50000]	Loss: 0.1497	LR: 0.012500
Training Epoch: 89 [49920/50000]	Loss: 0.1393	LR: 0.012500
Training Epoch: 89 [50000/50000]	Loss: 0.2368	LR: 0.012500
epoch 89 training time consumed: 53.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  326635 GB |  326635 GB |
|       from large pool |  123392 KB |    1034 MB |  326313 GB |  326313 GB |
|       from small pool |   10798 KB |      13 MB |     321 GB |     321 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  326635 GB |  326635 GB |
|       from large pool |  123392 KB |    1034 MB |  326313 GB |  326313 GB |
|       from small pool |   10798 KB |      13 MB |     321 GB |     321 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  143738 GB |  143738 GB |
|       from large pool |  155136 KB |  433088 KB |  143382 GB |  143382 GB |
|       from small pool |    1489 KB |    3494 KB |     355 GB |     355 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   12603 K  |   12603 K  |
|       from large pool |      24    |      65    |    6578 K  |    6578 K  |
|       from small pool |     232    |     275    |    6024 K  |    6024 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   12603 K  |   12603 K  |
|       from large pool |      24    |      65    |    6578 K  |    6578 K  |
|       from small pool |     232    |     275    |    6024 K  |    6024 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6242 K  |    6242 K  |
|       from large pool |       9    |      14    |    3184 K  |    3184 K  |
|       from small pool |      12    |      17    |    3058 K  |    3058 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 89, Average loss: 0.0097, Accuracy: 0.7077, Time consumed:3.46s

Training Epoch: 90 [128/50000]	Loss: 0.0686	LR: 0.012500
Training Epoch: 90 [256/50000]	Loss: 0.1012	LR: 0.012500
Training Epoch: 90 [384/50000]	Loss: 0.1374	LR: 0.012500
Training Epoch: 90 [512/50000]	Loss: 0.1044	LR: 0.012500
Training Epoch: 90 [640/50000]	Loss: 0.1343	LR: 0.012500
Training Epoch: 90 [768/50000]	Loss: 0.1826	LR: 0.012500
Training Epoch: 90 [896/50000]	Loss: 0.2276	LR: 0.012500
Training Epoch: 90 [1024/50000]	Loss: 0.1184	LR: 0.012500
Training Epoch: 90 [1152/50000]	Loss: 0.0885	LR: 0.012500
Training Epoch: 90 [1280/50000]	Loss: 0.1570	LR: 0.012500
Training Epoch: 90 [1408/50000]	Loss: 0.1751	LR: 0.012500
Training Epoch: 90 [1536/50000]	Loss: 0.1207	LR: 0.012500
Training Epoch: 90 [1664/50000]	Loss: 0.1404	LR: 0.012500
Training Epoch: 90 [1792/50000]	Loss: 0.1505	LR: 0.012500
Training Epoch: 90 [1920/50000]	Loss: 0.1278	LR: 0.012500
Training Epoch: 90 [2048/50000]	Loss: 0.1239	LR: 0.012500
Training Epoch: 90 [2176/50000]	Loss: 0.1611	LR: 0.012500
Training Epoch: 90 [2304/50000]	Loss: 0.0920	LR: 0.012500
Training Epoch: 90 [2432/50000]	Loss: 0.2025	LR: 0.012500
Training Epoch: 90 [2560/50000]	Loss: 0.0735	LR: 0.012500
Training Epoch: 90 [2688/50000]	Loss: 0.1323	LR: 0.012500
Training Epoch: 90 [2816/50000]	Loss: 0.1065	LR: 0.012500
Training Epoch: 90 [2944/50000]	Loss: 0.0609	LR: 0.012500
Training Epoch: 90 [3072/50000]	Loss: 0.1306	LR: 0.012500
Training Epoch: 90 [3200/50000]	Loss: 0.0949	LR: 0.012500
Training Epoch: 90 [3328/50000]	Loss: 0.1139	LR: 0.012500
Training Epoch: 90 [3456/50000]	Loss: 0.1161	LR: 0.012500
Training Epoch: 90 [3584/50000]	Loss: 0.1516	LR: 0.012500
Training Epoch: 90 [3712/50000]	Loss: 0.0826	LR: 0.012500
Training Epoch: 90 [3840/50000]	Loss: 0.0891	LR: 0.012500
Training Epoch: 90 [3968/50000]	Loss: 0.1484	LR: 0.012500
Training Epoch: 90 [4096/50000]	Loss: 0.1012	LR: 0.012500
Training Epoch: 90 [4224/50000]	Loss: 0.0879	LR: 0.012500
Training Epoch: 90 [4352/50000]	Loss: 0.0675	LR: 0.012500
Training Epoch: 90 [4480/50000]	Loss: 0.1134	LR: 0.012500
Training Epoch: 90 [4608/50000]	Loss: 0.1375	LR: 0.012500
Training Epoch: 90 [4736/50000]	Loss: 0.0938	LR: 0.012500
Training Epoch: 90 [4864/50000]	Loss: 0.0879	LR: 0.012500
Training Epoch: 90 [4992/50000]	Loss: 0.1330	LR: 0.012500
Training Epoch: 90 [5120/50000]	Loss: 0.0862	LR: 0.012500
Training Epoch: 90 [5248/50000]	Loss: 0.0795	LR: 0.012500
Training Epoch: 90 [5376/50000]	Loss: 0.0721	LR: 0.012500
Training Epoch: 90 [5504/50000]	Loss: 0.1076	LR: 0.012500
Training Epoch: 90 [5632/50000]	Loss: 0.0869	LR: 0.012500
Training Epoch: 90 [5760/50000]	Loss: 0.1519	LR: 0.012500
Training Epoch: 90 [5888/50000]	Loss: 0.1958	LR: 0.012500
Training Epoch: 90 [6016/50000]	Loss: 0.0909	LR: 0.012500
Training Epoch: 90 [6144/50000]	Loss: 0.0671	LR: 0.012500
Training Epoch: 90 [6272/50000]	Loss: 0.1742	LR: 0.012500
Training Epoch: 90 [6400/50000]	Loss: 0.1038	LR: 0.012500
Training Epoch: 90 [6528/50000]	Loss: 0.0799	LR: 0.012500
Training Epoch: 90 [6656/50000]	Loss: 0.1107	LR: 0.012500
Training Epoch: 90 [6784/50000]	Loss: 0.1023	LR: 0.012500
Training Epoch: 90 [6912/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 90 [7040/50000]	Loss: 0.1002	LR: 0.012500
Training Epoch: 90 [7168/50000]	Loss: 0.0899	LR: 0.012500
Training Epoch: 90 [7296/50000]	Loss: 0.1174	LR: 0.012500
Training Epoch: 90 [7424/50000]	Loss: 0.1440	LR: 0.012500
Training Epoch: 90 [7552/50000]	Loss: 0.1001	LR: 0.012500
Training Epoch: 90 [7680/50000]	Loss: 0.1389	LR: 0.012500
Training Epoch: 90 [7808/50000]	Loss: 0.1217	LR: 0.012500
Training Epoch: 90 [7936/50000]	Loss: 0.1379	LR: 0.012500
Training Epoch: 90 [8064/50000]	Loss: 0.1006	LR: 0.012500
Training Epoch: 90 [8192/50000]	Loss: 0.1153	LR: 0.012500
Training Epoch: 90 [8320/50000]	Loss: 0.1670	LR: 0.012500
Training Epoch: 90 [8448/50000]	Loss: 0.1180	LR: 0.012500
Training Epoch: 90 [8576/50000]	Loss: 0.1137	LR: 0.012500
Training Epoch: 90 [8704/50000]	Loss: 0.0955	LR: 0.012500
Training Epoch: 90 [8832/50000]	Loss: 0.0972	LR: 0.012500
Training Epoch: 90 [8960/50000]	Loss: 0.1362	LR: 0.012500
Training Epoch: 90 [9088/50000]	Loss: 0.1016	LR: 0.012500
Training Epoch: 90 [9216/50000]	Loss: 0.1325	LR: 0.012500
Training Epoch: 90 [9344/50000]	Loss: 0.1170	LR: 0.012500
Training Epoch: 90 [9472/50000]	Loss: 0.1144	LR: 0.012500
Training Epoch: 90 [9600/50000]	Loss: 0.0755	LR: 0.012500
Training Epoch: 90 [9728/50000]	Loss: 0.1191	LR: 0.012500
Training Epoch: 90 [9856/50000]	Loss: 0.1426	LR: 0.012500
Training Epoch: 90 [9984/50000]	Loss: 0.1347	LR: 0.012500
Training Epoch: 90 [10112/50000]	Loss: 0.1381	LR: 0.012500
Training Epoch: 90 [10240/50000]	Loss: 0.1673	LR: 0.012500
Training Epoch: 90 [10368/50000]	Loss: 0.1141	LR: 0.012500
Training Epoch: 90 [10496/50000]	Loss: 0.1025	LR: 0.012500
Training Epoch: 90 [10624/50000]	Loss: 0.1023	LR: 0.012500
Training Epoch: 90 [10752/50000]	Loss: 0.1257	LR: 0.012500
Training Epoch: 90 [10880/50000]	Loss: 0.0824	LR: 0.012500
Training Epoch: 90 [11008/50000]	Loss: 0.1647	LR: 0.012500
Training Epoch: 90 [11136/50000]	Loss: 0.1636	LR: 0.012500
Training Epoch: 90 [11264/50000]	Loss: 0.1753	LR: 0.012500
Training Epoch: 90 [11392/50000]	Loss: 0.1603	LR: 0.012500
Training Epoch: 90 [11520/50000]	Loss: 0.1019	LR: 0.012500
Training Epoch: 90 [11648/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 90 [11776/50000]	Loss: 0.1353	LR: 0.012500
Training Epoch: 90 [11904/50000]	Loss: 0.1300	LR: 0.012500
Training Epoch: 90 [12032/50000]	Loss: 0.1116	LR: 0.012500
Training Epoch: 90 [12160/50000]	Loss: 0.1666	LR: 0.012500
Training Epoch: 90 [12288/50000]	Loss: 0.0939	LR: 0.012500
Training Epoch: 90 [12416/50000]	Loss: 0.1311	LR: 0.012500
Training Epoch: 90 [12544/50000]	Loss: 0.1474	LR: 0.012500
Training Epoch: 90 [12672/50000]	Loss: 0.1795	LR: 0.012500
Training Epoch: 90 [12800/50000]	Loss: 0.1026	LR: 0.012500
Training Epoch: 90 [12928/50000]	Loss: 0.1231	LR: 0.012500
Training Epoch: 90 [13056/50000]	Loss: 0.1066	LR: 0.012500
Training Epoch: 90 [13184/50000]	Loss: 0.0919	LR: 0.012500
Training Epoch: 90 [13312/50000]	Loss: 0.1226	LR: 0.012500
Training Epoch: 90 [13440/50000]	Loss: 0.0870	LR: 0.012500
Training Epoch: 90 [13568/50000]	Loss: 0.0614	LR: 0.012500
Training Epoch: 90 [13696/50000]	Loss: 0.1085	LR: 0.012500
Training Epoch: 90 [13824/50000]	Loss: 0.0647	LR: 0.012500
Training Epoch: 90 [13952/50000]	Loss: 0.1134	LR: 0.012500
Training Epoch: 90 [14080/50000]	Loss: 0.1330	LR: 0.012500
Training Epoch: 90 [14208/50000]	Loss: 0.1383	LR: 0.012500
Training Epoch: 90 [14336/50000]	Loss: 0.1376	LR: 0.012500
Training Epoch: 90 [14464/50000]	Loss: 0.1003	LR: 0.012500
Training Epoch: 90 [14592/50000]	Loss: 0.1303	LR: 0.012500
Training Epoch: 90 [14720/50000]	Loss: 0.0892	LR: 0.012500
Training Epoch: 90 [14848/50000]	Loss: 0.1624	LR: 0.012500
Training Epoch: 90 [14976/50000]	Loss: 0.1107	LR: 0.012500
Training Epoch: 90 [15104/50000]	Loss: 0.1129	LR: 0.012500
Training Epoch: 90 [15232/50000]	Loss: 0.1004	LR: 0.012500
Training Epoch: 90 [15360/50000]	Loss: 0.1213	LR: 0.012500
Training Epoch: 90 [15488/50000]	Loss: 0.0824	LR: 0.012500
Training Epoch: 90 [15616/50000]	Loss: 0.1530	LR: 0.012500
Training Epoch: 90 [15744/50000]	Loss: 0.1427	LR: 0.012500
Training Epoch: 90 [15872/50000]	Loss: 0.0806	LR: 0.012500
Training Epoch: 90 [16000/50000]	Loss: 0.1372	LR: 0.012500
Training Epoch: 90 [16128/50000]	Loss: 0.1583	LR: 0.012500
Training Epoch: 90 [16256/50000]	Loss: 0.1383	LR: 0.012500
Training Epoch: 90 [16384/50000]	Loss: 0.1031	LR: 0.012500
Training Epoch: 90 [16512/50000]	Loss: 0.1418	LR: 0.012500
Training Epoch: 90 [16640/50000]	Loss: 0.0697	LR: 0.012500
Training Epoch: 90 [16768/50000]	Loss: 0.0850	LR: 0.012500
Training Epoch: 90 [16896/50000]	Loss: 0.0908	LR: 0.012500
Training Epoch: 90 [17024/50000]	Loss: 0.1152	LR: 0.012500
Training Epoch: 90 [17152/50000]	Loss: 0.1211	LR: 0.012500
Training Epoch: 90 [17280/50000]	Loss: 0.1469	LR: 0.012500
Training Epoch: 90 [17408/50000]	Loss: 0.1610	LR: 0.012500
Training Epoch: 90 [17536/50000]	Loss: 0.1773	LR: 0.012500
Training Epoch: 90 [17664/50000]	Loss: 0.0754	LR: 0.012500
Training Epoch: 90 [17792/50000]	Loss: 0.0615	LR: 0.012500
Training Epoch: 90 [17920/50000]	Loss: 0.0593	LR: 0.012500
Training Epoch: 90 [18048/50000]	Loss: 0.1430	LR: 0.012500
Training Epoch: 90 [18176/50000]	Loss: 0.0804	LR: 0.012500
Training Epoch: 90 [18304/50000]	Loss: 0.1322	LR: 0.012500
Training Epoch: 90 [18432/50000]	Loss: 0.1806	LR: 0.012500
Training Epoch: 90 [18560/50000]	Loss: 0.1477	LR: 0.012500
Training Epoch: 90 [18688/50000]	Loss: 0.1419	LR: 0.012500
Training Epoch: 90 [18816/50000]	Loss: 0.0965	LR: 0.012500
Training Epoch: 90 [18944/50000]	Loss: 0.1696	LR: 0.012500
Training Epoch: 90 [19072/50000]	Loss: 0.1436	LR: 0.012500
Training Epoch: 90 [19200/50000]	Loss: 0.1008	LR: 0.012500
Training Epoch: 90 [19328/50000]	Loss: 0.1171	LR: 0.012500
Training Epoch: 90 [19456/50000]	Loss: 0.1685	LR: 0.012500
Training Epoch: 90 [19584/50000]	Loss: 0.0723	LR: 0.012500
Training Epoch: 90 [19712/50000]	Loss: 0.1817	LR: 0.012500
Training Epoch: 90 [19840/50000]	Loss: 0.0765	LR: 0.012500
Training Epoch: 90 [19968/50000]	Loss: 0.0909	LR: 0.012500
Training Epoch: 90 [20096/50000]	Loss: 0.1381	LR: 0.012500
Training Epoch: 90 [20224/50000]	Loss: 0.1480	LR: 0.012500
Training Epoch: 90 [20352/50000]	Loss: 0.1128	LR: 0.012500
Training Epoch: 90 [20480/50000]	Loss: 0.0952	LR: 0.012500
Training Epoch: 90 [20608/50000]	Loss: 0.0894	LR: 0.012500
Training Epoch: 90 [20736/50000]	Loss: 0.1560	LR: 0.012500
Training Epoch: 90 [20864/50000]	Loss: 0.1271	LR: 0.012500
Training Epoch: 90 [20992/50000]	Loss: 0.1128	LR: 0.012500
Training Epoch: 90 [21120/50000]	Loss: 0.1280	LR: 0.012500
Training Epoch: 90 [21248/50000]	Loss: 0.0912	LR: 0.012500
Training Epoch: 90 [21376/50000]	Loss: 0.1409	LR: 0.012500
Training Epoch: 90 [21504/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 90 [21632/50000]	Loss: 0.0791	LR: 0.012500
Training Epoch: 90 [21760/50000]	Loss: 0.1447	LR: 0.012500
Training Epoch: 90 [21888/50000]	Loss: 0.1021	LR: 0.012500
Training Epoch: 90 [22016/50000]	Loss: 0.1740	LR: 0.012500
Training Epoch: 90 [22144/50000]	Loss: 0.0750	LR: 0.012500
Training Epoch: 90 [22272/50000]	Loss: 0.1061	LR: 0.012500
Training Epoch: 90 [22400/50000]	Loss: 0.1040	LR: 0.012500
Training Epoch: 90 [22528/50000]	Loss: 0.0958	LR: 0.012500
Training Epoch: 90 [22656/50000]	Loss: 0.1290	LR: 0.012500
Training Epoch: 90 [22784/50000]	Loss: 0.1763	LR: 0.012500
Training Epoch: 90 [22912/50000]	Loss: 0.1341	LR: 0.012500
Training Epoch: 90 [23040/50000]	Loss: 0.1073	LR: 0.012500
Training Epoch: 90 [23168/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 90 [23296/50000]	Loss: 0.1026	LR: 0.012500
Training Epoch: 90 [23424/50000]	Loss: 0.1413	LR: 0.012500
Training Epoch: 90 [23552/50000]	Loss: 0.0643	LR: 0.012500
Training Epoch: 90 [23680/50000]	Loss: 0.1640	LR: 0.012500
Training Epoch: 90 [23808/50000]	Loss: 0.0878	LR: 0.012500
Training Epoch: 90 [23936/50000]	Loss: 0.0976	LR: 0.012500
Training Epoch: 90 [24064/50000]	Loss: 0.1214	LR: 0.012500
Training Epoch: 90 [24192/50000]	Loss: 0.1009	LR: 0.012500
Training Epoch: 90 [24320/50000]	Loss: 0.0992	LR: 0.012500
Training Epoch: 90 [24448/50000]	Loss: 0.1308	LR: 0.012500
Training Epoch: 90 [24576/50000]	Loss: 0.1606	LR: 0.012500
Training Epoch: 90 [24704/50000]	Loss: 0.1432	LR: 0.012500
Training Epoch: 90 [24832/50000]	Loss: 0.1075	LR: 0.012500
Training Epoch: 90 [24960/50000]	Loss: 0.1017	LR: 0.012500
Training Epoch: 90 [25088/50000]	Loss: 0.1191	LR: 0.012500
Training Epoch: 90 [25216/50000]	Loss: 0.1145	LR: 0.012500
Training Epoch: 90 [25344/50000]	Loss: 0.0877	LR: 0.012500
Training Epoch: 90 [25472/50000]	Loss: 0.1001	LR: 0.012500
Training Epoch: 90 [25600/50000]	Loss: 0.1530	LR: 0.012500
Training Epoch: 90 [25728/50000]	Loss: 0.1363	LR: 0.012500
Training Epoch: 90 [25856/50000]	Loss: 0.1695	LR: 0.012500
Training Epoch: 90 [25984/50000]	Loss: 0.1748	LR: 0.012500
Training Epoch: 90 [26112/50000]	Loss: 0.1065	LR: 0.012500
Training Epoch: 90 [26240/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 90 [26368/50000]	Loss: 0.0717	LR: 0.012500
Training Epoch: 90 [26496/50000]	Loss: 0.1420	LR: 0.012500
Training Epoch: 90 [26624/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 90 [26752/50000]	Loss: 0.1404	LR: 0.012500
Training Epoch: 90 [26880/50000]	Loss: 0.1218	LR: 0.012500
Training Epoch: 90 [27008/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 90 [27136/50000]	Loss: 0.0741	LR: 0.012500
Training Epoch: 90 [27264/50000]	Loss: 0.1928	LR: 0.012500
Training Epoch: 90 [27392/50000]	Loss: 0.0982	LR: 0.012500
Training Epoch: 90 [27520/50000]	Loss: 0.1667	LR: 0.012500
Training Epoch: 90 [27648/50000]	Loss: 0.1125	LR: 0.012500
Training Epoch: 90 [27776/50000]	Loss: 0.1378	LR: 0.012500
Training Epoch: 90 [27904/50000]	Loss: 0.1769	LR: 0.012500
Training Epoch: 90 [28032/50000]	Loss: 0.1419	LR: 0.012500
Training Epoch: 90 [28160/50000]	Loss: 0.1595	LR: 0.012500
Training Epoch: 90 [28288/50000]	Loss: 0.1498	LR: 0.012500
Training Epoch: 90 [28416/50000]	Loss: 0.0839	LR: 0.012500
Training Epoch: 90 [28544/50000]	Loss: 0.0991	LR: 0.012500
Training Epoch: 90 [28672/50000]	Loss: 0.2282	LR: 0.012500
Training Epoch: 90 [28800/50000]	Loss: 0.0849	LR: 0.012500
Training Epoch: 90 [28928/50000]	Loss: 0.0773	LR: 0.012500
Training Epoch: 90 [29056/50000]	Loss: 0.1376	LR: 0.012500
Training Epoch: 90 [29184/50000]	Loss: 0.1381	LR: 0.012500
Training Epoch: 90 [29312/50000]	Loss: 0.1664	LR: 0.012500
Training Epoch: 90 [29440/50000]	Loss: 0.1213	LR: 0.012500
Training Epoch: 90 [29568/50000]	Loss: 0.1004	LR: 0.012500
Training Epoch: 90 [29696/50000]	Loss: 0.0832	LR: 0.012500
Training Epoch: 90 [29824/50000]	Loss: 0.1503	LR: 0.012500
Training Epoch: 90 [29952/50000]	Loss: 0.1788	LR: 0.012500
Training Epoch: 90 [30080/50000]	Loss: 0.1517	LR: 0.012500
Training Epoch: 90 [30208/50000]	Loss: 0.1027	LR: 0.012500
Training Epoch: 90 [30336/50000]	Loss: 0.1713	LR: 0.012500
Training Epoch: 90 [30464/50000]	Loss: 0.1495	LR: 0.012500
Training Epoch: 90 [30592/50000]	Loss: 0.0929	LR: 0.012500
Training Epoch: 90 [30720/50000]	Loss: 0.1736	LR: 0.012500
Training Epoch: 90 [30848/50000]	Loss: 0.1168	LR: 0.012500
Training Epoch: 90 [30976/50000]	Loss: 0.1557	LR: 0.012500
Training Epoch: 90 [31104/50000]	Loss: 0.1407	LR: 0.012500
Training Epoch: 90 [31232/50000]	Loss: 0.0989	LR: 0.012500
Training Epoch: 90 [31360/50000]	Loss: 0.1330	LR: 0.012500
Training Epoch: 90 [31488/50000]	Loss: 0.1837	LR: 0.012500
Training Epoch: 90 [31616/50000]	Loss: 0.1512	LR: 0.012500
Training Epoch: 90 [31744/50000]	Loss: 0.1744	LR: 0.012500
Training Epoch: 90 [31872/50000]	Loss: 0.1712	LR: 0.012500
Training Epoch: 90 [32000/50000]	Loss: 0.1244	LR: 0.012500
Training Epoch: 90 [32128/50000]	Loss: 0.1461	LR: 0.012500
Training Epoch: 90 [32256/50000]	Loss: 0.1351	LR: 0.012500
Training Epoch: 90 [32384/50000]	Loss: 0.0880	LR: 0.012500
Training Epoch: 90 [32512/50000]	Loss: 0.1715	LR: 0.012500
Training Epoch: 90 [32640/50000]	Loss: 0.0948	LR: 0.012500
Training Epoch: 90 [32768/50000]	Loss: 0.1495	LR: 0.012500
Training Epoch: 90 [32896/50000]	Loss: 0.1589	LR: 0.012500
Training Epoch: 90 [33024/50000]	Loss: 0.1723	LR: 0.012500
Training Epoch: 90 [33152/50000]	Loss: 0.1420	LR: 0.012500
Training Epoch: 90 [33280/50000]	Loss: 0.1068	LR: 0.012500
Training Epoch: 90 [33408/50000]	Loss: 0.1334	LR: 0.012500
Training Epoch: 90 [33536/50000]	Loss: 0.1256	LR: 0.012500
Training Epoch: 90 [33664/50000]	Loss: 0.1247	LR: 0.012500
Training Epoch: 90 [33792/50000]	Loss: 0.1783	LR: 0.012500
Training Epoch: 90 [33920/50000]	Loss: 0.1790	LR: 0.012500
Training Epoch: 90 [34048/50000]	Loss: 0.1605	LR: 0.012500
Training Epoch: 90 [34176/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 90 [34304/50000]	Loss: 0.1219	LR: 0.012500
Training Epoch: 90 [34432/50000]	Loss: 0.1140	LR: 0.012500
Training Epoch: 90 [34560/50000]	Loss: 0.1502	LR: 0.012500
Training Epoch: 90 [34688/50000]	Loss: 0.1494	LR: 0.012500
Training Epoch: 90 [34816/50000]	Loss: 0.1269	LR: 0.012500
Training Epoch: 90 [34944/50000]	Loss: 0.1501	LR: 0.012500
Training Epoch: 90 [35072/50000]	Loss: 0.1812	LR: 0.012500
Training Epoch: 90 [35200/50000]	Loss: 0.0950	LR: 0.012500
Training Epoch: 90 [35328/50000]	Loss: 0.1740	LR: 0.012500
Training Epoch: 90 [35456/50000]	Loss: 0.1901	LR: 0.012500
Training Epoch: 90 [35584/50000]	Loss: 0.1082	LR: 0.012500
Training Epoch: 90 [35712/50000]	Loss: 0.1742	LR: 0.012500
Training Epoch: 90 [35840/50000]	Loss: 0.1793	LR: 0.012500
Training Epoch: 90 [35968/50000]	Loss: 0.1098	LR: 0.012500
Training Epoch: 90 [36096/50000]	Loss: 0.1260	LR: 0.012500
Training Epoch: 90 [36224/50000]	Loss: 0.1678	LR: 0.012500
Training Epoch: 90 [36352/50000]	Loss: 0.1572	LR: 0.012500
Training Epoch: 90 [36480/50000]	Loss: 0.1180	LR: 0.012500
Training Epoch: 90 [36608/50000]	Loss: 0.1075	LR: 0.012500
Training Epoch: 90 [36736/50000]	Loss: 0.1669	LR: 0.012500
Training Epoch: 90 [36864/50000]	Loss: 0.1081	LR: 0.012500
Training Epoch: 90 [36992/50000]	Loss: 0.1847	LR: 0.012500
Training Epoch: 90 [37120/50000]	Loss: 0.1489	LR: 0.012500
Training Epoch: 90 [37248/50000]	Loss: 0.1315	LR: 0.012500
Training Epoch: 90 [37376/50000]	Loss: 0.1436	LR: 0.012500
Training Epoch: 90 [37504/50000]	Loss: 0.1275	LR: 0.012500
Training Epoch: 90 [37632/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 90 [37760/50000]	Loss: 0.1559	LR: 0.012500
Training Epoch: 90 [37888/50000]	Loss: 0.1494	LR: 0.012500
Training Epoch: 90 [38016/50000]	Loss: 0.0861	LR: 0.012500
Training Epoch: 90 [38144/50000]	Loss: 0.1184	LR: 0.012500
Training Epoch: 90 [38272/50000]	Loss: 0.0976	LR: 0.012500
Training Epoch: 90 [38400/50000]	Loss: 0.1193	LR: 0.012500
Training Epoch: 90 [38528/50000]	Loss: 0.0906	LR: 0.012500
Training Epoch: 90 [38656/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 90 [38784/50000]	Loss: 0.1206	LR: 0.012500
Training Epoch: 90 [38912/50000]	Loss: 0.0727	LR: 0.012500
Training Epoch: 90 [39040/50000]	Loss: 0.1164	LR: 0.012500
Training Epoch: 90 [39168/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 90 [39296/50000]	Loss: 0.0721	LR: 0.012500
Training Epoch: 90 [39424/50000]	Loss: 0.1669	LR: 0.012500
Training Epoch: 90 [39552/50000]	Loss: 0.1811	LR: 0.012500
Training Epoch: 90 [39680/50000]	Loss: 0.1011	LR: 0.012500
Training Epoch: 90 [39808/50000]	Loss: 0.2188	LR: 0.012500
Training Epoch: 90 [39936/50000]	Loss: 0.1167	LR: 0.012500
Training Epoch: 90 [40064/50000]	Loss: 0.1743	LR: 0.012500
Training Epoch: 90 [40192/50000]	Loss: 0.1261	LR: 0.012500
Training Epoch: 90 [40320/50000]	Loss: 0.1344	LR: 0.012500
Training Epoch: 90 [40448/50000]	Loss: 0.1711	LR: 0.012500
Training Epoch: 90 [40576/50000]	Loss: 0.0778	LR: 0.012500
Training Epoch: 90 [40704/50000]	Loss: 0.1684	LR: 0.012500
Training Epoch: 90 [40832/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 90 [40960/50000]	Loss: 0.1231	LR: 0.012500
Training Epoch: 90 [41088/50000]	Loss: 0.1174	LR: 0.012500
Training Epoch: 90 [41216/50000]	Loss: 0.1408	LR: 0.012500
Training Epoch: 90 [41344/50000]	Loss: 0.1690	LR: 0.012500
Training Epoch: 90 [41472/50000]	Loss: 0.1489	LR: 0.012500
Training Epoch: 90 [41600/50000]	Loss: 0.1124	LR: 0.012500
Training Epoch: 90 [41728/50000]	Loss: 0.1209	LR: 0.012500
Training Epoch: 90 [41856/50000]	Loss: 0.2100	LR: 0.012500
Training Epoch: 90 [41984/50000]	Loss: 0.1147	LR: 0.012500
Training Epoch: 90 [42112/50000]	Loss: 0.1870	LR: 0.012500
Training Epoch: 90 [42240/50000]	Loss: 0.1067	LR: 0.012500
Training Epoch: 90 [42368/50000]	Loss: 0.1690	LR: 0.012500
Training Epoch: 90 [42496/50000]	Loss: 0.1183	LR: 0.012500
Training Epoch: 90 [42624/50000]	Loss: 0.1287	LR: 0.012500
Training Epoch: 90 [42752/50000]	Loss: 0.1648	LR: 0.012500
Training Epoch: 90 [42880/50000]	Loss: 0.1813	LR: 0.012500
Training Epoch: 90 [43008/50000]	Loss: 0.0984	LR: 0.012500
Training Epoch: 90 [43136/50000]	Loss: 0.1134	LR: 0.012500
Training Epoch: 90 [43264/50000]	Loss: 0.1914	LR: 0.012500
Training Epoch: 90 [43392/50000]	Loss: 0.0900	LR: 0.012500
Training Epoch: 90 [43520/50000]	Loss: 0.1043	LR: 0.012500
Training Epoch: 90 [43648/50000]	Loss: 0.1805	LR: 0.012500
Training Epoch: 90 [43776/50000]	Loss: 0.1292	LR: 0.012500
Training Epoch: 90 [43904/50000]	Loss: 0.1544	LR: 0.012500
Training Epoch: 90 [44032/50000]	Loss: 0.1231	LR: 0.012500
Training Epoch: 90 [44160/50000]	Loss: 0.1491	LR: 0.012500
Training Epoch: 90 [44288/50000]	Loss: 0.1525	LR: 0.012500
Training Epoch: 90 [44416/50000]	Loss: 0.0960	LR: 0.012500
Training Epoch: 90 [44544/50000]	Loss: 0.1044	LR: 0.012500
Training Epoch: 90 [44672/50000]	Loss: 0.2545	LR: 0.012500
Training Epoch: 90 [44800/50000]	Loss: 0.1916	LR: 0.012500
Training Epoch: 90 [44928/50000]	Loss: 0.1210	LR: 0.012500
Training Epoch: 90 [45056/50000]	Loss: 0.1436	LR: 0.012500
Training Epoch: 90 [45184/50000]	Loss: 0.1798	LR: 0.012500
Training Epoch: 90 [45312/50000]	Loss: 0.1337	LR: 0.012500
Training Epoch: 90 [45440/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 90 [45568/50000]	Loss: 0.1072	LR: 0.012500
Training Epoch: 90 [45696/50000]	Loss: 0.1301	LR: 0.012500
Training Epoch: 90 [45824/50000]	Loss: 0.2346	LR: 0.012500
Training Epoch: 90 [45952/50000]	Loss: 0.1455	LR: 0.012500
Training Epoch: 90 [46080/50000]	Loss: 0.0849	LR: 0.012500
Training Epoch: 90 [46208/50000]	Loss: 0.0993	LR: 0.012500
Training Epoch: 90 [46336/50000]	Loss: 0.1256	LR: 0.012500
Training Epoch: 90 [46464/50000]	Loss: 0.1576	LR: 0.012500
Training Epoch: 90 [46592/50000]	Loss: 0.0779	LR: 0.012500
Training Epoch: 90 [46720/50000]	Loss: 0.1084	LR: 0.012500
Training Epoch: 90 [46848/50000]	Loss: 0.1129	LR: 0.012500
Training Epoch: 90 [46976/50000]	Loss: 0.1704	LR: 0.012500
Training Epoch: 90 [47104/50000]	Loss: 0.1474	LR: 0.012500
Training Epoch: 90 [47232/50000]	Loss: 0.1518	LR: 0.012500
Training Epoch: 90 [47360/50000]	Loss: 0.1469	LR: 0.012500
Training Epoch: 90 [47488/50000]	Loss: 0.0922	LR: 0.012500
Training Epoch: 90 [47616/50000]	Loss: 0.1636	LR: 0.012500
Training Epoch: 90 [47744/50000]	Loss: 0.0986	LR: 0.012500
Training Epoch: 90 [47872/50000]	Loss: 0.1073	LR: 0.012500
Training Epoch: 90 [48000/50000]	Loss: 0.1002	LR: 0.012500
Training Epoch: 90 [48128/50000]	Loss: 0.1742	LR: 0.012500
Training Epoch: 90 [48256/50000]	Loss: 0.1153	LR: 0.012500
Training Epoch: 90 [48384/50000]	Loss: 0.1326	LR: 0.012500
Training Epoch: 90 [48512/50000]	Loss: 0.1881	LR: 0.012500
Training Epoch: 90 [48640/50000]	Loss: 0.1394	LR: 0.012500
Training Epoch: 90 [48768/50000]	Loss: 0.1677	LR: 0.012500
Training Epoch: 90 [48896/50000]	Loss: 0.1130	LR: 0.012500
Training Epoch: 90 [49024/50000]	Loss: 0.2255	LR: 0.012500
Training Epoch: 90 [49152/50000]	Loss: 0.1347	LR: 0.012500
Training Epoch: 90 [49280/50000]	Loss: 0.1603	LR: 0.012500
Training Epoch: 90 [49408/50000]	Loss: 0.1167	LR: 0.012500
Training Epoch: 90 [49536/50000]	Loss: 0.0941	LR: 0.012500
Training Epoch: 90 [49664/50000]	Loss: 0.1820	LR: 0.012500
Training Epoch: 90 [49792/50000]	Loss: 0.1377	LR: 0.012500
Training Epoch: 90 [49920/50000]	Loss: 0.1460	LR: 0.012500
Training Epoch: 90 [50000/50000]	Loss: 0.1739	LR: 0.012500
epoch 90 training time consumed: 53.90s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  330305 GB |  330305 GB |
|       from large pool |  123392 KB |    1034 MB |  329979 GB |  329979 GB |
|       from small pool |   10798 KB |      13 MB |     325 GB |     325 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  330305 GB |  330305 GB |
|       from large pool |  123392 KB |    1034 MB |  329979 GB |  329979 GB |
|       from small pool |   10798 KB |      13 MB |     325 GB |     325 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  145353 GB |  145353 GB |
|       from large pool |  155136 KB |  433088 KB |  144993 GB |  144993 GB |
|       from small pool |    1489 KB |    3494 KB |     359 GB |     359 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   12745 K  |   12745 K  |
|       from large pool |      24    |      65    |    6652 K  |    6652 K  |
|       from small pool |     232    |     275    |    6092 K  |    6092 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   12745 K  |   12745 K  |
|       from large pool |      24    |      65    |    6652 K  |    6652 K  |
|       from small pool |     232    |     275    |    6092 K  |    6092 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6315 K  |    6315 K  |
|       from large pool |       9    |      14    |    3220 K  |    3220 K  |
|       from small pool |      12    |      17    |    3095 K  |    3095 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 90, Average loss: 0.0098, Accuracy: 0.7131, Time consumed:3.45s

Training Epoch: 91 [128/50000]	Loss: 0.1420	LR: 0.012500
Training Epoch: 91 [256/50000]	Loss: 0.0758	LR: 0.012500
Training Epoch: 91 [384/50000]	Loss: 0.1724	LR: 0.012500
Training Epoch: 91 [512/50000]	Loss: 0.1311	LR: 0.012500
Training Epoch: 91 [640/50000]	Loss: 0.0883	LR: 0.012500
Training Epoch: 91 [768/50000]	Loss: 0.1008	LR: 0.012500
Training Epoch: 91 [896/50000]	Loss: 0.1252	LR: 0.012500
Training Epoch: 91 [1024/50000]	Loss: 0.0728	LR: 0.012500
Training Epoch: 91 [1152/50000]	Loss: 0.1417	LR: 0.012500
Training Epoch: 91 [1280/50000]	Loss: 0.1076	LR: 0.012500
Training Epoch: 91 [1408/50000]	Loss: 0.0640	LR: 0.012500
Training Epoch: 91 [1536/50000]	Loss: 0.1324	LR: 0.012500
Training Epoch: 91 [1664/50000]	Loss: 0.1263	LR: 0.012500
Training Epoch: 91 [1792/50000]	Loss: 0.0893	LR: 0.012500
Training Epoch: 91 [1920/50000]	Loss: 0.1404	LR: 0.012500
Training Epoch: 91 [2048/50000]	Loss: 0.1342	LR: 0.012500
Training Epoch: 91 [2176/50000]	Loss: 0.0953	LR: 0.012500
Training Epoch: 91 [2304/50000]	Loss: 0.0759	LR: 0.012500
Training Epoch: 91 [2432/50000]	Loss: 0.0772	LR: 0.012500
Training Epoch: 91 [2560/50000]	Loss: 0.0923	LR: 0.012500
Training Epoch: 91 [2688/50000]	Loss: 0.0865	LR: 0.012500
Training Epoch: 91 [2816/50000]	Loss: 0.1758	LR: 0.012500
Training Epoch: 91 [2944/50000]	Loss: 0.1163	LR: 0.012500
Training Epoch: 91 [3072/50000]	Loss: 0.0682	LR: 0.012500
Training Epoch: 91 [3200/50000]	Loss: 0.1011	LR: 0.012500
Training Epoch: 91 [3328/50000]	Loss: 0.0647	LR: 0.012500
Training Epoch: 91 [3456/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 91 [3584/50000]	Loss: 0.0787	LR: 0.012500
Training Epoch: 91 [3712/50000]	Loss: 0.0836	LR: 0.012500
Training Epoch: 91 [3840/50000]	Loss: 0.1503	LR: 0.012500
Training Epoch: 91 [3968/50000]	Loss: 0.0782	LR: 0.012500
Training Epoch: 91 [4096/50000]	Loss: 0.1310	LR: 0.012500
Training Epoch: 91 [4224/50000]	Loss: 0.1139	LR: 0.012500
Training Epoch: 91 [4352/50000]	Loss: 0.1112	LR: 0.012500
Training Epoch: 91 [4480/50000]	Loss: 0.0925	LR: 0.012500
Training Epoch: 91 [4608/50000]	Loss: 0.0941	LR: 0.012500
Training Epoch: 91 [4736/50000]	Loss: 0.1306	LR: 0.012500
Training Epoch: 91 [4864/50000]	Loss: 0.0643	LR: 0.012500
Training Epoch: 91 [4992/50000]	Loss: 0.1045	LR: 0.012500
Training Epoch: 91 [5120/50000]	Loss: 0.0559	LR: 0.012500
Training Epoch: 91 [5248/50000]	Loss: 0.0882	LR: 0.012500
Training Epoch: 91 [5376/50000]	Loss: 0.1603	LR: 0.012500
Training Epoch: 91 [5504/50000]	Loss: 0.0980	LR: 0.012500
Training Epoch: 91 [5632/50000]	Loss: 0.1179	LR: 0.012500
Training Epoch: 91 [5760/50000]	Loss: 0.0964	LR: 0.012500
Training Epoch: 91 [5888/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 91 [6016/50000]	Loss: 0.1080	LR: 0.012500
Training Epoch: 91 [6144/50000]	Loss: 0.1384	LR: 0.012500
Training Epoch: 91 [6272/50000]	Loss: 0.1282	LR: 0.012500
Training Epoch: 91 [6400/50000]	Loss: 0.1260	LR: 0.012500
Training Epoch: 91 [6528/50000]	Loss: 0.1396	LR: 0.012500
Training Epoch: 91 [6656/50000]	Loss: 0.1446	LR: 0.012500
Training Epoch: 91 [6784/50000]	Loss: 0.1034	LR: 0.012500
Training Epoch: 91 [6912/50000]	Loss: 0.0958	LR: 0.012500
Training Epoch: 91 [7040/50000]	Loss: 0.1182	LR: 0.012500
Training Epoch: 91 [7168/50000]	Loss: 0.1139	LR: 0.012500
Training Epoch: 91 [7296/50000]	Loss: 0.1343	LR: 0.012500
Training Epoch: 91 [7424/50000]	Loss: 0.1116	LR: 0.012500
Training Epoch: 91 [7552/50000]	Loss: 0.0982	LR: 0.012500
Training Epoch: 91 [7680/50000]	Loss: 0.1306	LR: 0.012500
Training Epoch: 91 [7808/50000]	Loss: 0.0685	LR: 0.012500
Training Epoch: 91 [7936/50000]	Loss: 0.1261	LR: 0.012500
Training Epoch: 91 [8064/50000]	Loss: 0.1025	LR: 0.012500
Training Epoch: 91 [8192/50000]	Loss: 0.0786	LR: 0.012500
Training Epoch: 91 [8320/50000]	Loss: 0.0894	LR: 0.012500
Training Epoch: 91 [8448/50000]	Loss: 0.1774	LR: 0.012500
Training Epoch: 91 [8576/50000]	Loss: 0.1372	LR: 0.012500
Training Epoch: 91 [8704/50000]	Loss: 0.1016	LR: 0.012500
Training Epoch: 91 [8832/50000]	Loss: 0.1301	LR: 0.012500
Training Epoch: 91 [8960/50000]	Loss: 0.1047	LR: 0.012500
Training Epoch: 91 [9088/50000]	Loss: 0.1135	LR: 0.012500
Training Epoch: 91 [9216/50000]	Loss: 0.1296	LR: 0.012500
Training Epoch: 91 [9344/50000]	Loss: 0.1036	LR: 0.012500
Training Epoch: 91 [9472/50000]	Loss: 0.0936	LR: 0.012500
Training Epoch: 91 [9600/50000]	Loss: 0.1226	LR: 0.012500
Training Epoch: 91 [9728/50000]	Loss: 0.0938	LR: 0.012500
Training Epoch: 91 [9856/50000]	Loss: 0.0962	LR: 0.012500
Training Epoch: 91 [9984/50000]	Loss: 0.1107	LR: 0.012500
Training Epoch: 91 [10112/50000]	Loss: 0.1771	LR: 0.012500
Training Epoch: 91 [10240/50000]	Loss: 0.1342	LR: 0.012500
Training Epoch: 91 [10368/50000]	Loss: 0.2284	LR: 0.012500
Training Epoch: 91 [10496/50000]	Loss: 0.0920	LR: 0.012500
Training Epoch: 91 [10624/50000]	Loss: 0.1336	LR: 0.012500
Training Epoch: 91 [10752/50000]	Loss: 0.0995	LR: 0.012500
Training Epoch: 91 [10880/50000]	Loss: 0.1067	LR: 0.012500
Training Epoch: 91 [11008/50000]	Loss: 0.0655	LR: 0.012500
Training Epoch: 91 [11136/50000]	Loss: 0.1285	LR: 0.012500
Training Epoch: 91 [11264/50000]	Loss: 0.1172	LR: 0.012500
Training Epoch: 91 [11392/50000]	Loss: 0.0890	LR: 0.012500
Training Epoch: 91 [11520/50000]	Loss: 0.1642	LR: 0.012500
Training Epoch: 91 [11648/50000]	Loss: 0.0875	LR: 0.012500
Training Epoch: 91 [11776/50000]	Loss: 0.1210	LR: 0.012500
Training Epoch: 91 [11904/50000]	Loss: 0.1683	LR: 0.012500
Training Epoch: 91 [12032/50000]	Loss: 0.1121	LR: 0.012500
Training Epoch: 91 [12160/50000]	Loss: 0.0940	LR: 0.012500
Training Epoch: 91 [12288/50000]	Loss: 0.1937	LR: 0.012500
Training Epoch: 91 [12416/50000]	Loss: 0.1103	LR: 0.012500
Training Epoch: 91 [12544/50000]	Loss: 0.1049	LR: 0.012500
Training Epoch: 91 [12672/50000]	Loss: 0.0651	LR: 0.012500
Training Epoch: 91 [12800/50000]	Loss: 0.1337	LR: 0.012500
Training Epoch: 91 [12928/50000]	Loss: 0.0928	LR: 0.012500
Training Epoch: 91 [13056/50000]	Loss: 0.1446	LR: 0.012500
Training Epoch: 91 [13184/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 91 [13312/50000]	Loss: 0.1416	LR: 0.012500
Training Epoch: 91 [13440/50000]	Loss: 0.1319	LR: 0.012500
Training Epoch: 91 [13568/50000]	Loss: 0.1229	LR: 0.012500
Training Epoch: 91 [13696/50000]	Loss: 0.1284	LR: 0.012500
Training Epoch: 91 [13824/50000]	Loss: 0.1153	LR: 0.012500
Training Epoch: 91 [13952/50000]	Loss: 0.1585	LR: 0.012500
Training Epoch: 91 [14080/50000]	Loss: 0.2022	LR: 0.012500
Training Epoch: 91 [14208/50000]	Loss: 0.1806	LR: 0.012500
Training Epoch: 91 [14336/50000]	Loss: 0.1490	LR: 0.012500
Training Epoch: 91 [14464/50000]	Loss: 0.0593	LR: 0.012500
Training Epoch: 91 [14592/50000]	Loss: 0.0651	LR: 0.012500
Training Epoch: 91 [14720/50000]	Loss: 0.0938	LR: 0.012500
Training Epoch: 91 [14848/50000]	Loss: 0.1548	LR: 0.012500
Training Epoch: 91 [14976/50000]	Loss: 0.0547	LR: 0.012500
Training Epoch: 91 [15104/50000]	Loss: 0.0634	LR: 0.012500
Training Epoch: 91 [15232/50000]	Loss: 0.1524	LR: 0.012500
Training Epoch: 91 [15360/50000]	Loss: 0.0657	LR: 0.012500
Training Epoch: 91 [15488/50000]	Loss: 0.2167	LR: 0.012500
Training Epoch: 91 [15616/50000]	Loss: 0.1441	LR: 0.012500
Training Epoch: 91 [15744/50000]	Loss: 0.1544	LR: 0.012500
Training Epoch: 91 [15872/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 91 [16000/50000]	Loss: 0.1957	LR: 0.012500
Training Epoch: 91 [16128/50000]	Loss: 0.1634	LR: 0.012500
Training Epoch: 91 [16256/50000]	Loss: 0.1467	LR: 0.012500
Training Epoch: 91 [16384/50000]	Loss: 0.1172	LR: 0.012500
Training Epoch: 91 [16512/50000]	Loss: 0.0834	LR: 0.012500
Training Epoch: 91 [16640/50000]	Loss: 0.0856	LR: 0.012500
Training Epoch: 91 [16768/50000]	Loss: 0.1501	LR: 0.012500
Training Epoch: 91 [16896/50000]	Loss: 0.1396	LR: 0.012500
Training Epoch: 91 [17024/50000]	Loss: 0.1586	LR: 0.012500
Training Epoch: 91 [17152/50000]	Loss: 0.1298	LR: 0.012500
Training Epoch: 91 [17280/50000]	Loss: 0.0794	LR: 0.012500
Training Epoch: 91 [17408/50000]	Loss: 0.0672	LR: 0.012500
Training Epoch: 91 [17536/50000]	Loss: 0.0526	LR: 0.012500
Training Epoch: 91 [17664/50000]	Loss: 0.1099	LR: 0.012500
Training Epoch: 91 [17792/50000]	Loss: 0.1655	LR: 0.012500
Training Epoch: 91 [17920/50000]	Loss: 0.1599	LR: 0.012500
Training Epoch: 91 [18048/50000]	Loss: 0.0983	LR: 0.012500
Training Epoch: 91 [18176/50000]	Loss: 0.1868	LR: 0.012500
Training Epoch: 91 [18304/50000]	Loss: 0.1692	LR: 0.012500
Training Epoch: 91 [18432/50000]	Loss: 0.1601	LR: 0.012500
Training Epoch: 91 [18560/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 91 [18688/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 91 [18816/50000]	Loss: 0.1425	LR: 0.012500
Training Epoch: 91 [18944/50000]	Loss: 0.0720	LR: 0.012500
Training Epoch: 91 [19072/50000]	Loss: 0.1282	LR: 0.012500
Training Epoch: 91 [19200/50000]	Loss: 0.1076	LR: 0.012500
Training Epoch: 91 [19328/50000]	Loss: 0.0964	LR: 0.012500
Training Epoch: 91 [19456/50000]	Loss: 0.1389	LR: 0.012500
Training Epoch: 91 [19584/50000]	Loss: 0.0936	LR: 0.012500
Training Epoch: 91 [19712/50000]	Loss: 0.0728	LR: 0.012500
Training Epoch: 91 [19840/50000]	Loss: 0.0760	LR: 0.012500
Training Epoch: 91 [19968/50000]	Loss: 0.1501	LR: 0.012500
Training Epoch: 91 [20096/50000]	Loss: 0.1965	LR: 0.012500
Training Epoch: 91 [20224/50000]	Loss: 0.1424	LR: 0.012500
Training Epoch: 91 [20352/50000]	Loss: 0.0940	LR: 0.012500
Training Epoch: 91 [20480/50000]	Loss: 0.1371	LR: 0.012500
Training Epoch: 91 [20608/50000]	Loss: 0.2496	LR: 0.012500
Training Epoch: 91 [20736/50000]	Loss: 0.0844	LR: 0.012500
Training Epoch: 91 [20864/50000]	Loss: 0.1384	LR: 0.012500
Training Epoch: 91 [20992/50000]	Loss: 0.1455	LR: 0.012500
Training Epoch: 91 [21120/50000]	Loss: 0.1053	LR: 0.012500
Training Epoch: 91 [21248/50000]	Loss: 0.1065	LR: 0.012500
Training Epoch: 91 [21376/50000]	Loss: 0.0952	LR: 0.012500
Training Epoch: 91 [21504/50000]	Loss: 0.0744	LR: 0.012500
Training Epoch: 91 [21632/50000]	Loss: 0.1343	LR: 0.012500
Training Epoch: 91 [21760/50000]	Loss: 0.1906	LR: 0.012500
Training Epoch: 91 [21888/50000]	Loss: 0.1223	LR: 0.012500
Training Epoch: 91 [22016/50000]	Loss: 0.1414	LR: 0.012500
Training Epoch: 91 [22144/50000]	Loss: 0.0844	LR: 0.012500
Training Epoch: 91 [22272/50000]	Loss: 0.0850	LR: 0.012500
Training Epoch: 91 [22400/50000]	Loss: 0.0978	LR: 0.012500
Training Epoch: 91 [22528/50000]	Loss: 0.1499	LR: 0.012500
Training Epoch: 91 [22656/50000]	Loss: 0.1514	LR: 0.012500
Training Epoch: 91 [22784/50000]	Loss: 0.0845	LR: 0.012500
Training Epoch: 91 [22912/50000]	Loss: 0.1454	LR: 0.012500
Training Epoch: 91 [23040/50000]	Loss: 0.1412	LR: 0.012500
Training Epoch: 91 [23168/50000]	Loss: 0.1821	LR: 0.012500
Training Epoch: 91 [23296/50000]	Loss: 0.1160	LR: 0.012500
Training Epoch: 91 [23424/50000]	Loss: 0.1601	LR: 0.012500
Training Epoch: 91 [23552/50000]	Loss: 0.1499	LR: 0.012500
Training Epoch: 91 [23680/50000]	Loss: 0.2119	LR: 0.012500
Training Epoch: 91 [23808/50000]	Loss: 0.1071	LR: 0.012500
Training Epoch: 91 [23936/50000]	Loss: 0.1189	LR: 0.012500
Training Epoch: 91 [24064/50000]	Loss: 0.0857	LR: 0.012500
Training Epoch: 91 [24192/50000]	Loss: 0.1196	LR: 0.012500
Training Epoch: 91 [24320/50000]	Loss: 0.1024	LR: 0.012500
Training Epoch: 91 [24448/50000]	Loss: 0.2078	LR: 0.012500
Training Epoch: 91 [24576/50000]	Loss: 0.0931	LR: 0.012500
Training Epoch: 91 [24704/50000]	Loss: 0.1492	LR: 0.012500
Training Epoch: 91 [24832/50000]	Loss: 0.0996	LR: 0.012500
Training Epoch: 91 [24960/50000]	Loss: 0.1715	LR: 0.012500
Training Epoch: 91 [25088/50000]	Loss: 0.0744	LR: 0.012500
Training Epoch: 91 [25216/50000]	Loss: 0.1510	LR: 0.012500
Training Epoch: 91 [25344/50000]	Loss: 0.1541	LR: 0.012500
Training Epoch: 91 [25472/50000]	Loss: 0.0814	LR: 0.012500
Training Epoch: 91 [25600/50000]	Loss: 0.1603	LR: 0.012500
Training Epoch: 91 [25728/50000]	Loss: 0.1516	LR: 0.012500
Training Epoch: 91 [25856/50000]	Loss: 0.1338	LR: 0.012500
Training Epoch: 91 [25984/50000]	Loss: 0.1752	LR: 0.012500
Training Epoch: 91 [26112/50000]	Loss: 0.1786	LR: 0.012500
Training Epoch: 91 [26240/50000]	Loss: 0.1665	LR: 0.012500
Training Epoch: 91 [26368/50000]	Loss: 0.1236	LR: 0.012500
Training Epoch: 91 [26496/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 91 [26624/50000]	Loss: 0.1580	LR: 0.012500
Training Epoch: 91 [26752/50000]	Loss: 0.1334	LR: 0.012500
Training Epoch: 91 [26880/50000]	Loss: 0.1482	LR: 0.012500
Training Epoch: 91 [27008/50000]	Loss: 0.1838	LR: 0.012500
Training Epoch: 91 [27136/50000]	Loss: 0.1650	LR: 0.012500
Training Epoch: 91 [27264/50000]	Loss: 0.1167	LR: 0.012500
Training Epoch: 91 [27392/50000]	Loss: 0.1011	LR: 0.012500
Training Epoch: 91 [27520/50000]	Loss: 0.2169	LR: 0.012500
Training Epoch: 91 [27648/50000]	Loss: 0.1049	LR: 0.012500
Training Epoch: 91 [27776/50000]	Loss: 0.1913	LR: 0.012500
Training Epoch: 91 [27904/50000]	Loss: 0.1374	LR: 0.012500
Training Epoch: 91 [28032/50000]	Loss: 0.1479	LR: 0.012500
Training Epoch: 91 [28160/50000]	Loss: 0.2034	LR: 0.012500
Training Epoch: 91 [28288/50000]	Loss: 0.1406	LR: 0.012500
Training Epoch: 91 [28416/50000]	Loss: 0.0952	LR: 0.012500
Training Epoch: 91 [28544/50000]	Loss: 0.0908	LR: 0.012500
Training Epoch: 91 [28672/50000]	Loss: 0.0982	LR: 0.012500
Training Epoch: 91 [28800/50000]	Loss: 0.1054	LR: 0.012500
Training Epoch: 91 [28928/50000]	Loss: 0.0871	LR: 0.012500
Training Epoch: 91 [29056/50000]	Loss: 0.1059	LR: 0.012500
Training Epoch: 91 [29184/50000]	Loss: 0.1162	LR: 0.012500
Training Epoch: 91 [29312/50000]	Loss: 0.1350	LR: 0.012500
Training Epoch: 91 [29440/50000]	Loss: 0.1754	LR: 0.012500
Training Epoch: 91 [29568/50000]	Loss: 0.1078	LR: 0.012500
Training Epoch: 91 [29696/50000]	Loss: 0.0806	LR: 0.012500
Training Epoch: 91 [29824/50000]	Loss: 0.1212	LR: 0.012500
Training Epoch: 91 [29952/50000]	Loss: 0.1941	LR: 0.012500
Training Epoch: 91 [30080/50000]	Loss: 0.1273	LR: 0.012500
Training Epoch: 91 [30208/50000]	Loss: 0.1492	LR: 0.012500
Training Epoch: 91 [30336/50000]	Loss: 0.1000	LR: 0.012500
Training Epoch: 91 [30464/50000]	Loss: 0.1559	LR: 0.012500
Training Epoch: 91 [30592/50000]	Loss: 0.1348	LR: 0.012500
Training Epoch: 91 [30720/50000]	Loss: 0.1452	LR: 0.012500
Training Epoch: 91 [30848/50000]	Loss: 0.1435	LR: 0.012500
Training Epoch: 91 [30976/50000]	Loss: 0.2279	LR: 0.012500
Training Epoch: 91 [31104/50000]	Loss: 0.1036	LR: 0.012500
Training Epoch: 91 [31232/50000]	Loss: 0.0879	LR: 0.012500
Training Epoch: 91 [31360/50000]	Loss: 0.1720	LR: 0.012500
Training Epoch: 91 [31488/50000]	Loss: 0.1125	LR: 0.012500
Training Epoch: 91 [31616/50000]	Loss: 0.1694	LR: 0.012500
Training Epoch: 91 [31744/50000]	Loss: 0.1296	LR: 0.012500
Training Epoch: 91 [31872/50000]	Loss: 0.1567	LR: 0.012500
Training Epoch: 91 [32000/50000]	Loss: 0.0978	LR: 0.012500
Training Epoch: 91 [32128/50000]	Loss: 0.0915	LR: 0.012500
Training Epoch: 91 [32256/50000]	Loss: 0.0870	LR: 0.012500
Training Epoch: 91 [32384/50000]	Loss: 0.1578	LR: 0.012500
Training Epoch: 91 [32512/50000]	Loss: 0.1518	LR: 0.012500
Training Epoch: 91 [32640/50000]	Loss: 0.1823	LR: 0.012500
Training Epoch: 91 [32768/50000]	Loss: 0.1092	LR: 0.012500
Training Epoch: 91 [32896/50000]	Loss: 0.1914	LR: 0.012500
Training Epoch: 91 [33024/50000]	Loss: 0.1260	LR: 0.012500
Training Epoch: 91 [33152/50000]	Loss: 0.1099	LR: 0.012500
Training Epoch: 91 [33280/50000]	Loss: 0.1276	LR: 0.012500
Training Epoch: 91 [33408/50000]	Loss: 0.1377	LR: 0.012500
Training Epoch: 91 [33536/50000]	Loss: 0.1567	LR: 0.012500
Training Epoch: 91 [33664/50000]	Loss: 0.2493	LR: 0.012500
Training Epoch: 91 [33792/50000]	Loss: 0.1236	LR: 0.012500
Training Epoch: 91 [33920/50000]	Loss: 0.0700	LR: 0.012500
Training Epoch: 91 [34048/50000]	Loss: 0.1324	LR: 0.012500
Training Epoch: 91 [34176/50000]	Loss: 0.1837	LR: 0.012500
Training Epoch: 91 [34304/50000]	Loss: 0.0898	LR: 0.012500
Training Epoch: 91 [34432/50000]	Loss: 0.1204	LR: 0.012500
Training Epoch: 91 [34560/50000]	Loss: 0.1360	LR: 0.012500
Training Epoch: 91 [34688/50000]	Loss: 0.1783	LR: 0.012500
Training Epoch: 91 [34816/50000]	Loss: 0.1623	LR: 0.012500
Training Epoch: 91 [34944/50000]	Loss: 0.2152	LR: 0.012500
Training Epoch: 91 [35072/50000]	Loss: 0.1007	LR: 0.012500
Training Epoch: 91 [35200/50000]	Loss: 0.1229	LR: 0.012500
Training Epoch: 91 [35328/50000]	Loss: 0.1091	LR: 0.012500
Training Epoch: 91 [35456/50000]	Loss: 0.0896	LR: 0.012500
Training Epoch: 91 [35584/50000]	Loss: 0.2686	LR: 0.012500
Training Epoch: 91 [35712/50000]	Loss: 0.2277	LR: 0.012500
Training Epoch: 91 [35840/50000]	Loss: 0.2182	LR: 0.012500
Training Epoch: 91 [35968/50000]	Loss: 0.0498	LR: 0.012500
Training Epoch: 91 [36096/50000]	Loss: 0.1387	LR: 0.012500
Training Epoch: 91 [36224/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 91 [36352/50000]	Loss: 0.1943	LR: 0.012500
Training Epoch: 91 [36480/50000]	Loss: 0.1372	LR: 0.012500
Training Epoch: 91 [36608/50000]	Loss: 0.0864	LR: 0.012500
Training Epoch: 91 [36736/50000]	Loss: 0.1457	LR: 0.012500
Training Epoch: 91 [36864/50000]	Loss: 0.1143	LR: 0.012500
Training Epoch: 91 [36992/50000]	Loss: 0.1377	LR: 0.012500
Training Epoch: 91 [37120/50000]	Loss: 0.1482	LR: 0.012500
Training Epoch: 91 [37248/50000]	Loss: 0.1615	LR: 0.012500
Training Epoch: 91 [37376/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 91 [37504/50000]	Loss: 0.1883	LR: 0.012500
Training Epoch: 91 [37632/50000]	Loss: 0.1368	LR: 0.012500
Training Epoch: 91 [37760/50000]	Loss: 0.1440	LR: 0.012500
Training Epoch: 91 [37888/50000]	Loss: 0.0913	LR: 0.012500
Training Epoch: 91 [38016/50000]	Loss: 0.2176	LR: 0.012500
Training Epoch: 91 [38144/50000]	Loss: 0.1291	LR: 0.012500
Training Epoch: 91 [38272/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 91 [38400/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 91 [38528/50000]	Loss: 0.1094	LR: 0.012500
Training Epoch: 91 [38656/50000]	Loss: 0.1643	LR: 0.012500
Training Epoch: 91 [38784/50000]	Loss: 0.1387	LR: 0.012500
Training Epoch: 91 [38912/50000]	Loss: 0.2018	LR: 0.012500
Training Epoch: 91 [39040/50000]	Loss: 0.0827	LR: 0.012500
Training Epoch: 91 [39168/50000]	Loss: 0.1670	LR: 0.012500
Training Epoch: 91 [39296/50000]	Loss: 0.0830	LR: 0.012500
Training Epoch: 91 [39424/50000]	Loss: 0.1321	LR: 0.012500
Training Epoch: 91 [39552/50000]	Loss: 0.1125	LR: 0.012500
Training Epoch: 91 [39680/50000]	Loss: 0.1212	LR: 0.012500
Training Epoch: 91 [39808/50000]	Loss: 0.1805	LR: 0.012500
Training Epoch: 91 [39936/50000]	Loss: 0.1180	LR: 0.012500
Training Epoch: 91 [40064/50000]	Loss: 0.1030	LR: 0.012500
Training Epoch: 91 [40192/50000]	Loss: 0.1775	LR: 0.012500
Training Epoch: 91 [40320/50000]	Loss: 0.1647	LR: 0.012500
Training Epoch: 91 [40448/50000]	Loss: 0.1267	LR: 0.012500
Training Epoch: 91 [40576/50000]	Loss: 0.1651	LR: 0.012500
Training Epoch: 91 [40704/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 91 [40832/50000]	Loss: 0.1157	LR: 0.012500
Training Epoch: 91 [40960/50000]	Loss: 0.1228	LR: 0.012500
Training Epoch: 91 [41088/50000]	Loss: 0.1212	LR: 0.012500
Training Epoch: 91 [41216/50000]	Loss: 0.2079	LR: 0.012500
Training Epoch: 91 [41344/50000]	Loss: 0.1670	LR: 0.012500
Training Epoch: 91 [41472/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 91 [41600/50000]	Loss: 0.1774	LR: 0.012500
Training Epoch: 91 [41728/50000]	Loss: 0.0963	LR: 0.012500
Training Epoch: 91 [41856/50000]	Loss: 0.1584	LR: 0.012500
Training Epoch: 91 [41984/50000]	Loss: 0.0980	LR: 0.012500
Training Epoch: 91 [42112/50000]	Loss: 0.1814	LR: 0.012500
Training Epoch: 91 [42240/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 91 [42368/50000]	Loss: 0.1435	LR: 0.012500
Training Epoch: 91 [42496/50000]	Loss: 0.1226	LR: 0.012500
Training Epoch: 91 [42624/50000]	Loss: 0.0733	LR: 0.012500
Training Epoch: 91 [42752/50000]	Loss: 0.1990	LR: 0.012500
Training Epoch: 91 [42880/50000]	Loss: 0.1683	LR: 0.012500
Training Epoch: 91 [43008/50000]	Loss: 0.1616	LR: 0.012500
Training Epoch: 91 [43136/50000]	Loss: 0.1059	LR: 0.012500
Training Epoch: 91 [43264/50000]	Loss: 0.1899	LR: 0.012500
Training Epoch: 91 [43392/50000]	Loss: 0.1173	LR: 0.012500
Training Epoch: 91 [43520/50000]	Loss: 0.1278	LR: 0.012500
Training Epoch: 91 [43648/50000]	Loss: 0.2327	LR: 0.012500
Training Epoch: 91 [43776/50000]	Loss: 0.2118	LR: 0.012500
Training Epoch: 91 [43904/50000]	Loss: 0.1311	LR: 0.012500
Training Epoch: 91 [44032/50000]	Loss: 0.2048	LR: 0.012500
Training Epoch: 91 [44160/50000]	Loss: 0.1214	LR: 0.012500
Training Epoch: 91 [44288/50000]	Loss: 0.1091	LR: 0.012500
Training Epoch: 91 [44416/50000]	Loss: 0.1323	LR: 0.012500
Training Epoch: 91 [44544/50000]	Loss: 0.1099	LR: 0.012500
Training Epoch: 91 [44672/50000]	Loss: 0.1130	LR: 0.012500
Training Epoch: 91 [44800/50000]	Loss: 0.0902	LR: 0.012500
Training Epoch: 91 [44928/50000]	Loss: 0.1427	LR: 0.012500
Training Epoch: 91 [45056/50000]	Loss: 0.1020	LR: 0.012500
Training Epoch: 91 [45184/50000]	Loss: 0.0896	LR: 0.012500
Training Epoch: 91 [45312/50000]	Loss: 0.2557	LR: 0.012500
Training Epoch: 91 [45440/50000]	Loss: 0.1620	LR: 0.012500
Training Epoch: 91 [45568/50000]	Loss: 0.1490	LR: 0.012500
Training Epoch: 91 [45696/50000]	Loss: 0.1128	LR: 0.012500
Training Epoch: 91 [45824/50000]	Loss: 0.1144	LR: 0.012500
Training Epoch: 91 [45952/50000]	Loss: 0.1250	LR: 0.012500
Training Epoch: 91 [46080/50000]	Loss: 0.1576	LR: 0.012500
Training Epoch: 91 [46208/50000]	Loss: 0.1227	LR: 0.012500
Training Epoch: 91 [46336/50000]	Loss: 0.1272	LR: 0.012500
Training Epoch: 91 [46464/50000]	Loss: 0.1635	LR: 0.012500
Training Epoch: 91 [46592/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 91 [46720/50000]	Loss: 0.1012	LR: 0.012500
Training Epoch: 91 [46848/50000]	Loss: 0.1539	LR: 0.012500
Training Epoch: 91 [46976/50000]	Loss: 0.1929	LR: 0.012500
Training Epoch: 91 [47104/50000]	Loss: 0.1426	LR: 0.012500
Training Epoch: 91 [47232/50000]	Loss: 0.1171	LR: 0.012500
Training Epoch: 91 [47360/50000]	Loss: 0.1761	LR: 0.012500
Training Epoch: 91 [47488/50000]	Loss: 0.2012	LR: 0.012500
Training Epoch: 91 [47616/50000]	Loss: 0.1181	LR: 0.012500
Training Epoch: 91 [47744/50000]	Loss: 0.1035	LR: 0.012500
Training Epoch: 91 [47872/50000]	Loss: 0.1563	LR: 0.012500
Training Epoch: 91 [48000/50000]	Loss: 0.1417	LR: 0.012500
Training Epoch: 91 [48128/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 91 [48256/50000]	Loss: 0.1149	LR: 0.012500
Training Epoch: 91 [48384/50000]	Loss: 0.1778	LR: 0.012500
Training Epoch: 91 [48512/50000]	Loss: 0.1316	LR: 0.012500
Training Epoch: 91 [48640/50000]	Loss: 0.1582	LR: 0.012500
Training Epoch: 91 [48768/50000]	Loss: 0.1164	LR: 0.012500
Training Epoch: 91 [48896/50000]	Loss: 0.1161	LR: 0.012500
Training Epoch: 91 [49024/50000]	Loss: 0.1518	LR: 0.012500
Training Epoch: 91 [49152/50000]	Loss: 0.1290	LR: 0.012500
Training Epoch: 91 [49280/50000]	Loss: 0.1652	LR: 0.012500
Training Epoch: 91 [49408/50000]	Loss: 0.1158	LR: 0.012500
Training Epoch: 91 [49536/50000]	Loss: 0.1134	LR: 0.012500
Training Epoch: 91 [49664/50000]	Loss: 0.1170	LR: 0.012500
Training Epoch: 91 [49792/50000]	Loss: 0.1788	LR: 0.012500
Training Epoch: 91 [49920/50000]	Loss: 0.1250	LR: 0.012500
Training Epoch: 91 [50000/50000]	Loss: 0.1902	LR: 0.012500
epoch 91 training time consumed: 53.91s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  333975 GB |  333975 GB |
|       from large pool |  123392 KB |    1034 MB |  333646 GB |  333646 GB |
|       from small pool |   10798 KB |      13 MB |     329 GB |     329 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  333975 GB |  333975 GB |
|       from large pool |  123392 KB |    1034 MB |  333646 GB |  333646 GB |
|       from small pool |   10798 KB |      13 MB |     329 GB |     329 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  146968 GB |  146968 GB |
|       from large pool |  155136 KB |  433088 KB |  146604 GB |  146604 GB |
|       from small pool |    1489 KB |    3494 KB |     363 GB |     363 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   12886 K  |   12886 K  |
|       from large pool |      24    |      65    |    6726 K  |    6726 K  |
|       from small pool |     232    |     275    |    6160 K  |    6159 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   12886 K  |   12886 K  |
|       from large pool |      24    |      65    |    6726 K  |    6726 K  |
|       from small pool |     232    |     275    |    6160 K  |    6159 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6387 K  |    6387 K  |
|       from large pool |       9    |      14    |    3255 K  |    3255 K  |
|       from small pool |      12    |      17    |    3131 K  |    3131 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 91, Average loss: 0.0094, Accuracy: 0.7153, Time consumed:3.46s

Training Epoch: 92 [128/50000]	Loss: 0.1759	LR: 0.012500
Training Epoch: 92 [256/50000]	Loss: 0.1158	LR: 0.012500
Training Epoch: 92 [384/50000]	Loss: 0.0974	LR: 0.012500
Training Epoch: 92 [512/50000]	Loss: 0.1409	LR: 0.012500
Training Epoch: 92 [640/50000]	Loss: 0.2021	LR: 0.012500
Training Epoch: 92 [768/50000]	Loss: 0.0713	LR: 0.012500
Training Epoch: 92 [896/50000]	Loss: 0.0715	LR: 0.012500
Training Epoch: 92 [1024/50000]	Loss: 0.0829	LR: 0.012500
Training Epoch: 92 [1152/50000]	Loss: 0.1140	LR: 0.012500
Training Epoch: 92 [1280/50000]	Loss: 0.1556	LR: 0.012500
Training Epoch: 92 [1408/50000]	Loss: 0.1224	LR: 0.012500
Training Epoch: 92 [1536/50000]	Loss: 0.0858	LR: 0.012500
Training Epoch: 92 [1664/50000]	Loss: 0.1067	LR: 0.012500
Training Epoch: 92 [1792/50000]	Loss: 0.1486	LR: 0.012500
Training Epoch: 92 [1920/50000]	Loss: 0.1156	LR: 0.012500
Training Epoch: 92 [2048/50000]	Loss: 0.0798	LR: 0.012500
Training Epoch: 92 [2176/50000]	Loss: 0.1802	LR: 0.012500
Training Epoch: 92 [2304/50000]	Loss: 0.0887	LR: 0.012500
Training Epoch: 92 [2432/50000]	Loss: 0.0870	LR: 0.012500
Training Epoch: 92 [2560/50000]	Loss: 0.1311	LR: 0.012500
Training Epoch: 92 [2688/50000]	Loss: 0.0750	LR: 0.012500
Training Epoch: 92 [2816/50000]	Loss: 0.1396	LR: 0.012500
Training Epoch: 92 [2944/50000]	Loss: 0.0779	LR: 0.012500
Training Epoch: 92 [3072/50000]	Loss: 0.1214	LR: 0.012500
Training Epoch: 92 [3200/50000]	Loss: 0.1074	LR: 0.012500
Training Epoch: 92 [3328/50000]	Loss: 0.1108	LR: 0.012500
Training Epoch: 92 [3456/50000]	Loss: 0.1833	LR: 0.012500
Training Epoch: 92 [3584/50000]	Loss: 0.0756	LR: 0.012500
Training Epoch: 92 [3712/50000]	Loss: 0.1271	LR: 0.012500
Training Epoch: 92 [3840/50000]	Loss: 0.1152	LR: 0.012500
Training Epoch: 92 [3968/50000]	Loss: 0.0647	LR: 0.012500
Training Epoch: 92 [4096/50000]	Loss: 0.1089	LR: 0.012500
Training Epoch: 92 [4224/50000]	Loss: 0.0814	LR: 0.012500
Training Epoch: 92 [4352/50000]	Loss: 0.1314	LR: 0.012500
Training Epoch: 92 [4480/50000]	Loss: 0.1175	LR: 0.012500
Training Epoch: 92 [4608/50000]	Loss: 0.1177	LR: 0.012500
Training Epoch: 92 [4736/50000]	Loss: 0.2102	LR: 0.012500
Training Epoch: 92 [4864/50000]	Loss: 0.1574	LR: 0.012500
Training Epoch: 92 [4992/50000]	Loss: 0.0699	LR: 0.012500
Training Epoch: 92 [5120/50000]	Loss: 0.1724	LR: 0.012500
Training Epoch: 92 [5248/50000]	Loss: 0.1176	LR: 0.012500
Training Epoch: 92 [5376/50000]	Loss: 0.0671	LR: 0.012500
Training Epoch: 92 [5504/50000]	Loss: 0.1420	LR: 0.012500
Training Epoch: 92 [5632/50000]	Loss: 0.1256	LR: 0.012500
Training Epoch: 92 [5760/50000]	Loss: 0.1641	LR: 0.012500
Training Epoch: 92 [5888/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 92 [6016/50000]	Loss: 0.0977	LR: 0.012500
Training Epoch: 92 [6144/50000]	Loss: 0.1319	LR: 0.012500
Training Epoch: 92 [6272/50000]	Loss: 0.0760	LR: 0.012500
Training Epoch: 92 [6400/50000]	Loss: 0.1418	LR: 0.012500
Training Epoch: 92 [6528/50000]	Loss: 0.1605	LR: 0.012500
Training Epoch: 92 [6656/50000]	Loss: 0.1068	LR: 0.012500
Training Epoch: 92 [6784/50000]	Loss: 0.0936	LR: 0.012500
Training Epoch: 92 [6912/50000]	Loss: 0.1329	LR: 0.012500
Training Epoch: 92 [7040/50000]	Loss: 0.1466	LR: 0.012500
Training Epoch: 92 [7168/50000]	Loss: 0.1378	LR: 0.012500
Training Epoch: 92 [7296/50000]	Loss: 0.1039	LR: 0.012500
Training Epoch: 92 [7424/50000]	Loss: 0.1754	LR: 0.012500
Training Epoch: 92 [7552/50000]	Loss: 0.1227	LR: 0.012500
Training Epoch: 92 [7680/50000]	Loss: 0.1031	LR: 0.012500
Training Epoch: 92 [7808/50000]	Loss: 0.1209	LR: 0.012500
Training Epoch: 92 [7936/50000]	Loss: 0.1178	LR: 0.012500
Training Epoch: 92 [8064/50000]	Loss: 0.1780	LR: 0.012500
Training Epoch: 92 [8192/50000]	Loss: 0.1316	LR: 0.012500
Training Epoch: 92 [8320/50000]	Loss: 0.1422	LR: 0.012500
Training Epoch: 92 [8448/50000]	Loss: 0.1189	LR: 0.012500
Training Epoch: 92 [8576/50000]	Loss: 0.1299	LR: 0.012500
Training Epoch: 92 [8704/50000]	Loss: 0.0967	LR: 0.012500
Training Epoch: 92 [8832/50000]	Loss: 0.1649	LR: 0.012500
Training Epoch: 92 [8960/50000]	Loss: 0.1651	LR: 0.012500
Training Epoch: 92 [9088/50000]	Loss: 0.0767	LR: 0.012500
Training Epoch: 92 [9216/50000]	Loss: 0.1411	LR: 0.012500
Training Epoch: 92 [9344/50000]	Loss: 0.2482	LR: 0.012500
Training Epoch: 92 [9472/50000]	Loss: 0.1694	LR: 0.012500
Training Epoch: 92 [9600/50000]	Loss: 0.1224	LR: 0.012500
Training Epoch: 92 [9728/50000]	Loss: 0.1104	LR: 0.012500
Training Epoch: 92 [9856/50000]	Loss: 0.1538	LR: 0.012500
Training Epoch: 92 [9984/50000]	Loss: 0.1879	LR: 0.012500
Training Epoch: 92 [10112/50000]	Loss: 0.1232	LR: 0.012500
Training Epoch: 92 [10240/50000]	Loss: 0.1677	LR: 0.012500
Training Epoch: 92 [10368/50000]	Loss: 0.1113	LR: 0.012500
Training Epoch: 92 [10496/50000]	Loss: 0.1773	LR: 0.012500
Training Epoch: 92 [10624/50000]	Loss: 0.1244	LR: 0.012500
Training Epoch: 92 [10752/50000]	Loss: 0.1513	LR: 0.012500
Training Epoch: 92 [10880/50000]	Loss: 0.1206	LR: 0.012500
Training Epoch: 92 [11008/50000]	Loss: 0.1380	LR: 0.012500
Training Epoch: 92 [11136/50000]	Loss: 0.2619	LR: 0.012500
Training Epoch: 92 [11264/50000]	Loss: 0.1381	LR: 0.012500
Training Epoch: 92 [11392/50000]	Loss: 0.1208	LR: 0.012500
Training Epoch: 92 [11520/50000]	Loss: 0.1002	LR: 0.012500
Training Epoch: 92 [11648/50000]	Loss: 0.1564	LR: 0.012500
Training Epoch: 92 [11776/50000]	Loss: 0.1329	LR: 0.012500
Training Epoch: 92 [11904/50000]	Loss: 0.0831	LR: 0.012500
Training Epoch: 92 [12032/50000]	Loss: 0.1525	LR: 0.012500
Training Epoch: 92 [12160/50000]	Loss: 0.0811	LR: 0.012500
Training Epoch: 92 [12288/50000]	Loss: 0.1591	LR: 0.012500
Training Epoch: 92 [12416/50000]	Loss: 0.1830	LR: 0.012500
Training Epoch: 92 [12544/50000]	Loss: 0.0951	LR: 0.012500
Training Epoch: 92 [12672/50000]	Loss: 0.1009	LR: 0.012500
Training Epoch: 92 [12800/50000]	Loss: 0.0636	LR: 0.012500
Training Epoch: 92 [12928/50000]	Loss: 0.2313	LR: 0.012500
Training Epoch: 92 [13056/50000]	Loss: 0.1370	LR: 0.012500
Training Epoch: 92 [13184/50000]	Loss: 0.1166	LR: 0.012500
Training Epoch: 92 [13312/50000]	Loss: 0.0672	LR: 0.012500
Training Epoch: 92 [13440/50000]	Loss: 0.1448	LR: 0.012500
Training Epoch: 92 [13568/50000]	Loss: 0.1257	LR: 0.012500
Training Epoch: 92 [13696/50000]	Loss: 0.1295	LR: 0.012500
Training Epoch: 92 [13824/50000]	Loss: 0.1014	LR: 0.012500
Training Epoch: 92 [13952/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 92 [14080/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 92 [14208/50000]	Loss: 0.1171	LR: 0.012500
Training Epoch: 92 [14336/50000]	Loss: 0.1676	LR: 0.012500
Training Epoch: 92 [14464/50000]	Loss: 0.1806	LR: 0.012500
Training Epoch: 92 [14592/50000]	Loss: 0.1002	LR: 0.012500
Training Epoch: 92 [14720/50000]	Loss: 0.1151	LR: 0.012500
Training Epoch: 92 [14848/50000]	Loss: 0.1058	LR: 0.012500
Training Epoch: 92 [14976/50000]	Loss: 0.1508	LR: 0.012500
Training Epoch: 92 [15104/50000]	Loss: 0.1241	LR: 0.012500
Training Epoch: 92 [15232/50000]	Loss: 0.2144	LR: 0.012500
Training Epoch: 92 [15360/50000]	Loss: 0.1685	LR: 0.012500
Training Epoch: 92 [15488/50000]	Loss: 0.1047	LR: 0.012500
Training Epoch: 92 [15616/50000]	Loss: 0.0758	LR: 0.012500
Training Epoch: 92 [15744/50000]	Loss: 0.1484	LR: 0.012500
Training Epoch: 92 [15872/50000]	Loss: 0.1453	LR: 0.012500
Training Epoch: 92 [16000/50000]	Loss: 0.1401	LR: 0.012500
Training Epoch: 92 [16128/50000]	Loss: 0.1180	LR: 0.012500
Training Epoch: 92 [16256/50000]	Loss: 0.0753	LR: 0.012500
Training Epoch: 92 [16384/50000]	Loss: 0.1902	LR: 0.012500
Training Epoch: 92 [16512/50000]	Loss: 0.1131	LR: 0.012500
Training Epoch: 92 [16640/50000]	Loss: 0.1461	LR: 0.012500
Training Epoch: 92 [16768/50000]	Loss: 0.1507	LR: 0.012500
Training Epoch: 92 [16896/50000]	Loss: 0.2147	LR: 0.012500
Training Epoch: 92 [17024/50000]	Loss: 0.1548	LR: 0.012500
Training Epoch: 92 [17152/50000]	Loss: 0.1844	LR: 0.012500
Training Epoch: 92 [17280/50000]	Loss: 0.1600	LR: 0.012500
Training Epoch: 92 [17408/50000]	Loss: 0.0708	LR: 0.012500
Training Epoch: 92 [17536/50000]	Loss: 0.2610	LR: 0.012500
Training Epoch: 92 [17664/50000]	Loss: 0.1595	LR: 0.012500
Training Epoch: 92 [17792/50000]	Loss: 0.1502	LR: 0.012500
Training Epoch: 92 [17920/50000]	Loss: 0.2048	LR: 0.012500
Training Epoch: 92 [18048/50000]	Loss: 0.1734	LR: 0.012500
Training Epoch: 92 [18176/50000]	Loss: 0.1512	LR: 0.012500
Training Epoch: 92 [18304/50000]	Loss: 0.1712	LR: 0.012500
Training Epoch: 92 [18432/50000]	Loss: 0.0907	LR: 0.012500
Training Epoch: 92 [18560/50000]	Loss: 0.1191	LR: 0.012500
Training Epoch: 92 [18688/50000]	Loss: 0.1383	LR: 0.012500
Training Epoch: 92 [18816/50000]	Loss: 0.1190	LR: 0.012500
Training Epoch: 92 [18944/50000]	Loss: 0.1735	LR: 0.012500
Training Epoch: 92 [19072/50000]	Loss: 0.0569	LR: 0.012500
Training Epoch: 92 [19200/50000]	Loss: 0.1164	LR: 0.012500
Training Epoch: 92 [19328/50000]	Loss: 0.0994	LR: 0.012500
Training Epoch: 92 [19456/50000]	Loss: 0.1115	LR: 0.012500
Training Epoch: 92 [19584/50000]	Loss: 0.1792	LR: 0.012500
Training Epoch: 92 [19712/50000]	Loss: 0.0661	LR: 0.012500
Training Epoch: 92 [19840/50000]	Loss: 0.1348	LR: 0.012500
Training Epoch: 92 [19968/50000]	Loss: 0.1198	LR: 0.012500
Training Epoch: 92 [20096/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 92 [20224/50000]	Loss: 0.1199	LR: 0.012500
Training Epoch: 92 [20352/50000]	Loss: 0.0747	LR: 0.012500
Training Epoch: 92 [20480/50000]	Loss: 0.1674	LR: 0.012500
Training Epoch: 92 [20608/50000]	Loss: 0.1520	LR: 0.012500
Training Epoch: 92 [20736/50000]	Loss: 0.0872	LR: 0.012500
Training Epoch: 92 [20864/50000]	Loss: 0.1420	LR: 0.012500
Training Epoch: 92 [20992/50000]	Loss: 0.1303	LR: 0.012500
Training Epoch: 92 [21120/50000]	Loss: 0.1730	LR: 0.012500
Training Epoch: 92 [21248/50000]	Loss: 0.1533	LR: 0.012500
Training Epoch: 92 [21376/50000]	Loss: 0.0838	LR: 0.012500
Training Epoch: 92 [21504/50000]	Loss: 0.1562	LR: 0.012500
Training Epoch: 92 [21632/50000]	Loss: 0.1220	LR: 0.012500
Training Epoch: 92 [21760/50000]	Loss: 0.0938	LR: 0.012500
Training Epoch: 92 [21888/50000]	Loss: 0.1353	LR: 0.012500
Training Epoch: 92 [22016/50000]	Loss: 0.1209	LR: 0.012500
Training Epoch: 92 [22144/50000]	Loss: 0.1070	LR: 0.012500
Training Epoch: 92 [22272/50000]	Loss: 0.1276	LR: 0.012500
Training Epoch: 92 [22400/50000]	Loss: 0.1354	LR: 0.012500
Training Epoch: 92 [22528/50000]	Loss: 0.0663	LR: 0.012500
Training Epoch: 92 [22656/50000]	Loss: 0.1401	LR: 0.012500
Training Epoch: 92 [22784/50000]	Loss: 0.2281	LR: 0.012500
Training Epoch: 92 [22912/50000]	Loss: 0.0814	LR: 0.012500
Training Epoch: 92 [23040/50000]	Loss: 0.1736	LR: 0.012500
Training Epoch: 92 [23168/50000]	Loss: 0.1924	LR: 0.012500
Training Epoch: 92 [23296/50000]	Loss: 0.0966	LR: 0.012500
Training Epoch: 92 [23424/50000]	Loss: 0.1210	LR: 0.012500
Training Epoch: 92 [23552/50000]	Loss: 0.0692	LR: 0.012500
Training Epoch: 92 [23680/50000]	Loss: 0.1085	LR: 0.012500
Training Epoch: 92 [23808/50000]	Loss: 0.1777	LR: 0.012500
Training Epoch: 92 [23936/50000]	Loss: 0.1639	LR: 0.012500
Training Epoch: 92 [24064/50000]	Loss: 0.1409	LR: 0.012500
Training Epoch: 92 [24192/50000]	Loss: 0.1996	LR: 0.012500
Training Epoch: 92 [24320/50000]	Loss: 0.1608	LR: 0.012500
Training Epoch: 92 [24448/50000]	Loss: 0.1171	LR: 0.012500
Training Epoch: 92 [24576/50000]	Loss: 0.0967	LR: 0.012500
Training Epoch: 92 [24704/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 92 [24832/50000]	Loss: 0.1264	LR: 0.012500
Training Epoch: 92 [24960/50000]	Loss: 0.2421	LR: 0.012500
Training Epoch: 92 [25088/50000]	Loss: 0.1391	LR: 0.012500
Training Epoch: 92 [25216/50000]	Loss: 0.0681	LR: 0.012500
Training Epoch: 92 [25344/50000]	Loss: 0.1266	LR: 0.012500
Training Epoch: 92 [25472/50000]	Loss: 0.1134	LR: 0.012500
Training Epoch: 92 [25600/50000]	Loss: 0.0997	LR: 0.012500
Training Epoch: 92 [25728/50000]	Loss: 0.1228	LR: 0.012500
Training Epoch: 92 [25856/50000]	Loss: 0.1303	LR: 0.012500
Training Epoch: 92 [25984/50000]	Loss: 0.1357	LR: 0.012500
Training Epoch: 92 [26112/50000]	Loss: 0.1221	LR: 0.012500
Training Epoch: 92 [26240/50000]	Loss: 0.1809	LR: 0.012500
Training Epoch: 92 [26368/50000]	Loss: 0.1417	LR: 0.012500
Training Epoch: 92 [26496/50000]	Loss: 0.1356	LR: 0.012500
Training Epoch: 92 [26624/50000]	Loss: 0.1367	LR: 0.012500
Training Epoch: 92 [26752/50000]	Loss: 0.0882	LR: 0.012500
Training Epoch: 92 [26880/50000]	Loss: 0.0838	LR: 0.012500
Training Epoch: 92 [27008/50000]	Loss: 0.2141	LR: 0.012500
Training Epoch: 92 [27136/50000]	Loss: 0.1693	LR: 0.012500
Training Epoch: 92 [27264/50000]	Loss: 0.1636	LR: 0.012500
Training Epoch: 92 [27392/50000]	Loss: 0.0800	LR: 0.012500
Training Epoch: 92 [27520/50000]	Loss: 0.1341	LR: 0.012500
Training Epoch: 92 [27648/50000]	Loss: 0.1118	LR: 0.012500
Training Epoch: 92 [27776/50000]	Loss: 0.1727	LR: 0.012500
Training Epoch: 92 [27904/50000]	Loss: 0.1389	LR: 0.012500
Training Epoch: 92 [28032/50000]	Loss: 0.1148	LR: 0.012500
Training Epoch: 92 [28160/50000]	Loss: 0.1305	LR: 0.012500
Training Epoch: 92 [28288/50000]	Loss: 0.1326	LR: 0.012500
Training Epoch: 92 [28416/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 92 [28544/50000]	Loss: 0.1300	LR: 0.012500
Training Epoch: 92 [28672/50000]	Loss: 0.1517	LR: 0.012500
Training Epoch: 92 [28800/50000]	Loss: 0.1802	LR: 0.012500
Training Epoch: 92 [28928/50000]	Loss: 0.1819	LR: 0.012500
Training Epoch: 92 [29056/50000]	Loss: 0.1397	LR: 0.012500
Training Epoch: 92 [29184/50000]	Loss: 0.1288	LR: 0.012500
Training Epoch: 92 [29312/50000]	Loss: 0.1336	LR: 0.012500
Training Epoch: 92 [29440/50000]	Loss: 0.1746	LR: 0.012500
Training Epoch: 92 [29568/50000]	Loss: 0.1798	LR: 0.012500
Training Epoch: 92 [29696/50000]	Loss: 0.1370	LR: 0.012500
Training Epoch: 92 [29824/50000]	Loss: 0.1766	LR: 0.012500
Training Epoch: 92 [29952/50000]	Loss: 0.1567	LR: 0.012500
Training Epoch: 92 [30080/50000]	Loss: 0.1946	LR: 0.012500
Training Epoch: 92 [30208/50000]	Loss: 0.1371	LR: 0.012500
Training Epoch: 92 [30336/50000]	Loss: 0.1102	LR: 0.012500
Training Epoch: 92 [30464/50000]	Loss: 0.1735	LR: 0.012500
Training Epoch: 92 [30592/50000]	Loss: 0.1711	LR: 0.012500
Training Epoch: 92 [30720/50000]	Loss: 0.0836	LR: 0.012500
Training Epoch: 92 [30848/50000]	Loss: 0.1964	LR: 0.012500
Training Epoch: 92 [30976/50000]	Loss: 0.2245	LR: 0.012500
Training Epoch: 92 [31104/50000]	Loss: 0.1922	LR: 0.012500
Training Epoch: 92 [31232/50000]	Loss: 0.1708	LR: 0.012500
Training Epoch: 92 [31360/50000]	Loss: 0.1725	LR: 0.012500
Training Epoch: 92 [31488/50000]	Loss: 0.0683	LR: 0.012500
Training Epoch: 92 [31616/50000]	Loss: 0.1257	LR: 0.012500
Training Epoch: 92 [31744/50000]	Loss: 0.1607	LR: 0.012500
Training Epoch: 92 [31872/50000]	Loss: 0.1314	LR: 0.012500
Training Epoch: 92 [32000/50000]	Loss: 0.1515	LR: 0.012500
Training Epoch: 92 [32128/50000]	Loss: 0.1049	LR: 0.012500
Training Epoch: 92 [32256/50000]	Loss: 0.1130	LR: 0.012500
Training Epoch: 92 [32384/50000]	Loss: 0.1586	LR: 0.012500
Training Epoch: 92 [32512/50000]	Loss: 0.1760	LR: 0.012500
Training Epoch: 92 [32640/50000]	Loss: 0.1240	LR: 0.012500
Training Epoch: 92 [32768/50000]	Loss: 0.1688	LR: 0.012500
Training Epoch: 92 [32896/50000]	Loss: 0.1469	LR: 0.012500
Training Epoch: 92 [33024/50000]	Loss: 0.1641	LR: 0.012500
Training Epoch: 92 [33152/50000]	Loss: 0.1940	LR: 0.012500
Training Epoch: 92 [33280/50000]	Loss: 0.1941	LR: 0.012500
Training Epoch: 92 [33408/50000]	Loss: 0.1039	LR: 0.012500
Training Epoch: 92 [33536/50000]	Loss: 0.2077	LR: 0.012500
Training Epoch: 92 [33664/50000]	Loss: 0.1621	LR: 0.012500
Training Epoch: 92 [33792/50000]	Loss: 0.2074	LR: 0.012500
Training Epoch: 92 [33920/50000]	Loss: 0.1183	LR: 0.012500
Training Epoch: 92 [34048/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 92 [34176/50000]	Loss: 0.1974	LR: 0.012500
Training Epoch: 92 [34304/50000]	Loss: 0.1038	LR: 0.012500
Training Epoch: 92 [34432/50000]	Loss: 0.1283	LR: 0.012500
Training Epoch: 92 [34560/50000]	Loss: 0.2030	LR: 0.012500
Training Epoch: 92 [34688/50000]	Loss: 0.1328	LR: 0.012500
Training Epoch: 92 [34816/50000]	Loss: 0.1474	LR: 0.012500
Training Epoch: 92 [34944/50000]	Loss: 0.0982	LR: 0.012500
Training Epoch: 92 [35072/50000]	Loss: 0.2199	LR: 0.012500
Training Epoch: 92 [35200/50000]	Loss: 0.1194	LR: 0.012500
Training Epoch: 92 [35328/50000]	Loss: 0.1359	LR: 0.012500
Training Epoch: 92 [35456/50000]	Loss: 0.1698	LR: 0.012500
Training Epoch: 92 [35584/50000]	Loss: 0.1411	LR: 0.012500
Training Epoch: 92 [35712/50000]	Loss: 0.2644	LR: 0.012500
Training Epoch: 92 [35840/50000]	Loss: 0.2198	LR: 0.012500
Training Epoch: 92 [35968/50000]	Loss: 0.1686	LR: 0.012500
Training Epoch: 92 [36096/50000]	Loss: 0.1146	LR: 0.012500
Training Epoch: 92 [36224/50000]	Loss: 0.1517	LR: 0.012500
Training Epoch: 92 [36352/50000]	Loss: 0.1753	LR: 0.012500
Training Epoch: 92 [36480/50000]	Loss: 0.1782	LR: 0.012500
Training Epoch: 92 [36608/50000]	Loss: 0.1247	LR: 0.012500
Training Epoch: 92 [36736/50000]	Loss: 0.1312	LR: 0.012500
Training Epoch: 92 [36864/50000]	Loss: 0.2092	LR: 0.012500
Training Epoch: 92 [36992/50000]	Loss: 0.1712	LR: 0.012500
Training Epoch: 92 [37120/50000]	Loss: 0.1374	LR: 0.012500
Training Epoch: 92 [37248/50000]	Loss: 0.1924	LR: 0.012500
Training Epoch: 92 [37376/50000]	Loss: 0.1274	LR: 0.012500
Training Epoch: 92 [37504/50000]	Loss: 0.1159	LR: 0.012500
Training Epoch: 92 [37632/50000]	Loss: 0.1757	LR: 0.012500
Training Epoch: 92 [37760/50000]	Loss: 0.2247	LR: 0.012500
Training Epoch: 92 [37888/50000]	Loss: 0.1144	LR: 0.012500
Training Epoch: 92 [38016/50000]	Loss: 0.1767	LR: 0.012500
Training Epoch: 92 [38144/50000]	Loss: 0.1536	LR: 0.012500
Training Epoch: 92 [38272/50000]	Loss: 0.1078	LR: 0.012500
Training Epoch: 92 [38400/50000]	Loss: 0.1890	LR: 0.012500
Training Epoch: 92 [38528/50000]	Loss: 0.1623	LR: 0.012500
Training Epoch: 92 [38656/50000]	Loss: 0.1411	LR: 0.012500
Training Epoch: 92 [38784/50000]	Loss: 0.2036	LR: 0.012500
Training Epoch: 92 [38912/50000]	Loss: 0.1444	LR: 0.012500
Training Epoch: 92 [39040/50000]	Loss: 0.1150	LR: 0.012500
Training Epoch: 92 [39168/50000]	Loss: 0.2268	LR: 0.012500
Training Epoch: 92 [39296/50000]	Loss: 0.0690	LR: 0.012500
Training Epoch: 92 [39424/50000]	Loss: 0.1000	LR: 0.012500
Training Epoch: 92 [39552/50000]	Loss: 0.1744	LR: 0.012500
Training Epoch: 92 [39680/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 92 [39808/50000]	Loss: 0.1590	LR: 0.012500
Training Epoch: 92 [39936/50000]	Loss: 0.2093	LR: 0.012500
Training Epoch: 92 [40064/50000]	Loss: 0.2949	LR: 0.012500
Training Epoch: 92 [40192/50000]	Loss: 0.1249	LR: 0.012500
Training Epoch: 92 [40320/50000]	Loss: 0.1615	LR: 0.012500
Training Epoch: 92 [40448/50000]	Loss: 0.1750	LR: 0.012500
Training Epoch: 92 [40576/50000]	Loss: 0.1729	LR: 0.012500
Training Epoch: 92 [40704/50000]	Loss: 0.1608	LR: 0.012500
Training Epoch: 92 [40832/50000]	Loss: 0.1486	LR: 0.012500
Training Epoch: 92 [40960/50000]	Loss: 0.0906	LR: 0.012500
Training Epoch: 92 [41088/50000]	Loss: 0.1165	LR: 0.012500
Training Epoch: 92 [41216/50000]	Loss: 0.1787	LR: 0.012500
Training Epoch: 92 [41344/50000]	Loss: 0.1375	LR: 0.012500
Training Epoch: 92 [41472/50000]	Loss: 0.0859	LR: 0.012500
Training Epoch: 92 [41600/50000]	Loss: 0.1370	LR: 0.012500
Training Epoch: 92 [41728/50000]	Loss: 0.1356	LR: 0.012500
Training Epoch: 92 [41856/50000]	Loss: 0.1176	LR: 0.012500
Training Epoch: 92 [41984/50000]	Loss: 0.1751	LR: 0.012500
Training Epoch: 92 [42112/50000]	Loss: 0.2699	LR: 0.012500
Training Epoch: 92 [42240/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 92 [42368/50000]	Loss: 0.2410	LR: 0.012500
Training Epoch: 92 [42496/50000]	Loss: 0.1376	LR: 0.012500
Training Epoch: 92 [42624/50000]	Loss: 0.1515	LR: 0.012500
Training Epoch: 92 [42752/50000]	Loss: 0.2497	LR: 0.012500
Training Epoch: 92 [42880/50000]	Loss: 0.2424	LR: 0.012500
Training Epoch: 92 [43008/50000]	Loss: 0.1849	LR: 0.012500
Training Epoch: 92 [43136/50000]	Loss: 0.1523	LR: 0.012500
Training Epoch: 92 [43264/50000]	Loss: 0.0994	LR: 0.012500
Training Epoch: 92 [43392/50000]	Loss: 0.1346	LR: 0.012500
Training Epoch: 92 [43520/50000]	Loss: 0.2119	LR: 0.012500
Training Epoch: 92 [43648/50000]	Loss: 0.2124	LR: 0.012500
Training Epoch: 92 [43776/50000]	Loss: 0.1646	LR: 0.012500
Training Epoch: 92 [43904/50000]	Loss: 0.2328	LR: 0.012500
Training Epoch: 92 [44032/50000]	Loss: 0.1478	LR: 0.012500
Training Epoch: 92 [44160/50000]	Loss: 0.1622	LR: 0.012500
Training Epoch: 92 [44288/50000]	Loss: 0.1501	LR: 0.012500
Training Epoch: 92 [44416/50000]	Loss: 0.1414	LR: 0.012500
Training Epoch: 92 [44544/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 92 [44672/50000]	Loss: 0.2117	LR: 0.012500
Training Epoch: 92 [44800/50000]	Loss: 0.1300	LR: 0.012500
Training Epoch: 92 [44928/50000]	Loss: 0.2003	LR: 0.012500
Training Epoch: 92 [45056/50000]	Loss: 0.1823	LR: 0.012500
Training Epoch: 92 [45184/50000]	Loss: 0.2325	LR: 0.012500
Training Epoch: 92 [45312/50000]	Loss: 0.1432	LR: 0.012500
Training Epoch: 92 [45440/50000]	Loss: 0.1820	LR: 0.012500
Training Epoch: 92 [45568/50000]	Loss: 0.1416	LR: 0.012500
Training Epoch: 92 [45696/50000]	Loss: 0.1520	LR: 0.012500
Training Epoch: 92 [45824/50000]	Loss: 0.1821	LR: 0.012500
Training Epoch: 92 [45952/50000]	Loss: 0.1445	LR: 0.012500
Training Epoch: 92 [46080/50000]	Loss: 0.2188	LR: 0.012500
Training Epoch: 92 [46208/50000]	Loss: 0.1851	LR: 0.012500
Training Epoch: 92 [46336/50000]	Loss: 0.1148	LR: 0.012500
Training Epoch: 92 [46464/50000]	Loss: 0.1445	LR: 0.012500
Training Epoch: 92 [46592/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 92 [46720/50000]	Loss: 0.1504	LR: 0.012500
Training Epoch: 92 [46848/50000]	Loss: 0.2781	LR: 0.012500
Training Epoch: 92 [46976/50000]	Loss: 0.1317	LR: 0.012500
Training Epoch: 92 [47104/50000]	Loss: 0.2304	LR: 0.012500
Training Epoch: 92 [47232/50000]	Loss: 0.1828	LR: 0.012500
Training Epoch: 92 [47360/50000]	Loss: 0.1619	LR: 0.012500
Training Epoch: 92 [47488/50000]	Loss: 0.2375	LR: 0.012500
Training Epoch: 92 [47616/50000]	Loss: 0.1630	LR: 0.012500
Training Epoch: 92 [47744/50000]	Loss: 0.1694	LR: 0.012500
Training Epoch: 92 [47872/50000]	Loss: 0.1784	LR: 0.012500
Training Epoch: 92 [48000/50000]	Loss: 0.2052	LR: 0.012500
Training Epoch: 92 [48128/50000]	Loss: 0.1335	LR: 0.012500
Training Epoch: 92 [48256/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 92 [48384/50000]	Loss: 0.2416	LR: 0.012500
Training Epoch: 92 [48512/50000]	Loss: 0.0963	LR: 0.012500
Training Epoch: 92 [48640/50000]	Loss: 0.1403	LR: 0.012500
Training Epoch: 92 [48768/50000]	Loss: 0.2681	LR: 0.012500
Training Epoch: 92 [48896/50000]	Loss: 0.1911	LR: 0.012500
Training Epoch: 92 [49024/50000]	Loss: 0.1887	LR: 0.012500
Training Epoch: 92 [49152/50000]	Loss: 0.1630	LR: 0.012500
Training Epoch: 92 [49280/50000]	Loss: 0.1351	LR: 0.012500
Training Epoch: 92 [49408/50000]	Loss: 0.1060	LR: 0.012500
Training Epoch: 92 [49536/50000]	Loss: 0.2716	LR: 0.012500
Training Epoch: 92 [49664/50000]	Loss: 0.2314	LR: 0.012500
Training Epoch: 92 [49792/50000]	Loss: 0.1041	LR: 0.012500
Training Epoch: 92 [49920/50000]	Loss: 0.1773	LR: 0.012500
Training Epoch: 92 [50000/50000]	Loss: 0.2332	LR: 0.012500
epoch 92 training time consumed: 53.89s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  337645 GB |  337645 GB |
|       from large pool |  123392 KB |    1034 MB |  337312 GB |  337312 GB |
|       from small pool |   10798 KB |      13 MB |     332 GB |     332 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  337645 GB |  337645 GB |
|       from large pool |  123392 KB |    1034 MB |  337312 GB |  337312 GB |
|       from small pool |   10798 KB |      13 MB |     332 GB |     332 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  148583 GB |  148583 GB |
|       from large pool |  155136 KB |  433088 KB |  148215 GB |  148215 GB |
|       from small pool |    1489 KB |    3494 KB |     367 GB |     367 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   13028 K  |   13028 K  |
|       from large pool |      24    |      65    |    6800 K  |    6800 K  |
|       from small pool |     232    |     275    |    6227 K  |    6227 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   13028 K  |   13028 K  |
|       from large pool |      24    |      65    |    6800 K  |    6800 K  |
|       from small pool |     232    |     275    |    6227 K  |    6227 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6460 K  |    6460 K  |
|       from large pool |       9    |      14    |    3291 K  |    3291 K  |
|       from small pool |      12    |      17    |    3168 K  |    3168 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 92, Average loss: 0.0104, Accuracy: 0.6877, Time consumed:3.48s

Training Epoch: 93 [128/50000]	Loss: 0.0953	LR: 0.012500
Training Epoch: 93 [256/50000]	Loss: 0.1158	LR: 0.012500
Training Epoch: 93 [384/50000]	Loss: 0.1988	LR: 0.012500
Training Epoch: 93 [512/50000]	Loss: 0.1260	LR: 0.012500
Training Epoch: 93 [640/50000]	Loss: 0.1158	LR: 0.012500
Training Epoch: 93 [768/50000]	Loss: 0.2119	LR: 0.012500
Training Epoch: 93 [896/50000]	Loss: 0.1561	LR: 0.012500
Training Epoch: 93 [1024/50000]	Loss: 0.1973	LR: 0.012500
Training Epoch: 93 [1152/50000]	Loss: 0.1844	LR: 0.012500
Training Epoch: 93 [1280/50000]	Loss: 0.1465	LR: 0.012500
Training Epoch: 93 [1408/50000]	Loss: 0.1435	LR: 0.012500
Training Epoch: 93 [1536/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 93 [1664/50000]	Loss: 0.1871	LR: 0.012500
Training Epoch: 93 [1792/50000]	Loss: 0.1362	LR: 0.012500
Training Epoch: 93 [1920/50000]	Loss: 0.1535	LR: 0.012500
Training Epoch: 93 [2048/50000]	Loss: 0.0924	LR: 0.012500
Training Epoch: 93 [2176/50000]	Loss: 0.1131	LR: 0.012500
Training Epoch: 93 [2304/50000]	Loss: 0.0803	LR: 0.012500
Training Epoch: 93 [2432/50000]	Loss: 0.1386	LR: 0.012500
Training Epoch: 93 [2560/50000]	Loss: 0.2132	LR: 0.012500
Training Epoch: 93 [2688/50000]	Loss: 0.1309	LR: 0.012500
Training Epoch: 93 [2816/50000]	Loss: 0.1224	LR: 0.012500
Training Epoch: 93 [2944/50000]	Loss: 0.0998	LR: 0.012500
Training Epoch: 93 [3072/50000]	Loss: 0.1551	LR: 0.012500
Training Epoch: 93 [3200/50000]	Loss: 0.2050	LR: 0.012500
Training Epoch: 93 [3328/50000]	Loss: 0.1448	LR: 0.012500
Training Epoch: 93 [3456/50000]	Loss: 0.1521	LR: 0.012500
Training Epoch: 93 [3584/50000]	Loss: 0.0963	LR: 0.012500
Training Epoch: 93 [3712/50000]	Loss: 0.1148	LR: 0.012500
Training Epoch: 93 [3840/50000]	Loss: 0.1074	LR: 0.012500
Training Epoch: 93 [3968/50000]	Loss: 0.1420	LR: 0.012500
Training Epoch: 93 [4096/50000]	Loss: 0.2019	LR: 0.012500
Training Epoch: 93 [4224/50000]	Loss: 0.1390	LR: 0.012500
Training Epoch: 93 [4352/50000]	Loss: 0.1194	LR: 0.012500
Training Epoch: 93 [4480/50000]	Loss: 0.1164	LR: 0.012500
Training Epoch: 93 [4608/50000]	Loss: 0.1318	LR: 0.012500
Training Epoch: 93 [4736/50000]	Loss: 0.1279	LR: 0.012500
Training Epoch: 93 [4864/50000]	Loss: 0.1458	LR: 0.012500
Training Epoch: 93 [4992/50000]	Loss: 0.1439	LR: 0.012500
Training Epoch: 93 [5120/50000]	Loss: 0.1732	LR: 0.012500
Training Epoch: 93 [5248/50000]	Loss: 0.1953	LR: 0.012500
Training Epoch: 93 [5376/50000]	Loss: 0.1516	LR: 0.012500
Training Epoch: 93 [5504/50000]	Loss: 0.1222	LR: 0.012500
Training Epoch: 93 [5632/50000]	Loss: 0.1653	LR: 0.012500
Training Epoch: 93 [5760/50000]	Loss: 0.2339	LR: 0.012500
Training Epoch: 93 [5888/50000]	Loss: 0.1425	LR: 0.012500
Training Epoch: 93 [6016/50000]	Loss: 0.1610	LR: 0.012500
Training Epoch: 93 [6144/50000]	Loss: 0.1329	LR: 0.012500
Training Epoch: 93 [6272/50000]	Loss: 0.1334	LR: 0.012500
Training Epoch: 93 [6400/50000]	Loss: 0.1706	LR: 0.012500
Training Epoch: 93 [6528/50000]	Loss: 0.1446	LR: 0.012500
Training Epoch: 93 [6656/50000]	Loss: 0.1062	LR: 0.012500
Training Epoch: 93 [6784/50000]	Loss: 0.1192	LR: 0.012500
Training Epoch: 93 [6912/50000]	Loss: 0.1226	LR: 0.012500
Training Epoch: 93 [7040/50000]	Loss: 0.1406	LR: 0.012500
Training Epoch: 93 [7168/50000]	Loss: 0.1674	LR: 0.012500
Training Epoch: 93 [7296/50000]	Loss: 0.0958	LR: 0.012500
Training Epoch: 93 [7424/50000]	Loss: 0.1163	LR: 0.012500
Training Epoch: 93 [7552/50000]	Loss: 0.1331	LR: 0.012500
Training Epoch: 93 [7680/50000]	Loss: 0.2123	LR: 0.012500
Training Epoch: 93 [7808/50000]	Loss: 0.1526	LR: 0.012500
Training Epoch: 93 [7936/50000]	Loss: 0.1383	LR: 0.012500
Training Epoch: 93 [8064/50000]	Loss: 0.1665	LR: 0.012500
Training Epoch: 93 [8192/50000]	Loss: 0.1324	LR: 0.012500
Training Epoch: 93 [8320/50000]	Loss: 0.0770	LR: 0.012500
Training Epoch: 93 [8448/50000]	Loss: 0.1407	LR: 0.012500
Training Epoch: 93 [8576/50000]	Loss: 0.1634	LR: 0.012500
Training Epoch: 93 [8704/50000]	Loss: 0.1521	LR: 0.012500
Training Epoch: 93 [8832/50000]	Loss: 0.1593	LR: 0.012500
Training Epoch: 93 [8960/50000]	Loss: 0.0782	LR: 0.012500
Training Epoch: 93 [9088/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 93 [9216/50000]	Loss: 0.1186	LR: 0.012500
Training Epoch: 93 [9344/50000]	Loss: 0.1445	LR: 0.012500
Training Epoch: 93 [9472/50000]	Loss: 0.1243	LR: 0.012500
Training Epoch: 93 [9600/50000]	Loss: 0.1859	LR: 0.012500
Training Epoch: 93 [9728/50000]	Loss: 0.1345	LR: 0.012500
Training Epoch: 93 [9856/50000]	Loss: 0.1526	LR: 0.012500
Training Epoch: 93 [9984/50000]	Loss: 0.1089	LR: 0.012500
Training Epoch: 93 [10112/50000]	Loss: 0.1057	LR: 0.012500
Training Epoch: 93 [10240/50000]	Loss: 0.1063	LR: 0.012500
Training Epoch: 93 [10368/50000]	Loss: 0.1708	LR: 0.012500
Training Epoch: 93 [10496/50000]	Loss: 0.0891	LR: 0.012500
Training Epoch: 93 [10624/50000]	Loss: 0.1167	LR: 0.012500
Training Epoch: 93 [10752/50000]	Loss: 0.0951	LR: 0.012500
Training Epoch: 93 [10880/50000]	Loss: 0.1346	LR: 0.012500
Training Epoch: 93 [11008/50000]	Loss: 0.0821	LR: 0.012500
Training Epoch: 93 [11136/50000]	Loss: 0.0641	LR: 0.012500
Training Epoch: 93 [11264/50000]	Loss: 0.1095	LR: 0.012500
Training Epoch: 93 [11392/50000]	Loss: 0.2328	LR: 0.012500
Training Epoch: 93 [11520/50000]	Loss: 0.1201	LR: 0.012500
Training Epoch: 93 [11648/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 93 [11776/50000]	Loss: 0.1456	LR: 0.012500
Training Epoch: 93 [11904/50000]	Loss: 0.0615	LR: 0.012500
Training Epoch: 93 [12032/50000]	Loss: 0.0884	LR: 0.012500
Training Epoch: 93 [12160/50000]	Loss: 0.1410	LR: 0.012500
Training Epoch: 93 [12288/50000]	Loss: 0.1569	LR: 0.012500
Training Epoch: 93 [12416/50000]	Loss: 0.1486	LR: 0.012500
Training Epoch: 93 [12544/50000]	Loss: 0.1329	LR: 0.012500
Training Epoch: 93 [12672/50000]	Loss: 0.2180	LR: 0.012500
Training Epoch: 93 [12800/50000]	Loss: 0.1164	LR: 0.012500
Training Epoch: 93 [12928/50000]	Loss: 0.1314	LR: 0.012500
Training Epoch: 93 [13056/50000]	Loss: 0.1384	LR: 0.012500
Training Epoch: 93 [13184/50000]	Loss: 0.0977	LR: 0.012500
Training Epoch: 93 [13312/50000]	Loss: 0.1568	LR: 0.012500
Training Epoch: 93 [13440/50000]	Loss: 0.1137	LR: 0.012500
Training Epoch: 93 [13568/50000]	Loss: 0.0876	LR: 0.012500
Training Epoch: 93 [13696/50000]	Loss: 0.1375	LR: 0.012500
Training Epoch: 93 [13824/50000]	Loss: 0.1181	LR: 0.012500
Training Epoch: 93 [13952/50000]	Loss: 0.1028	LR: 0.012500
Training Epoch: 93 [14080/50000]	Loss: 0.1264	LR: 0.012500
Training Epoch: 93 [14208/50000]	Loss: 0.2302	LR: 0.012500
Training Epoch: 93 [14336/50000]	Loss: 0.0875	LR: 0.012500
Training Epoch: 93 [14464/50000]	Loss: 0.1417	LR: 0.012500
Training Epoch: 93 [14592/50000]	Loss: 0.0883	LR: 0.012500
Training Epoch: 93 [14720/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 93 [14848/50000]	Loss: 0.1230	LR: 0.012500
Training Epoch: 93 [14976/50000]	Loss: 0.1449	LR: 0.012500
Training Epoch: 93 [15104/50000]	Loss: 0.1526	LR: 0.012500
Training Epoch: 93 [15232/50000]	Loss: 0.1083	LR: 0.012500
Training Epoch: 93 [15360/50000]	Loss: 0.1858	LR: 0.012500
Training Epoch: 93 [15488/50000]	Loss: 0.1893	LR: 0.012500
Training Epoch: 93 [15616/50000]	Loss: 0.1125	LR: 0.012500
Training Epoch: 93 [15744/50000]	Loss: 0.1306	LR: 0.012500
Training Epoch: 93 [15872/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 93 [16000/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 93 [16128/50000]	Loss: 0.0962	LR: 0.012500
Training Epoch: 93 [16256/50000]	Loss: 0.1321	LR: 0.012500
Training Epoch: 93 [16384/50000]	Loss: 0.0764	LR: 0.012500
Training Epoch: 93 [16512/50000]	Loss: 0.1656	LR: 0.012500
Training Epoch: 93 [16640/50000]	Loss: 0.1254	LR: 0.012500
Training Epoch: 93 [16768/50000]	Loss: 0.1664	LR: 0.012500
Training Epoch: 93 [16896/50000]	Loss: 0.1277	LR: 0.012500
Training Epoch: 93 [17024/50000]	Loss: 0.1362	LR: 0.012500
Training Epoch: 93 [17152/50000]	Loss: 0.1257	LR: 0.012500
Training Epoch: 93 [17280/50000]	Loss: 0.1611	LR: 0.012500
Training Epoch: 93 [17408/50000]	Loss: 0.1542	LR: 0.012500
Training Epoch: 93 [17536/50000]	Loss: 0.1184	LR: 0.012500
Training Epoch: 93 [17664/50000]	Loss: 0.1374	LR: 0.012500
Training Epoch: 93 [17792/50000]	Loss: 0.1204	LR: 0.012500
Training Epoch: 93 [17920/50000]	Loss: 0.1147	LR: 0.012500
Training Epoch: 93 [18048/50000]	Loss: 0.1077	LR: 0.012500
Training Epoch: 93 [18176/50000]	Loss: 0.1724	LR: 0.012500
Training Epoch: 93 [18304/50000]	Loss: 0.2116	LR: 0.012500
Training Epoch: 93 [18432/50000]	Loss: 0.2057	LR: 0.012500
Training Epoch: 93 [18560/50000]	Loss: 0.1552	LR: 0.012500
Training Epoch: 93 [18688/50000]	Loss: 0.1841	LR: 0.012500
Training Epoch: 93 [18816/50000]	Loss: 0.0911	LR: 0.012500
Training Epoch: 93 [18944/50000]	Loss: 0.0882	LR: 0.012500
Training Epoch: 93 [19072/50000]	Loss: 0.1282	LR: 0.012500
Training Epoch: 93 [19200/50000]	Loss: 0.1228	LR: 0.012500
Training Epoch: 93 [19328/50000]	Loss: 0.1885	LR: 0.012500
Training Epoch: 93 [19456/50000]	Loss: 0.2244	LR: 0.012500
Training Epoch: 93 [19584/50000]	Loss: 0.1128	LR: 0.012500
Training Epoch: 93 [19712/50000]	Loss: 0.1920	LR: 0.012500
Training Epoch: 93 [19840/50000]	Loss: 0.1816	LR: 0.012500
Training Epoch: 93 [19968/50000]	Loss: 0.1021	LR: 0.012500
Training Epoch: 93 [20096/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 93 [20224/50000]	Loss: 0.1365	LR: 0.012500
Training Epoch: 93 [20352/50000]	Loss: 0.1210	LR: 0.012500
Training Epoch: 93 [20480/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 93 [20608/50000]	Loss: 0.1009	LR: 0.012500
Training Epoch: 93 [20736/50000]	Loss: 0.1055	LR: 0.012500
Training Epoch: 93 [20864/50000]	Loss: 0.1028	LR: 0.012500
Training Epoch: 93 [20992/50000]	Loss: 0.1528	LR: 0.012500
Training Epoch: 93 [21120/50000]	Loss: 0.1506	LR: 0.012500
Training Epoch: 93 [21248/50000]	Loss: 0.2403	LR: 0.012500
Training Epoch: 93 [21376/50000]	Loss: 0.1797	LR: 0.012500
Training Epoch: 93 [21504/50000]	Loss: 0.1493	LR: 0.012500
Training Epoch: 93 [21632/50000]	Loss: 0.1789	LR: 0.012500
Training Epoch: 93 [21760/50000]	Loss: 0.2530	LR: 0.012500
Training Epoch: 93 [21888/50000]	Loss: 0.1462	LR: 0.012500
Training Epoch: 93 [22016/50000]	Loss: 0.1111	LR: 0.012500
Training Epoch: 93 [22144/50000]	Loss: 0.1326	LR: 0.012500
Training Epoch: 93 [22272/50000]	Loss: 0.1100	LR: 0.012500
Training Epoch: 93 [22400/50000]	Loss: 0.2168	LR: 0.012500
Training Epoch: 93 [22528/50000]	Loss: 0.1180	LR: 0.012500
Training Epoch: 93 [22656/50000]	Loss: 0.2681	LR: 0.012500
Training Epoch: 93 [22784/50000]	Loss: 0.1098	LR: 0.012500
Training Epoch: 93 [22912/50000]	Loss: 0.1618	LR: 0.012500
Training Epoch: 93 [23040/50000]	Loss: 0.1626	LR: 0.012500
Training Epoch: 93 [23168/50000]	Loss: 0.1421	LR: 0.012500
Training Epoch: 93 [23296/50000]	Loss: 0.1653	LR: 0.012500
Training Epoch: 93 [23424/50000]	Loss: 0.1567	LR: 0.012500
Training Epoch: 93 [23552/50000]	Loss: 0.1607	LR: 0.012500
Training Epoch: 93 [23680/50000]	Loss: 0.0990	LR: 0.012500
Training Epoch: 93 [23808/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 93 [23936/50000]	Loss: 0.1019	LR: 0.012500
Training Epoch: 93 [24064/50000]	Loss: 0.1948	LR: 0.012500
Training Epoch: 93 [24192/50000]	Loss: 0.1047	LR: 0.012500
Training Epoch: 93 [24320/50000]	Loss: 0.1399	LR: 0.012500
Training Epoch: 93 [24448/50000]	Loss: 0.1917	LR: 0.012500
Training Epoch: 93 [24576/50000]	Loss: 0.1461	LR: 0.012500
Training Epoch: 93 [24704/50000]	Loss: 0.1676	LR: 0.012500
Training Epoch: 93 [24832/50000]	Loss: 0.0880	LR: 0.012500
Training Epoch: 93 [24960/50000]	Loss: 0.0866	LR: 0.012500
Training Epoch: 93 [25088/50000]	Loss: 0.2220	LR: 0.012500
Training Epoch: 93 [25216/50000]	Loss: 0.0949	LR: 0.012500
Training Epoch: 93 [25344/50000]	Loss: 0.1354	LR: 0.012500
Training Epoch: 93 [25472/50000]	Loss: 0.1209	LR: 0.012500
Training Epoch: 93 [25600/50000]	Loss: 0.1703	LR: 0.012500
Training Epoch: 93 [25728/50000]	Loss: 0.1292	LR: 0.012500
Training Epoch: 93 [25856/50000]	Loss: 0.1245	LR: 0.012500
Training Epoch: 93 [25984/50000]	Loss: 0.1569	LR: 0.012500
Training Epoch: 93 [26112/50000]	Loss: 0.1453	LR: 0.012500
Training Epoch: 93 [26240/50000]	Loss: 0.1216	LR: 0.012500
Training Epoch: 93 [26368/50000]	Loss: 0.1729	LR: 0.012500
Training Epoch: 93 [26496/50000]	Loss: 0.1237	LR: 0.012500
Training Epoch: 93 [26624/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 93 [26752/50000]	Loss: 0.1469	LR: 0.012500
Training Epoch: 93 [26880/50000]	Loss: 0.1205	LR: 0.012500
Training Epoch: 93 [27008/50000]	Loss: 0.1707	LR: 0.012500
Training Epoch: 93 [27136/50000]	Loss: 0.2439	LR: 0.012500
Training Epoch: 93 [27264/50000]	Loss: 0.1167	LR: 0.012500
Training Epoch: 93 [27392/50000]	Loss: 0.1594	LR: 0.012500
Training Epoch: 93 [27520/50000]	Loss: 0.2124	LR: 0.012500
Training Epoch: 93 [27648/50000]	Loss: 0.1494	LR: 0.012500
Training Epoch: 93 [27776/50000]	Loss: 0.1575	LR: 0.012500
Training Epoch: 93 [27904/50000]	Loss: 0.0949	LR: 0.012500
Training Epoch: 93 [28032/50000]	Loss: 0.1289	LR: 0.012500
Training Epoch: 93 [28160/50000]	Loss: 0.0869	LR: 0.012500
Training Epoch: 93 [28288/50000]	Loss: 0.2373	LR: 0.012500
Training Epoch: 93 [28416/50000]	Loss: 0.1641	LR: 0.012500
Training Epoch: 93 [28544/50000]	Loss: 0.0985	LR: 0.012500
Training Epoch: 93 [28672/50000]	Loss: 0.2189	LR: 0.012500
Training Epoch: 93 [28800/50000]	Loss: 0.2408	LR: 0.012500
Training Epoch: 93 [28928/50000]	Loss: 0.1787	LR: 0.012500
Training Epoch: 93 [29056/50000]	Loss: 0.1643	LR: 0.012500
Training Epoch: 93 [29184/50000]	Loss: 0.1424	LR: 0.012500
Training Epoch: 93 [29312/50000]	Loss: 0.2129	LR: 0.012500
Training Epoch: 93 [29440/50000]	Loss: 0.1941	LR: 0.012500
Training Epoch: 93 [29568/50000]	Loss: 0.1461	LR: 0.012500
Training Epoch: 93 [29696/50000]	Loss: 0.1538	LR: 0.012500
Training Epoch: 93 [29824/50000]	Loss: 0.1678	LR: 0.012500
Training Epoch: 93 [29952/50000]	Loss: 0.2854	LR: 0.012500
Training Epoch: 93 [30080/50000]	Loss: 0.1311	LR: 0.012500
Training Epoch: 93 [30208/50000]	Loss: 0.1847	LR: 0.012500
Training Epoch: 93 [30336/50000]	Loss: 0.1333	LR: 0.012500
Training Epoch: 93 [30464/50000]	Loss: 0.2120	LR: 0.012500
Training Epoch: 93 [30592/50000]	Loss: 0.1559	LR: 0.012500
Training Epoch: 93 [30720/50000]	Loss: 0.1852	LR: 0.012500
Training Epoch: 93 [30848/50000]	Loss: 0.2372	LR: 0.012500
Training Epoch: 93 [30976/50000]	Loss: 0.1705	LR: 0.012500
Training Epoch: 93 [31104/50000]	Loss: 0.1656	LR: 0.012500
Training Epoch: 93 [31232/50000]	Loss: 0.1326	LR: 0.012500
Training Epoch: 93 [31360/50000]	Loss: 0.1378	LR: 0.012500
Training Epoch: 93 [31488/50000]	Loss: 0.0995	LR: 0.012500
Training Epoch: 93 [31616/50000]	Loss: 0.2181	LR: 0.012500
Training Epoch: 93 [31744/50000]	Loss: 0.1332	LR: 0.012500
Training Epoch: 93 [31872/50000]	Loss: 0.1420	LR: 0.012500
Training Epoch: 93 [32000/50000]	Loss: 0.1752	LR: 0.012500
Training Epoch: 93 [32128/50000]	Loss: 0.1508	LR: 0.012500
Training Epoch: 93 [32256/50000]	Loss: 0.1051	LR: 0.012500
Training Epoch: 93 [32384/50000]	Loss: 0.1315	LR: 0.012500
Training Epoch: 93 [32512/50000]	Loss: 0.2433	LR: 0.012500
Training Epoch: 93 [32640/50000]	Loss: 0.1679	LR: 0.012500
Training Epoch: 93 [32768/50000]	Loss: 0.1619	LR: 0.012500
Training Epoch: 93 [32896/50000]	Loss: 0.1816	LR: 0.012500
Training Epoch: 93 [33024/50000]	Loss: 0.1290	LR: 0.012500
Training Epoch: 93 [33152/50000]	Loss: 0.1671	LR: 0.012500
Training Epoch: 93 [33280/50000]	Loss: 0.1770	LR: 0.012500
Training Epoch: 93 [33408/50000]	Loss: 0.1742	LR: 0.012500
Training Epoch: 93 [33536/50000]	Loss: 0.1945	LR: 0.012500
Training Epoch: 93 [33664/50000]	Loss: 0.2167	LR: 0.012500
Training Epoch: 93 [33792/50000]	Loss: 0.1585	LR: 0.012500
Training Epoch: 93 [33920/50000]	Loss: 0.2271	LR: 0.012500
Training Epoch: 93 [34048/50000]	Loss: 0.1001	LR: 0.012500
Training Epoch: 93 [34176/50000]	Loss: 0.1499	LR: 0.012500
Training Epoch: 93 [34304/50000]	Loss: 0.2207	LR: 0.012500
Training Epoch: 93 [34432/50000]	Loss: 0.1618	LR: 0.012500
Training Epoch: 93 [34560/50000]	Loss: 0.1499	LR: 0.012500
Training Epoch: 93 [34688/50000]	Loss: 0.1714	LR: 0.012500
Training Epoch: 93 [34816/50000]	Loss: 0.1861	LR: 0.012500
Training Epoch: 93 [34944/50000]	Loss: 0.1378	LR: 0.012500
Training Epoch: 93 [35072/50000]	Loss: 0.2385	LR: 0.012500
Training Epoch: 93 [35200/50000]	Loss: 0.1946	LR: 0.012500
Training Epoch: 93 [35328/50000]	Loss: 0.1394	LR: 0.012500
Training Epoch: 93 [35456/50000]	Loss: 0.1376	LR: 0.012500
Training Epoch: 93 [35584/50000]	Loss: 0.2023	LR: 0.012500
Training Epoch: 93 [35712/50000]	Loss: 0.0816	LR: 0.012500
Training Epoch: 93 [35840/50000]	Loss: 0.1753	LR: 0.012500
Training Epoch: 93 [35968/50000]	Loss: 0.2107	LR: 0.012500
Training Epoch: 93 [36096/50000]	Loss: 0.1818	LR: 0.012500
Training Epoch: 93 [36224/50000]	Loss: 0.2053	LR: 0.012500
Training Epoch: 93 [36352/50000]	Loss: 0.1900	LR: 0.012500
Training Epoch: 93 [36480/50000]	Loss: 0.2287	LR: 0.012500
Training Epoch: 93 [36608/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 93 [36736/50000]	Loss: 0.2312	LR: 0.012500
Training Epoch: 93 [36864/50000]	Loss: 0.1734	LR: 0.012500
Training Epoch: 93 [36992/50000]	Loss: 0.1660	LR: 0.012500
Training Epoch: 93 [37120/50000]	Loss: 0.1837	LR: 0.012500
Training Epoch: 93 [37248/50000]	Loss: 0.1927	LR: 0.012500
Training Epoch: 93 [37376/50000]	Loss: 0.1133	LR: 0.012500
Training Epoch: 93 [37504/50000]	Loss: 0.2122	LR: 0.012500
Training Epoch: 93 [37632/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 93 [37760/50000]	Loss: 0.1558	LR: 0.012500
Training Epoch: 93 [37888/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 93 [38016/50000]	Loss: 0.1075	LR: 0.012500
Training Epoch: 93 [38144/50000]	Loss: 0.1468	LR: 0.012500
Training Epoch: 93 [38272/50000]	Loss: 0.1849	LR: 0.012500
Training Epoch: 93 [38400/50000]	Loss: 0.1915	LR: 0.012500
Training Epoch: 93 [38528/50000]	Loss: 0.1704	LR: 0.012500
Training Epoch: 93 [38656/50000]	Loss: 0.2232	LR: 0.012500
Training Epoch: 93 [38784/50000]	Loss: 0.1218	LR: 0.012500
Training Epoch: 93 [38912/50000]	Loss: 0.2583	LR: 0.012500
Training Epoch: 93 [39040/50000]	Loss: 0.2223	LR: 0.012500
Training Epoch: 93 [39168/50000]	Loss: 0.1340	LR: 0.012500
Training Epoch: 93 [39296/50000]	Loss: 0.1563	LR: 0.012500
Training Epoch: 93 [39424/50000]	Loss: 0.1119	LR: 0.012500
Training Epoch: 93 [39552/50000]	Loss: 0.1502	LR: 0.012500
Training Epoch: 93 [39680/50000]	Loss: 0.1665	LR: 0.012500
Training Epoch: 93 [39808/50000]	Loss: 0.2073	LR: 0.012500
Training Epoch: 93 [39936/50000]	Loss: 0.1576	LR: 0.012500
Training Epoch: 93 [40064/50000]	Loss: 0.1100	LR: 0.012500
Training Epoch: 93 [40192/50000]	Loss: 0.1434	LR: 0.012500
Training Epoch: 93 [40320/50000]	Loss: 0.1651	LR: 0.012500
Training Epoch: 93 [40448/50000]	Loss: 0.2110	LR: 0.012500
Training Epoch: 93 [40576/50000]	Loss: 0.2487	LR: 0.012500
Training Epoch: 93 [40704/50000]	Loss: 0.1174	LR: 0.012500
Training Epoch: 93 [40832/50000]	Loss: 0.1907	LR: 0.012500
Training Epoch: 93 [40960/50000]	Loss: 0.1327	LR: 0.012500
Training Epoch: 93 [41088/50000]	Loss: 0.1807	LR: 0.012500
Training Epoch: 93 [41216/50000]	Loss: 0.2196	LR: 0.012500
Training Epoch: 93 [41344/50000]	Loss: 0.1526	LR: 0.012500
Training Epoch: 93 [41472/50000]	Loss: 0.2026	LR: 0.012500
Training Epoch: 93 [41600/50000]	Loss: 0.1961	LR: 0.012500
Training Epoch: 93 [41728/50000]	Loss: 0.1948	LR: 0.012500
Training Epoch: 93 [41856/50000]	Loss: 0.1466	LR: 0.012500
Training Epoch: 93 [41984/50000]	Loss: 0.2441	LR: 0.012500
Training Epoch: 93 [42112/50000]	Loss: 0.1984	LR: 0.012500
Training Epoch: 93 [42240/50000]	Loss: 0.1794	LR: 0.012500
Training Epoch: 93 [42368/50000]	Loss: 0.1596	LR: 0.012500
Training Epoch: 93 [42496/50000]	Loss: 0.1651	LR: 0.012500
Training Epoch: 93 [42624/50000]	Loss: 0.2050	LR: 0.012500
Training Epoch: 93 [42752/50000]	Loss: 0.1491	LR: 0.012500
Training Epoch: 93 [42880/50000]	Loss: 0.1917	LR: 0.012500
Training Epoch: 93 [43008/50000]	Loss: 0.2171	LR: 0.012500
Training Epoch: 93 [43136/50000]	Loss: 0.2370	LR: 0.012500
Training Epoch: 93 [43264/50000]	Loss: 0.1338	LR: 0.012500
Training Epoch: 93 [43392/50000]	Loss: 0.2271	LR: 0.012500
Training Epoch: 93 [43520/50000]	Loss: 0.1530	LR: 0.012500
Training Epoch: 93 [43648/50000]	Loss: 0.1413	LR: 0.012500
Training Epoch: 93 [43776/50000]	Loss: 0.1092	LR: 0.012500
Training Epoch: 93 [43904/50000]	Loss: 0.1476	LR: 0.012500
Training Epoch: 93 [44032/50000]	Loss: 0.1262	LR: 0.012500
Training Epoch: 93 [44160/50000]	Loss: 0.1539	LR: 0.012500
Training Epoch: 93 [44288/50000]	Loss: 0.2534	LR: 0.012500
Training Epoch: 93 [44416/50000]	Loss: 0.2033	LR: 0.012500
Training Epoch: 93 [44544/50000]	Loss: 0.2582	LR: 0.012500
Training Epoch: 93 [44672/50000]	Loss: 0.1997	LR: 0.012500
Training Epoch: 93 [44800/50000]	Loss: 0.2433	LR: 0.012500
Training Epoch: 93 [44928/50000]	Loss: 0.1480	LR: 0.012500
Training Epoch: 93 [45056/50000]	Loss: 0.1301	LR: 0.012500
Training Epoch: 93 [45184/50000]	Loss: 0.1582	LR: 0.012500
Training Epoch: 93 [45312/50000]	Loss: 0.1478	LR: 0.012500
Training Epoch: 93 [45440/50000]	Loss: 0.2204	LR: 0.012500
Training Epoch: 93 [45568/50000]	Loss: 0.1411	LR: 0.012500
Training Epoch: 93 [45696/50000]	Loss: 0.1248	LR: 0.012500
Training Epoch: 93 [45824/50000]	Loss: 0.2053	LR: 0.012500
Training Epoch: 93 [45952/50000]	Loss: 0.1400	LR: 0.012500
Training Epoch: 93 [46080/50000]	Loss: 0.2682	LR: 0.012500
Training Epoch: 93 [46208/50000]	Loss: 0.2061	LR: 0.012500
Training Epoch: 93 [46336/50000]	Loss: 0.1412	LR: 0.012500
Training Epoch: 93 [46464/50000]	Loss: 0.2576	LR: 0.012500
Training Epoch: 93 [46592/50000]	Loss: 0.1821	LR: 0.012500
Training Epoch: 93 [46720/50000]	Loss: 0.1995	LR: 0.012500
Training Epoch: 93 [46848/50000]	Loss: 0.2366	LR: 0.012500
Training Epoch: 93 [46976/50000]	Loss: 0.2195	LR: 0.012500
Training Epoch: 93 [47104/50000]	Loss: 0.1412	LR: 0.012500
Training Epoch: 93 [47232/50000]	Loss: 0.1317	LR: 0.012500
Training Epoch: 93 [47360/50000]	Loss: 0.2614	LR: 0.012500
Training Epoch: 93 [47488/50000]	Loss: 0.1937	LR: 0.012500
Training Epoch: 93 [47616/50000]	Loss: 0.2405	LR: 0.012500
Training Epoch: 93 [47744/50000]	Loss: 0.2918	LR: 0.012500
Training Epoch: 93 [47872/50000]	Loss: 0.2455	LR: 0.012500
Training Epoch: 93 [48000/50000]	Loss: 0.1967	LR: 0.012500
Training Epoch: 93 [48128/50000]	Loss: 0.2175	LR: 0.012500
Training Epoch: 93 [48256/50000]	Loss: 0.2128	LR: 0.012500
Training Epoch: 93 [48384/50000]	Loss: 0.2067	LR: 0.012500
Training Epoch: 93 [48512/50000]	Loss: 0.1821	LR: 0.012500
Training Epoch: 93 [48640/50000]	Loss: 0.2592	LR: 0.012500
Training Epoch: 93 [48768/50000]	Loss: 0.1828	LR: 0.012500
Training Epoch: 93 [48896/50000]	Loss: 0.2065	LR: 0.012500
Training Epoch: 93 [49024/50000]	Loss: 0.1852	LR: 0.012500
Training Epoch: 93 [49152/50000]	Loss: 0.1841	LR: 0.012500
Training Epoch: 93 [49280/50000]	Loss: 0.2193	LR: 0.012500
Training Epoch: 93 [49408/50000]	Loss: 0.1944	LR: 0.012500
Training Epoch: 93 [49536/50000]	Loss: 0.1834	LR: 0.012500
Training Epoch: 93 [49664/50000]	Loss: 0.2004	LR: 0.012500
Training Epoch: 93 [49792/50000]	Loss: 0.1973	LR: 0.012500
Training Epoch: 93 [49920/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 93 [50000/50000]	Loss: 0.2639	LR: 0.012500
epoch 93 training time consumed: 53.94s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  341315 GB |  341315 GB |
|       from large pool |  123392 KB |    1034 MB |  340979 GB |  340979 GB |
|       from small pool |   10798 KB |      13 MB |     336 GB |     336 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  341315 GB |  341315 GB |
|       from large pool |  123392 KB |    1034 MB |  340979 GB |  340979 GB |
|       from small pool |   10798 KB |      13 MB |     336 GB |     336 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  150198 GB |  150198 GB |
|       from large pool |  155136 KB |  433088 KB |  149826 GB |  149826 GB |
|       from small pool |    1489 KB |    3494 KB |     371 GB |     371 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   13170 K  |   13169 K  |
|       from large pool |      24    |      65    |    6874 K  |    6874 K  |
|       from small pool |     232    |     275    |    6295 K  |    6295 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   13170 K  |   13169 K  |
|       from large pool |      24    |      65    |    6874 K  |    6874 K  |
|       from small pool |     232    |     275    |    6295 K  |    6295 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6532 K  |    6532 K  |
|       from large pool |       9    |      14    |    3327 K  |    3327 K  |
|       from small pool |      12    |      17    |    3205 K  |    3205 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 93, Average loss: 0.0105, Accuracy: 0.6902, Time consumed:3.46s

Training Epoch: 94 [128/50000]	Loss: 0.0936	LR: 0.012500
Training Epoch: 94 [256/50000]	Loss: 0.1547	LR: 0.012500
Training Epoch: 94 [384/50000]	Loss: 0.2229	LR: 0.012500
Training Epoch: 94 [512/50000]	Loss: 0.1306	LR: 0.012500
Training Epoch: 94 [640/50000]	Loss: 0.1968	LR: 0.012500
Training Epoch: 94 [768/50000]	Loss: 0.1722	LR: 0.012500
Training Epoch: 94 [896/50000]	Loss: 0.1031	LR: 0.012500
Training Epoch: 94 [1024/50000]	Loss: 0.1872	LR: 0.012500
Training Epoch: 94 [1152/50000]	Loss: 0.1761	LR: 0.012500
Training Epoch: 94 [1280/50000]	Loss: 0.2101	LR: 0.012500
Training Epoch: 94 [1408/50000]	Loss: 0.2156	LR: 0.012500
Training Epoch: 94 [1536/50000]	Loss: 0.1549	LR: 0.012500
Training Epoch: 94 [1664/50000]	Loss: 0.1716	LR: 0.012500
Training Epoch: 94 [1792/50000]	Loss: 0.1251	LR: 0.012500
Training Epoch: 94 [1920/50000]	Loss: 0.1893	LR: 0.012500
Training Epoch: 94 [2048/50000]	Loss: 0.1835	LR: 0.012500
Training Epoch: 94 [2176/50000]	Loss: 0.1145	LR: 0.012500
Training Epoch: 94 [2304/50000]	Loss: 0.1095	LR: 0.012500
Training Epoch: 94 [2432/50000]	Loss: 0.1397	LR: 0.012500
Training Epoch: 94 [2560/50000]	Loss: 0.1693	LR: 0.012500
Training Epoch: 94 [2688/50000]	Loss: 0.1063	LR: 0.012500
Training Epoch: 94 [2816/50000]	Loss: 0.1660	LR: 0.012500
Training Epoch: 94 [2944/50000]	Loss: 0.1441	LR: 0.012500
Training Epoch: 94 [3072/50000]	Loss: 0.1405	LR: 0.012500
Training Epoch: 94 [3200/50000]	Loss: 0.1549	LR: 0.012500
Training Epoch: 94 [3328/50000]	Loss: 0.1432	LR: 0.012500
Training Epoch: 94 [3456/50000]	Loss: 0.1833	LR: 0.012500
Training Epoch: 94 [3584/50000]	Loss: 0.2208	LR: 0.012500
Training Epoch: 94 [3712/50000]	Loss: 0.2485	LR: 0.012500
Training Epoch: 94 [3840/50000]	Loss: 0.1827	LR: 0.012500
Training Epoch: 94 [3968/50000]	Loss: 0.1355	LR: 0.012500
Training Epoch: 94 [4096/50000]	Loss: 0.1130	LR: 0.012500
Training Epoch: 94 [4224/50000]	Loss: 0.1713	LR: 0.012500
Training Epoch: 94 [4352/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 94 [4480/50000]	Loss: 0.1242	LR: 0.012500
Training Epoch: 94 [4608/50000]	Loss: 0.2190	LR: 0.012500
Training Epoch: 94 [4736/50000]	Loss: 0.0982	LR: 0.012500
Training Epoch: 94 [4864/50000]	Loss: 0.1952	LR: 0.012500
Training Epoch: 94 [4992/50000]	Loss: 0.2197	LR: 0.012500
Training Epoch: 94 [5120/50000]	Loss: 0.1375	LR: 0.012500
Training Epoch: 94 [5248/50000]	Loss: 0.2403	LR: 0.012500
Training Epoch: 94 [5376/50000]	Loss: 0.2026	LR: 0.012500
Training Epoch: 94 [5504/50000]	Loss: 0.0944	LR: 0.012500
Training Epoch: 94 [5632/50000]	Loss: 0.1206	LR: 0.012500
Training Epoch: 94 [5760/50000]	Loss: 0.0993	LR: 0.012500
Training Epoch: 94 [5888/50000]	Loss: 0.1266	LR: 0.012500
Training Epoch: 94 [6016/50000]	Loss: 0.0870	LR: 0.012500
Training Epoch: 94 [6144/50000]	Loss: 0.1451	LR: 0.012500
Training Epoch: 94 [6272/50000]	Loss: 0.1445	LR: 0.012500
Training Epoch: 94 [6400/50000]	Loss: 0.1241	LR: 0.012500
Training Epoch: 94 [6528/50000]	Loss: 0.2054	LR: 0.012500
Training Epoch: 94 [6656/50000]	Loss: 0.2970	LR: 0.012500
Training Epoch: 94 [6784/50000]	Loss: 0.1367	LR: 0.012500
Training Epoch: 94 [6912/50000]	Loss: 0.1458	LR: 0.012500
Training Epoch: 94 [7040/50000]	Loss: 0.1073	LR: 0.012500
Training Epoch: 94 [7168/50000]	Loss: 0.1252	LR: 0.012500
Training Epoch: 94 [7296/50000]	Loss: 0.1047	LR: 0.012500
Training Epoch: 94 [7424/50000]	Loss: 0.1543	LR: 0.012500
Training Epoch: 94 [7552/50000]	Loss: 0.1605	LR: 0.012500
Training Epoch: 94 [7680/50000]	Loss: 0.1089	LR: 0.012500
Training Epoch: 94 [7808/50000]	Loss: 0.1165	LR: 0.012500
Training Epoch: 94 [7936/50000]	Loss: 0.2188	LR: 0.012500
Training Epoch: 94 [8064/50000]	Loss: 0.1952	LR: 0.012500
Training Epoch: 94 [8192/50000]	Loss: 0.0893	LR: 0.012500
Training Epoch: 94 [8320/50000]	Loss: 0.1865	LR: 0.012500
Training Epoch: 94 [8448/50000]	Loss: 0.0691	LR: 0.012500
Training Epoch: 94 [8576/50000]	Loss: 0.1054	LR: 0.012500
Training Epoch: 94 [8704/50000]	Loss: 0.1396	LR: 0.012500
Training Epoch: 94 [8832/50000]	Loss: 0.2002	LR: 0.012500
Training Epoch: 94 [8960/50000]	Loss: 0.1893	LR: 0.012500
Training Epoch: 94 [9088/50000]	Loss: 0.1025	LR: 0.012500
Training Epoch: 94 [9216/50000]	Loss: 0.1922	LR: 0.012500
Training Epoch: 94 [9344/50000]	Loss: 0.2335	LR: 0.012500
Training Epoch: 94 [9472/50000]	Loss: 0.1132	LR: 0.012500
Training Epoch: 94 [9600/50000]	Loss: 0.1169	LR: 0.012500
Training Epoch: 94 [9728/50000]	Loss: 0.1120	LR: 0.012500
Training Epoch: 94 [9856/50000]	Loss: 0.1542	LR: 0.012500
Training Epoch: 94 [9984/50000]	Loss: 0.1301	LR: 0.012500
Training Epoch: 94 [10112/50000]	Loss: 0.1242	LR: 0.012500
Training Epoch: 94 [10240/50000]	Loss: 0.2118	LR: 0.012500
Training Epoch: 94 [10368/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 94 [10496/50000]	Loss: 0.1757	LR: 0.012500
Training Epoch: 94 [10624/50000]	Loss: 0.1453	LR: 0.012500
Training Epoch: 94 [10752/50000]	Loss: 0.1416	LR: 0.012500
Training Epoch: 94 [10880/50000]	Loss: 0.1460	LR: 0.012500
Training Epoch: 94 [11008/50000]	Loss: 0.1400	LR: 0.012500
Training Epoch: 94 [11136/50000]	Loss: 0.0931	LR: 0.012500
Training Epoch: 94 [11264/50000]	Loss: 0.1607	LR: 0.012500
Training Epoch: 94 [11392/50000]	Loss: 0.1141	LR: 0.012500
Training Epoch: 94 [11520/50000]	Loss: 0.1451	LR: 0.012500
Training Epoch: 94 [11648/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 94 [11776/50000]	Loss: 0.1652	LR: 0.012500
Training Epoch: 94 [11904/50000]	Loss: 0.1410	LR: 0.012500
Training Epoch: 94 [12032/50000]	Loss: 0.2414	LR: 0.012500
Training Epoch: 94 [12160/50000]	Loss: 0.1435	LR: 0.012500
Training Epoch: 94 [12288/50000]	Loss: 0.1698	LR: 0.012500
Training Epoch: 94 [12416/50000]	Loss: 0.1298	LR: 0.012500
Training Epoch: 94 [12544/50000]	Loss: 0.2213	LR: 0.012500
Training Epoch: 94 [12672/50000]	Loss: 0.1470	LR: 0.012500
Training Epoch: 94 [12800/50000]	Loss: 0.1622	LR: 0.012500
Training Epoch: 94 [12928/50000]	Loss: 0.0673	LR: 0.012500
Training Epoch: 94 [13056/50000]	Loss: 0.1731	LR: 0.012500
Training Epoch: 94 [13184/50000]	Loss: 0.0974	LR: 0.012500
Training Epoch: 94 [13312/50000]	Loss: 0.0875	LR: 0.012500
Training Epoch: 94 [13440/50000]	Loss: 0.1318	LR: 0.012500
Training Epoch: 94 [13568/50000]	Loss: 0.0932	LR: 0.012500
Training Epoch: 94 [13696/50000]	Loss: 0.1060	LR: 0.012500
Training Epoch: 94 [13824/50000]	Loss: 0.1451	LR: 0.012500
Training Epoch: 94 [13952/50000]	Loss: 0.0856	LR: 0.012500
Training Epoch: 94 [14080/50000]	Loss: 0.2190	LR: 0.012500
Training Epoch: 94 [14208/50000]	Loss: 0.0760	LR: 0.012500
Training Epoch: 94 [14336/50000]	Loss: 0.1604	LR: 0.012500
Training Epoch: 94 [14464/50000]	Loss: 0.1710	LR: 0.012500
Training Epoch: 94 [14592/50000]	Loss: 0.2426	LR: 0.012500
Training Epoch: 94 [14720/50000]	Loss: 0.1272	LR: 0.012500
Training Epoch: 94 [14848/50000]	Loss: 0.1165	LR: 0.012500
Training Epoch: 94 [14976/50000]	Loss: 0.1408	LR: 0.012500
Training Epoch: 94 [15104/50000]	Loss: 0.1533	LR: 0.012500
Training Epoch: 94 [15232/50000]	Loss: 0.1231	LR: 0.012500
Training Epoch: 94 [15360/50000]	Loss: 0.1505	LR: 0.012500
Training Epoch: 94 [15488/50000]	Loss: 0.1339	LR: 0.012500
Training Epoch: 94 [15616/50000]	Loss: 0.1430	LR: 0.012500
Training Epoch: 94 [15744/50000]	Loss: 0.1748	LR: 0.012500
Training Epoch: 94 [15872/50000]	Loss: 0.1490	LR: 0.012500
Training Epoch: 94 [16000/50000]	Loss: 0.1406	LR: 0.012500
Training Epoch: 94 [16128/50000]	Loss: 0.0580	LR: 0.012500
Training Epoch: 94 [16256/50000]	Loss: 0.1484	LR: 0.012500
Training Epoch: 94 [16384/50000]	Loss: 0.1709	LR: 0.012500
Training Epoch: 94 [16512/50000]	Loss: 0.1276	LR: 0.012500
Training Epoch: 94 [16640/50000]	Loss: 0.1238	LR: 0.012500
Training Epoch: 94 [16768/50000]	Loss: 0.1360	LR: 0.012500
Training Epoch: 94 [16896/50000]	Loss: 0.1559	LR: 0.012500
Training Epoch: 94 [17024/50000]	Loss: 0.2278	LR: 0.012500
Training Epoch: 94 [17152/50000]	Loss: 0.1220	LR: 0.012500
Training Epoch: 94 [17280/50000]	Loss: 0.2027	LR: 0.012500
Training Epoch: 94 [17408/50000]	Loss: 0.1431	LR: 0.012500
Training Epoch: 94 [17536/50000]	Loss: 0.1610	LR: 0.012500
Training Epoch: 94 [17664/50000]	Loss: 0.1661	LR: 0.012500
Training Epoch: 94 [17792/50000]	Loss: 0.1805	LR: 0.012500
Training Epoch: 94 [17920/50000]	Loss: 0.1604	LR: 0.012500
Training Epoch: 94 [18048/50000]	Loss: 0.2163	LR: 0.012500
Training Epoch: 94 [18176/50000]	Loss: 0.2086	LR: 0.012500
Training Epoch: 94 [18304/50000]	Loss: 0.1949	LR: 0.012500
Training Epoch: 94 [18432/50000]	Loss: 0.1549	LR: 0.012500
Training Epoch: 94 [18560/50000]	Loss: 0.1414	LR: 0.012500
Training Epoch: 94 [18688/50000]	Loss: 0.1963	LR: 0.012500
Training Epoch: 94 [18816/50000]	Loss: 0.1428	LR: 0.012500
Training Epoch: 94 [18944/50000]	Loss: 0.1439	LR: 0.012500
Training Epoch: 94 [19072/50000]	Loss: 0.1288	LR: 0.012500
Training Epoch: 94 [19200/50000]	Loss: 0.1394	LR: 0.012500
Training Epoch: 94 [19328/50000]	Loss: 0.2443	LR: 0.012500
Training Epoch: 94 [19456/50000]	Loss: 0.1230	LR: 0.012500
Training Epoch: 94 [19584/50000]	Loss: 0.1122	LR: 0.012500
Training Epoch: 94 [19712/50000]	Loss: 0.0887	LR: 0.012500
Training Epoch: 94 [19840/50000]	Loss: 0.1460	LR: 0.012500
Training Epoch: 94 [19968/50000]	Loss: 0.2552	LR: 0.012500
Training Epoch: 94 [20096/50000]	Loss: 0.1814	LR: 0.012500
Training Epoch: 94 [20224/50000]	Loss: 0.1466	LR: 0.012500
Training Epoch: 94 [20352/50000]	Loss: 0.1220	LR: 0.012500
Training Epoch: 94 [20480/50000]	Loss: 0.1292	LR: 0.012500
Training Epoch: 94 [20608/50000]	Loss: 0.2442	LR: 0.012500
Training Epoch: 94 [20736/50000]	Loss: 0.1709	LR: 0.012500
Training Epoch: 94 [20864/50000]	Loss: 0.1479	LR: 0.012500
Training Epoch: 94 [20992/50000]	Loss: 0.2010	LR: 0.012500
Training Epoch: 94 [21120/50000]	Loss: 0.1716	LR: 0.012500
Training Epoch: 94 [21248/50000]	Loss: 0.1463	LR: 0.012500
Training Epoch: 94 [21376/50000]	Loss: 0.1249	LR: 0.012500
Training Epoch: 94 [21504/50000]	Loss: 0.2370	LR: 0.012500
Training Epoch: 94 [21632/50000]	Loss: 0.1113	LR: 0.012500
Training Epoch: 94 [21760/50000]	Loss: 0.1076	LR: 0.012500
Training Epoch: 94 [21888/50000]	Loss: 0.1904	LR: 0.012500
Training Epoch: 94 [22016/50000]	Loss: 0.1223	LR: 0.012500
Training Epoch: 94 [22144/50000]	Loss: 0.1119	LR: 0.012500
Training Epoch: 94 [22272/50000]	Loss: 0.1606	LR: 0.012500
Training Epoch: 94 [22400/50000]	Loss: 0.1442	LR: 0.012500
Training Epoch: 94 [22528/50000]	Loss: 0.1358	LR: 0.012500
Training Epoch: 94 [22656/50000]	Loss: 0.1534	LR: 0.012500
Training Epoch: 94 [22784/50000]	Loss: 0.1963	LR: 0.012500
Training Epoch: 94 [22912/50000]	Loss: 0.1163	LR: 0.012500
Training Epoch: 94 [23040/50000]	Loss: 0.2298	LR: 0.012500
Training Epoch: 94 [23168/50000]	Loss: 0.1450	LR: 0.012500
Training Epoch: 94 [23296/50000]	Loss: 0.1389	LR: 0.012500
Training Epoch: 94 [23424/50000]	Loss: 0.0522	LR: 0.012500
Training Epoch: 94 [23552/50000]	Loss: 0.1909	LR: 0.012500
Training Epoch: 94 [23680/50000]	Loss: 0.1763	LR: 0.012500
Training Epoch: 94 [23808/50000]	Loss: 0.2190	LR: 0.012500
Training Epoch: 94 [23936/50000]	Loss: 0.1635	LR: 0.012500
Training Epoch: 94 [24064/50000]	Loss: 0.1563	LR: 0.012500
Training Epoch: 94 [24192/50000]	Loss: 0.2018	LR: 0.012500
Training Epoch: 94 [24320/50000]	Loss: 0.1474	LR: 0.012500
Training Epoch: 94 [24448/50000]	Loss: 0.1132	LR: 0.012500
Training Epoch: 94 [24576/50000]	Loss: 0.2051	LR: 0.012500
Training Epoch: 94 [24704/50000]	Loss: 0.1834	LR: 0.012500
Training Epoch: 94 [24832/50000]	Loss: 0.1534	LR: 0.012500
Training Epoch: 94 [24960/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 94 [25088/50000]	Loss: 0.1865	LR: 0.012500
Training Epoch: 94 [25216/50000]	Loss: 0.1605	LR: 0.012500
Training Epoch: 94 [25344/50000]	Loss: 0.1677	LR: 0.012500
Training Epoch: 94 [25472/50000]	Loss: 0.1446	LR: 0.012500
Training Epoch: 94 [25600/50000]	Loss: 0.2277	LR: 0.012500
Training Epoch: 94 [25728/50000]	Loss: 0.1704	LR: 0.012500
Training Epoch: 94 [25856/50000]	Loss: 0.1387	LR: 0.012500
Training Epoch: 94 [25984/50000]	Loss: 0.1100	LR: 0.012500
Training Epoch: 94 [26112/50000]	Loss: 0.1299	LR: 0.012500
Training Epoch: 94 [26240/50000]	Loss: 0.1701	LR: 0.012500
Training Epoch: 94 [26368/50000]	Loss: 0.1334	LR: 0.012500
Training Epoch: 94 [26496/50000]	Loss: 0.1124	LR: 0.012500
Training Epoch: 94 [26624/50000]	Loss: 0.1443	LR: 0.012500
Training Epoch: 94 [26752/50000]	Loss: 0.1716	LR: 0.012500
Training Epoch: 94 [26880/50000]	Loss: 0.0954	LR: 0.012500
Training Epoch: 94 [27008/50000]	Loss: 0.1145	LR: 0.012500
Training Epoch: 94 [27136/50000]	Loss: 0.1492	LR: 0.012500
Training Epoch: 94 [27264/50000]	Loss: 0.1081	LR: 0.012500
Training Epoch: 94 [27392/50000]	Loss: 0.1593	LR: 0.012500
Training Epoch: 94 [27520/50000]	Loss: 0.1431	LR: 0.012500
Training Epoch: 94 [27648/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 94 [27776/50000]	Loss: 0.1424	LR: 0.012500
Training Epoch: 94 [27904/50000]	Loss: 0.1027	LR: 0.012500
Training Epoch: 94 [28032/50000]	Loss: 0.2091	LR: 0.012500
Training Epoch: 94 [28160/50000]	Loss: 0.1353	LR: 0.012500
Training Epoch: 94 [28288/50000]	Loss: 0.1705	LR: 0.012500
Training Epoch: 94 [28416/50000]	Loss: 0.1496	LR: 0.012500
Training Epoch: 94 [28544/50000]	Loss: 0.2071	LR: 0.012500
Training Epoch: 94 [28672/50000]	Loss: 0.1936	LR: 0.012500
Training Epoch: 94 [28800/50000]	Loss: 0.2230	LR: 0.012500
Training Epoch: 94 [28928/50000]	Loss: 0.1513	LR: 0.012500
Training Epoch: 94 [29056/50000]	Loss: 0.1408	LR: 0.012500
Training Epoch: 94 [29184/50000]	Loss: 0.1748	LR: 0.012500
Training Epoch: 94 [29312/50000]	Loss: 0.1359	LR: 0.012500
Training Epoch: 94 [29440/50000]	Loss: 0.1672	LR: 0.012500
Training Epoch: 94 [29568/50000]	Loss: 0.1614	LR: 0.012500
Training Epoch: 94 [29696/50000]	Loss: 0.1005	LR: 0.012500
Training Epoch: 94 [29824/50000]	Loss: 0.1597	LR: 0.012500
Training Epoch: 94 [29952/50000]	Loss: 0.0703	LR: 0.012500
Training Epoch: 94 [30080/50000]	Loss: 0.1141	LR: 0.012500
Training Epoch: 94 [30208/50000]	Loss: 0.1403	LR: 0.012500
Training Epoch: 94 [30336/50000]	Loss: 0.1640	LR: 0.012500
Training Epoch: 94 [30464/50000]	Loss: 0.2317	LR: 0.012500
Training Epoch: 94 [30592/50000]	Loss: 0.1922	LR: 0.012500
Training Epoch: 94 [30720/50000]	Loss: 0.1139	LR: 0.012500
Training Epoch: 94 [30848/50000]	Loss: 0.1341	LR: 0.012500
Training Epoch: 94 [30976/50000]	Loss: 0.1756	LR: 0.012500
Training Epoch: 94 [31104/50000]	Loss: 0.1450	LR: 0.012500
Training Epoch: 94 [31232/50000]	Loss: 0.2387	LR: 0.012500
Training Epoch: 94 [31360/50000]	Loss: 0.1521	LR: 0.012500
Training Epoch: 94 [31488/50000]	Loss: 0.1232	LR: 0.012500
Training Epoch: 94 [31616/50000]	Loss: 0.1034	LR: 0.012500
Training Epoch: 94 [31744/50000]	Loss: 0.1512	LR: 0.012500
Training Epoch: 94 [31872/50000]	Loss: 0.1393	LR: 0.012500
Training Epoch: 94 [32000/50000]	Loss: 0.1678	LR: 0.012500
Training Epoch: 94 [32128/50000]	Loss: 0.2346	LR: 0.012500
Training Epoch: 94 [32256/50000]	Loss: 0.0803	LR: 0.012500
Training Epoch: 94 [32384/50000]	Loss: 0.1467	LR: 0.012500
Training Epoch: 94 [32512/50000]	Loss: 0.1830	LR: 0.012500
Training Epoch: 94 [32640/50000]	Loss: 0.1473	LR: 0.012500
Training Epoch: 94 [32768/50000]	Loss: 0.1549	LR: 0.012500
Training Epoch: 94 [32896/50000]	Loss: 0.2644	LR: 0.012500
Training Epoch: 94 [33024/50000]	Loss: 0.2223	LR: 0.012500
Training Epoch: 94 [33152/50000]	Loss: 0.1717	LR: 0.012500
Training Epoch: 94 [33280/50000]	Loss: 0.1461	LR: 0.012500
Training Epoch: 94 [33408/50000]	Loss: 0.1617	LR: 0.012500
Training Epoch: 94 [33536/50000]	Loss: 0.1466	LR: 0.012500
Training Epoch: 94 [33664/50000]	Loss: 0.1267	LR: 0.012500
Training Epoch: 94 [33792/50000]	Loss: 0.1865	LR: 0.012500
Training Epoch: 94 [33920/50000]	Loss: 0.2219	LR: 0.012500
Training Epoch: 94 [34048/50000]	Loss: 0.1119	LR: 0.012500
Training Epoch: 94 [34176/50000]	Loss: 0.1521	LR: 0.012500
Training Epoch: 94 [34304/50000]	Loss: 0.1735	LR: 0.012500
Training Epoch: 94 [34432/50000]	Loss: 0.1131	LR: 0.012500
Training Epoch: 94 [34560/50000]	Loss: 0.2894	LR: 0.012500
Training Epoch: 94 [34688/50000]	Loss: 0.1864	LR: 0.012500
Training Epoch: 94 [34816/50000]	Loss: 0.1710	LR: 0.012500
Training Epoch: 94 [34944/50000]	Loss: 0.2298	LR: 0.012500
Training Epoch: 94 [35072/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 94 [35200/50000]	Loss: 0.2515	LR: 0.012500
Training Epoch: 94 [35328/50000]	Loss: 0.0925	LR: 0.012500
Training Epoch: 94 [35456/50000]	Loss: 0.0927	LR: 0.012500
Training Epoch: 94 [35584/50000]	Loss: 0.1343	LR: 0.012500
Training Epoch: 94 [35712/50000]	Loss: 0.2516	LR: 0.012500
Training Epoch: 94 [35840/50000]	Loss: 0.1419	LR: 0.012500
Training Epoch: 94 [35968/50000]	Loss: 0.2185	LR: 0.012500
Training Epoch: 94 [36096/50000]	Loss: 0.1851	LR: 0.012500
Training Epoch: 94 [36224/50000]	Loss: 0.1178	LR: 0.012500
Training Epoch: 94 [36352/50000]	Loss: 0.2299	LR: 0.012500
Training Epoch: 94 [36480/50000]	Loss: 0.2409	LR: 0.012500
Training Epoch: 94 [36608/50000]	Loss: 0.1592	LR: 0.012500
Training Epoch: 94 [36736/50000]	Loss: 0.2499	LR: 0.012500
Training Epoch: 94 [36864/50000]	Loss: 0.1920	LR: 0.012500
Training Epoch: 94 [36992/50000]	Loss: 0.1764	LR: 0.012500
Training Epoch: 94 [37120/50000]	Loss: 0.1238	LR: 0.012500
Training Epoch: 94 [37248/50000]	Loss: 0.2051	LR: 0.012500
Training Epoch: 94 [37376/50000]	Loss: 0.2785	LR: 0.012500
Training Epoch: 94 [37504/50000]	Loss: 0.2035	LR: 0.012500
Training Epoch: 94 [37632/50000]	Loss: 0.1829	LR: 0.012500
Training Epoch: 94 [37760/50000]	Loss: 0.1998	LR: 0.012500
Training Epoch: 94 [37888/50000]	Loss: 0.1347	LR: 0.012500
Training Epoch: 94 [38016/50000]	Loss: 0.1080	LR: 0.012500
Training Epoch: 94 [38144/50000]	Loss: 0.1722	LR: 0.012500
Training Epoch: 94 [38272/50000]	Loss: 0.1597	LR: 0.012500
Training Epoch: 94 [38400/50000]	Loss: 0.1560	LR: 0.012500
Training Epoch: 94 [38528/50000]	Loss: 0.1392	LR: 0.012500
Training Epoch: 94 [38656/50000]	Loss: 0.1087	LR: 0.012500
Training Epoch: 94 [38784/50000]	Loss: 0.1580	LR: 0.012500
Training Epoch: 94 [38912/50000]	Loss: 0.1770	LR: 0.012500
Training Epoch: 94 [39040/50000]	Loss: 0.2294	LR: 0.012500
Training Epoch: 94 [39168/50000]	Loss: 0.1606	LR: 0.012500
Training Epoch: 94 [39296/50000]	Loss: 0.1857	LR: 0.012500
Training Epoch: 94 [39424/50000]	Loss: 0.1723	LR: 0.012500
Training Epoch: 94 [39552/50000]	Loss: 0.1367	LR: 0.012500
Training Epoch: 94 [39680/50000]	Loss: 0.1966	LR: 0.012500
Training Epoch: 94 [39808/50000]	Loss: 0.1666	LR: 0.012500
Training Epoch: 94 [39936/50000]	Loss: 0.1882	LR: 0.012500
Training Epoch: 94 [40064/50000]	Loss: 0.1342	LR: 0.012500
Training Epoch: 94 [40192/50000]	Loss: 0.1377	LR: 0.012500
Training Epoch: 94 [40320/50000]	Loss: 0.1923	LR: 0.012500
Training Epoch: 94 [40448/50000]	Loss: 0.1441	LR: 0.012500
Training Epoch: 94 [40576/50000]	Loss: 0.1990	LR: 0.012500
Training Epoch: 94 [40704/50000]	Loss: 0.1309	LR: 0.012500
Training Epoch: 94 [40832/50000]	Loss: 0.1844	LR: 0.012500
Training Epoch: 94 [40960/50000]	Loss: 0.1788	LR: 0.012500
Training Epoch: 94 [41088/50000]	Loss: 0.1806	LR: 0.012500
Training Epoch: 94 [41216/50000]	Loss: 0.2680	LR: 0.012500
Training Epoch: 94 [41344/50000]	Loss: 0.1896	LR: 0.012500
Training Epoch: 94 [41472/50000]	Loss: 0.2012	LR: 0.012500
Training Epoch: 94 [41600/50000]	Loss: 0.1978	LR: 0.012500
Training Epoch: 94 [41728/50000]	Loss: 0.1736	LR: 0.012500
Training Epoch: 94 [41856/50000]	Loss: 0.2580	LR: 0.012500
Training Epoch: 94 [41984/50000]	Loss: 0.2788	LR: 0.012500
Training Epoch: 94 [42112/50000]	Loss: 0.1577	LR: 0.012500
Training Epoch: 94 [42240/50000]	Loss: 0.1829	LR: 0.012500
Training Epoch: 94 [42368/50000]	Loss: 0.2082	LR: 0.012500
Training Epoch: 94 [42496/50000]	Loss: 0.2247	LR: 0.012500
Training Epoch: 94 [42624/50000]	Loss: 0.2320	LR: 0.012500
Training Epoch: 94 [42752/50000]	Loss: 0.1335	LR: 0.012500
Training Epoch: 94 [42880/50000]	Loss: 0.2254	LR: 0.012500
Training Epoch: 94 [43008/50000]	Loss: 0.1318	LR: 0.012500
Training Epoch: 94 [43136/50000]	Loss: 0.1578	LR: 0.012500
Training Epoch: 94 [43264/50000]	Loss: 0.1423	LR: 0.012500
Training Epoch: 94 [43392/50000]	Loss: 0.1893	LR: 0.012500
Training Epoch: 94 [43520/50000]	Loss: 0.2272	LR: 0.012500
Training Epoch: 94 [43648/50000]	Loss: 0.1724	LR: 0.012500
Training Epoch: 94 [43776/50000]	Loss: 0.2591	LR: 0.012500
Training Epoch: 94 [43904/50000]	Loss: 0.2129	LR: 0.012500
Training Epoch: 94 [44032/50000]	Loss: 0.2370	LR: 0.012500
Training Epoch: 94 [44160/50000]	Loss: 0.1990	LR: 0.012500
Training Epoch: 94 [44288/50000]	Loss: 0.1234	LR: 0.012500
Training Epoch: 94 [44416/50000]	Loss: 0.1200	LR: 0.012500
Training Epoch: 94 [44544/50000]	Loss: 0.1244	LR: 0.012500
Training Epoch: 94 [44672/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 94 [44800/50000]	Loss: 0.1603	LR: 0.012500
Training Epoch: 94 [44928/50000]	Loss: 0.1492	LR: 0.012500
Training Epoch: 94 [45056/50000]	Loss: 0.1508	LR: 0.012500
Training Epoch: 94 [45184/50000]	Loss: 0.2171	LR: 0.012500
Training Epoch: 94 [45312/50000]	Loss: 0.2211	LR: 0.012500
Training Epoch: 94 [45440/50000]	Loss: 0.1571	LR: 0.012500
Training Epoch: 94 [45568/50000]	Loss: 0.1529	LR: 0.012500
Training Epoch: 94 [45696/50000]	Loss: 0.1756	LR: 0.012500
Training Epoch: 94 [45824/50000]	Loss: 0.1333	LR: 0.012500
Training Epoch: 94 [45952/50000]	Loss: 0.1283	LR: 0.012500
Training Epoch: 94 [46080/50000]	Loss: 0.1444	LR: 0.012500
Training Epoch: 94 [46208/50000]	Loss: 0.2071	LR: 0.012500
Training Epoch: 94 [46336/50000]	Loss: 0.1056	LR: 0.012500
Training Epoch: 94 [46464/50000]	Loss: 0.1960	LR: 0.012500
Training Epoch: 94 [46592/50000]	Loss: 0.1658	LR: 0.012500
Training Epoch: 94 [46720/50000]	Loss: 0.2630	LR: 0.012500
Training Epoch: 94 [46848/50000]	Loss: 0.1504	LR: 0.012500
Training Epoch: 94 [46976/50000]	Loss: 0.1393	LR: 0.012500
Training Epoch: 94 [47104/50000]	Loss: 0.2242	LR: 0.012500
Training Epoch: 94 [47232/50000]	Loss: 0.1512	LR: 0.012500
Training Epoch: 94 [47360/50000]	Loss: 0.1678	LR: 0.012500
Training Epoch: 94 [47488/50000]	Loss: 0.1570	LR: 0.012500
Training Epoch: 94 [47616/50000]	Loss: 0.2365	LR: 0.012500
Training Epoch: 94 [47744/50000]	Loss: 0.1516	LR: 0.012500
Training Epoch: 94 [47872/50000]	Loss: 0.2063	LR: 0.012500
Training Epoch: 94 [48000/50000]	Loss: 0.2644	LR: 0.012500
Training Epoch: 94 [48128/50000]	Loss: 0.1292	LR: 0.012500
Training Epoch: 94 [48256/50000]	Loss: 0.1913	LR: 0.012500
Training Epoch: 94 [48384/50000]	Loss: 0.1465	LR: 0.012500
Training Epoch: 94 [48512/50000]	Loss: 0.2079	LR: 0.012500
Training Epoch: 94 [48640/50000]	Loss: 0.2273	LR: 0.012500
Training Epoch: 94 [48768/50000]	Loss: 0.1197	LR: 0.012500
Training Epoch: 94 [48896/50000]	Loss: 0.1709	LR: 0.012500
Training Epoch: 94 [49024/50000]	Loss: 0.1830	LR: 0.012500
Training Epoch: 94 [49152/50000]	Loss: 0.1572	LR: 0.012500
Training Epoch: 94 [49280/50000]	Loss: 0.1668	LR: 0.012500
Training Epoch: 94 [49408/50000]	Loss: 0.2136	LR: 0.012500
Training Epoch: 94 [49536/50000]	Loss: 0.2074	LR: 0.012500
Training Epoch: 94 [49664/50000]	Loss: 0.1261	LR: 0.012500
Training Epoch: 94 [49792/50000]	Loss: 0.1524	LR: 0.012500
Training Epoch: 94 [49920/50000]	Loss: 0.2602	LR: 0.012500
Training Epoch: 94 [50000/50000]	Loss: 0.2440	LR: 0.012500
epoch 94 training time consumed: 54.03s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  344985 GB |  344985 GB |
|       from large pool |  123392 KB |    1034 MB |  344645 GB |  344645 GB |
|       from small pool |   10798 KB |      13 MB |     339 GB |     339 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  344985 GB |  344985 GB |
|       from large pool |  123392 KB |    1034 MB |  344645 GB |  344645 GB |
|       from small pool |   10798 KB |      13 MB |     339 GB |     339 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  151813 GB |  151813 GB |
|       from large pool |  155136 KB |  433088 KB |  151438 GB |  151437 GB |
|       from small pool |    1489 KB |    3494 KB |     375 GB |     375 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   13311 K  |   13311 K  |
|       from large pool |      24    |      65    |    6948 K  |    6948 K  |
|       from small pool |     232    |     275    |    6363 K  |    6363 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   13311 K  |   13311 K  |
|       from large pool |      24    |      65    |    6948 K  |    6948 K  |
|       from small pool |     232    |     275    |    6363 K  |    6363 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6605 K  |    6605 K  |
|       from large pool |       9    |      14    |    3363 K  |    3363 K  |
|       from small pool |      12    |      17    |    3241 K  |    3241 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 94, Average loss: 0.0102, Accuracy: 0.6981, Time consumed:3.47s

Training Epoch: 95 [128/50000]	Loss: 0.1201	LR: 0.012500
Training Epoch: 95 [256/50000]	Loss: 0.0972	LR: 0.012500
Training Epoch: 95 [384/50000]	Loss: 0.1425	LR: 0.012500
Training Epoch: 95 [512/50000]	Loss: 0.1615	LR: 0.012500
Training Epoch: 95 [640/50000]	Loss: 0.1615	LR: 0.012500
Training Epoch: 95 [768/50000]	Loss: 0.1125	LR: 0.012500
Training Epoch: 95 [896/50000]	Loss: 0.1072	LR: 0.012500
Training Epoch: 95 [1024/50000]	Loss: 0.1739	LR: 0.012500
Training Epoch: 95 [1152/50000]	Loss: 0.1537	LR: 0.012500
Training Epoch: 95 [1280/50000]	Loss: 0.2208	LR: 0.012500
Training Epoch: 95 [1408/50000]	Loss: 0.0943	LR: 0.012500
Training Epoch: 95 [1536/50000]	Loss: 0.2046	LR: 0.012500
Training Epoch: 95 [1664/50000]	Loss: 0.1022	LR: 0.012500
Training Epoch: 95 [1792/50000]	Loss: 0.1239	LR: 0.012500
Training Epoch: 95 [1920/50000]	Loss: 0.1940	LR: 0.012500
Training Epoch: 95 [2048/50000]	Loss: 0.1322	LR: 0.012500
Training Epoch: 95 [2176/50000]	Loss: 0.1899	LR: 0.012500
Training Epoch: 95 [2304/50000]	Loss: 0.2174	LR: 0.012500
Training Epoch: 95 [2432/50000]	Loss: 0.1846	LR: 0.012500
Training Epoch: 95 [2560/50000]	Loss: 0.1600	LR: 0.012500
Training Epoch: 95 [2688/50000]	Loss: 0.1641	LR: 0.012500
Training Epoch: 95 [2816/50000]	Loss: 0.1863	LR: 0.012500
Training Epoch: 95 [2944/50000]	Loss: 0.2075	LR: 0.012500
Training Epoch: 95 [3072/50000]	Loss: 0.0858	LR: 0.012500
Training Epoch: 95 [3200/50000]	Loss: 0.1221	LR: 0.012500
Training Epoch: 95 [3328/50000]	Loss: 0.1382	LR: 0.012500
Training Epoch: 95 [3456/50000]	Loss: 0.1355	LR: 0.012500
Training Epoch: 95 [3584/50000]	Loss: 0.1645	LR: 0.012500
Training Epoch: 95 [3712/50000]	Loss: 0.1443	LR: 0.012500
Training Epoch: 95 [3840/50000]	Loss: 0.1758	LR: 0.012500
Training Epoch: 95 [3968/50000]	Loss: 0.1398	LR: 0.012500
Training Epoch: 95 [4096/50000]	Loss: 0.1698	LR: 0.012500
Training Epoch: 95 [4224/50000]	Loss: 0.1567	LR: 0.012500
Training Epoch: 95 [4352/50000]	Loss: 0.1230	LR: 0.012500
Training Epoch: 95 [4480/50000]	Loss: 0.1406	LR: 0.012500
Training Epoch: 95 [4608/50000]	Loss: 0.1411	LR: 0.012500
Training Epoch: 95 [4736/50000]	Loss: 0.1649	LR: 0.012500
Training Epoch: 95 [4864/50000]	Loss: 0.1500	LR: 0.012500
Training Epoch: 95 [4992/50000]	Loss: 0.1305	LR: 0.012500
Training Epoch: 95 [5120/50000]	Loss: 0.1947	LR: 0.012500
Training Epoch: 95 [5248/50000]	Loss: 0.1959	LR: 0.012500
Training Epoch: 95 [5376/50000]	Loss: 0.1461	LR: 0.012500
Training Epoch: 95 [5504/50000]	Loss: 0.1433	LR: 0.012500
Training Epoch: 95 [5632/50000]	Loss: 0.1019	LR: 0.012500
Training Epoch: 95 [5760/50000]	Loss: 0.1736	LR: 0.012500
Training Epoch: 95 [5888/50000]	Loss: 0.2385	LR: 0.012500
Training Epoch: 95 [6016/50000]	Loss: 0.1549	LR: 0.012500
Training Epoch: 95 [6144/50000]	Loss: 0.1360	LR: 0.012500
Training Epoch: 95 [6272/50000]	Loss: 0.1514	LR: 0.012500
Training Epoch: 95 [6400/50000]	Loss: 0.1935	LR: 0.012500
Training Epoch: 95 [6528/50000]	Loss: 0.2005	LR: 0.012500
Training Epoch: 95 [6656/50000]	Loss: 0.2192	LR: 0.012500
Training Epoch: 95 [6784/50000]	Loss: 0.1428	LR: 0.012500
Training Epoch: 95 [6912/50000]	Loss: 0.1515	LR: 0.012500
Training Epoch: 95 [7040/50000]	Loss: 0.1106	LR: 0.012500
Training Epoch: 95 [7168/50000]	Loss: 0.1365	LR: 0.012500
Training Epoch: 95 [7296/50000]	Loss: 0.2119	LR: 0.012500
Training Epoch: 95 [7424/50000]	Loss: 0.0861	LR: 0.012500
Training Epoch: 95 [7552/50000]	Loss: 0.1947	LR: 0.012500
Training Epoch: 95 [7680/50000]	Loss: 0.1494	LR: 0.012500
Training Epoch: 95 [7808/50000]	Loss: 0.1701	LR: 0.012500
Training Epoch: 95 [7936/50000]	Loss: 0.1480	LR: 0.012500
Training Epoch: 95 [8064/50000]	Loss: 0.1473	LR: 0.012500
Training Epoch: 95 [8192/50000]	Loss: 0.1552	LR: 0.012500
Training Epoch: 95 [8320/50000]	Loss: 0.1413	LR: 0.012500
Training Epoch: 95 [8448/50000]	Loss: 0.1452	LR: 0.012500
Training Epoch: 95 [8576/50000]	Loss: 0.1206	LR: 0.012500
Training Epoch: 95 [8704/50000]	Loss: 0.1055	LR: 0.012500
Training Epoch: 95 [8832/50000]	Loss: 0.1674	LR: 0.012500
Training Epoch: 95 [8960/50000]	Loss: 0.2039	LR: 0.012500
Training Epoch: 95 [9088/50000]	Loss: 0.1889	LR: 0.012500
Training Epoch: 95 [9216/50000]	Loss: 0.1700	LR: 0.012500
Training Epoch: 95 [9344/50000]	Loss: 0.1288	LR: 0.012500
Training Epoch: 95 [9472/50000]	Loss: 0.1042	LR: 0.012500
Training Epoch: 95 [9600/50000]	Loss: 0.1889	LR: 0.012500
Training Epoch: 95 [9728/50000]	Loss: 0.1527	LR: 0.012500
Training Epoch: 95 [9856/50000]	Loss: 0.0683	LR: 0.012500
Training Epoch: 95 [9984/50000]	Loss: 0.2296	LR: 0.012500
Training Epoch: 95 [10112/50000]	Loss: 0.1513	LR: 0.012500
Training Epoch: 95 [10240/50000]	Loss: 0.0966	LR: 0.012500
Training Epoch: 95 [10368/50000]	Loss: 0.1172	LR: 0.012500
Training Epoch: 95 [10496/50000]	Loss: 0.1917	LR: 0.012500
Training Epoch: 95 [10624/50000]	Loss: 0.1792	LR: 0.012500
Training Epoch: 95 [10752/50000]	Loss: 0.1912	LR: 0.012500
Training Epoch: 95 [10880/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 95 [11008/50000]	Loss: 0.1530	LR: 0.012500
Training Epoch: 95 [11136/50000]	Loss: 0.1249	LR: 0.012500
Training Epoch: 95 [11264/50000]	Loss: 0.1507	LR: 0.012500
Training Epoch: 95 [11392/50000]	Loss: 0.1693	LR: 0.012500
Training Epoch: 95 [11520/50000]	Loss: 0.1855	LR: 0.012500
Training Epoch: 95 [11648/50000]	Loss: 0.1377	LR: 0.012500
Training Epoch: 95 [11776/50000]	Loss: 0.1374	LR: 0.012500
Training Epoch: 95 [11904/50000]	Loss: 0.1203	LR: 0.012500
Training Epoch: 95 [12032/50000]	Loss: 0.1475	LR: 0.012500
Training Epoch: 95 [12160/50000]	Loss: 0.1127	LR: 0.012500
Training Epoch: 95 [12288/50000]	Loss: 0.1561	LR: 0.012500
Training Epoch: 95 [12416/50000]	Loss: 0.1357	LR: 0.012500
Training Epoch: 95 [12544/50000]	Loss: 0.1355	LR: 0.012500
Training Epoch: 95 [12672/50000]	Loss: 0.1278	LR: 0.012500
Training Epoch: 95 [12800/50000]	Loss: 0.1928	LR: 0.012500
Training Epoch: 95 [12928/50000]	Loss: 0.1636	LR: 0.012500
Training Epoch: 95 [13056/50000]	Loss: 0.1629	LR: 0.012500
Training Epoch: 95 [13184/50000]	Loss: 0.1200	LR: 0.012500
Training Epoch: 95 [13312/50000]	Loss: 0.1727	LR: 0.012500
Training Epoch: 95 [13440/50000]	Loss: 0.1645	LR: 0.012500
Training Epoch: 95 [13568/50000]	Loss: 0.1132	LR: 0.012500
Training Epoch: 95 [13696/50000]	Loss: 0.1311	LR: 0.012500
Training Epoch: 95 [13824/50000]	Loss: 0.1289	LR: 0.012500
Training Epoch: 95 [13952/50000]	Loss: 0.1282	LR: 0.012500
Training Epoch: 95 [14080/50000]	Loss: 0.1776	LR: 0.012500
Training Epoch: 95 [14208/50000]	Loss: 0.1638	LR: 0.012500
Training Epoch: 95 [14336/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 95 [14464/50000]	Loss: 0.1751	LR: 0.012500
Training Epoch: 95 [14592/50000]	Loss: 0.1346	LR: 0.012500
Training Epoch: 95 [14720/50000]	Loss: 0.1500	LR: 0.012500
Training Epoch: 95 [14848/50000]	Loss: 0.1170	LR: 0.012500
Training Epoch: 95 [14976/50000]	Loss: 0.1307	LR: 0.012500
Training Epoch: 95 [15104/50000]	Loss: 0.1582	LR: 0.012500
Training Epoch: 95 [15232/50000]	Loss: 0.2052	LR: 0.012500
Training Epoch: 95 [15360/50000]	Loss: 0.1996	LR: 0.012500
Training Epoch: 95 [15488/50000]	Loss: 0.1337	LR: 0.012500
Training Epoch: 95 [15616/50000]	Loss: 0.1432	LR: 0.012500
Training Epoch: 95 [15744/50000]	Loss: 0.1249	LR: 0.012500
Training Epoch: 95 [15872/50000]	Loss: 0.1591	LR: 0.012500
Training Epoch: 95 [16000/50000]	Loss: 0.1717	LR: 0.012500
Training Epoch: 95 [16128/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 95 [16256/50000]	Loss: 0.1892	LR: 0.012500
Training Epoch: 95 [16384/50000]	Loss: 0.2487	LR: 0.012500
Training Epoch: 95 [16512/50000]	Loss: 0.1301	LR: 0.012500
Training Epoch: 95 [16640/50000]	Loss: 0.1826	LR: 0.012500
Training Epoch: 95 [16768/50000]	Loss: 0.2620	LR: 0.012500
Training Epoch: 95 [16896/50000]	Loss: 0.1514	LR: 0.012500
Training Epoch: 95 [17024/50000]	Loss: 0.1731	LR: 0.012500
Training Epoch: 95 [17152/50000]	Loss: 0.1080	LR: 0.012500
Training Epoch: 95 [17280/50000]	Loss: 0.1422	LR: 0.012500
Training Epoch: 95 [17408/50000]	Loss: 0.2193	LR: 0.012500
Training Epoch: 95 [17536/50000]	Loss: 0.1486	LR: 0.012500
Training Epoch: 95 [17664/50000]	Loss: 0.1118	LR: 0.012500
Training Epoch: 95 [17792/50000]	Loss: 0.1740	LR: 0.012500
Training Epoch: 95 [17920/50000]	Loss: 0.1356	LR: 0.012500
Training Epoch: 95 [18048/50000]	Loss: 0.0951	LR: 0.012500
Training Epoch: 95 [18176/50000]	Loss: 0.1197	LR: 0.012500
Training Epoch: 95 [18304/50000]	Loss: 0.1005	LR: 0.012500
Training Epoch: 95 [18432/50000]	Loss: 0.1712	LR: 0.012500
Training Epoch: 95 [18560/50000]	Loss: 0.2530	LR: 0.012500
Training Epoch: 95 [18688/50000]	Loss: 0.0837	LR: 0.012500
Training Epoch: 95 [18816/50000]	Loss: 0.2208	LR: 0.012500
Training Epoch: 95 [18944/50000]	Loss: 0.1510	LR: 0.012500
Training Epoch: 95 [19072/50000]	Loss: 0.1361	LR: 0.012500
Training Epoch: 95 [19200/50000]	Loss: 0.1931	LR: 0.012500
Training Epoch: 95 [19328/50000]	Loss: 0.1669	LR: 0.012500
Training Epoch: 95 [19456/50000]	Loss: 0.1829	LR: 0.012500
Training Epoch: 95 [19584/50000]	Loss: 0.1577	LR: 0.012500
Training Epoch: 95 [19712/50000]	Loss: 0.1951	LR: 0.012500
Training Epoch: 95 [19840/50000]	Loss: 0.0920	LR: 0.012500
Training Epoch: 95 [19968/50000]	Loss: 0.1878	LR: 0.012500
Training Epoch: 95 [20096/50000]	Loss: 0.1479	LR: 0.012500
Training Epoch: 95 [20224/50000]	Loss: 0.1080	LR: 0.012500
Training Epoch: 95 [20352/50000]	Loss: 0.1138	LR: 0.012500
Training Epoch: 95 [20480/50000]	Loss: 0.0621	LR: 0.012500
Training Epoch: 95 [20608/50000]	Loss: 0.1697	LR: 0.012500
Training Epoch: 95 [20736/50000]	Loss: 0.1061	LR: 0.012500
Training Epoch: 95 [20864/50000]	Loss: 0.2266	LR: 0.012500
Training Epoch: 95 [20992/50000]	Loss: 0.1973	LR: 0.012500
Training Epoch: 95 [21120/50000]	Loss: 0.1766	LR: 0.012500
Training Epoch: 95 [21248/50000]	Loss: 0.1784	LR: 0.012500
Training Epoch: 95 [21376/50000]	Loss: 0.1177	LR: 0.012500
Training Epoch: 95 [21504/50000]	Loss: 0.1740	LR: 0.012500
Training Epoch: 95 [21632/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 95 [21760/50000]	Loss: 0.1172	LR: 0.012500
Training Epoch: 95 [21888/50000]	Loss: 0.2038	LR: 0.012500
Training Epoch: 95 [22016/50000]	Loss: 0.2041	LR: 0.012500
Training Epoch: 95 [22144/50000]	Loss: 0.1363	LR: 0.012500
Training Epoch: 95 [22272/50000]	Loss: 0.1315	LR: 0.012500
Training Epoch: 95 [22400/50000]	Loss: 0.1170	LR: 0.012500
Training Epoch: 95 [22528/50000]	Loss: 0.2767	LR: 0.012500
Training Epoch: 95 [22656/50000]	Loss: 0.1580	LR: 0.012500
Training Epoch: 95 [22784/50000]	Loss: 0.1688	LR: 0.012500
Training Epoch: 95 [22912/50000]	Loss: 0.1807	LR: 0.012500
Training Epoch: 95 [23040/50000]	Loss: 0.1944	LR: 0.012500
Training Epoch: 95 [23168/50000]	Loss: 0.1584	LR: 0.012500
Training Epoch: 95 [23296/50000]	Loss: 0.1428	LR: 0.012500
Training Epoch: 95 [23424/50000]	Loss: 0.1694	LR: 0.012500
Training Epoch: 95 [23552/50000]	Loss: 0.1635	LR: 0.012500
Training Epoch: 95 [23680/50000]	Loss: 0.2159	LR: 0.012500
Training Epoch: 95 [23808/50000]	Loss: 0.1353	LR: 0.012500
Training Epoch: 95 [23936/50000]	Loss: 0.1833	LR: 0.012500
Training Epoch: 95 [24064/50000]	Loss: 0.1472	LR: 0.012500
Training Epoch: 95 [24192/50000]	Loss: 0.1536	LR: 0.012500
Training Epoch: 95 [24320/50000]	Loss: 0.2090	LR: 0.012500
Training Epoch: 95 [24448/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 95 [24576/50000]	Loss: 0.2222	LR: 0.012500
Training Epoch: 95 [24704/50000]	Loss: 0.1034	LR: 0.012500
Training Epoch: 95 [24832/50000]	Loss: 0.1635	LR: 0.012500
Training Epoch: 95 [24960/50000]	Loss: 0.2448	LR: 0.012500
Training Epoch: 95 [25088/50000]	Loss: 0.1763	LR: 0.012500
Training Epoch: 95 [25216/50000]	Loss: 0.2242	LR: 0.012500
Training Epoch: 95 [25344/50000]	Loss: 0.2918	LR: 0.012500
Training Epoch: 95 [25472/50000]	Loss: 0.1559	LR: 0.012500
Training Epoch: 95 [25600/50000]	Loss: 0.1485	LR: 0.012500
Training Epoch: 95 [25728/50000]	Loss: 0.1027	LR: 0.012500
Training Epoch: 95 [25856/50000]	Loss: 0.2741	LR: 0.012500
Training Epoch: 95 [25984/50000]	Loss: 0.1625	LR: 0.012500
Training Epoch: 95 [26112/50000]	Loss: 0.2206	LR: 0.012500
Training Epoch: 95 [26240/50000]	Loss: 0.1443	LR: 0.012500
Training Epoch: 95 [26368/50000]	Loss: 0.2093	LR: 0.012500
Training Epoch: 95 [26496/50000]	Loss: 0.2029	LR: 0.012500
Training Epoch: 95 [26624/50000]	Loss: 0.1561	LR: 0.012500
Training Epoch: 95 [26752/50000]	Loss: 0.1798	LR: 0.012500
Training Epoch: 95 [26880/50000]	Loss: 0.2882	LR: 0.012500
Training Epoch: 95 [27008/50000]	Loss: 0.1320	LR: 0.012500
Training Epoch: 95 [27136/50000]	Loss: 0.2307	LR: 0.012500
Training Epoch: 95 [27264/50000]	Loss: 0.2149	LR: 0.012500
Training Epoch: 95 [27392/50000]	Loss: 0.1563	LR: 0.012500
Training Epoch: 95 [27520/50000]	Loss: 0.2092	LR: 0.012500
Training Epoch: 95 [27648/50000]	Loss: 0.1347	LR: 0.012500
Training Epoch: 95 [27776/50000]	Loss: 0.2029	LR: 0.012500
Training Epoch: 95 [27904/50000]	Loss: 0.2381	LR: 0.012500
Training Epoch: 95 [28032/50000]	Loss: 0.1345	LR: 0.012500
Training Epoch: 95 [28160/50000]	Loss: 0.1567	LR: 0.012500
Training Epoch: 95 [28288/50000]	Loss: 0.1857	LR: 0.012500
Training Epoch: 95 [28416/50000]	Loss: 0.1595	LR: 0.012500
Training Epoch: 95 [28544/50000]	Loss: 0.2866	LR: 0.012500
Training Epoch: 95 [28672/50000]	Loss: 0.1223	LR: 0.012500
Training Epoch: 95 [28800/50000]	Loss: 0.1584	LR: 0.012500
Training Epoch: 95 [28928/50000]	Loss: 0.1531	LR: 0.012500
Training Epoch: 95 [29056/50000]	Loss: 0.1884	LR: 0.012500
Training Epoch: 95 [29184/50000]	Loss: 0.2667	LR: 0.012500
Training Epoch: 95 [29312/50000]	Loss: 0.1439	LR: 0.012500
Training Epoch: 95 [29440/50000]	Loss: 0.1522	LR: 0.012500
Training Epoch: 95 [29568/50000]	Loss: 0.2145	LR: 0.012500
Training Epoch: 95 [29696/50000]	Loss: 0.1827	LR: 0.012500
Training Epoch: 95 [29824/50000]	Loss: 0.1555	LR: 0.012500
Training Epoch: 95 [29952/50000]	Loss: 0.2513	LR: 0.012500
Training Epoch: 95 [30080/50000]	Loss: 0.2060	LR: 0.012500
Training Epoch: 95 [30208/50000]	Loss: 0.1505	LR: 0.012500
Training Epoch: 95 [30336/50000]	Loss: 0.1402	LR: 0.012500
Training Epoch: 95 [30464/50000]	Loss: 0.1499	LR: 0.012500
Training Epoch: 95 [30592/50000]	Loss: 0.1521	LR: 0.012500
Training Epoch: 95 [30720/50000]	Loss: 0.1199	LR: 0.012500
Training Epoch: 95 [30848/50000]	Loss: 0.2790	LR: 0.012500
Training Epoch: 95 [30976/50000]	Loss: 0.1363	LR: 0.012500
Training Epoch: 95 [31104/50000]	Loss: 0.2974	LR: 0.012500
Training Epoch: 95 [31232/50000]	Loss: 0.1738	LR: 0.012500
Training Epoch: 95 [31360/50000]	Loss: 0.1616	LR: 0.012500
Training Epoch: 95 [31488/50000]	Loss: 0.1609	LR: 0.012500
Training Epoch: 95 [31616/50000]	Loss: 0.1160	LR: 0.012500
Training Epoch: 95 [31744/50000]	Loss: 0.1981	LR: 0.012500
Training Epoch: 95 [31872/50000]	Loss: 0.1424	LR: 0.012500
Training Epoch: 95 [32000/50000]	Loss: 0.1426	LR: 0.012500
Training Epoch: 95 [32128/50000]	Loss: 0.1269	LR: 0.012500
Training Epoch: 95 [32256/50000]	Loss: 0.2066	LR: 0.012500
Training Epoch: 95 [32384/50000]	Loss: 0.2296	LR: 0.012500
Training Epoch: 95 [32512/50000]	Loss: 0.2651	LR: 0.012500
Training Epoch: 95 [32640/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 95 [32768/50000]	Loss: 0.1785	LR: 0.012500
Training Epoch: 95 [32896/50000]	Loss: 0.2308	LR: 0.012500
Training Epoch: 95 [33024/50000]	Loss: 0.2454	LR: 0.012500
Training Epoch: 95 [33152/50000]	Loss: 0.1948	LR: 0.012500
Training Epoch: 95 [33280/50000]	Loss: 0.3641	LR: 0.012500
Training Epoch: 95 [33408/50000]	Loss: 0.2014	LR: 0.012500
Training Epoch: 95 [33536/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 95 [33664/50000]	Loss: 0.1862	LR: 0.012500
Training Epoch: 95 [33792/50000]	Loss: 0.1848	LR: 0.012500
Training Epoch: 95 [33920/50000]	Loss: 0.1655	LR: 0.012500
Training Epoch: 95 [34048/50000]	Loss: 0.1594	LR: 0.012500
Training Epoch: 95 [34176/50000]	Loss: 0.1988	LR: 0.012500
Training Epoch: 95 [34304/50000]	Loss: 0.1828	LR: 0.012500
Training Epoch: 95 [34432/50000]	Loss: 0.2002	LR: 0.012500
Training Epoch: 95 [34560/50000]	Loss: 0.1823	LR: 0.012500
Training Epoch: 95 [34688/50000]	Loss: 0.1615	LR: 0.012500
Training Epoch: 95 [34816/50000]	Loss: 0.2261	LR: 0.012500
Training Epoch: 95 [34944/50000]	Loss: 0.1950	LR: 0.012500
Training Epoch: 95 [35072/50000]	Loss: 0.1289	LR: 0.012500
Training Epoch: 95 [35200/50000]	Loss: 0.2375	LR: 0.012500
Training Epoch: 95 [35328/50000]	Loss: 0.1776	LR: 0.012500
Training Epoch: 95 [35456/50000]	Loss: 0.1405	LR: 0.012500
Training Epoch: 95 [35584/50000]	Loss: 0.1676	LR: 0.012500
Training Epoch: 95 [35712/50000]	Loss: 0.2085	LR: 0.012500
Training Epoch: 95 [35840/50000]	Loss: 0.2342	LR: 0.012500
Training Epoch: 95 [35968/50000]	Loss: 0.1800	LR: 0.012500
Training Epoch: 95 [36096/50000]	Loss: 0.1901	LR: 0.012500
Training Epoch: 95 [36224/50000]	Loss: 0.1007	LR: 0.012500
Training Epoch: 95 [36352/50000]	Loss: 0.1387	LR: 0.012500
Training Epoch: 95 [36480/50000]	Loss: 0.2210	LR: 0.012500
Training Epoch: 95 [36608/50000]	Loss: 0.1685	LR: 0.012500
Training Epoch: 95 [36736/50000]	Loss: 0.2631	LR: 0.012500
Training Epoch: 95 [36864/50000]	Loss: 0.3028	LR: 0.012500
Training Epoch: 95 [36992/50000]	Loss: 0.1650	LR: 0.012500
Training Epoch: 95 [37120/50000]	Loss: 0.1038	LR: 0.012500
Training Epoch: 95 [37248/50000]	Loss: 0.2509	LR: 0.012500
Training Epoch: 95 [37376/50000]	Loss: 0.2261	LR: 0.012500
Training Epoch: 95 [37504/50000]	Loss: 0.1057	LR: 0.012500
Training Epoch: 95 [37632/50000]	Loss: 0.1588	LR: 0.012500
Training Epoch: 95 [37760/50000]	Loss: 0.2742	LR: 0.012500
Training Epoch: 95 [37888/50000]	Loss: 0.1702	LR: 0.012500
Training Epoch: 95 [38016/50000]	Loss: 0.1784	LR: 0.012500
Training Epoch: 95 [38144/50000]	Loss: 0.2040	LR: 0.012500
Training Epoch: 95 [38272/50000]	Loss: 0.1400	LR: 0.012500
Training Epoch: 95 [38400/50000]	Loss: 0.1804	LR: 0.012500
Training Epoch: 95 [38528/50000]	Loss: 0.2427	LR: 0.012500
Training Epoch: 95 [38656/50000]	Loss: 0.2046	LR: 0.012500
Training Epoch: 95 [38784/50000]	Loss: 0.2040	LR: 0.012500
Training Epoch: 95 [38912/50000]	Loss: 0.1115	LR: 0.012500
Training Epoch: 95 [39040/50000]	Loss: 0.2282	LR: 0.012500
Training Epoch: 95 [39168/50000]	Loss: 0.1922	LR: 0.012500
Training Epoch: 95 [39296/50000]	Loss: 0.2290	LR: 0.012500
Training Epoch: 95 [39424/50000]	Loss: 0.1170	LR: 0.012500
Training Epoch: 95 [39552/50000]	Loss: 0.1426	LR: 0.012500
Training Epoch: 95 [39680/50000]	Loss: 0.1691	LR: 0.012500
Training Epoch: 95 [39808/50000]	Loss: 0.2030	LR: 0.012500
Training Epoch: 95 [39936/50000]	Loss: 0.2143	LR: 0.012500
Training Epoch: 95 [40064/50000]	Loss: 0.1686	LR: 0.012500
Training Epoch: 95 [40192/50000]	Loss: 0.2182	LR: 0.012500
Training Epoch: 95 [40320/50000]	Loss: 0.1344	LR: 0.012500
Training Epoch: 95 [40448/50000]	Loss: 0.2041	LR: 0.012500
Training Epoch: 95 [40576/50000]	Loss: 0.1588	LR: 0.012500
Training Epoch: 95 [40704/50000]	Loss: 0.2041	LR: 0.012500
Training Epoch: 95 [40832/50000]	Loss: 0.1722	LR: 0.012500
Training Epoch: 95 [40960/50000]	Loss: 0.2164	LR: 0.012500
Training Epoch: 95 [41088/50000]	Loss: 0.3141	LR: 0.012500
Training Epoch: 95 [41216/50000]	Loss: 0.2144	LR: 0.012500
Training Epoch: 95 [41344/50000]	Loss: 0.2343	LR: 0.012500
Training Epoch: 95 [41472/50000]	Loss: 0.2535	LR: 0.012500
Training Epoch: 95 [41600/50000]	Loss: 0.2455	LR: 0.012500
Training Epoch: 95 [41728/50000]	Loss: 0.1289	LR: 0.012500
Training Epoch: 95 [41856/50000]	Loss: 0.3230	LR: 0.012500
Training Epoch: 95 [41984/50000]	Loss: 0.1572	LR: 0.012500
Training Epoch: 95 [42112/50000]	Loss: 0.1627	LR: 0.012500
Training Epoch: 95 [42240/50000]	Loss: 0.2144	LR: 0.012500
Training Epoch: 95 [42368/50000]	Loss: 0.2127	LR: 0.012500
Training Epoch: 95 [42496/50000]	Loss: 0.3150	LR: 0.012500
Training Epoch: 95 [42624/50000]	Loss: 0.3818	LR: 0.012500
Training Epoch: 95 [42752/50000]	Loss: 0.3069	LR: 0.012500
Training Epoch: 95 [42880/50000]	Loss: 0.1752	LR: 0.012500
Training Epoch: 95 [43008/50000]	Loss: 0.2484	LR: 0.012500
Training Epoch: 95 [43136/50000]	Loss: 0.1950	LR: 0.012500
Training Epoch: 95 [43264/50000]	Loss: 0.1798	LR: 0.012500
Training Epoch: 95 [43392/50000]	Loss: 0.1831	LR: 0.012500
Training Epoch: 95 [43520/50000]	Loss: 0.1863	LR: 0.012500
Training Epoch: 95 [43648/50000]	Loss: 0.2591	LR: 0.012500
Training Epoch: 95 [43776/50000]	Loss: 0.2526	LR: 0.012500
Training Epoch: 95 [43904/50000]	Loss: 0.2971	LR: 0.012500
Training Epoch: 95 [44032/50000]	Loss: 0.1780	LR: 0.012500
Training Epoch: 95 [44160/50000]	Loss: 0.1877	LR: 0.012500
Training Epoch: 95 [44288/50000]	Loss: 0.2228	LR: 0.012500
Training Epoch: 95 [44416/50000]	Loss: 0.1894	LR: 0.012500
Training Epoch: 95 [44544/50000]	Loss: 0.3010	LR: 0.012500
Training Epoch: 95 [44672/50000]	Loss: 0.2535	LR: 0.012500
Training Epoch: 95 [44800/50000]	Loss: 0.2379	LR: 0.012500
Training Epoch: 95 [44928/50000]	Loss: 0.2499	LR: 0.012500
Training Epoch: 95 [45056/50000]	Loss: 0.1991	LR: 0.012500
Training Epoch: 95 [45184/50000]	Loss: 0.1629	LR: 0.012500
Training Epoch: 95 [45312/50000]	Loss: 0.2424	LR: 0.012500
Training Epoch: 95 [45440/50000]	Loss: 0.2401	LR: 0.012500
Training Epoch: 95 [45568/50000]	Loss: 0.2366	LR: 0.012500
Training Epoch: 95 [45696/50000]	Loss: 0.1915	LR: 0.012500
Training Epoch: 95 [45824/50000]	Loss: 0.2224	LR: 0.012500
Training Epoch: 95 [45952/50000]	Loss: 0.1341	LR: 0.012500
Training Epoch: 95 [46080/50000]	Loss: 0.1829	LR: 0.012500
Training Epoch: 95 [46208/50000]	Loss: 0.1422	LR: 0.012500
Training Epoch: 95 [46336/50000]	Loss: 0.1213	LR: 0.012500
Training Epoch: 95 [46464/50000]	Loss: 0.1811	LR: 0.012500
Training Epoch: 95 [46592/50000]	Loss: 0.2061	LR: 0.012500
Training Epoch: 95 [46720/50000]	Loss: 0.1790	LR: 0.012500
Training Epoch: 95 [46848/50000]	Loss: 0.2384	LR: 0.012500
Training Epoch: 95 [46976/50000]	Loss: 0.2363	LR: 0.012500
Training Epoch: 95 [47104/50000]	Loss: 0.2485	LR: 0.012500
Training Epoch: 95 [47232/50000]	Loss: 0.1767	LR: 0.012500
Training Epoch: 95 [47360/50000]	Loss: 0.2011	LR: 0.012500
Training Epoch: 95 [47488/50000]	Loss: 0.1936	LR: 0.012500
Training Epoch: 95 [47616/50000]	Loss: 0.2518	LR: 0.012500
Training Epoch: 95 [47744/50000]	Loss: 0.1926	LR: 0.012500
Training Epoch: 95 [47872/50000]	Loss: 0.1756	LR: 0.012500
Training Epoch: 95 [48000/50000]	Loss: 0.2594	LR: 0.012500
Training Epoch: 95 [48128/50000]	Loss: 0.2002	LR: 0.012500
Training Epoch: 95 [48256/50000]	Loss: 0.1812	LR: 0.012500
Training Epoch: 95 [48384/50000]	Loss: 0.1545	LR: 0.012500
Training Epoch: 95 [48512/50000]	Loss: 0.1791	LR: 0.012500
Training Epoch: 95 [48640/50000]	Loss: 0.2633	LR: 0.012500
Training Epoch: 95 [48768/50000]	Loss: 0.1510	LR: 0.012500
Training Epoch: 95 [48896/50000]	Loss: 0.2345	LR: 0.012500
Training Epoch: 95 [49024/50000]	Loss: 0.1684	LR: 0.012500
Training Epoch: 95 [49152/50000]	Loss: 0.2597	LR: 0.012500
Training Epoch: 95 [49280/50000]	Loss: 0.2261	LR: 0.012500
Training Epoch: 95 [49408/50000]	Loss: 0.2002	LR: 0.012500
Training Epoch: 95 [49536/50000]	Loss: 0.1765	LR: 0.012500
Training Epoch: 95 [49664/50000]	Loss: 0.2148	LR: 0.012500
Training Epoch: 95 [49792/50000]	Loss: 0.1429	LR: 0.012500
Training Epoch: 95 [49920/50000]	Loss: 0.1683	LR: 0.012500
Training Epoch: 95 [50000/50000]	Loss: 0.2917	LR: 0.012500
epoch 95 training time consumed: 53.92s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  348655 GB |  348655 GB |
|       from large pool |  123392 KB |    1034 MB |  348312 GB |  348312 GB |
|       from small pool |   10798 KB |      13 MB |     343 GB |     343 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  348655 GB |  348655 GB |
|       from large pool |  123392 KB |    1034 MB |  348312 GB |  348312 GB |
|       from small pool |   10798 KB |      13 MB |     343 GB |     343 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  153428 GB |  153428 GB |
|       from large pool |  155136 KB |  433088 KB |  153049 GB |  153048 GB |
|       from small pool |    1489 KB |    3494 KB |     379 GB |     379 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   13453 K  |   13453 K  |
|       from large pool |      24    |      65    |    7022 K  |    7022 K  |
|       from small pool |     232    |     275    |    6430 K  |    6430 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   13453 K  |   13453 K  |
|       from large pool |      24    |      65    |    7022 K  |    7022 K  |
|       from small pool |     232    |     275    |    6430 K  |    6430 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6677 K  |    6677 K  |
|       from large pool |       9    |      14    |    3398 K  |    3398 K  |
|       from small pool |      12    |      17    |    3278 K  |    3278 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 95, Average loss: 0.0102, Accuracy: 0.6993, Time consumed:3.45s

Training Epoch: 96 [128/50000]	Loss: 0.1723	LR: 0.012500
Training Epoch: 96 [256/50000]	Loss: 0.1715	LR: 0.012500
Training Epoch: 96 [384/50000]	Loss: 0.1582	LR: 0.012500
Training Epoch: 96 [512/50000]	Loss: 0.1459	LR: 0.012500
Training Epoch: 96 [640/50000]	Loss: 0.2450	LR: 0.012500
Training Epoch: 96 [768/50000]	Loss: 0.1531	LR: 0.012500
Training Epoch: 96 [896/50000]	Loss: 0.1285	LR: 0.012500
Training Epoch: 96 [1024/50000]	Loss: 0.2676	LR: 0.012500
Training Epoch: 96 [1152/50000]	Loss: 0.2099	LR: 0.012500
Training Epoch: 96 [1280/50000]	Loss: 0.1986	LR: 0.012500
Training Epoch: 96 [1408/50000]	Loss: 0.1204	LR: 0.012500
Training Epoch: 96 [1536/50000]	Loss: 0.0897	LR: 0.012500
Training Epoch: 96 [1664/50000]	Loss: 0.1467	LR: 0.012500
Training Epoch: 96 [1792/50000]	Loss: 0.1053	LR: 0.012500
Training Epoch: 96 [1920/50000]	Loss: 0.1663	LR: 0.012500
Training Epoch: 96 [2048/50000]	Loss: 0.1911	LR: 0.012500
Training Epoch: 96 [2176/50000]	Loss: 0.1301	LR: 0.012500
Training Epoch: 96 [2304/50000]	Loss: 0.2184	LR: 0.012500
Training Epoch: 96 [2432/50000]	Loss: 0.1681	LR: 0.012500
Training Epoch: 96 [2560/50000]	Loss: 0.1716	LR: 0.012500
Training Epoch: 96 [2688/50000]	Loss: 0.1478	LR: 0.012500
Training Epoch: 96 [2816/50000]	Loss: 0.1174	LR: 0.012500
Training Epoch: 96 [2944/50000]	Loss: 0.2033	LR: 0.012500
Training Epoch: 96 [3072/50000]	Loss: 0.1677	LR: 0.012500
Training Epoch: 96 [3200/50000]	Loss: 0.2109	LR: 0.012500
Training Epoch: 96 [3328/50000]	Loss: 0.1220	LR: 0.012500
Training Epoch: 96 [3456/50000]	Loss: 0.1864	LR: 0.012500
Training Epoch: 96 [3584/50000]	Loss: 0.0963	LR: 0.012500
Training Epoch: 96 [3712/50000]	Loss: 0.2019	LR: 0.012500
Training Epoch: 96 [3840/50000]	Loss: 0.1328	LR: 0.012500
Training Epoch: 96 [3968/50000]	Loss: 0.1418	LR: 0.012500
Training Epoch: 96 [4096/50000]	Loss: 0.1857	LR: 0.012500
Training Epoch: 96 [4224/50000]	Loss: 0.1941	LR: 0.012500
Training Epoch: 96 [4352/50000]	Loss: 0.1840	LR: 0.012500
Training Epoch: 96 [4480/50000]	Loss: 0.1672	LR: 0.012500
Training Epoch: 96 [4608/50000]	Loss: 0.1530	LR: 0.012500
Training Epoch: 96 [4736/50000]	Loss: 0.1688	LR: 0.012500
Training Epoch: 96 [4864/50000]	Loss: 0.1516	LR: 0.012500
Training Epoch: 96 [4992/50000]	Loss: 0.1492	LR: 0.012500
Training Epoch: 96 [5120/50000]	Loss: 0.1601	LR: 0.012500
Training Epoch: 96 [5248/50000]	Loss: 0.1581	LR: 0.012500
Training Epoch: 96 [5376/50000]	Loss: 0.1166	LR: 0.012500
Training Epoch: 96 [5504/50000]	Loss: 0.2348	LR: 0.012500
Training Epoch: 96 [5632/50000]	Loss: 0.1953	LR: 0.012500
Training Epoch: 96 [5760/50000]	Loss: 0.1999	LR: 0.012500
Training Epoch: 96 [5888/50000]	Loss: 0.1928	LR: 0.012500
Training Epoch: 96 [6016/50000]	Loss: 0.2048	LR: 0.012500
Training Epoch: 96 [6144/50000]	Loss: 0.1276	LR: 0.012500
Training Epoch: 96 [6272/50000]	Loss: 0.2326	LR: 0.012500
Training Epoch: 96 [6400/50000]	Loss: 0.2501	LR: 0.012500
Training Epoch: 96 [6528/50000]	Loss: 0.1892	LR: 0.012500
Training Epoch: 96 [6656/50000]	Loss: 0.1699	LR: 0.012500
Training Epoch: 96 [6784/50000]	Loss: 0.1359	LR: 0.012500
Training Epoch: 96 [6912/50000]	Loss: 0.1813	LR: 0.012500
Training Epoch: 96 [7040/50000]	Loss: 0.1727	LR: 0.012500
Training Epoch: 96 [7168/50000]	Loss: 0.1604	LR: 0.012500
Training Epoch: 96 [7296/50000]	Loss: 0.1413	LR: 0.012500
Training Epoch: 96 [7424/50000]	Loss: 0.2162	LR: 0.012500
Training Epoch: 96 [7552/50000]	Loss: 0.0975	LR: 0.012500
Training Epoch: 96 [7680/50000]	Loss: 0.1202	LR: 0.012500
Training Epoch: 96 [7808/50000]	Loss: 0.1117	LR: 0.012500
Training Epoch: 96 [7936/50000]	Loss: 0.1113	LR: 0.012500
Training Epoch: 96 [8064/50000]	Loss: 0.1926	LR: 0.012500
Training Epoch: 96 [8192/50000]	Loss: 0.1691	LR: 0.012500
Training Epoch: 96 [8320/50000]	Loss: 0.1302	LR: 0.012500
Training Epoch: 96 [8448/50000]	Loss: 0.1229	LR: 0.012500
Training Epoch: 96 [8576/50000]	Loss: 0.2021	LR: 0.012500
Training Epoch: 96 [8704/50000]	Loss: 0.1517	LR: 0.012500
Training Epoch: 96 [8832/50000]	Loss: 0.2455	LR: 0.012500
Training Epoch: 96 [8960/50000]	Loss: 0.1339	LR: 0.012500
Training Epoch: 96 [9088/50000]	Loss: 0.1562	LR: 0.012500
Training Epoch: 96 [9216/50000]	Loss: 0.0982	LR: 0.012500
Training Epoch: 96 [9344/50000]	Loss: 0.2041	LR: 0.012500
Training Epoch: 96 [9472/50000]	Loss: 0.1668	LR: 0.012500
Training Epoch: 96 [9600/50000]	Loss: 0.1127	LR: 0.012500
Training Epoch: 96 [9728/50000]	Loss: 0.1406	LR: 0.012500
Training Epoch: 96 [9856/50000]	Loss: 0.1327	LR: 0.012500
Training Epoch: 96 [9984/50000]	Loss: 0.1636	LR: 0.012500
Training Epoch: 96 [10112/50000]	Loss: 0.0849	LR: 0.012500
Training Epoch: 96 [10240/50000]	Loss: 0.1049	LR: 0.012500
Training Epoch: 96 [10368/50000]	Loss: 0.2283	LR: 0.012500
Training Epoch: 96 [10496/50000]	Loss: 0.1379	LR: 0.012500
Training Epoch: 96 [10624/50000]	Loss: 0.2291	LR: 0.012500
Training Epoch: 96 [10752/50000]	Loss: 0.0755	LR: 0.012500
Training Epoch: 96 [10880/50000]	Loss: 0.1706	LR: 0.012500
Training Epoch: 96 [11008/50000]	Loss: 0.1316	LR: 0.012500
Training Epoch: 96 [11136/50000]	Loss: 0.2291	LR: 0.012500
Training Epoch: 96 [11264/50000]	Loss: 0.1160	LR: 0.012500
Training Epoch: 96 [11392/50000]	Loss: 0.1941	LR: 0.012500
Training Epoch: 96 [11520/50000]	Loss: 0.1806	LR: 0.012500
Training Epoch: 96 [11648/50000]	Loss: 0.1762	LR: 0.012500
Training Epoch: 96 [11776/50000]	Loss: 0.1127	LR: 0.012500
Training Epoch: 96 [11904/50000]	Loss: 0.2015	LR: 0.012500
Training Epoch: 96 [12032/50000]	Loss: 0.1869	LR: 0.012500
Training Epoch: 96 [12160/50000]	Loss: 0.1672	LR: 0.012500
Training Epoch: 96 [12288/50000]	Loss: 0.1237	LR: 0.012500
Training Epoch: 96 [12416/50000]	Loss: 0.1441	LR: 0.012500
Training Epoch: 96 [12544/50000]	Loss: 0.1644	LR: 0.012500
Training Epoch: 96 [12672/50000]	Loss: 0.2309	LR: 0.012500
Training Epoch: 96 [12800/50000]	Loss: 0.1327	LR: 0.012500
Training Epoch: 96 [12928/50000]	Loss: 0.1443	LR: 0.012500
Training Epoch: 96 [13056/50000]	Loss: 0.1185	LR: 0.012500
Training Epoch: 96 [13184/50000]	Loss: 0.2141	LR: 0.012500
Training Epoch: 96 [13312/50000]	Loss: 0.2130	LR: 0.012500
Training Epoch: 96 [13440/50000]	Loss: 0.1188	LR: 0.012500
Training Epoch: 96 [13568/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 96 [13696/50000]	Loss: 0.2511	LR: 0.012500
Training Epoch: 96 [13824/50000]	Loss: 0.2200	LR: 0.012500
Training Epoch: 96 [13952/50000]	Loss: 0.1076	LR: 0.012500
Training Epoch: 96 [14080/50000]	Loss: 0.2249	LR: 0.012500
Training Epoch: 96 [14208/50000]	Loss: 0.1929	LR: 0.012500
Training Epoch: 96 [14336/50000]	Loss: 0.1720	LR: 0.012500
Training Epoch: 96 [14464/50000]	Loss: 0.1078	LR: 0.012500
Training Epoch: 96 [14592/50000]	Loss: 0.2067	LR: 0.012500
Training Epoch: 96 [14720/50000]	Loss: 0.1970	LR: 0.012500
Training Epoch: 96 [14848/50000]	Loss: 0.2471	LR: 0.012500
Training Epoch: 96 [14976/50000]	Loss: 0.1660	LR: 0.012500
Training Epoch: 96 [15104/50000]	Loss: 0.1348	LR: 0.012500
Training Epoch: 96 [15232/50000]	Loss: 0.1927	LR: 0.012500
Training Epoch: 96 [15360/50000]	Loss: 0.2252	LR: 0.012500
Training Epoch: 96 [15488/50000]	Loss: 0.1683	LR: 0.012500
Training Epoch: 96 [15616/50000]	Loss: 0.1621	LR: 0.012500
Training Epoch: 96 [15744/50000]	Loss: 0.1655	LR: 0.012500
Training Epoch: 96 [15872/50000]	Loss: 0.1492	LR: 0.012500
Training Epoch: 96 [16000/50000]	Loss: 0.1200	LR: 0.012500
Training Epoch: 96 [16128/50000]	Loss: 0.1730	LR: 0.012500
Training Epoch: 96 [16256/50000]	Loss: 0.1039	LR: 0.012500
Training Epoch: 96 [16384/50000]	Loss: 0.1699	LR: 0.012500
Training Epoch: 96 [16512/50000]	Loss: 0.1678	LR: 0.012500
Training Epoch: 96 [16640/50000]	Loss: 0.1684	LR: 0.012500
Training Epoch: 96 [16768/50000]	Loss: 0.1753	LR: 0.012500
Training Epoch: 96 [16896/50000]	Loss: 0.2376	LR: 0.012500
Training Epoch: 96 [17024/50000]	Loss: 0.1857	LR: 0.012500
Training Epoch: 96 [17152/50000]	Loss: 0.2064	LR: 0.012500
Training Epoch: 96 [17280/50000]	Loss: 0.1760	LR: 0.012500
Training Epoch: 96 [17408/50000]	Loss: 0.0922	LR: 0.012500
Training Epoch: 96 [17536/50000]	Loss: 0.1502	LR: 0.012500
Training Epoch: 96 [17664/50000]	Loss: 0.1196	LR: 0.012500
Training Epoch: 96 [17792/50000]	Loss: 0.2359	LR: 0.012500
Training Epoch: 96 [17920/50000]	Loss: 0.1847	LR: 0.012500
Training Epoch: 96 [18048/50000]	Loss: 0.1579	LR: 0.012500
Training Epoch: 96 [18176/50000]	Loss: 0.2233	LR: 0.012500
Training Epoch: 96 [18304/50000]	Loss: 0.1860	LR: 0.012500
Training Epoch: 96 [18432/50000]	Loss: 0.2035	LR: 0.012500
Training Epoch: 96 [18560/50000]	Loss: 0.1742	LR: 0.012500
Training Epoch: 96 [18688/50000]	Loss: 0.1369	LR: 0.012500
Training Epoch: 96 [18816/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 96 [18944/50000]	Loss: 0.1819	LR: 0.012500
Training Epoch: 96 [19072/50000]	Loss: 0.2387	LR: 0.012500
Training Epoch: 96 [19200/50000]	Loss: 0.1983	LR: 0.012500
Training Epoch: 96 [19328/50000]	Loss: 0.1204	LR: 0.012500
Training Epoch: 96 [19456/50000]	Loss: 0.1468	LR: 0.012500
Training Epoch: 96 [19584/50000]	Loss: 0.1290	LR: 0.012500
Training Epoch: 96 [19712/50000]	Loss: 0.1905	LR: 0.012500
Training Epoch: 96 [19840/50000]	Loss: 0.1315	LR: 0.012500
Training Epoch: 96 [19968/50000]	Loss: 0.2213	LR: 0.012500
Training Epoch: 96 [20096/50000]	Loss: 0.1144	LR: 0.012500
Training Epoch: 96 [20224/50000]	Loss: 0.1455	LR: 0.012500
Training Epoch: 96 [20352/50000]	Loss: 0.2566	LR: 0.012500
Training Epoch: 96 [20480/50000]	Loss: 0.1730	LR: 0.012500
Training Epoch: 96 [20608/50000]	Loss: 0.2204	LR: 0.012500
Training Epoch: 96 [20736/50000]	Loss: 0.1527	LR: 0.012500
Training Epoch: 96 [20864/50000]	Loss: 0.1427	LR: 0.012500
Training Epoch: 96 [20992/50000]	Loss: 0.1415	LR: 0.012500
Training Epoch: 96 [21120/50000]	Loss: 0.2750	LR: 0.012500
Training Epoch: 96 [21248/50000]	Loss: 0.1448	LR: 0.012500
Training Epoch: 96 [21376/50000]	Loss: 0.1752	LR: 0.012500
Training Epoch: 96 [21504/50000]	Loss: 0.0956	LR: 0.012500
Training Epoch: 96 [21632/50000]	Loss: 0.2001	LR: 0.012500
Training Epoch: 96 [21760/50000]	Loss: 0.2113	LR: 0.012500
Training Epoch: 96 [21888/50000]	Loss: 0.2027	LR: 0.012500
Training Epoch: 96 [22016/50000]	Loss: 0.1407	LR: 0.012500
Training Epoch: 96 [22144/50000]	Loss: 0.1142	LR: 0.012500
Training Epoch: 96 [22272/50000]	Loss: 0.1600	LR: 0.012500
Training Epoch: 96 [22400/50000]	Loss: 0.1652	LR: 0.012500
Training Epoch: 96 [22528/50000]	Loss: 0.1555	LR: 0.012500
Training Epoch: 96 [22656/50000]	Loss: 0.2128	LR: 0.012500
Training Epoch: 96 [22784/50000]	Loss: 0.1456	LR: 0.012500
Training Epoch: 96 [22912/50000]	Loss: 0.1530	LR: 0.012500
Training Epoch: 96 [23040/50000]	Loss: 0.0801	LR: 0.012500
Training Epoch: 96 [23168/50000]	Loss: 0.2333	LR: 0.012500
Training Epoch: 96 [23296/50000]	Loss: 0.0886	LR: 0.012500
Training Epoch: 96 [23424/50000]	Loss: 0.1951	LR: 0.012500
Training Epoch: 96 [23552/50000]	Loss: 0.1191	LR: 0.012500
Training Epoch: 96 [23680/50000]	Loss: 0.2389	LR: 0.012500
Training Epoch: 96 [23808/50000]	Loss: 0.1804	LR: 0.012500
Training Epoch: 96 [23936/50000]	Loss: 0.1958	LR: 0.012500
Training Epoch: 96 [24064/50000]	Loss: 0.1454	LR: 0.012500
Training Epoch: 96 [24192/50000]	Loss: 0.1186	LR: 0.012500
Training Epoch: 96 [24320/50000]	Loss: 0.1681	LR: 0.012500
Training Epoch: 96 [24448/50000]	Loss: 0.1918	LR: 0.012500
Training Epoch: 96 [24576/50000]	Loss: 0.2443	LR: 0.012500
Training Epoch: 96 [24704/50000]	Loss: 0.1539	LR: 0.012500
Training Epoch: 96 [24832/50000]	Loss: 0.2142	LR: 0.012500
Training Epoch: 96 [24960/50000]	Loss: 0.2487	LR: 0.012500
Training Epoch: 96 [25088/50000]	Loss: 0.1610	LR: 0.012500
Training Epoch: 96 [25216/50000]	Loss: 0.1680	LR: 0.012500
Training Epoch: 96 [25344/50000]	Loss: 0.2064	LR: 0.012500
Training Epoch: 96 [25472/50000]	Loss: 0.1527	LR: 0.012500
Training Epoch: 96 [25600/50000]	Loss: 0.2465	LR: 0.012500
Training Epoch: 96 [25728/50000]	Loss: 0.2550	LR: 0.012500
Training Epoch: 96 [25856/50000]	Loss: 0.1175	LR: 0.012500
Training Epoch: 96 [25984/50000]	Loss: 0.2281	LR: 0.012500
Training Epoch: 96 [26112/50000]	Loss: 0.2020	LR: 0.012500
Training Epoch: 96 [26240/50000]	Loss: 0.1911	LR: 0.012500
Training Epoch: 96 [26368/50000]	Loss: 0.1681	LR: 0.012500
Training Epoch: 96 [26496/50000]	Loss: 0.2150	LR: 0.012500
Training Epoch: 96 [26624/50000]	Loss: 0.2685	LR: 0.012500
Training Epoch: 96 [26752/50000]	Loss: 0.1335	LR: 0.012500
Training Epoch: 96 [26880/50000]	Loss: 0.0976	LR: 0.012500
Training Epoch: 96 [27008/50000]	Loss: 0.1667	LR: 0.012500
Training Epoch: 96 [27136/50000]	Loss: 0.1943	LR: 0.012500
Training Epoch: 96 [27264/50000]	Loss: 0.0862	LR: 0.012500
Training Epoch: 96 [27392/50000]	Loss: 0.2569	LR: 0.012500
Training Epoch: 96 [27520/50000]	Loss: 0.1604	LR: 0.012500
Training Epoch: 96 [27648/50000]	Loss: 0.1593	LR: 0.012500
Training Epoch: 96 [27776/50000]	Loss: 0.2368	LR: 0.012500
Training Epoch: 96 [27904/50000]	Loss: 0.1087	LR: 0.012500
Training Epoch: 96 [28032/50000]	Loss: 0.1912	LR: 0.012500
Training Epoch: 96 [28160/50000]	Loss: 0.2443	LR: 0.012500
Training Epoch: 96 [28288/50000]	Loss: 0.2125	LR: 0.012500
Training Epoch: 96 [28416/50000]	Loss: 0.1662	LR: 0.012500
Training Epoch: 96 [28544/50000]	Loss: 0.1695	LR: 0.012500
Training Epoch: 96 [28672/50000]	Loss: 0.2075	LR: 0.012500
Training Epoch: 96 [28800/50000]	Loss: 0.2021	LR: 0.012500
Training Epoch: 96 [28928/50000]	Loss: 0.2079	LR: 0.012500
Training Epoch: 96 [29056/50000]	Loss: 0.1802	LR: 0.012500
Training Epoch: 96 [29184/50000]	Loss: 0.1633	LR: 0.012500
Training Epoch: 96 [29312/50000]	Loss: 0.1505	LR: 0.012500
Training Epoch: 96 [29440/50000]	Loss: 0.1775	LR: 0.012500
Training Epoch: 96 [29568/50000]	Loss: 0.1943	LR: 0.012500
Training Epoch: 96 [29696/50000]	Loss: 0.1534	LR: 0.012500
Training Epoch: 96 [29824/50000]	Loss: 0.1164	LR: 0.012500
Training Epoch: 96 [29952/50000]	Loss: 0.2205	LR: 0.012500
Training Epoch: 96 [30080/50000]	Loss: 0.1808	LR: 0.012500
Training Epoch: 96 [30208/50000]	Loss: 0.1767	LR: 0.012500
Training Epoch: 96 [30336/50000]	Loss: 0.2011	LR: 0.012500
Training Epoch: 96 [30464/50000]	Loss: 0.1521	LR: 0.012500
Training Epoch: 96 [30592/50000]	Loss: 0.1200	LR: 0.012500
Training Epoch: 96 [30720/50000]	Loss: 0.2029	LR: 0.012500
Training Epoch: 96 [30848/50000]	Loss: 0.1619	LR: 0.012500
Training Epoch: 96 [30976/50000]	Loss: 0.1315	LR: 0.012500
Training Epoch: 96 [31104/50000]	Loss: 0.2803	LR: 0.012500
Training Epoch: 96 [31232/50000]	Loss: 0.3844	LR: 0.012500
Training Epoch: 96 [31360/50000]	Loss: 0.2161	LR: 0.012500
Training Epoch: 96 [31488/50000]	Loss: 0.1722	LR: 0.012500
Training Epoch: 96 [31616/50000]	Loss: 0.2730	LR: 0.012500
Training Epoch: 96 [31744/50000]	Loss: 0.2260	LR: 0.012500
Training Epoch: 96 [31872/50000]	Loss: 0.1102	LR: 0.012500
Training Epoch: 96 [32000/50000]	Loss: 0.1645	LR: 0.012500
Training Epoch: 96 [32128/50000]	Loss: 0.1767	LR: 0.012500
Training Epoch: 96 [32256/50000]	Loss: 0.2186	LR: 0.012500
Training Epoch: 96 [32384/50000]	Loss: 0.2461	LR: 0.012500
Training Epoch: 96 [32512/50000]	Loss: 0.2419	LR: 0.012500
Training Epoch: 96 [32640/50000]	Loss: 0.1851	LR: 0.012500
Training Epoch: 96 [32768/50000]	Loss: 0.1601	LR: 0.012500
Training Epoch: 96 [32896/50000]	Loss: 0.2479	LR: 0.012500
Training Epoch: 96 [33024/50000]	Loss: 0.1137	LR: 0.012500
Training Epoch: 96 [33152/50000]	Loss: 0.1908	LR: 0.012500
Training Epoch: 96 [33280/50000]	Loss: 0.1588	LR: 0.012500
Training Epoch: 96 [33408/50000]	Loss: 0.2226	LR: 0.012500
Training Epoch: 96 [33536/50000]	Loss: 0.1626	LR: 0.012500
Training Epoch: 96 [33664/50000]	Loss: 0.2446	LR: 0.012500
Training Epoch: 96 [33792/50000]	Loss: 0.2068	LR: 0.012500
Training Epoch: 96 [33920/50000]	Loss: 0.1886	LR: 0.012500
Training Epoch: 96 [34048/50000]	Loss: 0.1689	LR: 0.012500
Training Epoch: 96 [34176/50000]	Loss: 0.2257	LR: 0.012500
Training Epoch: 96 [34304/50000]	Loss: 0.1689	LR: 0.012500
Training Epoch: 96 [34432/50000]	Loss: 0.1165	LR: 0.012500
Training Epoch: 96 [34560/50000]	Loss: 0.3464	LR: 0.012500
Training Epoch: 96 [34688/50000]	Loss: 0.2543	LR: 0.012500
Training Epoch: 96 [34816/50000]	Loss: 0.1398	LR: 0.012500
Training Epoch: 96 [34944/50000]	Loss: 0.1649	LR: 0.012500
Training Epoch: 96 [35072/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 96 [35200/50000]	Loss: 0.2485	LR: 0.012500
Training Epoch: 96 [35328/50000]	Loss: 0.2731	LR: 0.012500
Training Epoch: 96 [35456/50000]	Loss: 0.1819	LR: 0.012500
Training Epoch: 96 [35584/50000]	Loss: 0.1714	LR: 0.012500
Training Epoch: 96 [35712/50000]	Loss: 0.1507	LR: 0.012500
Training Epoch: 96 [35840/50000]	Loss: 0.1163	LR: 0.012500
Training Epoch: 96 [35968/50000]	Loss: 0.2169	LR: 0.012500
Training Epoch: 96 [36096/50000]	Loss: 0.1627	LR: 0.012500
Training Epoch: 96 [36224/50000]	Loss: 0.1868	LR: 0.012500
Training Epoch: 96 [36352/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 96 [36480/50000]	Loss: 0.2533	LR: 0.012500
Training Epoch: 96 [36608/50000]	Loss: 0.2546	LR: 0.012500
Training Epoch: 96 [36736/50000]	Loss: 0.1203	LR: 0.012500
Training Epoch: 96 [36864/50000]	Loss: 0.1298	LR: 0.012500
Training Epoch: 96 [36992/50000]	Loss: 0.2532	LR: 0.012500
Training Epoch: 96 [37120/50000]	Loss: 0.1900	LR: 0.012500
Training Epoch: 96 [37248/50000]	Loss: 0.1567	LR: 0.012500
Training Epoch: 96 [37376/50000]	Loss: 0.1603	LR: 0.012500
Training Epoch: 96 [37504/50000]	Loss: 0.1916	LR: 0.012500
Training Epoch: 96 [37632/50000]	Loss: 0.1454	LR: 0.012500
Training Epoch: 96 [37760/50000]	Loss: 0.2233	LR: 0.012500
Training Epoch: 96 [37888/50000]	Loss: 0.1610	LR: 0.012500
Training Epoch: 96 [38016/50000]	Loss: 0.2550	LR: 0.012500
Training Epoch: 96 [38144/50000]	Loss: 0.1561	LR: 0.012500
Training Epoch: 96 [38272/50000]	Loss: 0.2410	LR: 0.012500
Training Epoch: 96 [38400/50000]	Loss: 0.1855	LR: 0.012500
Training Epoch: 96 [38528/50000]	Loss: 0.2309	LR: 0.012500
Training Epoch: 96 [38656/50000]	Loss: 0.2302	LR: 0.012500
Training Epoch: 96 [38784/50000]	Loss: 0.2112	LR: 0.012500
Training Epoch: 96 [38912/50000]	Loss: 0.1308	LR: 0.012500
Training Epoch: 96 [39040/50000]	Loss: 0.1199	LR: 0.012500
Training Epoch: 96 [39168/50000]	Loss: 0.2540	LR: 0.012500
Training Epoch: 96 [39296/50000]	Loss: 0.1242	LR: 0.012500
Training Epoch: 96 [39424/50000]	Loss: 0.2584	LR: 0.012500
Training Epoch: 96 [39552/50000]	Loss: 0.1308	LR: 0.012500
Training Epoch: 96 [39680/50000]	Loss: 0.1374	LR: 0.012500
Training Epoch: 96 [39808/50000]	Loss: 0.1959	LR: 0.012500
Training Epoch: 96 [39936/50000]	Loss: 0.2598	LR: 0.012500
Training Epoch: 96 [40064/50000]	Loss: 0.2636	LR: 0.012500
Training Epoch: 96 [40192/50000]	Loss: 0.2867	LR: 0.012500
Training Epoch: 96 [40320/50000]	Loss: 0.1776	LR: 0.012500
Training Epoch: 96 [40448/50000]	Loss: 0.1817	LR: 0.012500
Training Epoch: 96 [40576/50000]	Loss: 0.3128	LR: 0.012500
Training Epoch: 96 [40704/50000]	Loss: 0.2040	LR: 0.012500
Training Epoch: 96 [40832/50000]	Loss: 0.1496	LR: 0.012500
Training Epoch: 96 [40960/50000]	Loss: 0.2040	LR: 0.012500
Training Epoch: 96 [41088/50000]	Loss: 0.1524	LR: 0.012500
Training Epoch: 96 [41216/50000]	Loss: 0.1238	LR: 0.012500
Training Epoch: 96 [41344/50000]	Loss: 0.2457	LR: 0.012500
Training Epoch: 96 [41472/50000]	Loss: 0.1159	LR: 0.012500
Training Epoch: 96 [41600/50000]	Loss: 0.1637	LR: 0.012500
Training Epoch: 96 [41728/50000]	Loss: 0.3169	LR: 0.012500
Training Epoch: 96 [41856/50000]	Loss: 0.1847	LR: 0.012500
Training Epoch: 96 [41984/50000]	Loss: 0.1214	LR: 0.012500
Training Epoch: 96 [42112/50000]	Loss: 0.1698	LR: 0.012500
Training Epoch: 96 [42240/50000]	Loss: 0.2226	LR: 0.012500
Training Epoch: 96 [42368/50000]	Loss: 0.2229	LR: 0.012500
Training Epoch: 96 [42496/50000]	Loss: 0.1470	LR: 0.012500
Training Epoch: 96 [42624/50000]	Loss: 0.2160	LR: 0.012500
Training Epoch: 96 [42752/50000]	Loss: 0.1945	LR: 0.012500
Training Epoch: 96 [42880/50000]	Loss: 0.2512	LR: 0.012500
Training Epoch: 96 [43008/50000]	Loss: 0.2893	LR: 0.012500
Training Epoch: 96 [43136/50000]	Loss: 0.2332	LR: 0.012500
Training Epoch: 96 [43264/50000]	Loss: 0.2697	LR: 0.012500
Training Epoch: 96 [43392/50000]	Loss: 0.2730	LR: 0.012500
Training Epoch: 96 [43520/50000]	Loss: 0.2676	LR: 0.012500
Training Epoch: 96 [43648/50000]	Loss: 0.2534	LR: 0.012500
Training Epoch: 96 [43776/50000]	Loss: 0.1620	LR: 0.012500
Training Epoch: 96 [43904/50000]	Loss: 0.1825	LR: 0.012500
Training Epoch: 96 [44032/50000]	Loss: 0.2801	LR: 0.012500
Training Epoch: 96 [44160/50000]	Loss: 0.1218	LR: 0.012500
Training Epoch: 96 [44288/50000]	Loss: 0.2476	LR: 0.012500
Training Epoch: 96 [44416/50000]	Loss: 0.2567	LR: 0.012500
Training Epoch: 96 [44544/50000]	Loss: 0.1232	LR: 0.012500
Training Epoch: 96 [44672/50000]	Loss: 0.1281	LR: 0.012500
Training Epoch: 96 [44800/50000]	Loss: 0.2470	LR: 0.012500
Training Epoch: 96 [44928/50000]	Loss: 0.2679	LR: 0.012500
Training Epoch: 96 [45056/50000]	Loss: 0.2625	LR: 0.012500
Training Epoch: 96 [45184/50000]	Loss: 0.1994	LR: 0.012500
Training Epoch: 96 [45312/50000]	Loss: 0.2679	LR: 0.012500
Training Epoch: 96 [45440/50000]	Loss: 0.2067	LR: 0.012500
Training Epoch: 96 [45568/50000]	Loss: 0.2526	LR: 0.012500
Training Epoch: 96 [45696/50000]	Loss: 0.1802	LR: 0.012500
Training Epoch: 96 [45824/50000]	Loss: 0.1420	LR: 0.012500
Training Epoch: 96 [45952/50000]	Loss: 0.1765	LR: 0.012500
Training Epoch: 96 [46080/50000]	Loss: 0.2207	LR: 0.012500
Training Epoch: 96 [46208/50000]	Loss: 0.1337	LR: 0.012500
Training Epoch: 96 [46336/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 96 [46464/50000]	Loss: 0.1976	LR: 0.012500
Training Epoch: 96 [46592/50000]	Loss: 0.1969	LR: 0.012500
Training Epoch: 96 [46720/50000]	Loss: 0.2299	LR: 0.012500
Training Epoch: 96 [46848/50000]	Loss: 0.1827	LR: 0.012500
Training Epoch: 96 [46976/50000]	Loss: 0.1953	LR: 0.012500
Training Epoch: 96 [47104/50000]	Loss: 0.1408	LR: 0.012500
Training Epoch: 96 [47232/50000]	Loss: 0.0949	LR: 0.012500
Training Epoch: 96 [47360/50000]	Loss: 0.2069	LR: 0.012500
Training Epoch: 96 [47488/50000]	Loss: 0.1945	LR: 0.012500
Training Epoch: 96 [47616/50000]	Loss: 0.2047	LR: 0.012500
Training Epoch: 96 [47744/50000]	Loss: 0.2457	LR: 0.012500
Training Epoch: 96 [47872/50000]	Loss: 0.1669	LR: 0.012500
Training Epoch: 96 [48000/50000]	Loss: 0.2601	LR: 0.012500
Training Epoch: 96 [48128/50000]	Loss: 0.2271	LR: 0.012500
Training Epoch: 96 [48256/50000]	Loss: 0.1316	LR: 0.012500
Training Epoch: 96 [48384/50000]	Loss: 0.2148	LR: 0.012500
Training Epoch: 96 [48512/50000]	Loss: 0.2016	LR: 0.012500
Training Epoch: 96 [48640/50000]	Loss: 0.1955	LR: 0.012500
Training Epoch: 96 [48768/50000]	Loss: 0.1377	LR: 0.012500
Training Epoch: 96 [48896/50000]	Loss: 0.2408	LR: 0.012500
Training Epoch: 96 [49024/50000]	Loss: 0.2312	LR: 0.012500
Training Epoch: 96 [49152/50000]	Loss: 0.3164	LR: 0.012500
Training Epoch: 96 [49280/50000]	Loss: 0.2171	LR: 0.012500
Training Epoch: 96 [49408/50000]	Loss: 0.2584	LR: 0.012500
Training Epoch: 96 [49536/50000]	Loss: 0.1524	LR: 0.012500
Training Epoch: 96 [49664/50000]	Loss: 0.1964	LR: 0.012500
Training Epoch: 96 [49792/50000]	Loss: 0.2462	LR: 0.012500
Training Epoch: 96 [49920/50000]	Loss: 0.2058	LR: 0.012500
Training Epoch: 96 [50000/50000]	Loss: 0.2208	LR: 0.012500
epoch 96 training time consumed: 53.99s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  352325 GB |  352325 GB |
|       from large pool |  123392 KB |    1034 MB |  351978 GB |  351978 GB |
|       from small pool |   10798 KB |      13 MB |     347 GB |     347 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  352325 GB |  352325 GB |
|       from large pool |  123392 KB |    1034 MB |  351978 GB |  351978 GB |
|       from small pool |   10798 KB |      13 MB |     347 GB |     347 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  155043 GB |  155043 GB |
|       from large pool |  155136 KB |  433088 KB |  154660 GB |  154659 GB |
|       from small pool |    1489 KB |    3494 KB |     383 GB |     383 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   13595 K  |   13594 K  |
|       from large pool |      24    |      65    |    7096 K  |    7096 K  |
|       from small pool |     232    |     275    |    6498 K  |    6498 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   13595 K  |   13594 K  |
|       from large pool |      24    |      65    |    7096 K  |    7096 K  |
|       from small pool |     232    |     275    |    6498 K  |    6498 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6750 K  |    6750 K  |
|       from large pool |       9    |      14    |    3434 K  |    3434 K  |
|       from small pool |      12    |      17    |    3315 K  |    3315 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 96, Average loss: 0.0103, Accuracy: 0.6942, Time consumed:3.46s

Training Epoch: 97 [128/50000]	Loss: 0.1814	LR: 0.012500
Training Epoch: 97 [256/50000]	Loss: 0.2558	LR: 0.012500
Training Epoch: 97 [384/50000]	Loss: 0.1898	LR: 0.012500
Training Epoch: 97 [512/50000]	Loss: 0.2412	LR: 0.012500
Training Epoch: 97 [640/50000]	Loss: 0.1564	LR: 0.012500
Training Epoch: 97 [768/50000]	Loss: 0.1282	LR: 0.012500
Training Epoch: 97 [896/50000]	Loss: 0.1709	LR: 0.012500
Training Epoch: 97 [1024/50000]	Loss: 0.1974	LR: 0.012500
Training Epoch: 97 [1152/50000]	Loss: 0.1667	LR: 0.012500
Training Epoch: 97 [1280/50000]	Loss: 0.1633	LR: 0.012500
Training Epoch: 97 [1408/50000]	Loss: 0.1966	LR: 0.012500
Training Epoch: 97 [1536/50000]	Loss: 0.1433	LR: 0.012500
Training Epoch: 97 [1664/50000]	Loss: 0.1083	LR: 0.012500
Training Epoch: 97 [1792/50000]	Loss: 0.1788	LR: 0.012500
Training Epoch: 97 [1920/50000]	Loss: 0.2492	LR: 0.012500
Training Epoch: 97 [2048/50000]	Loss: 0.2618	LR: 0.012500
Training Epoch: 97 [2176/50000]	Loss: 0.2078	LR: 0.012500
Training Epoch: 97 [2304/50000]	Loss: 0.1651	LR: 0.012500
Training Epoch: 97 [2432/50000]	Loss: 0.0978	LR: 0.012500
Training Epoch: 97 [2560/50000]	Loss: 0.1405	LR: 0.012500
Training Epoch: 97 [2688/50000]	Loss: 0.1569	LR: 0.012500
Training Epoch: 97 [2816/50000]	Loss: 0.1319	LR: 0.012500
Training Epoch: 97 [2944/50000]	Loss: 0.2852	LR: 0.012500
Training Epoch: 97 [3072/50000]	Loss: 0.1909	LR: 0.012500
Training Epoch: 97 [3200/50000]	Loss: 0.1968	LR: 0.012500
Training Epoch: 97 [3328/50000]	Loss: 0.1708	LR: 0.012500
Training Epoch: 97 [3456/50000]	Loss: 0.2155	LR: 0.012500
Training Epoch: 97 [3584/50000]	Loss: 0.1712	LR: 0.012500
Training Epoch: 97 [3712/50000]	Loss: 0.0943	LR: 0.012500
Training Epoch: 97 [3840/50000]	Loss: 0.1008	LR: 0.012500
Training Epoch: 97 [3968/50000]	Loss: 0.1352	LR: 0.012500
Training Epoch: 97 [4096/50000]	Loss: 0.2067	LR: 0.012500
Training Epoch: 97 [4224/50000]	Loss: 0.1711	LR: 0.012500
Training Epoch: 97 [4352/50000]	Loss: 0.1882	LR: 0.012500
Training Epoch: 97 [4480/50000]	Loss: 0.1040	LR: 0.012500
Training Epoch: 97 [4608/50000]	Loss: 0.1828	LR: 0.012500
Training Epoch: 97 [4736/50000]	Loss: 0.1495	LR: 0.012500
Training Epoch: 97 [4864/50000]	Loss: 0.1603	LR: 0.012500
Training Epoch: 97 [4992/50000]	Loss: 0.1912	LR: 0.012500
Training Epoch: 97 [5120/50000]	Loss: 0.1069	LR: 0.012500
Training Epoch: 97 [5248/50000]	Loss: 0.1095	LR: 0.012500
Training Epoch: 97 [5376/50000]	Loss: 0.1855	LR: 0.012500
Training Epoch: 97 [5504/50000]	Loss: 0.2998	LR: 0.012500
Training Epoch: 97 [5632/50000]	Loss: 0.1836	LR: 0.012500
Training Epoch: 97 [5760/50000]	Loss: 0.2456	LR: 0.012500
Training Epoch: 97 [5888/50000]	Loss: 0.2048	LR: 0.012500
Training Epoch: 97 [6016/50000]	Loss: 0.1259	LR: 0.012500
Training Epoch: 97 [6144/50000]	Loss: 0.1789	LR: 0.012500
Training Epoch: 97 [6272/50000]	Loss: 0.1699	LR: 0.012500
Training Epoch: 97 [6400/50000]	Loss: 0.1615	LR: 0.012500
Training Epoch: 97 [6528/50000]	Loss: 0.1887	LR: 0.012500
Training Epoch: 97 [6656/50000]	Loss: 0.2169	LR: 0.012500
Training Epoch: 97 [6784/50000]	Loss: 0.1915	LR: 0.012500
Training Epoch: 97 [6912/50000]	Loss: 0.0983	LR: 0.012500
Training Epoch: 97 [7040/50000]	Loss: 0.2858	LR: 0.012500
Training Epoch: 97 [7168/50000]	Loss: 0.1731	LR: 0.012500
Training Epoch: 97 [7296/50000]	Loss: 0.1153	LR: 0.012500
Training Epoch: 97 [7424/50000]	Loss: 0.1651	LR: 0.012500
Training Epoch: 97 [7552/50000]	Loss: 0.0873	LR: 0.012500
Training Epoch: 97 [7680/50000]	Loss: 0.1850	LR: 0.012500
Training Epoch: 97 [7808/50000]	Loss: 0.1575	LR: 0.012500
Training Epoch: 97 [7936/50000]	Loss: 0.2336	LR: 0.012500
Training Epoch: 97 [8064/50000]	Loss: 0.1317	LR: 0.012500
Training Epoch: 97 [8192/50000]	Loss: 0.1739	LR: 0.012500
Training Epoch: 97 [8320/50000]	Loss: 0.1494	LR: 0.012500
Training Epoch: 97 [8448/50000]	Loss: 0.1945	LR: 0.012500
Training Epoch: 97 [8576/50000]	Loss: 0.2474	LR: 0.012500
Training Epoch: 97 [8704/50000]	Loss: 0.1485	LR: 0.012500
Training Epoch: 97 [8832/50000]	Loss: 0.1806	LR: 0.012500
Training Epoch: 97 [8960/50000]	Loss: 0.1962	LR: 0.012500
Training Epoch: 97 [9088/50000]	Loss: 0.1572	LR: 0.012500
Training Epoch: 97 [9216/50000]	Loss: 0.1987	LR: 0.012500
Training Epoch: 97 [9344/50000]	Loss: 0.1581	LR: 0.012500
Training Epoch: 97 [9472/50000]	Loss: 0.1535	LR: 0.012500
Training Epoch: 97 [9600/50000]	Loss: 0.1459	LR: 0.012500
Training Epoch: 97 [9728/50000]	Loss: 0.0675	LR: 0.012500
Training Epoch: 97 [9856/50000]	Loss: 0.1232	LR: 0.012500
Training Epoch: 97 [9984/50000]	Loss: 0.1624	LR: 0.012500
Training Epoch: 97 [10112/50000]	Loss: 0.3323	LR: 0.012500
Training Epoch: 97 [10240/50000]	Loss: 0.2507	LR: 0.012500
Training Epoch: 97 [10368/50000]	Loss: 0.2819	LR: 0.012500
Training Epoch: 97 [10496/50000]	Loss: 0.1439	LR: 0.012500
Training Epoch: 97 [10624/50000]	Loss: 0.2177	LR: 0.012500
Training Epoch: 97 [10752/50000]	Loss: 0.1119	LR: 0.012500
Training Epoch: 97 [10880/50000]	Loss: 0.1754	LR: 0.012500
Training Epoch: 97 [11008/50000]	Loss: 0.0981	LR: 0.012500
Training Epoch: 97 [11136/50000]	Loss: 0.2576	LR: 0.012500
Training Epoch: 97 [11264/50000]	Loss: 0.1586	LR: 0.012500
Training Epoch: 97 [11392/50000]	Loss: 0.1305	LR: 0.012500
Training Epoch: 97 [11520/50000]	Loss: 0.2233	LR: 0.012500
Training Epoch: 97 [11648/50000]	Loss: 0.1436	LR: 0.012500
Training Epoch: 97 [11776/50000]	Loss: 0.1553	LR: 0.012500
Training Epoch: 97 [11904/50000]	Loss: 0.1641	LR: 0.012500
Training Epoch: 97 [12032/50000]	Loss: 0.1956	LR: 0.012500
Training Epoch: 97 [12160/50000]	Loss: 0.0974	LR: 0.012500
Training Epoch: 97 [12288/50000]	Loss: 0.1927	LR: 0.012500
Training Epoch: 97 [12416/50000]	Loss: 0.2316	LR: 0.012500
Training Epoch: 97 [12544/50000]	Loss: 0.1277	LR: 0.012500
Training Epoch: 97 [12672/50000]	Loss: 0.1319	LR: 0.012500
Training Epoch: 97 [12800/50000]	Loss: 0.1671	LR: 0.012500
Training Epoch: 97 [12928/50000]	Loss: 0.1486	LR: 0.012500
Training Epoch: 97 [13056/50000]	Loss: 0.1533	LR: 0.012500
Training Epoch: 97 [13184/50000]	Loss: 0.1728	LR: 0.012500
Training Epoch: 97 [13312/50000]	Loss: 0.1650	LR: 0.012500
Training Epoch: 97 [13440/50000]	Loss: 0.1230	LR: 0.012500
Training Epoch: 97 [13568/50000]	Loss: 0.0890	LR: 0.012500
Training Epoch: 97 [13696/50000]	Loss: 0.1195	LR: 0.012500
Training Epoch: 97 [13824/50000]	Loss: 0.1604	LR: 0.012500
Training Epoch: 97 [13952/50000]	Loss: 0.1537	LR: 0.012500
Training Epoch: 97 [14080/50000]	Loss: 0.1779	LR: 0.012500
Training Epoch: 97 [14208/50000]	Loss: 0.1704	LR: 0.012500
Training Epoch: 97 [14336/50000]	Loss: 0.1028	LR: 0.012500
Training Epoch: 97 [14464/50000]	Loss: 0.1362	LR: 0.012500
Training Epoch: 97 [14592/50000]	Loss: 0.1594	LR: 0.012500
Training Epoch: 97 [14720/50000]	Loss: 0.1095	LR: 0.012500
Training Epoch: 97 [14848/50000]	Loss: 0.1579	LR: 0.012500
Training Epoch: 97 [14976/50000]	Loss: 0.1443	LR: 0.012500
Training Epoch: 97 [15104/50000]	Loss: 0.1187	LR: 0.012500
Training Epoch: 97 [15232/50000]	Loss: 0.1260	LR: 0.012500
Training Epoch: 97 [15360/50000]	Loss: 0.1685	LR: 0.012500
Training Epoch: 97 [15488/50000]	Loss: 0.1288	LR: 0.012500
Training Epoch: 97 [15616/50000]	Loss: 0.1773	LR: 0.012500
Training Epoch: 97 [15744/50000]	Loss: 0.1550	LR: 0.012500
Training Epoch: 97 [15872/50000]	Loss: 0.1715	LR: 0.012500
Training Epoch: 97 [16000/50000]	Loss: 0.2634	LR: 0.012500
Training Epoch: 97 [16128/50000]	Loss: 0.1545	LR: 0.012500
Training Epoch: 97 [16256/50000]	Loss: 0.1390	LR: 0.012500
Training Epoch: 97 [16384/50000]	Loss: 0.2368	LR: 0.012500
Training Epoch: 97 [16512/50000]	Loss: 0.1227	LR: 0.012500
Training Epoch: 97 [16640/50000]	Loss: 0.1428	LR: 0.012500
Training Epoch: 97 [16768/50000]	Loss: 0.1155	LR: 0.012500
Training Epoch: 97 [16896/50000]	Loss: 0.1902	LR: 0.012500
Training Epoch: 97 [17024/50000]	Loss: 0.2529	LR: 0.012500
Training Epoch: 97 [17152/50000]	Loss: 0.1248	LR: 0.012500
Training Epoch: 97 [17280/50000]	Loss: 0.1112	LR: 0.012500
Training Epoch: 97 [17408/50000]	Loss: 0.1792	LR: 0.012500
Training Epoch: 97 [17536/50000]	Loss: 0.1434	LR: 0.012500
Training Epoch: 97 [17664/50000]	Loss: 0.1605	LR: 0.012500
Training Epoch: 97 [17792/50000]	Loss: 0.1120	LR: 0.012500
Training Epoch: 97 [17920/50000]	Loss: 0.1300	LR: 0.012500
Training Epoch: 97 [18048/50000]	Loss: 0.1379	LR: 0.012500
Training Epoch: 97 [18176/50000]	Loss: 0.1969	LR: 0.012500
Training Epoch: 97 [18304/50000]	Loss: 0.1774	LR: 0.012500
Training Epoch: 97 [18432/50000]	Loss: 0.1518	LR: 0.012500
Training Epoch: 97 [18560/50000]	Loss: 0.1921	LR: 0.012500
Training Epoch: 97 [18688/50000]	Loss: 0.1522	LR: 0.012500
Training Epoch: 97 [18816/50000]	Loss: 0.1614	LR: 0.012500
Training Epoch: 97 [18944/50000]	Loss: 0.1789	LR: 0.012500
Training Epoch: 97 [19072/50000]	Loss: 0.1326	LR: 0.012500
Training Epoch: 97 [19200/50000]	Loss: 0.1200	LR: 0.012500
Training Epoch: 97 [19328/50000]	Loss: 0.1816	LR: 0.012500
Training Epoch: 97 [19456/50000]	Loss: 0.1351	LR: 0.012500
Training Epoch: 97 [19584/50000]	Loss: 0.1513	LR: 0.012500
Training Epoch: 97 [19712/50000]	Loss: 0.1570	LR: 0.012500
Training Epoch: 97 [19840/50000]	Loss: 0.1649	LR: 0.012500
Training Epoch: 97 [19968/50000]	Loss: 0.1889	LR: 0.012500
Training Epoch: 97 [20096/50000]	Loss: 0.2124	LR: 0.012500
Training Epoch: 97 [20224/50000]	Loss: 0.1794	LR: 0.012500
Training Epoch: 97 [20352/50000]	Loss: 0.1209	LR: 0.012500
Training Epoch: 97 [20480/50000]	Loss: 0.1498	LR: 0.012500
Training Epoch: 97 [20608/50000]	Loss: 0.2195	LR: 0.012500
Training Epoch: 97 [20736/50000]	Loss: 0.2094	LR: 0.012500
Training Epoch: 97 [20864/50000]	Loss: 0.1888	LR: 0.012500
Training Epoch: 97 [20992/50000]	Loss: 0.1543	LR: 0.012500
Training Epoch: 97 [21120/50000]	Loss: 0.1438	LR: 0.012500
Training Epoch: 97 [21248/50000]	Loss: 0.1900	LR: 0.012500
Training Epoch: 97 [21376/50000]	Loss: 0.2405	LR: 0.012500
Training Epoch: 97 [21504/50000]	Loss: 0.1831	LR: 0.012500
Training Epoch: 97 [21632/50000]	Loss: 0.2038	LR: 0.012500
Training Epoch: 97 [21760/50000]	Loss: 0.1790	LR: 0.012500
Training Epoch: 97 [21888/50000]	Loss: 0.1581	LR: 0.012500
Training Epoch: 97 [22016/50000]	Loss: 0.1393	LR: 0.012500
Training Epoch: 97 [22144/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 97 [22272/50000]	Loss: 0.1471	LR: 0.012500
Training Epoch: 97 [22400/50000]	Loss: 0.1385	LR: 0.012500
Training Epoch: 97 [22528/50000]	Loss: 0.1062	LR: 0.012500
Training Epoch: 97 [22656/50000]	Loss: 0.1614	LR: 0.012500
Training Epoch: 97 [22784/50000]	Loss: 0.2465	LR: 0.012500
Training Epoch: 97 [22912/50000]	Loss: 0.1842	LR: 0.012500
Training Epoch: 97 [23040/50000]	Loss: 0.2008	LR: 0.012500
Training Epoch: 97 [23168/50000]	Loss: 0.2231	LR: 0.012500
Training Epoch: 97 [23296/50000]	Loss: 0.1746	LR: 0.012500
Training Epoch: 97 [23424/50000]	Loss: 0.1812	LR: 0.012500
Training Epoch: 97 [23552/50000]	Loss: 0.1683	LR: 0.012500
Training Epoch: 97 [23680/50000]	Loss: 0.1877	LR: 0.012500
Training Epoch: 97 [23808/50000]	Loss: 0.1958	LR: 0.012500
Training Epoch: 97 [23936/50000]	Loss: 0.1870	LR: 0.012500
Training Epoch: 97 [24064/50000]	Loss: 0.2473	LR: 0.012500
Training Epoch: 97 [24192/50000]	Loss: 0.1683	LR: 0.012500
Training Epoch: 97 [24320/50000]	Loss: 0.1595	LR: 0.012500
Training Epoch: 97 [24448/50000]	Loss: 0.1566	LR: 0.012500
Training Epoch: 97 [24576/50000]	Loss: 0.2144	LR: 0.012500
Training Epoch: 97 [24704/50000]	Loss: 0.1478	LR: 0.012500
Training Epoch: 97 [24832/50000]	Loss: 0.2050	LR: 0.012500
Training Epoch: 97 [24960/50000]	Loss: 0.1283	LR: 0.012500
Training Epoch: 97 [25088/50000]	Loss: 0.1563	LR: 0.012500
Training Epoch: 97 [25216/50000]	Loss: 0.1189	LR: 0.012500
Training Epoch: 97 [25344/50000]	Loss: 0.1138	LR: 0.012500
Training Epoch: 97 [25472/50000]	Loss: 0.2177	LR: 0.012500
Training Epoch: 97 [25600/50000]	Loss: 0.1362	LR: 0.012500
Training Epoch: 97 [25728/50000]	Loss: 0.1766	LR: 0.012500
Training Epoch: 97 [25856/50000]	Loss: 0.1145	LR: 0.012500
Training Epoch: 97 [25984/50000]	Loss: 0.2404	LR: 0.012500
Training Epoch: 97 [26112/50000]	Loss: 0.2195	LR: 0.012500
Training Epoch: 97 [26240/50000]	Loss: 0.1303	LR: 0.012500
Training Epoch: 97 [26368/50000]	Loss: 0.2495	LR: 0.012500
Training Epoch: 97 [26496/50000]	Loss: 0.1486	LR: 0.012500
Training Epoch: 97 [26624/50000]	Loss: 0.1962	LR: 0.012500
Training Epoch: 97 [26752/50000]	Loss: 0.2498	LR: 0.012500
Training Epoch: 97 [26880/50000]	Loss: 0.1404	LR: 0.012500
Training Epoch: 97 [27008/50000]	Loss: 0.1357	LR: 0.012500
Training Epoch: 97 [27136/50000]	Loss: 0.1759	LR: 0.012500
Training Epoch: 97 [27264/50000]	Loss: 0.0942	LR: 0.012500
Training Epoch: 97 [27392/50000]	Loss: 0.2067	LR: 0.012500
Training Epoch: 97 [27520/50000]	Loss: 0.1196	LR: 0.012500
Training Epoch: 97 [27648/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 97 [27776/50000]	Loss: 0.1690	LR: 0.012500
Training Epoch: 97 [27904/50000]	Loss: 0.1078	LR: 0.012500
Training Epoch: 97 [28032/50000]	Loss: 0.2663	LR: 0.012500
Training Epoch: 97 [28160/50000]	Loss: 0.2048	LR: 0.012500
Training Epoch: 97 [28288/50000]	Loss: 0.1879	LR: 0.012500
Training Epoch: 97 [28416/50000]	Loss: 0.1384	LR: 0.012500
Training Epoch: 97 [28544/50000]	Loss: 0.1185	LR: 0.012500
Training Epoch: 97 [28672/50000]	Loss: 0.1519	LR: 0.012500
Training Epoch: 97 [28800/50000]	Loss: 0.1871	LR: 0.012500
Training Epoch: 97 [28928/50000]	Loss: 0.2530	LR: 0.012500
Training Epoch: 97 [29056/50000]	Loss: 0.1874	LR: 0.012500
Training Epoch: 97 [29184/50000]	Loss: 0.1338	LR: 0.012500
Training Epoch: 97 [29312/50000]	Loss: 0.1260	LR: 0.012500
Training Epoch: 97 [29440/50000]	Loss: 0.1108	LR: 0.012500
Training Epoch: 97 [29568/50000]	Loss: 0.2245	LR: 0.012500
Training Epoch: 97 [29696/50000]	Loss: 0.1942	LR: 0.012500
Training Epoch: 97 [29824/50000]	Loss: 0.1833	LR: 0.012500
Training Epoch: 97 [29952/50000]	Loss: 0.1650	LR: 0.012500
Training Epoch: 97 [30080/50000]	Loss: 0.2214	LR: 0.012500
Training Epoch: 97 [30208/50000]	Loss: 0.2255	LR: 0.012500
Training Epoch: 97 [30336/50000]	Loss: 0.1440	LR: 0.012500
Training Epoch: 97 [30464/50000]	Loss: 0.1458	LR: 0.012500
Training Epoch: 97 [30592/50000]	Loss: 0.1648	LR: 0.012500
Training Epoch: 97 [30720/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 97 [30848/50000]	Loss: 0.1796	LR: 0.012500
Training Epoch: 97 [30976/50000]	Loss: 0.1843	LR: 0.012500
Training Epoch: 97 [31104/50000]	Loss: 0.1676	LR: 0.012500
Training Epoch: 97 [31232/50000]	Loss: 0.1152	LR: 0.012500
Training Epoch: 97 [31360/50000]	Loss: 0.2015	LR: 0.012500
Training Epoch: 97 [31488/50000]	Loss: 0.2114	LR: 0.012500
Training Epoch: 97 [31616/50000]	Loss: 0.2849	LR: 0.012500
Training Epoch: 97 [31744/50000]	Loss: 0.1085	LR: 0.012500
Training Epoch: 97 [31872/50000]	Loss: 0.1633	LR: 0.012500
Training Epoch: 97 [32000/50000]	Loss: 0.1343	LR: 0.012500
Training Epoch: 97 [32128/50000]	Loss: 0.1922	LR: 0.012500
Training Epoch: 97 [32256/50000]	Loss: 0.2298	LR: 0.012500
Training Epoch: 97 [32384/50000]	Loss: 0.2133	LR: 0.012500
Training Epoch: 97 [32512/50000]	Loss: 0.2082	LR: 0.012500
Training Epoch: 97 [32640/50000]	Loss: 0.2527	LR: 0.012500
Training Epoch: 97 [32768/50000]	Loss: 0.2376	LR: 0.012500
Training Epoch: 97 [32896/50000]	Loss: 0.1766	LR: 0.012500
Training Epoch: 97 [33024/50000]	Loss: 0.1661	LR: 0.012500
Training Epoch: 97 [33152/50000]	Loss: 0.1078	LR: 0.012500
Training Epoch: 97 [33280/50000]	Loss: 0.2031	LR: 0.012500
Training Epoch: 97 [33408/50000]	Loss: 0.1319	LR: 0.012500
Training Epoch: 97 [33536/50000]	Loss: 0.2038	LR: 0.012500
Training Epoch: 97 [33664/50000]	Loss: 0.1659	LR: 0.012500
Training Epoch: 97 [33792/50000]	Loss: 0.2180	LR: 0.012500
Training Epoch: 97 [33920/50000]	Loss: 0.1536	LR: 0.012500
Training Epoch: 97 [34048/50000]	Loss: 0.2601	LR: 0.012500
Training Epoch: 97 [34176/50000]	Loss: 0.1555	LR: 0.012500
Training Epoch: 97 [34304/50000]	Loss: 0.2143	LR: 0.012500
Training Epoch: 97 [34432/50000]	Loss: 0.1702	LR: 0.012500
Training Epoch: 97 [34560/50000]	Loss: 0.1570	LR: 0.012500
Training Epoch: 97 [34688/50000]	Loss: 0.2136	LR: 0.012500
Training Epoch: 97 [34816/50000]	Loss: 0.1412	LR: 0.012500
Training Epoch: 97 [34944/50000]	Loss: 0.2469	LR: 0.012500
Training Epoch: 97 [35072/50000]	Loss: 0.1859	LR: 0.012500
Training Epoch: 97 [35200/50000]	Loss: 0.1367	LR: 0.012500
Training Epoch: 97 [35328/50000]	Loss: 0.1719	LR: 0.012500
Training Epoch: 97 [35456/50000]	Loss: 0.1480	LR: 0.012500
Training Epoch: 97 [35584/50000]	Loss: 0.2322	LR: 0.012500
Training Epoch: 97 [35712/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 97 [35840/50000]	Loss: 0.2660	LR: 0.012500
Training Epoch: 97 [35968/50000]	Loss: 0.2036	LR: 0.012500
Training Epoch: 97 [36096/50000]	Loss: 0.2047	LR: 0.012500
Training Epoch: 97 [36224/50000]	Loss: 0.1824	LR: 0.012500
Training Epoch: 97 [36352/50000]	Loss: 0.2116	LR: 0.012500
Training Epoch: 97 [36480/50000]	Loss: 0.1948	LR: 0.012500
Training Epoch: 97 [36608/50000]	Loss: 0.2078	LR: 0.012500
Training Epoch: 97 [36736/50000]	Loss: 0.1793	LR: 0.012500
Training Epoch: 97 [36864/50000]	Loss: 0.1819	LR: 0.012500
Training Epoch: 97 [36992/50000]	Loss: 0.1508	LR: 0.012500
Training Epoch: 97 [37120/50000]	Loss: 0.1909	LR: 0.012500
Training Epoch: 97 [37248/50000]	Loss: 0.1483	LR: 0.012500
Training Epoch: 97 [37376/50000]	Loss: 0.1693	LR: 0.012500
Training Epoch: 97 [37504/50000]	Loss: 0.1674	LR: 0.012500
Training Epoch: 97 [37632/50000]	Loss: 0.2328	LR: 0.012500
Training Epoch: 97 [37760/50000]	Loss: 0.1919	LR: 0.012500
Training Epoch: 97 [37888/50000]	Loss: 0.2593	LR: 0.012500
Training Epoch: 97 [38016/50000]	Loss: 0.2634	LR: 0.012500
Training Epoch: 97 [38144/50000]	Loss: 0.2431	LR: 0.012500
Training Epoch: 97 [38272/50000]	Loss: 0.2543	LR: 0.012500
Training Epoch: 97 [38400/50000]	Loss: 0.1883	LR: 0.012500
Training Epoch: 97 [38528/50000]	Loss: 0.2582	LR: 0.012500
Training Epoch: 97 [38656/50000]	Loss: 0.2604	LR: 0.012500
Training Epoch: 97 [38784/50000]	Loss: 0.1956	LR: 0.012500
Training Epoch: 97 [38912/50000]	Loss: 0.2083	LR: 0.012500
Training Epoch: 97 [39040/50000]	Loss: 0.1456	LR: 0.012500
Training Epoch: 97 [39168/50000]	Loss: 0.2399	LR: 0.012500
Training Epoch: 97 [39296/50000]	Loss: 0.2598	LR: 0.012500
Training Epoch: 97 [39424/50000]	Loss: 0.1817	LR: 0.012500
Training Epoch: 97 [39552/50000]	Loss: 0.1472	LR: 0.012500
Training Epoch: 97 [39680/50000]	Loss: 0.1145	LR: 0.012500
Training Epoch: 97 [39808/50000]	Loss: 0.2984	LR: 0.012500
Training Epoch: 97 [39936/50000]	Loss: 0.1829	LR: 0.012500
Training Epoch: 97 [40064/50000]	Loss: 0.1529	LR: 0.012500
Training Epoch: 97 [40192/50000]	Loss: 0.1991	LR: 0.012500
Training Epoch: 97 [40320/50000]	Loss: 0.2020	LR: 0.012500
Training Epoch: 97 [40448/50000]	Loss: 0.2389	LR: 0.012500
Training Epoch: 97 [40576/50000]	Loss: 0.1958	LR: 0.012500
Training Epoch: 97 [40704/50000]	Loss: 0.2489	LR: 0.012500
Training Epoch: 97 [40832/50000]	Loss: 0.2495	LR: 0.012500
Training Epoch: 97 [40960/50000]	Loss: 0.1885	LR: 0.012500
Training Epoch: 97 [41088/50000]	Loss: 0.2280	LR: 0.012500
Training Epoch: 97 [41216/50000]	Loss: 0.1791	LR: 0.012500
Training Epoch: 97 [41344/50000]	Loss: 0.2504	LR: 0.012500
Training Epoch: 97 [41472/50000]	Loss: 0.1424	LR: 0.012500
Training Epoch: 97 [41600/50000]	Loss: 0.2269	LR: 0.012500
Training Epoch: 97 [41728/50000]	Loss: 0.1620	LR: 0.012500
Training Epoch: 97 [41856/50000]	Loss: 0.1414	LR: 0.012500
Training Epoch: 97 [41984/50000]	Loss: 0.2822	LR: 0.012500
Training Epoch: 97 [42112/50000]	Loss: 0.2135	LR: 0.012500
Training Epoch: 97 [42240/50000]	Loss: 0.2205	LR: 0.012500
Training Epoch: 97 [42368/50000]	Loss: 0.2137	LR: 0.012500
Training Epoch: 97 [42496/50000]	Loss: 0.2186	LR: 0.012500
Training Epoch: 97 [42624/50000]	Loss: 0.2370	LR: 0.012500
Training Epoch: 97 [42752/50000]	Loss: 0.2858	LR: 0.012500
Training Epoch: 97 [42880/50000]	Loss: 0.2105	LR: 0.012500
Training Epoch: 97 [43008/50000]	Loss: 0.1254	LR: 0.012500
Training Epoch: 97 [43136/50000]	Loss: 0.1301	LR: 0.012500
Training Epoch: 97 [43264/50000]	Loss: 0.2411	LR: 0.012500
Training Epoch: 97 [43392/50000]	Loss: 0.1723	LR: 0.012500
Training Epoch: 97 [43520/50000]	Loss: 0.1330	LR: 0.012500
Training Epoch: 97 [43648/50000]	Loss: 0.2602	LR: 0.012500
Training Epoch: 97 [43776/50000]	Loss: 0.2083	LR: 0.012500
Training Epoch: 97 [43904/50000]	Loss: 0.1802	LR: 0.012500
Training Epoch: 97 [44032/50000]	Loss: 0.2318	LR: 0.012500
Training Epoch: 97 [44160/50000]	Loss: 0.2349	LR: 0.012500
Training Epoch: 97 [44288/50000]	Loss: 0.3771	LR: 0.012500
Training Epoch: 97 [44416/50000]	Loss: 0.1893	LR: 0.012500
Training Epoch: 97 [44544/50000]	Loss: 0.1265	LR: 0.012500
Training Epoch: 97 [44672/50000]	Loss: 0.1733	LR: 0.012500
Training Epoch: 97 [44800/50000]	Loss: 0.1936	LR: 0.012500
Training Epoch: 97 [44928/50000]	Loss: 0.1477	LR: 0.012500
Training Epoch: 97 [45056/50000]	Loss: 0.2099	LR: 0.012500
Training Epoch: 97 [45184/50000]	Loss: 0.1790	LR: 0.012500
Training Epoch: 97 [45312/50000]	Loss: 0.3037	LR: 0.012500
Training Epoch: 97 [45440/50000]	Loss: 0.2840	LR: 0.012500
Training Epoch: 97 [45568/50000]	Loss: 0.2288	LR: 0.012500
Training Epoch: 97 [45696/50000]	Loss: 0.2654	LR: 0.012500
Training Epoch: 97 [45824/50000]	Loss: 0.1958	LR: 0.012500
Training Epoch: 97 [45952/50000]	Loss: 0.1956	LR: 0.012500
Training Epoch: 97 [46080/50000]	Loss: 0.2101	LR: 0.012500
Training Epoch: 97 [46208/50000]	Loss: 0.1887	LR: 0.012500
Training Epoch: 97 [46336/50000]	Loss: 0.2482	LR: 0.012500
Training Epoch: 97 [46464/50000]	Loss: 0.1608	LR: 0.012500
Training Epoch: 97 [46592/50000]	Loss: 0.1848	LR: 0.012500
Training Epoch: 97 [46720/50000]	Loss: 0.1361	LR: 0.012500
Training Epoch: 97 [46848/50000]	Loss: 0.2498	LR: 0.012500
Training Epoch: 97 [46976/50000]	Loss: 0.2250	LR: 0.012500
Training Epoch: 97 [47104/50000]	Loss: 0.1683	LR: 0.012500
Training Epoch: 97 [47232/50000]	Loss: 0.2265	LR: 0.012500
Training Epoch: 97 [47360/50000]	Loss: 0.1704	LR: 0.012500
Training Epoch: 97 [47488/50000]	Loss: 0.2075	LR: 0.012500
Training Epoch: 97 [47616/50000]	Loss: 0.2019	LR: 0.012500
Training Epoch: 97 [47744/50000]	Loss: 0.2866	LR: 0.012500
Training Epoch: 97 [47872/50000]	Loss: 0.1409	LR: 0.012500
Training Epoch: 97 [48000/50000]	Loss: 0.2078	LR: 0.012500
Training Epoch: 97 [48128/50000]	Loss: 0.1791	LR: 0.012500
Training Epoch: 97 [48256/50000]	Loss: 0.3386	LR: 0.012500
Training Epoch: 97 [48384/50000]	Loss: 0.2352	LR: 0.012500
Training Epoch: 97 [48512/50000]	Loss: 0.1612	LR: 0.012500
Training Epoch: 97 [48640/50000]	Loss: 0.1991	LR: 0.012500
Training Epoch: 97 [48768/50000]	Loss: 0.1847	LR: 0.012500
Training Epoch: 97 [48896/50000]	Loss: 0.1831	LR: 0.012500
Training Epoch: 97 [49024/50000]	Loss: 0.1868	LR: 0.012500
Training Epoch: 97 [49152/50000]	Loss: 0.1752	LR: 0.012500
Training Epoch: 97 [49280/50000]	Loss: 0.1541	LR: 0.012500
Training Epoch: 97 [49408/50000]	Loss: 0.2643	LR: 0.012500
Training Epoch: 97 [49536/50000]	Loss: 0.2525	LR: 0.012500
Training Epoch: 97 [49664/50000]	Loss: 0.1618	LR: 0.012500
Training Epoch: 97 [49792/50000]	Loss: 0.2187	LR: 0.012500
Training Epoch: 97 [49920/50000]	Loss: 0.1074	LR: 0.012500
Training Epoch: 97 [50000/50000]	Loss: 0.3748	LR: 0.012500
epoch 97 training time consumed: 53.98s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  355995 GB |  355995 GB |
|       from large pool |  123392 KB |    1034 MB |  355645 GB |  355644 GB |
|       from small pool |   10798 KB |      13 MB |     350 GB |     350 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  355995 GB |  355995 GB |
|       from large pool |  123392 KB |    1034 MB |  355645 GB |  355644 GB |
|       from small pool |   10798 KB |      13 MB |     350 GB |     350 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  156658 GB |  156658 GB |
|       from large pool |  155136 KB |  433088 KB |  156271 GB |  156270 GB |
|       from small pool |    1489 KB |    3494 KB |     387 GB |     387 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   13736 K  |   13736 K  |
|       from large pool |      24    |      65    |    7170 K  |    7170 K  |
|       from small pool |     232    |     275    |    6566 K  |    6566 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   13736 K  |   13736 K  |
|       from large pool |      24    |      65    |    7170 K  |    7170 K  |
|       from small pool |     232    |     275    |    6566 K  |    6566 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6822 K  |    6822 K  |
|       from large pool |       9    |      14    |    3470 K  |    3470 K  |
|       from small pool |      12    |      17    |    3352 K  |    3352 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 97, Average loss: 0.0106, Accuracy: 0.6862, Time consumed:3.46s

Training Epoch: 98 [128/50000]	Loss: 0.1684	LR: 0.012500
Training Epoch: 98 [256/50000]	Loss: 0.1376	LR: 0.012500
Training Epoch: 98 [384/50000]	Loss: 0.1898	LR: 0.012500
Training Epoch: 98 [512/50000]	Loss: 0.2425	LR: 0.012500
Training Epoch: 98 [640/50000]	Loss: 0.2235	LR: 0.012500
Training Epoch: 98 [768/50000]	Loss: 0.2429	LR: 0.012500
Training Epoch: 98 [896/50000]	Loss: 0.1477	LR: 0.012500
Training Epoch: 98 [1024/50000]	Loss: 0.1094	LR: 0.012500
Training Epoch: 98 [1152/50000]	Loss: 0.2029	LR: 0.012500
Training Epoch: 98 [1280/50000]	Loss: 0.1391	LR: 0.012500
Training Epoch: 98 [1408/50000]	Loss: 0.2107	LR: 0.012500
Training Epoch: 98 [1536/50000]	Loss: 0.2869	LR: 0.012500
Training Epoch: 98 [1664/50000]	Loss: 0.1853	LR: 0.012500
Training Epoch: 98 [1792/50000]	Loss: 0.1664	LR: 0.012500
Training Epoch: 98 [1920/50000]	Loss: 0.1693	LR: 0.012500
Training Epoch: 98 [2048/50000]	Loss: 0.1798	LR: 0.012500
Training Epoch: 98 [2176/50000]	Loss: 0.1943	LR: 0.012500
Training Epoch: 98 [2304/50000]	Loss: 0.1728	LR: 0.012500
Training Epoch: 98 [2432/50000]	Loss: 0.1283	LR: 0.012500
Training Epoch: 98 [2560/50000]	Loss: 0.1505	LR: 0.012500
Training Epoch: 98 [2688/50000]	Loss: 0.2269	LR: 0.012500
Training Epoch: 98 [2816/50000]	Loss: 0.1287	LR: 0.012500
Training Epoch: 98 [2944/50000]	Loss: 0.2445	LR: 0.012500
Training Epoch: 98 [3072/50000]	Loss: 0.1780	LR: 0.012500
Training Epoch: 98 [3200/50000]	Loss: 0.1424	LR: 0.012500
Training Epoch: 98 [3328/50000]	Loss: 0.2553	LR: 0.012500
Training Epoch: 98 [3456/50000]	Loss: 0.1725	LR: 0.012500
Training Epoch: 98 [3584/50000]	Loss: 0.1566	LR: 0.012500
Training Epoch: 98 [3712/50000]	Loss: 0.2110	LR: 0.012500
Training Epoch: 98 [3840/50000]	Loss: 0.1645	LR: 0.012500
Training Epoch: 98 [3968/50000]	Loss: 0.1782	LR: 0.012500
Training Epoch: 98 [4096/50000]	Loss: 0.1447	LR: 0.012500
Training Epoch: 98 [4224/50000]	Loss: 0.2413	LR: 0.012500
Training Epoch: 98 [4352/50000]	Loss: 0.1835	LR: 0.012500
Training Epoch: 98 [4480/50000]	Loss: 0.1604	LR: 0.012500
Training Epoch: 98 [4608/50000]	Loss: 0.1468	LR: 0.012500
Training Epoch: 98 [4736/50000]	Loss: 0.1955	LR: 0.012500
Training Epoch: 98 [4864/50000]	Loss: 0.1788	LR: 0.012500
Training Epoch: 98 [4992/50000]	Loss: 0.1834	LR: 0.012500
Training Epoch: 98 [5120/50000]	Loss: 0.1371	LR: 0.012500
Training Epoch: 98 [5248/50000]	Loss: 0.2065	LR: 0.012500
Training Epoch: 98 [5376/50000]	Loss: 0.2204	LR: 0.012500
Training Epoch: 98 [5504/50000]	Loss: 0.1469	LR: 0.012500
Training Epoch: 98 [5632/50000]	Loss: 0.1781	LR: 0.012500
Training Epoch: 98 [5760/50000]	Loss: 0.1510	LR: 0.012500
Training Epoch: 98 [5888/50000]	Loss: 0.2592	LR: 0.012500
Training Epoch: 98 [6016/50000]	Loss: 0.1442	LR: 0.012500
Training Epoch: 98 [6144/50000]	Loss: 0.1682	LR: 0.012500
Training Epoch: 98 [6272/50000]	Loss: 0.1472	LR: 0.012500
Training Epoch: 98 [6400/50000]	Loss: 0.2394	LR: 0.012500
Training Epoch: 98 [6528/50000]	Loss: 0.0892	LR: 0.012500
Training Epoch: 98 [6656/50000]	Loss: 0.1436	LR: 0.012500
Training Epoch: 98 [6784/50000]	Loss: 0.1629	LR: 0.012500
Training Epoch: 98 [6912/50000]	Loss: 0.1129	LR: 0.012500
Training Epoch: 98 [7040/50000]	Loss: 0.1043	LR: 0.012500
Training Epoch: 98 [7168/50000]	Loss: 0.2145	LR: 0.012500
Training Epoch: 98 [7296/50000]	Loss: 0.1715	LR: 0.012500
Training Epoch: 98 [7424/50000]	Loss: 0.1427	LR: 0.012500
Training Epoch: 98 [7552/50000]	Loss: 0.2292	LR: 0.012500
Training Epoch: 98 [7680/50000]	Loss: 0.1326	LR: 0.012500
Training Epoch: 98 [7808/50000]	Loss: 0.1297	LR: 0.012500
Training Epoch: 98 [7936/50000]	Loss: 0.1917	LR: 0.012500
Training Epoch: 98 [8064/50000]	Loss: 0.2351	LR: 0.012500
Training Epoch: 98 [8192/50000]	Loss: 0.1219	LR: 0.012500
Training Epoch: 98 [8320/50000]	Loss: 0.1542	LR: 0.012500
Training Epoch: 98 [8448/50000]	Loss: 0.1433	LR: 0.012500
Training Epoch: 98 [8576/50000]	Loss: 0.2051	LR: 0.012500
Training Epoch: 98 [8704/50000]	Loss: 0.1699	LR: 0.012500
Training Epoch: 98 [8832/50000]	Loss: 0.1233	LR: 0.012500
Training Epoch: 98 [8960/50000]	Loss: 0.1894	LR: 0.012500
Training Epoch: 98 [9088/50000]	Loss: 0.1323	LR: 0.012500
Training Epoch: 98 [9216/50000]	Loss: 0.1627	LR: 0.012500
Training Epoch: 98 [9344/50000]	Loss: 0.1507	LR: 0.012500
Training Epoch: 98 [9472/50000]	Loss: 0.1493	LR: 0.012500
Training Epoch: 98 [9600/50000]	Loss: 0.1718	LR: 0.012500
Training Epoch: 98 [9728/50000]	Loss: 0.1718	LR: 0.012500
Training Epoch: 98 [9856/50000]	Loss: 0.1647	LR: 0.012500
Training Epoch: 98 [9984/50000]	Loss: 0.2580	LR: 0.012500
Training Epoch: 98 [10112/50000]	Loss: 0.1640	LR: 0.012500
Training Epoch: 98 [10240/50000]	Loss: 0.1649	LR: 0.012500
Training Epoch: 98 [10368/50000]	Loss: 0.1585	LR: 0.012500
Training Epoch: 98 [10496/50000]	Loss: 0.1507	LR: 0.012500
Training Epoch: 98 [10624/50000]	Loss: 0.1575	LR: 0.012500
Training Epoch: 98 [10752/50000]	Loss: 0.1721	LR: 0.012500
Training Epoch: 98 [10880/50000]	Loss: 0.1824	LR: 0.012500
Training Epoch: 98 [11008/50000]	Loss: 0.1864	LR: 0.012500
Training Epoch: 98 [11136/50000]	Loss: 0.1410	LR: 0.012500
Training Epoch: 98 [11264/50000]	Loss: 0.1656	LR: 0.012500
Training Epoch: 98 [11392/50000]	Loss: 0.1695	LR: 0.012500
Training Epoch: 98 [11520/50000]	Loss: 0.2180	LR: 0.012500
Training Epoch: 98 [11648/50000]	Loss: 0.2551	LR: 0.012500
Training Epoch: 98 [11776/50000]	Loss: 0.1610	LR: 0.012500
Training Epoch: 98 [11904/50000]	Loss: 0.1684	LR: 0.012500
Training Epoch: 98 [12032/50000]	Loss: 0.1673	LR: 0.012500
Training Epoch: 98 [12160/50000]	Loss: 0.1757	LR: 0.012500
Training Epoch: 98 [12288/50000]	Loss: 0.1593	LR: 0.012500
Training Epoch: 98 [12416/50000]	Loss: 0.2039	LR: 0.012500
Training Epoch: 98 [12544/50000]	Loss: 0.1967	LR: 0.012500
Training Epoch: 98 [12672/50000]	Loss: 0.1656	LR: 0.012500
Training Epoch: 98 [12800/50000]	Loss: 0.1731	LR: 0.012500
Training Epoch: 98 [12928/50000]	Loss: 0.2752	LR: 0.012500
Training Epoch: 98 [13056/50000]	Loss: 0.1643	LR: 0.012500
Training Epoch: 98 [13184/50000]	Loss: 0.1357	LR: 0.012500
Training Epoch: 98 [13312/50000]	Loss: 0.2525	LR: 0.012500
Training Epoch: 98 [13440/50000]	Loss: 0.1589	LR: 0.012500
Training Epoch: 98 [13568/50000]	Loss: 0.1910	LR: 0.012500
Training Epoch: 98 [13696/50000]	Loss: 0.1869	LR: 0.012500
Training Epoch: 98 [13824/50000]	Loss: 0.1414	LR: 0.012500
Training Epoch: 98 [13952/50000]	Loss: 0.2180	LR: 0.012500
Training Epoch: 98 [14080/50000]	Loss: 0.1375	LR: 0.012500
Training Epoch: 98 [14208/50000]	Loss: 0.2129	LR: 0.012500
Training Epoch: 98 [14336/50000]	Loss: 0.1520	LR: 0.012500
Training Epoch: 98 [14464/50000]	Loss: 0.1612	LR: 0.012500
Training Epoch: 98 [14592/50000]	Loss: 0.1612	LR: 0.012500
Training Epoch: 98 [14720/50000]	Loss: 0.1842	LR: 0.012500
Training Epoch: 98 [14848/50000]	Loss: 0.1798	LR: 0.012500
Training Epoch: 98 [14976/50000]	Loss: 0.1382	LR: 0.012500
Training Epoch: 98 [15104/50000]	Loss: 0.1842	LR: 0.012500
Training Epoch: 98 [15232/50000]	Loss: 0.1110	LR: 0.012500
Training Epoch: 98 [15360/50000]	Loss: 0.1001	LR: 0.012500
Training Epoch: 98 [15488/50000]	Loss: 0.1667	LR: 0.012500
Training Epoch: 98 [15616/50000]	Loss: 0.1341	LR: 0.012500
Training Epoch: 98 [15744/50000]	Loss: 0.1692	LR: 0.012500
Training Epoch: 98 [15872/50000]	Loss: 0.1362	LR: 0.012500
Training Epoch: 98 [16000/50000]	Loss: 0.2610	LR: 0.012500
Training Epoch: 98 [16128/50000]	Loss: 0.2086	LR: 0.012500
Training Epoch: 98 [16256/50000]	Loss: 0.1524	LR: 0.012500
Training Epoch: 98 [16384/50000]	Loss: 0.1458	LR: 0.012500
Training Epoch: 98 [16512/50000]	Loss: 0.1888	LR: 0.012500
Training Epoch: 98 [16640/50000]	Loss: 0.1635	LR: 0.012500
Training Epoch: 98 [16768/50000]	Loss: 0.1352	LR: 0.012500
Training Epoch: 98 [16896/50000]	Loss: 0.1742	LR: 0.012500
Training Epoch: 98 [17024/50000]	Loss: 0.2054	LR: 0.012500
Training Epoch: 98 [17152/50000]	Loss: 0.1630	LR: 0.012500
Training Epoch: 98 [17280/50000]	Loss: 0.1644	LR: 0.012500
Training Epoch: 98 [17408/50000]	Loss: 0.2188	LR: 0.012500
Training Epoch: 98 [17536/50000]	Loss: 0.1996	LR: 0.012500
Training Epoch: 98 [17664/50000]	Loss: 0.1752	LR: 0.012500
Training Epoch: 98 [17792/50000]	Loss: 0.1640	LR: 0.012500
Training Epoch: 98 [17920/50000]	Loss: 0.1558	LR: 0.012500
Training Epoch: 98 [18048/50000]	Loss: 0.1552	LR: 0.012500
Training Epoch: 98 [18176/50000]	Loss: 0.1909	LR: 0.012500
Training Epoch: 98 [18304/50000]	Loss: 0.1265	LR: 0.012500
Training Epoch: 98 [18432/50000]	Loss: 0.1347	LR: 0.012500
Training Epoch: 98 [18560/50000]	Loss: 0.1552	LR: 0.012500
Training Epoch: 98 [18688/50000]	Loss: 0.2072	LR: 0.012500
Training Epoch: 98 [18816/50000]	Loss: 0.1518	LR: 0.012500
Training Epoch: 98 [18944/50000]	Loss: 0.1963	LR: 0.012500
Training Epoch: 98 [19072/50000]	Loss: 0.2537	LR: 0.012500
Training Epoch: 98 [19200/50000]	Loss: 0.1730	LR: 0.012500
Training Epoch: 98 [19328/50000]	Loss: 0.1917	LR: 0.012500
Training Epoch: 98 [19456/50000]	Loss: 0.2405	LR: 0.012500
Training Epoch: 98 [19584/50000]	Loss: 0.1088	LR: 0.012500
Training Epoch: 98 [19712/50000]	Loss: 0.1167	LR: 0.012500
Training Epoch: 98 [19840/50000]	Loss: 0.1554	LR: 0.012500
Training Epoch: 98 [19968/50000]	Loss: 0.1647	LR: 0.012500
Training Epoch: 98 [20096/50000]	Loss: 0.2467	LR: 0.012500
Training Epoch: 98 [20224/50000]	Loss: 0.2377	LR: 0.012500
Training Epoch: 98 [20352/50000]	Loss: 0.1127	LR: 0.012500
Training Epoch: 98 [20480/50000]	Loss: 0.1622	LR: 0.012500
Training Epoch: 98 [20608/50000]	Loss: 0.2319	LR: 0.012500
Training Epoch: 98 [20736/50000]	Loss: 0.2015	LR: 0.012500
Training Epoch: 98 [20864/50000]	Loss: 0.1860	LR: 0.012500
Training Epoch: 98 [20992/50000]	Loss: 0.1569	LR: 0.012500
Training Epoch: 98 [21120/50000]	Loss: 0.1901	LR: 0.012500
Training Epoch: 98 [21248/50000]	Loss: 0.1442	LR: 0.012500
Training Epoch: 98 [21376/50000]	Loss: 0.2152	LR: 0.012500
Training Epoch: 98 [21504/50000]	Loss: 0.1438	LR: 0.012500
Training Epoch: 98 [21632/50000]	Loss: 0.2096	LR: 0.012500
Training Epoch: 98 [21760/50000]	Loss: 0.1987	LR: 0.012500
Training Epoch: 98 [21888/50000]	Loss: 0.1501	LR: 0.012500
Training Epoch: 98 [22016/50000]	Loss: 0.1143	LR: 0.012500
Training Epoch: 98 [22144/50000]	Loss: 0.1881	LR: 0.012500
Training Epoch: 98 [22272/50000]	Loss: 0.1836	LR: 0.012500
Training Epoch: 98 [22400/50000]	Loss: 0.2590	LR: 0.012500
Training Epoch: 98 [22528/50000]	Loss: 0.1318	LR: 0.012500
Training Epoch: 98 [22656/50000]	Loss: 0.2569	LR: 0.012500
Training Epoch: 98 [22784/50000]	Loss: 0.1767	LR: 0.012500
Training Epoch: 98 [22912/50000]	Loss: 0.1659	LR: 0.012500
Training Epoch: 98 [23040/50000]	Loss: 0.1706	LR: 0.012500
Training Epoch: 98 [23168/50000]	Loss: 0.2046	LR: 0.012500
Training Epoch: 98 [23296/50000]	Loss: 0.1757	LR: 0.012500
Training Epoch: 98 [23424/50000]	Loss: 0.2071	LR: 0.012500
Training Epoch: 98 [23552/50000]	Loss: 0.2044	LR: 0.012500
Training Epoch: 98 [23680/50000]	Loss: 0.2094	LR: 0.012500
Training Epoch: 98 [23808/50000]	Loss: 0.2374	LR: 0.012500
Training Epoch: 98 [23936/50000]	Loss: 0.1762	LR: 0.012500
Training Epoch: 98 [24064/50000]	Loss: 0.1729	LR: 0.012500
Training Epoch: 98 [24192/50000]	Loss: 0.1828	LR: 0.012500
Training Epoch: 98 [24320/50000]	Loss: 0.1972	LR: 0.012500
Training Epoch: 98 [24448/50000]	Loss: 0.2234	LR: 0.012500
Training Epoch: 98 [24576/50000]	Loss: 0.2903	LR: 0.012500
Training Epoch: 98 [24704/50000]	Loss: 0.1867	LR: 0.012500
Training Epoch: 98 [24832/50000]	Loss: 0.1812	LR: 0.012500
Training Epoch: 98 [24960/50000]	Loss: 0.2114	LR: 0.012500
Training Epoch: 98 [25088/50000]	Loss: 0.1857	LR: 0.012500
Training Epoch: 98 [25216/50000]	Loss: 0.2506	LR: 0.012500
Training Epoch: 98 [25344/50000]	Loss: 0.1317	LR: 0.012500
Training Epoch: 98 [25472/50000]	Loss: 0.2181	LR: 0.012500
Training Epoch: 98 [25600/50000]	Loss: 0.2666	LR: 0.012500
Training Epoch: 98 [25728/50000]	Loss: 0.1523	LR: 0.012500
Training Epoch: 98 [25856/50000]	Loss: 0.1362	LR: 0.012500
Training Epoch: 98 [25984/50000]	Loss: 0.1411	LR: 0.012500
Training Epoch: 98 [26112/50000]	Loss: 0.2203	LR: 0.012500
Training Epoch: 98 [26240/50000]	Loss: 0.2405	LR: 0.012500
Training Epoch: 98 [26368/50000]	Loss: 0.1443	LR: 0.012500
Training Epoch: 98 [26496/50000]	Loss: 0.1548	LR: 0.012500
Training Epoch: 98 [26624/50000]	Loss: 0.2625	LR: 0.012500
Training Epoch: 98 [26752/50000]	Loss: 0.1403	LR: 0.012500
Training Epoch: 98 [26880/50000]	Loss: 0.2358	LR: 0.012500
Training Epoch: 98 [27008/50000]	Loss: 0.1487	LR: 0.012500
Training Epoch: 98 [27136/50000]	Loss: 0.1603	LR: 0.012500
Training Epoch: 98 [27264/50000]	Loss: 0.3122	LR: 0.012500
Training Epoch: 98 [27392/50000]	Loss: 0.1822	LR: 0.012500
Training Epoch: 98 [27520/50000]	Loss: 0.2844	LR: 0.012500
Training Epoch: 98 [27648/50000]	Loss: 0.1447	LR: 0.012500
Training Epoch: 98 [27776/50000]	Loss: 0.2969	LR: 0.012500
Training Epoch: 98 [27904/50000]	Loss: 0.2113	LR: 0.012500
Training Epoch: 98 [28032/50000]	Loss: 0.2624	LR: 0.012500
Training Epoch: 98 [28160/50000]	Loss: 0.2093	LR: 0.012500
Training Epoch: 98 [28288/50000]	Loss: 0.1541	LR: 0.012500
Training Epoch: 98 [28416/50000]	Loss: 0.1640	LR: 0.012500
Training Epoch: 98 [28544/50000]	Loss: 0.2488	LR: 0.012500
Training Epoch: 98 [28672/50000]	Loss: 0.1782	LR: 0.012500
Training Epoch: 98 [28800/50000]	Loss: 0.2658	LR: 0.012500
Training Epoch: 98 [28928/50000]	Loss: 0.1646	LR: 0.012500
Training Epoch: 98 [29056/50000]	Loss: 0.2113	LR: 0.012500
Training Epoch: 98 [29184/50000]	Loss: 0.0934	LR: 0.012500
Training Epoch: 98 [29312/50000]	Loss: 0.2985	LR: 0.012500
Training Epoch: 98 [29440/50000]	Loss: 0.2136	LR: 0.012500
Training Epoch: 98 [29568/50000]	Loss: 0.2277	LR: 0.012500
Training Epoch: 98 [29696/50000]	Loss: 0.2163	LR: 0.012500
Training Epoch: 98 [29824/50000]	Loss: 0.1608	LR: 0.012500
Training Epoch: 98 [29952/50000]	Loss: 0.1933	LR: 0.012500
Training Epoch: 98 [30080/50000]	Loss: 0.1477	LR: 0.012500
Training Epoch: 98 [30208/50000]	Loss: 0.2104	LR: 0.012500
Training Epoch: 98 [30336/50000]	Loss: 0.2804	LR: 0.012500
Training Epoch: 98 [30464/50000]	Loss: 0.1817	LR: 0.012500
Training Epoch: 98 [30592/50000]	Loss: 0.1848	LR: 0.012500
Training Epoch: 98 [30720/50000]	Loss: 0.1972	LR: 0.012500
Training Epoch: 98 [30848/50000]	Loss: 0.1050	LR: 0.012500
Training Epoch: 98 [30976/50000]	Loss: 0.2194	LR: 0.012500
Training Epoch: 98 [31104/50000]	Loss: 0.2226	LR: 0.012500
Training Epoch: 98 [31232/50000]	Loss: 0.1848	LR: 0.012500
Training Epoch: 98 [31360/50000]	Loss: 0.1619	LR: 0.012500
Training Epoch: 98 [31488/50000]	Loss: 0.2134	LR: 0.012500
Training Epoch: 98 [31616/50000]	Loss: 0.1810	LR: 0.012500
Training Epoch: 98 [31744/50000]	Loss: 0.1959	LR: 0.012500
Training Epoch: 98 [31872/50000]	Loss: 0.2937	LR: 0.012500
Training Epoch: 98 [32000/50000]	Loss: 0.2220	LR: 0.012500
Training Epoch: 98 [32128/50000]	Loss: 0.1976	LR: 0.012500
Training Epoch: 98 [32256/50000]	Loss: 0.1521	LR: 0.012500
Training Epoch: 98 [32384/50000]	Loss: 0.1816	LR: 0.012500
Training Epoch: 98 [32512/50000]	Loss: 0.2001	LR: 0.012500
Training Epoch: 98 [32640/50000]	Loss: 0.2317	LR: 0.012500
Training Epoch: 98 [32768/50000]	Loss: 0.2445	LR: 0.012500
Training Epoch: 98 [32896/50000]	Loss: 0.1877	LR: 0.012500
Training Epoch: 98 [33024/50000]	Loss: 0.1526	LR: 0.012500
Training Epoch: 98 [33152/50000]	Loss: 0.3450	LR: 0.012500
Training Epoch: 98 [33280/50000]	Loss: 0.1349	LR: 0.012500
Training Epoch: 98 [33408/50000]	Loss: 0.2095	LR: 0.012500
Training Epoch: 98 [33536/50000]	Loss: 0.2116	LR: 0.012500
Training Epoch: 98 [33664/50000]	Loss: 0.2708	LR: 0.012500
Training Epoch: 98 [33792/50000]	Loss: 0.2990	LR: 0.012500
Training Epoch: 98 [33920/50000]	Loss: 0.1914	LR: 0.012500
Training Epoch: 98 [34048/50000]	Loss: 0.3214	LR: 0.012500
Training Epoch: 98 [34176/50000]	Loss: 0.2136	LR: 0.012500
Training Epoch: 98 [34304/50000]	Loss: 0.1613	LR: 0.012500
Training Epoch: 98 [34432/50000]	Loss: 0.1877	LR: 0.012500
Training Epoch: 98 [34560/50000]	Loss: 0.2229	LR: 0.012500
Training Epoch: 98 [34688/50000]	Loss: 0.2080	LR: 0.012500
Training Epoch: 98 [34816/50000]	Loss: 0.2012	LR: 0.012500
Training Epoch: 98 [34944/50000]	Loss: 0.1989	LR: 0.012500
Training Epoch: 98 [35072/50000]	Loss: 0.1683	LR: 0.012500
Training Epoch: 98 [35200/50000]	Loss: 0.1611	LR: 0.012500
Training Epoch: 98 [35328/50000]	Loss: 0.2367	LR: 0.012500
Training Epoch: 98 [35456/50000]	Loss: 0.1564	LR: 0.012500
Training Epoch: 98 [35584/50000]	Loss: 0.2128	LR: 0.012500
Training Epoch: 98 [35712/50000]	Loss: 0.3084	LR: 0.012500
Training Epoch: 98 [35840/50000]	Loss: 0.2113	LR: 0.012500
Training Epoch: 98 [35968/50000]	Loss: 0.3371	LR: 0.012500
Training Epoch: 98 [36096/50000]	Loss: 0.2284	LR: 0.012500
Training Epoch: 98 [36224/50000]	Loss: 0.2008	LR: 0.012500
Training Epoch: 98 [36352/50000]	Loss: 0.2614	LR: 0.012500
Training Epoch: 98 [36480/50000]	Loss: 0.2398	LR: 0.012500
Training Epoch: 98 [36608/50000]	Loss: 0.1347	LR: 0.012500
Training Epoch: 98 [36736/50000]	Loss: 0.2232	LR: 0.012500
Training Epoch: 98 [36864/50000]	Loss: 0.1638	LR: 0.012500
Training Epoch: 98 [36992/50000]	Loss: 0.2143	LR: 0.012500
Training Epoch: 98 [37120/50000]	Loss: 0.2360	LR: 0.012500
Training Epoch: 98 [37248/50000]	Loss: 0.2188	LR: 0.012500
Training Epoch: 98 [37376/50000]	Loss: 0.2365	LR: 0.012500
Training Epoch: 98 [37504/50000]	Loss: 0.2092	LR: 0.012500
Training Epoch: 98 [37632/50000]	Loss: 0.1392	LR: 0.012500
Training Epoch: 98 [37760/50000]	Loss: 0.1945	LR: 0.012500
Training Epoch: 98 [37888/50000]	Loss: 0.0894	LR: 0.012500
Training Epoch: 98 [38016/50000]	Loss: 0.1952	LR: 0.012500
Training Epoch: 98 [38144/50000]	Loss: 0.1736	LR: 0.012500
Training Epoch: 98 [38272/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 98 [38400/50000]	Loss: 0.1407	LR: 0.012500
Training Epoch: 98 [38528/50000]	Loss: 0.2272	LR: 0.012500
Training Epoch: 98 [38656/50000]	Loss: 0.2369	LR: 0.012500
Training Epoch: 98 [38784/50000]	Loss: 0.1196	LR: 0.012500
Training Epoch: 98 [38912/50000]	Loss: 0.2745	LR: 0.012500
Training Epoch: 98 [39040/50000]	Loss: 0.2222	LR: 0.012500
Training Epoch: 98 [39168/50000]	Loss: 0.2298	LR: 0.012500
Training Epoch: 98 [39296/50000]	Loss: 0.2470	LR: 0.012500
Training Epoch: 98 [39424/50000]	Loss: 0.2849	LR: 0.012500
Training Epoch: 98 [39552/50000]	Loss: 0.2070	LR: 0.012500
Training Epoch: 98 [39680/50000]	Loss: 0.1459	LR: 0.012500
Training Epoch: 98 [39808/50000]	Loss: 0.2833	LR: 0.012500
Training Epoch: 98 [39936/50000]	Loss: 0.1574	LR: 0.012500
Training Epoch: 98 [40064/50000]	Loss: 0.2083	LR: 0.012500
Training Epoch: 98 [40192/50000]	Loss: 0.2281	LR: 0.012500
Training Epoch: 98 [40320/50000]	Loss: 0.3123	LR: 0.012500
Training Epoch: 98 [40448/50000]	Loss: 0.1803	LR: 0.012500
Training Epoch: 98 [40576/50000]	Loss: 0.2079	LR: 0.012500
Training Epoch: 98 [40704/50000]	Loss: 0.2624	LR: 0.012500
Training Epoch: 98 [40832/50000]	Loss: 0.2443	LR: 0.012500
Training Epoch: 98 [40960/50000]	Loss: 0.1675	LR: 0.012500
Training Epoch: 98 [41088/50000]	Loss: 0.2551	LR: 0.012500
Training Epoch: 98 [41216/50000]	Loss: 0.2367	LR: 0.012500
Training Epoch: 98 [41344/50000]	Loss: 0.1563	LR: 0.012500
Training Epoch: 98 [41472/50000]	Loss: 0.2390	LR: 0.012500
Training Epoch: 98 [41600/50000]	Loss: 0.2378	LR: 0.012500
Training Epoch: 98 [41728/50000]	Loss: 0.2643	LR: 0.012500
Training Epoch: 98 [41856/50000]	Loss: 0.2635	LR: 0.012500
Training Epoch: 98 [41984/50000]	Loss: 0.2973	LR: 0.012500
Training Epoch: 98 [42112/50000]	Loss: 0.2393	LR: 0.012500
Training Epoch: 98 [42240/50000]	Loss: 0.1883	LR: 0.012500
Training Epoch: 98 [42368/50000]	Loss: 0.2458	LR: 0.012500
Training Epoch: 98 [42496/50000]	Loss: 0.2219	LR: 0.012500
Training Epoch: 98 [42624/50000]	Loss: 0.1816	LR: 0.012500
Training Epoch: 98 [42752/50000]	Loss: 0.2382	LR: 0.012500
Training Epoch: 98 [42880/50000]	Loss: 0.1673	LR: 0.012500
Training Epoch: 98 [43008/50000]	Loss: 0.2718	LR: 0.012500
Training Epoch: 98 [43136/50000]	Loss: 0.2564	LR: 0.012500
Training Epoch: 98 [43264/50000]	Loss: 0.2574	LR: 0.012500
Training Epoch: 98 [43392/50000]	Loss: 0.2949	LR: 0.012500
Training Epoch: 98 [43520/50000]	Loss: 0.2636	LR: 0.012500
Training Epoch: 98 [43648/50000]	Loss: 0.2560	LR: 0.012500
Training Epoch: 98 [43776/50000]	Loss: 0.1913	LR: 0.012500
Training Epoch: 98 [43904/50000]	Loss: 0.3074	LR: 0.012500
Training Epoch: 98 [44032/50000]	Loss: 0.2530	LR: 0.012500
Training Epoch: 98 [44160/50000]	Loss: 0.1240	LR: 0.012500
Training Epoch: 98 [44288/50000]	Loss: 0.2193	LR: 0.012500
Training Epoch: 98 [44416/50000]	Loss: 0.2372	LR: 0.012500
Training Epoch: 98 [44544/50000]	Loss: 0.2577	LR: 0.012500
Training Epoch: 98 [44672/50000]	Loss: 0.2011	LR: 0.012500
Training Epoch: 98 [44800/50000]	Loss: 0.2617	LR: 0.012500
Training Epoch: 98 [44928/50000]	Loss: 0.2472	LR: 0.012500
Training Epoch: 98 [45056/50000]	Loss: 0.2804	LR: 0.012500
Training Epoch: 98 [45184/50000]	Loss: 0.2148	LR: 0.012500
Training Epoch: 98 [45312/50000]	Loss: 0.2363	LR: 0.012500
Training Epoch: 98 [45440/50000]	Loss: 0.2113	LR: 0.012500
Training Epoch: 98 [45568/50000]	Loss: 0.2340	LR: 0.012500
Training Epoch: 98 [45696/50000]	Loss: 0.2007	LR: 0.012500
Training Epoch: 98 [45824/50000]	Loss: 0.2829	LR: 0.012500
Training Epoch: 98 [45952/50000]	Loss: 0.1980	LR: 0.012500
Training Epoch: 98 [46080/50000]	Loss: 0.3253	LR: 0.012500
Training Epoch: 98 [46208/50000]	Loss: 0.1743	LR: 0.012500
Training Epoch: 98 [46336/50000]	Loss: 0.1593	LR: 0.012500
Training Epoch: 98 [46464/50000]	Loss: 0.2627	LR: 0.012500
Training Epoch: 98 [46592/50000]	Loss: 0.2179	LR: 0.012500
Training Epoch: 98 [46720/50000]	Loss: 0.3491	LR: 0.012500
Training Epoch: 98 [46848/50000]	Loss: 0.2253	LR: 0.012500
Training Epoch: 98 [46976/50000]	Loss: 0.2761	LR: 0.012500
Training Epoch: 98 [47104/50000]	Loss: 0.2555	LR: 0.012500
Training Epoch: 98 [47232/50000]	Loss: 0.2548	LR: 0.012500
Training Epoch: 98 [47360/50000]	Loss: 0.2011	LR: 0.012500
Training Epoch: 98 [47488/50000]	Loss: 0.2003	LR: 0.012500
Training Epoch: 98 [47616/50000]	Loss: 0.3320	LR: 0.012500
Training Epoch: 98 [47744/50000]	Loss: 0.3019	LR: 0.012500
Training Epoch: 98 [47872/50000]	Loss: 0.2408	LR: 0.012500
Training Epoch: 98 [48000/50000]	Loss: 0.1237	LR: 0.012500
Training Epoch: 98 [48128/50000]	Loss: 0.2236	LR: 0.012500
Training Epoch: 98 [48256/50000]	Loss: 0.2520	LR: 0.012500
Training Epoch: 98 [48384/50000]	Loss: 0.2186	LR: 0.012500
Training Epoch: 98 [48512/50000]	Loss: 0.2349	LR: 0.012500
Training Epoch: 98 [48640/50000]	Loss: 0.2949	LR: 0.012500
Training Epoch: 98 [48768/50000]	Loss: 0.2496	LR: 0.012500
Training Epoch: 98 [48896/50000]	Loss: 0.2612	LR: 0.012500
Training Epoch: 98 [49024/50000]	Loss: 0.1993	LR: 0.012500
Training Epoch: 98 [49152/50000]	Loss: 0.2318	LR: 0.012500
Training Epoch: 98 [49280/50000]	Loss: 0.2157	LR: 0.012500
Training Epoch: 98 [49408/50000]	Loss: 0.2422	LR: 0.012500
Training Epoch: 98 [49536/50000]	Loss: 0.2348	LR: 0.012500
Training Epoch: 98 [49664/50000]	Loss: 0.2339	LR: 0.012500
Training Epoch: 98 [49792/50000]	Loss: 0.3934	LR: 0.012500
Training Epoch: 98 [49920/50000]	Loss: 0.2566	LR: 0.012500
Training Epoch: 98 [50000/50000]	Loss: 0.3123	LR: 0.012500
epoch 98 training time consumed: 53.87s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  359665 GB |  359665 GB |
|       from large pool |  123392 KB |    1034 MB |  359311 GB |  359311 GB |
|       from small pool |   10798 KB |      13 MB |     354 GB |     354 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  359665 GB |  359665 GB |
|       from large pool |  123392 KB |    1034 MB |  359311 GB |  359311 GB |
|       from small pool |   10798 KB |      13 MB |     354 GB |     354 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  158273 GB |  158273 GB |
|       from large pool |  155136 KB |  433088 KB |  157882 GB |  157881 GB |
|       from small pool |    1489 KB |    3494 KB |     391 GB |     391 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   13878 K  |   13878 K  |
|       from large pool |      24    |      65    |    7244 K  |    7244 K  |
|       from small pool |     232    |     275    |    6634 K  |    6633 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   13878 K  |   13878 K  |
|       from large pool |      24    |      65    |    7244 K  |    7244 K  |
|       from small pool |     232    |     275    |    6634 K  |    6633 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6895 K  |    6895 K  |
|       from large pool |       9    |      14    |    3506 K  |    3506 K  |
|       from small pool |      12    |      17    |    3388 K  |    3388 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 98, Average loss: 0.0109, Accuracy: 0.6842, Time consumed:3.45s

Training Epoch: 99 [128/50000]	Loss: 0.1744	LR: 0.012500
Training Epoch: 99 [256/50000]	Loss: 0.3003	LR: 0.012500
Training Epoch: 99 [384/50000]	Loss: 0.2158	LR: 0.012500
Training Epoch: 99 [512/50000]	Loss: 0.1972	LR: 0.012500
Training Epoch: 99 [640/50000]	Loss: 0.1936	LR: 0.012500
Training Epoch: 99 [768/50000]	Loss: 0.2416	LR: 0.012500
Training Epoch: 99 [896/50000]	Loss: 0.2685	LR: 0.012500
Training Epoch: 99 [1024/50000]	Loss: 0.2060	LR: 0.012500
Training Epoch: 99 [1152/50000]	Loss: 0.1203	LR: 0.012500
Training Epoch: 99 [1280/50000]	Loss: 0.2426	LR: 0.012500
Training Epoch: 99 [1408/50000]	Loss: 0.3380	LR: 0.012500
Training Epoch: 99 [1536/50000]	Loss: 0.1917	LR: 0.012500
Training Epoch: 99 [1664/50000]	Loss: 0.2386	LR: 0.012500
Training Epoch: 99 [1792/50000]	Loss: 0.2424	LR: 0.012500
Training Epoch: 99 [1920/50000]	Loss: 0.2196	LR: 0.012500
Training Epoch: 99 [2048/50000]	Loss: 0.1765	LR: 0.012500
Training Epoch: 99 [2176/50000]	Loss: 0.1516	LR: 0.012500
Training Epoch: 99 [2304/50000]	Loss: 0.1708	LR: 0.012500
Training Epoch: 99 [2432/50000]	Loss: 0.2158	LR: 0.012500
Training Epoch: 99 [2560/50000]	Loss: 0.1656	LR: 0.012500
Training Epoch: 99 [2688/50000]	Loss: 0.1105	LR: 0.012500
Training Epoch: 99 [2816/50000]	Loss: 0.2172	LR: 0.012500
Training Epoch: 99 [2944/50000]	Loss: 0.1107	LR: 0.012500
Training Epoch: 99 [3072/50000]	Loss: 0.1408	LR: 0.012500
Training Epoch: 99 [3200/50000]	Loss: 0.1649	LR: 0.012500
Training Epoch: 99 [3328/50000]	Loss: 0.1876	LR: 0.012500
Training Epoch: 99 [3456/50000]	Loss: 0.1602	LR: 0.012500
Training Epoch: 99 [3584/50000]	Loss: 0.2443	LR: 0.012500
Training Epoch: 99 [3712/50000]	Loss: 0.2532	LR: 0.012500
Training Epoch: 99 [3840/50000]	Loss: 0.1340	LR: 0.012500
Training Epoch: 99 [3968/50000]	Loss: 0.1495	LR: 0.012500
Training Epoch: 99 [4096/50000]	Loss: 0.1385	LR: 0.012500
Training Epoch: 99 [4224/50000]	Loss: 0.1258	LR: 0.012500
Training Epoch: 99 [4352/50000]	Loss: 0.2379	LR: 0.012500
Training Epoch: 99 [4480/50000]	Loss: 0.2346	LR: 0.012500
Training Epoch: 99 [4608/50000]	Loss: 0.1076	LR: 0.012500
Training Epoch: 99 [4736/50000]	Loss: 0.2422	LR: 0.012500
Training Epoch: 99 [4864/50000]	Loss: 0.2046	LR: 0.012500
Training Epoch: 99 [4992/50000]	Loss: 0.1244	LR: 0.012500
Training Epoch: 99 [5120/50000]	Loss: 0.1210	LR: 0.012500
Training Epoch: 99 [5248/50000]	Loss: 0.2063	LR: 0.012500
Training Epoch: 99 [5376/50000]	Loss: 0.1478	LR: 0.012500
Training Epoch: 99 [5504/50000]	Loss: 0.1107	LR: 0.012500
Training Epoch: 99 [5632/50000]	Loss: 0.1112	LR: 0.012500
Training Epoch: 99 [5760/50000]	Loss: 0.2234	LR: 0.012500
Training Epoch: 99 [5888/50000]	Loss: 0.1112	LR: 0.012500
Training Epoch: 99 [6016/50000]	Loss: 0.2221	LR: 0.012500
Training Epoch: 99 [6144/50000]	Loss: 0.2441	LR: 0.012500
Training Epoch: 99 [6272/50000]	Loss: 0.1878	LR: 0.012500
Training Epoch: 99 [6400/50000]	Loss: 0.1645	LR: 0.012500
Training Epoch: 99 [6528/50000]	Loss: 0.1575	LR: 0.012500
Training Epoch: 99 [6656/50000]	Loss: 0.2283	LR: 0.012500
Training Epoch: 99 [6784/50000]	Loss: 0.2020	LR: 0.012500
Training Epoch: 99 [6912/50000]	Loss: 0.2642	LR: 0.012500
Training Epoch: 99 [7040/50000]	Loss: 0.3453	LR: 0.012500
Training Epoch: 99 [7168/50000]	Loss: 0.1983	LR: 0.012500
Training Epoch: 99 [7296/50000]	Loss: 0.3260	LR: 0.012500
Training Epoch: 99 [7424/50000]	Loss: 0.1300	LR: 0.012500
Training Epoch: 99 [7552/50000]	Loss: 0.2552	LR: 0.012500
Training Epoch: 99 [7680/50000]	Loss: 0.1867	LR: 0.012500
Training Epoch: 99 [7808/50000]	Loss: 0.2579	LR: 0.012500
Training Epoch: 99 [7936/50000]	Loss: 0.2047	LR: 0.012500
Training Epoch: 99 [8064/50000]	Loss: 0.1237	LR: 0.012500
Training Epoch: 99 [8192/50000]	Loss: 0.1900	LR: 0.012500
Training Epoch: 99 [8320/50000]	Loss: 0.3062	LR: 0.012500
Training Epoch: 99 [8448/50000]	Loss: 0.2109	LR: 0.012500
Training Epoch: 99 [8576/50000]	Loss: 0.2118	LR: 0.012500
Training Epoch: 99 [8704/50000]	Loss: 0.0818	LR: 0.012500
Training Epoch: 99 [8832/50000]	Loss: 0.2666	LR: 0.012500
Training Epoch: 99 [8960/50000]	Loss: 0.1928	LR: 0.012500
Training Epoch: 99 [9088/50000]	Loss: 0.1598	LR: 0.012500
Training Epoch: 99 [9216/50000]	Loss: 0.1288	LR: 0.012500
Training Epoch: 99 [9344/50000]	Loss: 0.2334	LR: 0.012500
Training Epoch: 99 [9472/50000]	Loss: 0.1433	LR: 0.012500
Training Epoch: 99 [9600/50000]	Loss: 0.1651	LR: 0.012500
Training Epoch: 99 [9728/50000]	Loss: 0.1477	LR: 0.012500
Training Epoch: 99 [9856/50000]	Loss: 0.1747	LR: 0.012500
Training Epoch: 99 [9984/50000]	Loss: 0.2964	LR: 0.012500
Training Epoch: 99 [10112/50000]	Loss: 0.2322	LR: 0.012500
Training Epoch: 99 [10240/50000]	Loss: 0.1526	LR: 0.012500
Training Epoch: 99 [10368/50000]	Loss: 0.1397	LR: 0.012500
Training Epoch: 99 [10496/50000]	Loss: 0.1973	LR: 0.012500
Training Epoch: 99 [10624/50000]	Loss: 0.1739	LR: 0.012500
Training Epoch: 99 [10752/50000]	Loss: 0.1766	LR: 0.012500
Training Epoch: 99 [10880/50000]	Loss: 0.1179	LR: 0.012500
Training Epoch: 99 [11008/50000]	Loss: 0.2472	LR: 0.012500
Training Epoch: 99 [11136/50000]	Loss: 0.1525	LR: 0.012500
Training Epoch: 99 [11264/50000]	Loss: 0.1460	LR: 0.012500
Training Epoch: 99 [11392/50000]	Loss: 0.1921	LR: 0.012500
Training Epoch: 99 [11520/50000]	Loss: 0.2366	LR: 0.012500
Training Epoch: 99 [11648/50000]	Loss: 0.2376	LR: 0.012500
Training Epoch: 99 [11776/50000]	Loss: 0.2249	LR: 0.012500
Training Epoch: 99 [11904/50000]	Loss: 0.2882	LR: 0.012500
Training Epoch: 99 [12032/50000]	Loss: 0.1263	LR: 0.012500
Training Epoch: 99 [12160/50000]	Loss: 0.2787	LR: 0.012500
Training Epoch: 99 [12288/50000]	Loss: 0.1908	LR: 0.012500
Training Epoch: 99 [12416/50000]	Loss: 0.2614	LR: 0.012500
Training Epoch: 99 [12544/50000]	Loss: 0.1761	LR: 0.012500
Training Epoch: 99 [12672/50000]	Loss: 0.1553	LR: 0.012500
Training Epoch: 99 [12800/50000]	Loss: 0.2396	LR: 0.012500
Training Epoch: 99 [12928/50000]	Loss: 0.1945	LR: 0.012500
Training Epoch: 99 [13056/50000]	Loss: 0.1605	LR: 0.012500
Training Epoch: 99 [13184/50000]	Loss: 0.1711	LR: 0.012500
Training Epoch: 99 [13312/50000]	Loss: 0.2000	LR: 0.012500
Training Epoch: 99 [13440/50000]	Loss: 0.1589	LR: 0.012500
Training Epoch: 99 [13568/50000]	Loss: 0.1734	LR: 0.012500
Training Epoch: 99 [13696/50000]	Loss: 0.2042	LR: 0.012500
Training Epoch: 99 [13824/50000]	Loss: 0.1542	LR: 0.012500
Training Epoch: 99 [13952/50000]	Loss: 0.1718	LR: 0.012500
Training Epoch: 99 [14080/50000]	Loss: 0.1268	LR: 0.012500
Training Epoch: 99 [14208/50000]	Loss: 0.1884	LR: 0.012500
Training Epoch: 99 [14336/50000]	Loss: 0.1246	LR: 0.012500
Training Epoch: 99 [14464/50000]	Loss: 0.2029	LR: 0.012500
Training Epoch: 99 [14592/50000]	Loss: 0.1701	LR: 0.012500
Training Epoch: 99 [14720/50000]	Loss: 0.3663	LR: 0.012500
Training Epoch: 99 [14848/50000]	Loss: 0.2037	LR: 0.012500
Training Epoch: 99 [14976/50000]	Loss: 0.2296	LR: 0.012500
Training Epoch: 99 [15104/50000]	Loss: 0.2980	LR: 0.012500
Training Epoch: 99 [15232/50000]	Loss: 0.1581	LR: 0.012500
Training Epoch: 99 [15360/50000]	Loss: 0.1798	LR: 0.012500
Training Epoch: 99 [15488/50000]	Loss: 0.2030	LR: 0.012500
Training Epoch: 99 [15616/50000]	Loss: 0.2277	LR: 0.012500
Training Epoch: 99 [15744/50000]	Loss: 0.2130	LR: 0.012500
Training Epoch: 99 [15872/50000]	Loss: 0.2176	LR: 0.012500
Training Epoch: 99 [16000/50000]	Loss: 0.1659	LR: 0.012500
Training Epoch: 99 [16128/50000]	Loss: 0.1779	LR: 0.012500
Training Epoch: 99 [16256/50000]	Loss: 0.2034	LR: 0.012500
Training Epoch: 99 [16384/50000]	Loss: 0.2133	LR: 0.012500
Training Epoch: 99 [16512/50000]	Loss: 0.2441	LR: 0.012500
Training Epoch: 99 [16640/50000]	Loss: 0.1534	LR: 0.012500
Training Epoch: 99 [16768/50000]	Loss: 0.1129	LR: 0.012500
Training Epoch: 99 [16896/50000]	Loss: 0.3396	LR: 0.012500
Training Epoch: 99 [17024/50000]	Loss: 0.1628	LR: 0.012500
Training Epoch: 99 [17152/50000]	Loss: 0.1714	LR: 0.012500
Training Epoch: 99 [17280/50000]	Loss: 0.2129	LR: 0.012500
Training Epoch: 99 [17408/50000]	Loss: 0.2117	LR: 0.012500
Training Epoch: 99 [17536/50000]	Loss: 0.2338	LR: 0.012500
Training Epoch: 99 [17664/50000]	Loss: 0.1008	LR: 0.012500
Training Epoch: 99 [17792/50000]	Loss: 0.1168	LR: 0.012500
Training Epoch: 99 [17920/50000]	Loss: 0.2248	LR: 0.012500
Training Epoch: 99 [18048/50000]	Loss: 0.2209	LR: 0.012500
Training Epoch: 99 [18176/50000]	Loss: 0.2071	LR: 0.012500
Training Epoch: 99 [18304/50000]	Loss: 0.0929	LR: 0.012500
Training Epoch: 99 [18432/50000]	Loss: 0.2029	LR: 0.012500
Training Epoch: 99 [18560/50000]	Loss: 0.1813	LR: 0.012500
Training Epoch: 99 [18688/50000]	Loss: 0.1547	LR: 0.012500
Training Epoch: 99 [18816/50000]	Loss: 0.1917	LR: 0.012500
Training Epoch: 99 [18944/50000]	Loss: 0.1319	LR: 0.012500
Training Epoch: 99 [19072/50000]	Loss: 0.2363	LR: 0.012500
Training Epoch: 99 [19200/50000]	Loss: 0.1667	LR: 0.012500
Training Epoch: 99 [19328/50000]	Loss: 0.2922	LR: 0.012500
Training Epoch: 99 [19456/50000]	Loss: 0.1769	LR: 0.012500
Training Epoch: 99 [19584/50000]	Loss: 0.1699	LR: 0.012500
Training Epoch: 99 [19712/50000]	Loss: 0.1911	LR: 0.012500
Training Epoch: 99 [19840/50000]	Loss: 0.1756	LR: 0.012500
Training Epoch: 99 [19968/50000]	Loss: 0.1840	LR: 0.012500
Training Epoch: 99 [20096/50000]	Loss: 0.2560	LR: 0.012500
Training Epoch: 99 [20224/50000]	Loss: 0.2656	LR: 0.012500
Training Epoch: 99 [20352/50000]	Loss: 0.1539	LR: 0.012500
Training Epoch: 99 [20480/50000]	Loss: 0.3341	LR: 0.012500
Training Epoch: 99 [20608/50000]	Loss: 0.1334	LR: 0.012500
Training Epoch: 99 [20736/50000]	Loss: 0.2126	LR: 0.012500
Training Epoch: 99 [20864/50000]	Loss: 0.1933	LR: 0.012500
Training Epoch: 99 [20992/50000]	Loss: 0.1702	LR: 0.012500
Training Epoch: 99 [21120/50000]	Loss: 0.1477	LR: 0.012500
Training Epoch: 99 [21248/50000]	Loss: 0.1625	LR: 0.012500
Training Epoch: 99 [21376/50000]	Loss: 0.1902	LR: 0.012500
Training Epoch: 99 [21504/50000]	Loss: 0.1885	LR: 0.012500
Training Epoch: 99 [21632/50000]	Loss: 0.2553	LR: 0.012500
Training Epoch: 99 [21760/50000]	Loss: 0.2372	LR: 0.012500
Training Epoch: 99 [21888/50000]	Loss: 0.1697	LR: 0.012500
Training Epoch: 99 [22016/50000]	Loss: 0.1532	LR: 0.012500
Training Epoch: 99 [22144/50000]	Loss: 0.1728	LR: 0.012500
Training Epoch: 99 [22272/50000]	Loss: 0.2014	LR: 0.012500
Training Epoch: 99 [22400/50000]	Loss: 0.2157	LR: 0.012500
Training Epoch: 99 [22528/50000]	Loss: 0.1531	LR: 0.012500
Training Epoch: 99 [22656/50000]	Loss: 0.2065	LR: 0.012500
Training Epoch: 99 [22784/50000]	Loss: 0.1218	LR: 0.012500
Training Epoch: 99 [22912/50000]	Loss: 0.1661	LR: 0.012500
Training Epoch: 99 [23040/50000]	Loss: 0.2082	LR: 0.012500
Training Epoch: 99 [23168/50000]	Loss: 0.1665	LR: 0.012500
Training Epoch: 99 [23296/50000]	Loss: 0.2000	LR: 0.012500
Training Epoch: 99 [23424/50000]	Loss: 0.2268	LR: 0.012500
Training Epoch: 99 [23552/50000]	Loss: 0.1874	LR: 0.012500
Training Epoch: 99 [23680/50000]	Loss: 0.1601	LR: 0.012500
Training Epoch: 99 [23808/50000]	Loss: 0.1486	LR: 0.012500
Training Epoch: 99 [23936/50000]	Loss: 0.2306	LR: 0.012500
Training Epoch: 99 [24064/50000]	Loss: 0.1264	LR: 0.012500
Training Epoch: 99 [24192/50000]	Loss: 0.2899	LR: 0.012500
Training Epoch: 99 [24320/50000]	Loss: 0.2264	LR: 0.012500
Training Epoch: 99 [24448/50000]	Loss: 0.2435	LR: 0.012500
Training Epoch: 99 [24576/50000]	Loss: 0.2567	LR: 0.012500
Training Epoch: 99 [24704/50000]	Loss: 0.1743	LR: 0.012500
Training Epoch: 99 [24832/50000]	Loss: 0.1721	LR: 0.012500
Training Epoch: 99 [24960/50000]	Loss: 0.2680	LR: 0.012500
Training Epoch: 99 [25088/50000]	Loss: 0.1683	LR: 0.012500
Training Epoch: 99 [25216/50000]	Loss: 0.1697	LR: 0.012500
Training Epoch: 99 [25344/50000]	Loss: 0.2126	LR: 0.012500
Training Epoch: 99 [25472/50000]	Loss: 0.2092	LR: 0.012500
Training Epoch: 99 [25600/50000]	Loss: 0.1510	LR: 0.012500
Training Epoch: 99 [25728/50000]	Loss: 0.1785	LR: 0.012500
Training Epoch: 99 [25856/50000]	Loss: 0.0937	LR: 0.012500
Training Epoch: 99 [25984/50000]	Loss: 0.1234	LR: 0.012500
Training Epoch: 99 [26112/50000]	Loss: 0.1637	LR: 0.012500
Training Epoch: 99 [26240/50000]	Loss: 0.2221	LR: 0.012500
Training Epoch: 99 [26368/50000]	Loss: 0.1940	LR: 0.012500
Training Epoch: 99 [26496/50000]	Loss: 0.1919	LR: 0.012500
Training Epoch: 99 [26624/50000]	Loss: 0.2445	LR: 0.012500
Training Epoch: 99 [26752/50000]	Loss: 0.2097	LR: 0.012500
Training Epoch: 99 [26880/50000]	Loss: 0.1698	LR: 0.012500
Training Epoch: 99 [27008/50000]	Loss: 0.1928	LR: 0.012500
Training Epoch: 99 [27136/50000]	Loss: 0.2217	LR: 0.012500
Training Epoch: 99 [27264/50000]	Loss: 0.2168	LR: 0.012500
Training Epoch: 99 [27392/50000]	Loss: 0.1228	LR: 0.012500
Training Epoch: 99 [27520/50000]	Loss: 0.1804	LR: 0.012500
Training Epoch: 99 [27648/50000]	Loss: 0.1166	LR: 0.012500
Training Epoch: 99 [27776/50000]	Loss: 0.1665	LR: 0.012500
Training Epoch: 99 [27904/50000]	Loss: 0.1921	LR: 0.012500
Training Epoch: 99 [28032/50000]	Loss: 0.2379	LR: 0.012500
Training Epoch: 99 [28160/50000]	Loss: 0.2694	LR: 0.012500
Training Epoch: 99 [28288/50000]	Loss: 0.1528	LR: 0.012500
Training Epoch: 99 [28416/50000]	Loss: 0.2845	LR: 0.012500
Training Epoch: 99 [28544/50000]	Loss: 0.1917	LR: 0.012500
Training Epoch: 99 [28672/50000]	Loss: 0.1769	LR: 0.012500
Training Epoch: 99 [28800/50000]	Loss: 0.0861	LR: 0.012500
Training Epoch: 99 [28928/50000]	Loss: 0.1625	LR: 0.012500
Training Epoch: 99 [29056/50000]	Loss: 0.2120	LR: 0.012500
Training Epoch: 99 [29184/50000]	Loss: 0.2770	LR: 0.012500
Training Epoch: 99 [29312/50000]	Loss: 0.3035	LR: 0.012500
Training Epoch: 99 [29440/50000]	Loss: 0.1434	LR: 0.012500
Training Epoch: 99 [29568/50000]	Loss: 0.1511	LR: 0.012500
Training Epoch: 99 [29696/50000]	Loss: 0.1264	LR: 0.012500
Training Epoch: 99 [29824/50000]	Loss: 0.1932	LR: 0.012500
Training Epoch: 99 [29952/50000]	Loss: 0.2190	LR: 0.012500
Training Epoch: 99 [30080/50000]	Loss: 0.2615	LR: 0.012500
Training Epoch: 99 [30208/50000]	Loss: 0.1363	LR: 0.012500
Training Epoch: 99 [30336/50000]	Loss: 0.2026	LR: 0.012500
Training Epoch: 99 [30464/50000]	Loss: 0.1834	LR: 0.012500
Training Epoch: 99 [30592/50000]	Loss: 0.2189	LR: 0.012500
Training Epoch: 99 [30720/50000]	Loss: 0.1934	LR: 0.012500
Training Epoch: 99 [30848/50000]	Loss: 0.1275	LR: 0.012500
Training Epoch: 99 [30976/50000]	Loss: 0.2377	LR: 0.012500
Training Epoch: 99 [31104/50000]	Loss: 0.1537	LR: 0.012500
Training Epoch: 99 [31232/50000]	Loss: 0.2185	LR: 0.012500
Training Epoch: 99 [31360/50000]	Loss: 0.1147	LR: 0.012500
Training Epoch: 99 [31488/50000]	Loss: 0.2289	LR: 0.012500
Training Epoch: 99 [31616/50000]	Loss: 0.1838	LR: 0.012500
Training Epoch: 99 [31744/50000]	Loss: 0.2652	LR: 0.012500
Training Epoch: 99 [31872/50000]	Loss: 0.2102	LR: 0.012500
Training Epoch: 99 [32000/50000]	Loss: 0.1358	LR: 0.012500
Training Epoch: 99 [32128/50000]	Loss: 0.1842	LR: 0.012500
Training Epoch: 99 [32256/50000]	Loss: 0.1885	LR: 0.012500
Training Epoch: 99 [32384/50000]	Loss: 0.1572	LR: 0.012500
Training Epoch: 99 [32512/50000]	Loss: 0.1936	LR: 0.012500
Training Epoch: 99 [32640/50000]	Loss: 0.2005	LR: 0.012500
Training Epoch: 99 [32768/50000]	Loss: 0.2256	LR: 0.012500
Training Epoch: 99 [32896/50000]	Loss: 0.2604	LR: 0.012500
Training Epoch: 99 [33024/50000]	Loss: 0.2179	LR: 0.012500
Training Epoch: 99 [33152/50000]	Loss: 0.2407	LR: 0.012500
Training Epoch: 99 [33280/50000]	Loss: 0.1880	LR: 0.012500
Training Epoch: 99 [33408/50000]	Loss: 0.2672	LR: 0.012500
Training Epoch: 99 [33536/50000]	Loss: 0.1939	LR: 0.012500
Training Epoch: 99 [33664/50000]	Loss: 0.2028	LR: 0.012500
Training Epoch: 99 [33792/50000]	Loss: 0.1481	LR: 0.012500
Training Epoch: 99 [33920/50000]	Loss: 0.2033	LR: 0.012500
Training Epoch: 99 [34048/50000]	Loss: 0.1584	LR: 0.012500
Training Epoch: 99 [34176/50000]	Loss: 0.2095	LR: 0.012500
Training Epoch: 99 [34304/50000]	Loss: 0.2641	LR: 0.012500
Training Epoch: 99 [34432/50000]	Loss: 0.1903	LR: 0.012500
Training Epoch: 99 [34560/50000]	Loss: 0.1451	LR: 0.012500
Training Epoch: 99 [34688/50000]	Loss: 0.2098	LR: 0.012500
Training Epoch: 99 [34816/50000]	Loss: 0.1898	LR: 0.012500
Training Epoch: 99 [34944/50000]	Loss: 0.2481	LR: 0.012500
Training Epoch: 99 [35072/50000]	Loss: 0.2463	LR: 0.012500
Training Epoch: 99 [35200/50000]	Loss: 0.1492	LR: 0.012500
Training Epoch: 99 [35328/50000]	Loss: 0.1199	LR: 0.012500
Training Epoch: 99 [35456/50000]	Loss: 0.2289	LR: 0.012500
Training Epoch: 99 [35584/50000]	Loss: 0.2770	LR: 0.012500
Training Epoch: 99 [35712/50000]	Loss: 0.2625	LR: 0.012500
Training Epoch: 99 [35840/50000]	Loss: 0.2302	LR: 0.012500
Training Epoch: 99 [35968/50000]	Loss: 0.2198	LR: 0.012500
Training Epoch: 99 [36096/50000]	Loss: 0.2286	LR: 0.012500
Training Epoch: 99 [36224/50000]	Loss: 0.1751	LR: 0.012500
Training Epoch: 99 [36352/50000]	Loss: 0.1781	LR: 0.012500
Training Epoch: 99 [36480/50000]	Loss: 0.1681	LR: 0.012500
Training Epoch: 99 [36608/50000]	Loss: 0.3107	LR: 0.012500
Training Epoch: 99 [36736/50000]	Loss: 0.1602	LR: 0.012500
Training Epoch: 99 [36864/50000]	Loss: 0.1998	LR: 0.012500
Training Epoch: 99 [36992/50000]	Loss: 0.2634	LR: 0.012500
Training Epoch: 99 [37120/50000]	Loss: 0.2187	LR: 0.012500
Training Epoch: 99 [37248/50000]	Loss: 0.1975	LR: 0.012500
Training Epoch: 99 [37376/50000]	Loss: 0.2426	LR: 0.012500
Training Epoch: 99 [37504/50000]	Loss: 0.2296	LR: 0.012500
Training Epoch: 99 [37632/50000]	Loss: 0.2199	LR: 0.012500
Training Epoch: 99 [37760/50000]	Loss: 0.1442	LR: 0.012500
Training Epoch: 99 [37888/50000]	Loss: 0.2051	LR: 0.012500
Training Epoch: 99 [38016/50000]	Loss: 0.2132	LR: 0.012500
Training Epoch: 99 [38144/50000]	Loss: 0.1365	LR: 0.012500
Training Epoch: 99 [38272/50000]	Loss: 0.1770	LR: 0.012500
Training Epoch: 99 [38400/50000]	Loss: 0.2438	LR: 0.012500
Training Epoch: 99 [38528/50000]	Loss: 0.1361	LR: 0.012500
Training Epoch: 99 [38656/50000]	Loss: 0.3194	LR: 0.012500
Training Epoch: 99 [38784/50000]	Loss: 0.2415	LR: 0.012500
Training Epoch: 99 [38912/50000]	Loss: 0.2938	LR: 0.012500
Training Epoch: 99 [39040/50000]	Loss: 0.2231	LR: 0.012500
Training Epoch: 99 [39168/50000]	Loss: 0.1464	LR: 0.012500
Training Epoch: 99 [39296/50000]	Loss: 0.2014	LR: 0.012500
Training Epoch: 99 [39424/50000]	Loss: 0.1327	LR: 0.012500
Training Epoch: 99 [39552/50000]	Loss: 0.1980	LR: 0.012500
Training Epoch: 99 [39680/50000]	Loss: 0.2781	LR: 0.012500
Training Epoch: 99 [39808/50000]	Loss: 0.1944	LR: 0.012500
Training Epoch: 99 [39936/50000]	Loss: 0.2002	LR: 0.012500
Training Epoch: 99 [40064/50000]	Loss: 0.1519	LR: 0.012500
Training Epoch: 99 [40192/50000]	Loss: 0.1926	LR: 0.012500
Training Epoch: 99 [40320/50000]	Loss: 0.1739	LR: 0.012500
Training Epoch: 99 [40448/50000]	Loss: 0.2537	LR: 0.012500
Training Epoch: 99 [40576/50000]	Loss: 0.1905	LR: 0.012500
Training Epoch: 99 [40704/50000]	Loss: 0.1971	LR: 0.012500
Training Epoch: 99 [40832/50000]	Loss: 0.2095	LR: 0.012500
Training Epoch: 99 [40960/50000]	Loss: 0.2574	LR: 0.012500
Training Epoch: 99 [41088/50000]	Loss: 0.1667	LR: 0.012500
Training Epoch: 99 [41216/50000]	Loss: 0.1511	LR: 0.012500
Training Epoch: 99 [41344/50000]	Loss: 0.2668	LR: 0.012500
Training Epoch: 99 [41472/50000]	Loss: 0.1375	LR: 0.012500
Training Epoch: 99 [41600/50000]	Loss: 0.1868	LR: 0.012500
Training Epoch: 99 [41728/50000]	Loss: 0.2711	LR: 0.012500
Training Epoch: 99 [41856/50000]	Loss: 0.2033	LR: 0.012500
Training Epoch: 99 [41984/50000]	Loss: 0.1870	LR: 0.012500
Training Epoch: 99 [42112/50000]	Loss: 0.2901	LR: 0.012500
Training Epoch: 99 [42240/50000]	Loss: 0.1964	LR: 0.012500
Training Epoch: 99 [42368/50000]	Loss: 0.1751	LR: 0.012500
Training Epoch: 99 [42496/50000]	Loss: 0.2053	LR: 0.012500
Training Epoch: 99 [42624/50000]	Loss: 0.1981	LR: 0.012500
Training Epoch: 99 [42752/50000]	Loss: 0.2479	LR: 0.012500
Training Epoch: 99 [42880/50000]	Loss: 0.2327	LR: 0.012500
Training Epoch: 99 [43008/50000]	Loss: 0.2709	LR: 0.012500
Training Epoch: 99 [43136/50000]	Loss: 0.1426	LR: 0.012500
Training Epoch: 99 [43264/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 99 [43392/50000]	Loss: 0.1114	LR: 0.012500
Training Epoch: 99 [43520/50000]	Loss: 0.1847	LR: 0.012500
Training Epoch: 99 [43648/50000]	Loss: 0.2667	LR: 0.012500
Training Epoch: 99 [43776/50000]	Loss: 0.2299	LR: 0.012500
Training Epoch: 99 [43904/50000]	Loss: 0.2392	LR: 0.012500
Training Epoch: 99 [44032/50000]	Loss: 0.1947	LR: 0.012500
Training Epoch: 99 [44160/50000]	Loss: 0.2180	LR: 0.012500
Training Epoch: 99 [44288/50000]	Loss: 0.2987	LR: 0.012500
Training Epoch: 99 [44416/50000]	Loss: 0.1646	LR: 0.012500
Training Epoch: 99 [44544/50000]	Loss: 0.2796	LR: 0.012500
Training Epoch: 99 [44672/50000]	Loss: 0.1737	LR: 0.012500
Training Epoch: 99 [44800/50000]	Loss: 0.1502	LR: 0.012500
Training Epoch: 99 [44928/50000]	Loss: 0.2794	LR: 0.012500
Training Epoch: 99 [45056/50000]	Loss: 0.2544	LR: 0.012500
Training Epoch: 99 [45184/50000]	Loss: 0.2604	LR: 0.012500
Training Epoch: 99 [45312/50000]	Loss: 0.1928	LR: 0.012500
Training Epoch: 99 [45440/50000]	Loss: 0.1700	LR: 0.012500
Training Epoch: 99 [45568/50000]	Loss: 0.2865	LR: 0.012500
Training Epoch: 99 [45696/50000]	Loss: 0.3634	LR: 0.012500
Training Epoch: 99 [45824/50000]	Loss: 0.2762	LR: 0.012500
Training Epoch: 99 [45952/50000]	Loss: 0.2250	LR: 0.012500
Training Epoch: 99 [46080/50000]	Loss: 0.2168	LR: 0.012500
Training Epoch: 99 [46208/50000]	Loss: 0.2272	LR: 0.012500
Training Epoch: 99 [46336/50000]	Loss: 0.2588	LR: 0.012500
Training Epoch: 99 [46464/50000]	Loss: 0.2220	LR: 0.012500
Training Epoch: 99 [46592/50000]	Loss: 0.1913	LR: 0.012500
Training Epoch: 99 [46720/50000]	Loss: 0.1245	LR: 0.012500
Training Epoch: 99 [46848/50000]	Loss: 0.2976	LR: 0.012500
Training Epoch: 99 [46976/50000]	Loss: 0.1606	LR: 0.012500
Training Epoch: 99 [47104/50000]	Loss: 0.2071	LR: 0.012500
Training Epoch: 99 [47232/50000]	Loss: 0.2452	LR: 0.012500
Training Epoch: 99 [47360/50000]	Loss: 0.1784	LR: 0.012500
Training Epoch: 99 [47488/50000]	Loss: 0.2525	LR: 0.012500
Training Epoch: 99 [47616/50000]	Loss: 0.1931	LR: 0.012500
Training Epoch: 99 [47744/50000]	Loss: 0.1647	LR: 0.012500
Training Epoch: 99 [47872/50000]	Loss: 0.2159	LR: 0.012500
Training Epoch: 99 [48000/50000]	Loss: 0.2403	LR: 0.012500
Training Epoch: 99 [48128/50000]	Loss: 0.1724	LR: 0.012500
Training Epoch: 99 [48256/50000]	Loss: 0.2614	LR: 0.012500
Training Epoch: 99 [48384/50000]	Loss: 0.1647	LR: 0.012500
Training Epoch: 99 [48512/50000]	Loss: 0.2292	LR: 0.012500
Training Epoch: 99 [48640/50000]	Loss: 0.1706	LR: 0.012500
Training Epoch: 99 [48768/50000]	Loss: 0.2003	LR: 0.012500
Training Epoch: 99 [48896/50000]	Loss: 0.1499	LR: 0.012500
Training Epoch: 99 [49024/50000]	Loss: 0.2569	LR: 0.012500
Training Epoch: 99 [49152/50000]	Loss: 0.1819	LR: 0.012500
Training Epoch: 99 [49280/50000]	Loss: 0.2959	LR: 0.012500
Training Epoch: 99 [49408/50000]	Loss: 0.1898	LR: 0.012500
Training Epoch: 99 [49536/50000]	Loss: 0.2732	LR: 0.012500
Training Epoch: 99 [49664/50000]	Loss: 0.2462	LR: 0.012500
Training Epoch: 99 [49792/50000]	Loss: 0.3236	LR: 0.012500
Training Epoch: 99 [49920/50000]	Loss: 0.2146	LR: 0.012500
Training Epoch: 99 [50000/50000]	Loss: 0.3497	LR: 0.012500
epoch 99 training time consumed: 53.95s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  363335 GB |  363335 GB |
|       from large pool |  123392 KB |    1034 MB |  362977 GB |  362977 GB |
|       from small pool |   10798 KB |      13 MB |     357 GB |     357 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  363335 GB |  363335 GB |
|       from large pool |  123392 KB |    1034 MB |  362977 GB |  362977 GB |
|       from small pool |   10798 KB |      13 MB |     357 GB |     357 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  159888 GB |  159888 GB |
|       from large pool |  155136 KB |  433088 KB |  159493 GB |  159492 GB |
|       from small pool |    1489 KB |    3494 KB |     395 GB |     395 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   14019 K  |   14019 K  |
|       from large pool |      24    |      65    |    7318 K  |    7318 K  |
|       from small pool |     232    |     275    |    6701 K  |    6701 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   14019 K  |   14019 K  |
|       from large pool |      24    |      65    |    7318 K  |    7318 K  |
|       from small pool |     232    |     275    |    6701 K  |    6701 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    6967 K  |    6967 K  |
|       from large pool |       9    |      14    |    3542 K  |    3542 K  |
|       from small pool |      12    |      17    |    3425 K  |    3425 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 99, Average loss: 0.0107, Accuracy: 0.6921, Time consumed:3.46s

Training Epoch: 100 [128/50000]	Loss: 0.1907	LR: 0.012500
Training Epoch: 100 [256/50000]	Loss: 0.1730	LR: 0.012500
Training Epoch: 100 [384/50000]	Loss: 0.1557	LR: 0.012500
Training Epoch: 100 [512/50000]	Loss: 0.1309	LR: 0.012500
Training Epoch: 100 [640/50000]	Loss: 0.1487	LR: 0.012500
Training Epoch: 100 [768/50000]	Loss: 0.2231	LR: 0.012500
Training Epoch: 100 [896/50000]	Loss: 0.0855	LR: 0.012500
Training Epoch: 100 [1024/50000]	Loss: 0.1163	LR: 0.012500
Training Epoch: 100 [1152/50000]	Loss: 0.2008	LR: 0.012500
Training Epoch: 100 [1280/50000]	Loss: 0.2394	LR: 0.012500
Training Epoch: 100 [1408/50000]	Loss: 0.2238	LR: 0.012500
Training Epoch: 100 [1536/50000]	Loss: 0.1775	LR: 0.012500
Training Epoch: 100 [1664/50000]	Loss: 0.1460	LR: 0.012500
Training Epoch: 100 [1792/50000]	Loss: 0.1438	LR: 0.012500
Training Epoch: 100 [1920/50000]	Loss: 0.1919	LR: 0.012500
Training Epoch: 100 [2048/50000]	Loss: 0.2191	LR: 0.012500
Training Epoch: 100 [2176/50000]	Loss: 0.1606	LR: 0.012500
Training Epoch: 100 [2304/50000]	Loss: 0.2089	LR: 0.012500
Training Epoch: 100 [2432/50000]	Loss: 0.2186	LR: 0.012500
Training Epoch: 100 [2560/50000]	Loss: 0.1994	LR: 0.012500
Training Epoch: 100 [2688/50000]	Loss: 0.1029	LR: 0.012500
Training Epoch: 100 [2816/50000]	Loss: 0.2486	LR: 0.012500
Training Epoch: 100 [2944/50000]	Loss: 0.1627	LR: 0.012500
Training Epoch: 100 [3072/50000]	Loss: 0.1805	LR: 0.012500
Training Epoch: 100 [3200/50000]	Loss: 0.1427	LR: 0.012500
Training Epoch: 100 [3328/50000]	Loss: 0.0956	LR: 0.012500
Training Epoch: 100 [3456/50000]	Loss: 0.1790	LR: 0.012500
Training Epoch: 100 [3584/50000]	Loss: 0.2114	LR: 0.012500
Training Epoch: 100 [3712/50000]	Loss: 0.2047	LR: 0.012500
Training Epoch: 100 [3840/50000]	Loss: 0.2092	LR: 0.012500
Training Epoch: 100 [3968/50000]	Loss: 0.1522	LR: 0.012500
Training Epoch: 100 [4096/50000]	Loss: 0.1762	LR: 0.012500
Training Epoch: 100 [4224/50000]	Loss: 0.1267	LR: 0.012500
Training Epoch: 100 [4352/50000]	Loss: 0.1902	LR: 0.012500
Training Epoch: 100 [4480/50000]	Loss: 0.1763	LR: 0.012500
Training Epoch: 100 [4608/50000]	Loss: 0.1908	LR: 0.012500
Training Epoch: 100 [4736/50000]	Loss: 0.1981	LR: 0.012500
Training Epoch: 100 [4864/50000]	Loss: 0.2154	LR: 0.012500
Training Epoch: 100 [4992/50000]	Loss: 0.1292	LR: 0.012500
Training Epoch: 100 [5120/50000]	Loss: 0.1220	LR: 0.012500
Training Epoch: 100 [5248/50000]	Loss: 0.1873	LR: 0.012500
Training Epoch: 100 [5376/50000]	Loss: 0.0935	LR: 0.012500
Training Epoch: 100 [5504/50000]	Loss: 0.1766	LR: 0.012500
Training Epoch: 100 [5632/50000]	Loss: 0.1535	LR: 0.012500
Training Epoch: 100 [5760/50000]	Loss: 0.1894	LR: 0.012500
Training Epoch: 100 [5888/50000]	Loss: 0.2606	LR: 0.012500
Training Epoch: 100 [6016/50000]	Loss: 0.2014	LR: 0.012500
Training Epoch: 100 [6144/50000]	Loss: 0.1878	LR: 0.012500
Training Epoch: 100 [6272/50000]	Loss: 0.2371	LR: 0.012500
Training Epoch: 100 [6400/50000]	Loss: 0.1646	LR: 0.012500
Training Epoch: 100 [6528/50000]	Loss: 0.1572	LR: 0.012500
Training Epoch: 100 [6656/50000]	Loss: 0.1618	LR: 0.012500
Training Epoch: 100 [6784/50000]	Loss: 0.2629	LR: 0.012500
Training Epoch: 100 [6912/50000]	Loss: 0.2001	LR: 0.012500
Training Epoch: 100 [7040/50000]	Loss: 0.0680	LR: 0.012500
Training Epoch: 100 [7168/50000]	Loss: 0.1891	LR: 0.012500
Training Epoch: 100 [7296/50000]	Loss: 0.1570	LR: 0.012500
Training Epoch: 100 [7424/50000]	Loss: 0.1960	LR: 0.012500
Training Epoch: 100 [7552/50000]	Loss: 0.2321	LR: 0.012500
Training Epoch: 100 [7680/50000]	Loss: 0.1285	LR: 0.012500
Training Epoch: 100 [7808/50000]	Loss: 0.1849	LR: 0.012500
Training Epoch: 100 [7936/50000]	Loss: 0.1982	LR: 0.012500
Training Epoch: 100 [8064/50000]	Loss: 0.1466	LR: 0.012500
Training Epoch: 100 [8192/50000]	Loss: 0.2112	LR: 0.012500
Training Epoch: 100 [8320/50000]	Loss: 0.2389	LR: 0.012500
Training Epoch: 100 [8448/50000]	Loss: 0.1770	LR: 0.012500
Training Epoch: 100 [8576/50000]	Loss: 0.2650	LR: 0.012500
Training Epoch: 100 [8704/50000]	Loss: 0.1853	LR: 0.012500
Training Epoch: 100 [8832/50000]	Loss: 0.1804	LR: 0.012500
Training Epoch: 100 [8960/50000]	Loss: 0.1227	LR: 0.012500
Training Epoch: 100 [9088/50000]	Loss: 0.2263	LR: 0.012500
Training Epoch: 100 [9216/50000]	Loss: 0.2255	LR: 0.012500
Training Epoch: 100 [9344/50000]	Loss: 0.1079	LR: 0.012500
Training Epoch: 100 [9472/50000]	Loss: 0.1949	LR: 0.012500
Training Epoch: 100 [9600/50000]	Loss: 0.1837	LR: 0.012500
Training Epoch: 100 [9728/50000]	Loss: 0.2673	LR: 0.012500
Training Epoch: 100 [9856/50000]	Loss: 0.1942	LR: 0.012500
Training Epoch: 100 [9984/50000]	Loss: 0.1852	LR: 0.012500
Training Epoch: 100 [10112/50000]	Loss: 0.1738	LR: 0.012500
Training Epoch: 100 [10240/50000]	Loss: 0.2026	LR: 0.012500
Training Epoch: 100 [10368/50000]	Loss: 0.1292	LR: 0.012500
Training Epoch: 100 [10496/50000]	Loss: 0.1516	LR: 0.012500
Training Epoch: 100 [10624/50000]	Loss: 0.1861	LR: 0.012500
Training Epoch: 100 [10752/50000]	Loss: 0.1237	LR: 0.012500
Training Epoch: 100 [10880/50000]	Loss: 0.2741	LR: 0.012500
Training Epoch: 100 [11008/50000]	Loss: 0.1991	LR: 0.012500
Training Epoch: 100 [11136/50000]	Loss: 0.1652	LR: 0.012500
Training Epoch: 100 [11264/50000]	Loss: 0.3003	LR: 0.012500
Training Epoch: 100 [11392/50000]	Loss: 0.2958	LR: 0.012500
Training Epoch: 100 [11520/50000]	Loss: 0.1380	LR: 0.012500
Training Epoch: 100 [11648/50000]	Loss: 0.1979	LR: 0.012500
Training Epoch: 100 [11776/50000]	Loss: 0.1333	LR: 0.012500
Training Epoch: 100 [11904/50000]	Loss: 0.2701	LR: 0.012500
Training Epoch: 100 [12032/50000]	Loss: 0.1680	LR: 0.012500
Training Epoch: 100 [12160/50000]	Loss: 0.1270	LR: 0.012500
Training Epoch: 100 [12288/50000]	Loss: 0.1726	LR: 0.012500
Training Epoch: 100 [12416/50000]	Loss: 0.1989	LR: 0.012500
Training Epoch: 100 [12544/50000]	Loss: 0.1364	LR: 0.012500
Training Epoch: 100 [12672/50000]	Loss: 0.2083	LR: 0.012500
Training Epoch: 100 [12800/50000]	Loss: 0.1603	LR: 0.012500
Training Epoch: 100 [12928/50000]	Loss: 0.1102	LR: 0.012500
Training Epoch: 100 [13056/50000]	Loss: 0.2089	LR: 0.012500
Training Epoch: 100 [13184/50000]	Loss: 0.1903	LR: 0.012500
Training Epoch: 100 [13312/50000]	Loss: 0.1952	LR: 0.012500
Training Epoch: 100 [13440/50000]	Loss: 0.1841	LR: 0.012500
Training Epoch: 100 [13568/50000]	Loss: 0.2116	LR: 0.012500
Training Epoch: 100 [13696/50000]	Loss: 0.2703	LR: 0.012500
Training Epoch: 100 [13824/50000]	Loss: 0.1908	LR: 0.012500
Training Epoch: 100 [13952/50000]	Loss: 0.1439	LR: 0.012500
Training Epoch: 100 [14080/50000]	Loss: 0.3207	LR: 0.012500
Training Epoch: 100 [14208/50000]	Loss: 0.2312	LR: 0.012500
Training Epoch: 100 [14336/50000]	Loss: 0.2070	LR: 0.012500
Training Epoch: 100 [14464/50000]	Loss: 0.2191	LR: 0.012500
Training Epoch: 100 [14592/50000]	Loss: 0.2029	LR: 0.012500
Training Epoch: 100 [14720/50000]	Loss: 0.2731	LR: 0.012500
Training Epoch: 100 [14848/50000]	Loss: 0.2199	LR: 0.012500
Training Epoch: 100 [14976/50000]	Loss: 0.1467	LR: 0.012500
Training Epoch: 100 [15104/50000]	Loss: 0.2488	LR: 0.012500
Training Epoch: 100 [15232/50000]	Loss: 0.1757	LR: 0.012500
Training Epoch: 100 [15360/50000]	Loss: 0.1197	LR: 0.012500
Training Epoch: 100 [15488/50000]	Loss: 0.1921	LR: 0.012500
Training Epoch: 100 [15616/50000]	Loss: 0.2334	LR: 0.012500
Training Epoch: 100 [15744/50000]	Loss: 0.1919	LR: 0.012500
Training Epoch: 100 [15872/50000]	Loss: 0.1319	LR: 0.012500
Training Epoch: 100 [16000/50000]	Loss: 0.1136	LR: 0.012500
Training Epoch: 100 [16128/50000]	Loss: 0.1885	LR: 0.012500
Training Epoch: 100 [16256/50000]	Loss: 0.2005	LR: 0.012500
Training Epoch: 100 [16384/50000]	Loss: 0.1371	LR: 0.012500
Training Epoch: 100 [16512/50000]	Loss: 0.1621	LR: 0.012500
Training Epoch: 100 [16640/50000]	Loss: 0.2310	LR: 0.012500
Training Epoch: 100 [16768/50000]	Loss: 0.2012	LR: 0.012500
Training Epoch: 100 [16896/50000]	Loss: 0.1411	LR: 0.012500
Training Epoch: 100 [17024/50000]	Loss: 0.1588	LR: 0.012500
Training Epoch: 100 [17152/50000]	Loss: 0.2161	LR: 0.012500
Training Epoch: 100 [17280/50000]	Loss: 0.3180	LR: 0.012500
Training Epoch: 100 [17408/50000]	Loss: 0.2460	LR: 0.012500
Training Epoch: 100 [17536/50000]	Loss: 0.2143	LR: 0.012500
Training Epoch: 100 [17664/50000]	Loss: 0.2508	LR: 0.012500
Training Epoch: 100 [17792/50000]	Loss: 0.2386	LR: 0.012500
Training Epoch: 100 [17920/50000]	Loss: 0.2140	LR: 0.012500
Training Epoch: 100 [18048/50000]	Loss: 0.1967	LR: 0.012500
Training Epoch: 100 [18176/50000]	Loss: 0.1879	LR: 0.012500
Training Epoch: 100 [18304/50000]	Loss: 0.1629	LR: 0.012500
Training Epoch: 100 [18432/50000]	Loss: 0.1736	LR: 0.012500
Training Epoch: 100 [18560/50000]	Loss: 0.1902	LR: 0.012500
Training Epoch: 100 [18688/50000]	Loss: 0.1465	LR: 0.012500
Training Epoch: 100 [18816/50000]	Loss: 0.1135	LR: 0.012500
Training Epoch: 100 [18944/50000]	Loss: 0.1471	LR: 0.012500
Training Epoch: 100 [19072/50000]	Loss: 0.2084	LR: 0.012500
Training Epoch: 100 [19200/50000]	Loss: 0.1542	LR: 0.012500
Training Epoch: 100 [19328/50000]	Loss: 0.1150	LR: 0.012500
Training Epoch: 100 [19456/50000]	Loss: 0.1862	LR: 0.012500
Training Epoch: 100 [19584/50000]	Loss: 0.2431	LR: 0.012500
Training Epoch: 100 [19712/50000]	Loss: 0.2291	LR: 0.012500
Training Epoch: 100 [19840/50000]	Loss: 0.1779	LR: 0.012500
Training Epoch: 100 [19968/50000]	Loss: 0.2272	LR: 0.012500
Training Epoch: 100 [20096/50000]	Loss: 0.1821	LR: 0.012500
Training Epoch: 100 [20224/50000]	Loss: 0.2430	LR: 0.012500
Training Epoch: 100 [20352/50000]	Loss: 0.1781	LR: 0.012500
Training Epoch: 100 [20480/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 100 [20608/50000]	Loss: 0.1941	LR: 0.012500
Training Epoch: 100 [20736/50000]	Loss: 0.1453	LR: 0.012500
Training Epoch: 100 [20864/50000]	Loss: 0.1805	LR: 0.012500
Training Epoch: 100 [20992/50000]	Loss: 0.1545	LR: 0.012500
Training Epoch: 100 [21120/50000]	Loss: 0.1915	LR: 0.012500
Training Epoch: 100 [21248/50000]	Loss: 0.2296	LR: 0.012500
Training Epoch: 100 [21376/50000]	Loss: 0.1914	LR: 0.012500
Training Epoch: 100 [21504/50000]	Loss: 0.1346	LR: 0.012500
Training Epoch: 100 [21632/50000]	Loss: 0.1739	LR: 0.012500
Training Epoch: 100 [21760/50000]	Loss: 0.1368	LR: 0.012500
Training Epoch: 100 [21888/50000]	Loss: 0.1993	LR: 0.012500
Training Epoch: 100 [22016/50000]	Loss: 0.1826	LR: 0.012500
Training Epoch: 100 [22144/50000]	Loss: 0.2344	LR: 0.012500
Training Epoch: 100 [22272/50000]	Loss: 0.2148	LR: 0.012500
Training Epoch: 100 [22400/50000]	Loss: 0.1674	LR: 0.012500
Training Epoch: 100 [22528/50000]	Loss: 0.1770	LR: 0.012500
Training Epoch: 100 [22656/50000]	Loss: 0.2023	LR: 0.012500
Training Epoch: 100 [22784/50000]	Loss: 0.1331	LR: 0.012500
Training Epoch: 100 [22912/50000]	Loss: 0.2627	LR: 0.012500
Training Epoch: 100 [23040/50000]	Loss: 0.1708	LR: 0.012500
Training Epoch: 100 [23168/50000]	Loss: 0.2108	LR: 0.012500
Training Epoch: 100 [23296/50000]	Loss: 0.2801	LR: 0.012500
Training Epoch: 100 [23424/50000]	Loss: 0.1993	LR: 0.012500
Training Epoch: 100 [23552/50000]	Loss: 0.2618	LR: 0.012500
Training Epoch: 100 [23680/50000]	Loss: 0.1654	LR: 0.012500
Training Epoch: 100 [23808/50000]	Loss: 0.2649	LR: 0.012500
Training Epoch: 100 [23936/50000]	Loss: 0.2340	LR: 0.012500
Training Epoch: 100 [24064/50000]	Loss: 0.2058	LR: 0.012500
Training Epoch: 100 [24192/50000]	Loss: 0.1769	LR: 0.012500
Training Epoch: 100 [24320/50000]	Loss: 0.1354	LR: 0.012500
Training Epoch: 100 [24448/50000]	Loss: 0.2140	LR: 0.012500
Training Epoch: 100 [24576/50000]	Loss: 0.1935	LR: 0.012500
Training Epoch: 100 [24704/50000]	Loss: 0.1932	LR: 0.012500
Training Epoch: 100 [24832/50000]	Loss: 0.2144	LR: 0.012500
Training Epoch: 100 [24960/50000]	Loss: 0.2517	LR: 0.012500
Training Epoch: 100 [25088/50000]	Loss: 0.2153	LR: 0.012500
Training Epoch: 100 [25216/50000]	Loss: 0.2102	LR: 0.012500
Training Epoch: 100 [25344/50000]	Loss: 0.1203	LR: 0.012500
Training Epoch: 100 [25472/50000]	Loss: 0.1866	LR: 0.012500
Training Epoch: 100 [25600/50000]	Loss: 0.2411	LR: 0.012500
Training Epoch: 100 [25728/50000]	Loss: 0.2061	LR: 0.012500
Training Epoch: 100 [25856/50000]	Loss: 0.3350	LR: 0.012500
Training Epoch: 100 [25984/50000]	Loss: 0.1913	LR: 0.012500
Training Epoch: 100 [26112/50000]	Loss: 0.2014	LR: 0.012500
Training Epoch: 100 [26240/50000]	Loss: 0.2280	LR: 0.012500
Training Epoch: 100 [26368/50000]	Loss: 0.2158	LR: 0.012500
Training Epoch: 100 [26496/50000]	Loss: 0.1995	LR: 0.012500
Training Epoch: 100 [26624/50000]	Loss: 0.1962	LR: 0.012500
Training Epoch: 100 [26752/50000]	Loss: 0.2654	LR: 0.012500
Training Epoch: 100 [26880/50000]	Loss: 0.1235	LR: 0.012500
Training Epoch: 100 [27008/50000]	Loss: 0.2057	LR: 0.012500
Training Epoch: 100 [27136/50000]	Loss: 0.2628	LR: 0.012500
Training Epoch: 100 [27264/50000]	Loss: 0.1805	LR: 0.012500
Training Epoch: 100 [27392/50000]	Loss: 0.2236	LR: 0.012500
Training Epoch: 100 [27520/50000]	Loss: 0.1901	LR: 0.012500
Training Epoch: 100 [27648/50000]	Loss: 0.1633	LR: 0.012500
Training Epoch: 100 [27776/50000]	Loss: 0.1594	LR: 0.012500
Training Epoch: 100 [27904/50000]	Loss: 0.2080	LR: 0.012500
Training Epoch: 100 [28032/50000]	Loss: 0.1933	LR: 0.012500
Training Epoch: 100 [28160/50000]	Loss: 0.1822	LR: 0.012500
Training Epoch: 100 [28288/50000]	Loss: 0.1649	LR: 0.012500
Training Epoch: 100 [28416/50000]	Loss: 0.2300	LR: 0.012500
Training Epoch: 100 [28544/50000]	Loss: 0.1413	LR: 0.012500
Training Epoch: 100 [28672/50000]	Loss: 0.1294	LR: 0.012500
Training Epoch: 100 [28800/50000]	Loss: 0.2220	LR: 0.012500
Training Epoch: 100 [28928/50000]	Loss: 0.2053	LR: 0.012500
Training Epoch: 100 [29056/50000]	Loss: 0.1601	LR: 0.012500
Training Epoch: 100 [29184/50000]	Loss: 0.1440	LR: 0.012500
Training Epoch: 100 [29312/50000]	Loss: 0.2329	LR: 0.012500
Training Epoch: 100 [29440/50000]	Loss: 0.1973	LR: 0.012500
Training Epoch: 100 [29568/50000]	Loss: 0.2142	LR: 0.012500
Training Epoch: 100 [29696/50000]	Loss: 0.2201	LR: 0.012500
Training Epoch: 100 [29824/50000]	Loss: 0.1583	LR: 0.012500
Training Epoch: 100 [29952/50000]	Loss: 0.2321	LR: 0.012500
Training Epoch: 100 [30080/50000]	Loss: 0.2228	LR: 0.012500
Training Epoch: 100 [30208/50000]	Loss: 0.2085	LR: 0.012500
Training Epoch: 100 [30336/50000]	Loss: 0.2073	LR: 0.012500
Training Epoch: 100 [30464/50000]	Loss: 0.1829	LR: 0.012500
Training Epoch: 100 [30592/50000]	Loss: 0.1423	LR: 0.012500
Training Epoch: 100 [30720/50000]	Loss: 0.2190	LR: 0.012500
Training Epoch: 100 [30848/50000]	Loss: 0.2605	LR: 0.012500
Training Epoch: 100 [30976/50000]	Loss: 0.2443	LR: 0.012500
Training Epoch: 100 [31104/50000]	Loss: 0.1972	LR: 0.012500
Training Epoch: 100 [31232/50000]	Loss: 0.1859	LR: 0.012500
Training Epoch: 100 [31360/50000]	Loss: 0.3095	LR: 0.012500
Training Epoch: 100 [31488/50000]	Loss: 0.1735	LR: 0.012500
Training Epoch: 100 [31616/50000]	Loss: 0.2274	LR: 0.012500
Training Epoch: 100 [31744/50000]	Loss: 0.1953	LR: 0.012500
Training Epoch: 100 [31872/50000]	Loss: 0.2039	LR: 0.012500
Training Epoch: 100 [32000/50000]	Loss: 0.3217	LR: 0.012500
Training Epoch: 100 [32128/50000]	Loss: 0.1798	LR: 0.012500
Training Epoch: 100 [32256/50000]	Loss: 0.1546	LR: 0.012500
Training Epoch: 100 [32384/50000]	Loss: 0.1437	LR: 0.012500
Training Epoch: 100 [32512/50000]	Loss: 0.2591	LR: 0.012500
Training Epoch: 100 [32640/50000]	Loss: 0.2145	LR: 0.012500
Training Epoch: 100 [32768/50000]	Loss: 0.2481	LR: 0.012500
Training Epoch: 100 [32896/50000]	Loss: 0.1429	LR: 0.012500
Training Epoch: 100 [33024/50000]	Loss: 0.2340	LR: 0.012500
Training Epoch: 100 [33152/50000]	Loss: 0.1591	LR: 0.012500
Training Epoch: 100 [33280/50000]	Loss: 0.2016	LR: 0.012500
Training Epoch: 100 [33408/50000]	Loss: 0.1552	LR: 0.012500
Training Epoch: 100 [33536/50000]	Loss: 0.1959	LR: 0.012500
Training Epoch: 100 [33664/50000]	Loss: 0.1462	LR: 0.012500
Training Epoch: 100 [33792/50000]	Loss: 0.1193	LR: 0.012500
Training Epoch: 100 [33920/50000]	Loss: 0.2109	LR: 0.012500
Training Epoch: 100 [34048/50000]	Loss: 0.2393	LR: 0.012500
Training Epoch: 100 [34176/50000]	Loss: 0.2492	LR: 0.012500
Training Epoch: 100 [34304/50000]	Loss: 0.2460	LR: 0.012500
Training Epoch: 100 [34432/50000]	Loss: 0.2771	LR: 0.012500
Training Epoch: 100 [34560/50000]	Loss: 0.1845	LR: 0.012500
Training Epoch: 100 [34688/50000]	Loss: 0.2163	LR: 0.012500
Training Epoch: 100 [34816/50000]	Loss: 0.3924	LR: 0.012500
Training Epoch: 100 [34944/50000]	Loss: 0.1657	LR: 0.012500
Training Epoch: 100 [35072/50000]	Loss: 0.2284	LR: 0.012500
Training Epoch: 100 [35200/50000]	Loss: 0.1847	LR: 0.012500
Training Epoch: 100 [35328/50000]	Loss: 0.2364	LR: 0.012500
Training Epoch: 100 [35456/50000]	Loss: 0.1668	LR: 0.012500
Training Epoch: 100 [35584/50000]	Loss: 0.2749	LR: 0.012500
Training Epoch: 100 [35712/50000]	Loss: 0.1555	LR: 0.012500
Training Epoch: 100 [35840/50000]	Loss: 0.2463	LR: 0.012500
Training Epoch: 100 [35968/50000]	Loss: 0.2602	LR: 0.012500
Training Epoch: 100 [36096/50000]	Loss: 0.2229	LR: 0.012500
Training Epoch: 100 [36224/50000]	Loss: 0.2440	LR: 0.012500
Training Epoch: 100 [36352/50000]	Loss: 0.2656	LR: 0.012500
Training Epoch: 100 [36480/50000]	Loss: 0.2167	LR: 0.012500
Training Epoch: 100 [36608/50000]	Loss: 0.2126	LR: 0.012500
Training Epoch: 100 [36736/50000]	Loss: 0.2251	LR: 0.012500
Training Epoch: 100 [36864/50000]	Loss: 0.2277	LR: 0.012500
Training Epoch: 100 [36992/50000]	Loss: 0.2763	LR: 0.012500
Training Epoch: 100 [37120/50000]	Loss: 0.1678	LR: 0.012500
Training Epoch: 100 [37248/50000]	Loss: 0.2146	LR: 0.012500
Training Epoch: 100 [37376/50000]	Loss: 0.2521	LR: 0.012500
Training Epoch: 100 [37504/50000]	Loss: 0.2061	LR: 0.012500
Training Epoch: 100 [37632/50000]	Loss: 0.2379	LR: 0.012500
Training Epoch: 100 [37760/50000]	Loss: 0.1589	LR: 0.012500
Training Epoch: 100 [37888/50000]	Loss: 0.1480	LR: 0.012500
Training Epoch: 100 [38016/50000]	Loss: 0.2986	LR: 0.012500
Training Epoch: 100 [38144/50000]	Loss: 0.2299	LR: 0.012500
Training Epoch: 100 [38272/50000]	Loss: 0.1390	LR: 0.012500
Training Epoch: 100 [38400/50000]	Loss: 0.1513	LR: 0.012500
Training Epoch: 100 [38528/50000]	Loss: 0.2887	LR: 0.012500
Training Epoch: 100 [38656/50000]	Loss: 0.1820	LR: 0.012500
Training Epoch: 100 [38784/50000]	Loss: 0.2188	LR: 0.012500
Training Epoch: 100 [38912/50000]	Loss: 0.2621	LR: 0.012500
Training Epoch: 100 [39040/50000]	Loss: 0.2113	LR: 0.012500
Training Epoch: 100 [39168/50000]	Loss: 0.1982	LR: 0.012500
Training Epoch: 100 [39296/50000]	Loss: 0.1020	LR: 0.012500
Training Epoch: 100 [39424/50000]	Loss: 0.3010	LR: 0.012500
Training Epoch: 100 [39552/50000]	Loss: 0.2109	LR: 0.012500
Training Epoch: 100 [39680/50000]	Loss: 0.2221	LR: 0.012500
Training Epoch: 100 [39808/50000]	Loss: 0.2108	LR: 0.012500
Training Epoch: 100 [39936/50000]	Loss: 0.2210	LR: 0.012500
Training Epoch: 100 [40064/50000]	Loss: 0.2151	LR: 0.012500
Training Epoch: 100 [40192/50000]	Loss: 0.2155	LR: 0.012500
Training Epoch: 100 [40320/50000]	Loss: 0.2271	LR: 0.012500
Training Epoch: 100 [40448/50000]	Loss: 0.2095	LR: 0.012500
Training Epoch: 100 [40576/50000]	Loss: 0.2055	LR: 0.012500
Training Epoch: 100 [40704/50000]	Loss: 0.2505	LR: 0.012500
Training Epoch: 100 [40832/50000]	Loss: 0.1789	LR: 0.012500
Training Epoch: 100 [40960/50000]	Loss: 0.2804	LR: 0.012500
Training Epoch: 100 [41088/50000]	Loss: 0.3012	LR: 0.012500
Training Epoch: 100 [41216/50000]	Loss: 0.2474	LR: 0.012500
Training Epoch: 100 [41344/50000]	Loss: 0.3952	LR: 0.012500
Training Epoch: 100 [41472/50000]	Loss: 0.2144	LR: 0.012500
Training Epoch: 100 [41600/50000]	Loss: 0.2378	LR: 0.012500
Training Epoch: 100 [41728/50000]	Loss: 0.2130	LR: 0.012500
Training Epoch: 100 [41856/50000]	Loss: 0.1692	LR: 0.012500
Training Epoch: 100 [41984/50000]	Loss: 0.2516	LR: 0.012500
Training Epoch: 100 [42112/50000]	Loss: 0.2529	LR: 0.012500
Training Epoch: 100 [42240/50000]	Loss: 0.2310	LR: 0.012500
Training Epoch: 100 [42368/50000]	Loss: 0.2766	LR: 0.012500
Training Epoch: 100 [42496/50000]	Loss: 0.2759	LR: 0.012500
Training Epoch: 100 [42624/50000]	Loss: 0.2802	LR: 0.012500
Training Epoch: 100 [42752/50000]	Loss: 0.2177	LR: 0.012500
Training Epoch: 100 [42880/50000]	Loss: 0.2681	LR: 0.012500
Training Epoch: 100 [43008/50000]	Loss: 0.1799	LR: 0.012500
Training Epoch: 100 [43136/50000]	Loss: 0.2133	LR: 0.012500
Training Epoch: 100 [43264/50000]	Loss: 0.3048	LR: 0.012500
Training Epoch: 100 [43392/50000]	Loss: 0.3071	LR: 0.012500
Training Epoch: 100 [43520/50000]	Loss: 0.1197	LR: 0.012500
Training Epoch: 100 [43648/50000]	Loss: 0.2361	LR: 0.012500
Training Epoch: 100 [43776/50000]	Loss: 0.2869	LR: 0.012500
Training Epoch: 100 [43904/50000]	Loss: 0.2468	LR: 0.012500
Training Epoch: 100 [44032/50000]	Loss: 0.2285	LR: 0.012500
Training Epoch: 100 [44160/50000]	Loss: 0.2030	LR: 0.012500
Training Epoch: 100 [44288/50000]	Loss: 0.2945	LR: 0.012500
Training Epoch: 100 [44416/50000]	Loss: 0.2153	LR: 0.012500
Training Epoch: 100 [44544/50000]	Loss: 0.1914	LR: 0.012500
Training Epoch: 100 [44672/50000]	Loss: 0.1886	LR: 0.012500
Training Epoch: 100 [44800/50000]	Loss: 0.2152	LR: 0.012500
Training Epoch: 100 [44928/50000]	Loss: 0.3796	LR: 0.012500
Training Epoch: 100 [45056/50000]	Loss: 0.2009	LR: 0.012500
Training Epoch: 100 [45184/50000]	Loss: 0.2393	LR: 0.012500
Training Epoch: 100 [45312/50000]	Loss: 0.3148	LR: 0.012500
Training Epoch: 100 [45440/50000]	Loss: 0.1640	LR: 0.012500
Training Epoch: 100 [45568/50000]	Loss: 0.4237	LR: 0.012500
Training Epoch: 100 [45696/50000]	Loss: 0.2322	LR: 0.012500
Training Epoch: 100 [45824/50000]	Loss: 0.2464	LR: 0.012500
Training Epoch: 100 [45952/50000]	Loss: 0.2691	LR: 0.012500
Training Epoch: 100 [46080/50000]	Loss: 0.1895	LR: 0.012500
Training Epoch: 100 [46208/50000]	Loss: 0.1705	LR: 0.012500
Training Epoch: 100 [46336/50000]	Loss: 0.2553	LR: 0.012500
Training Epoch: 100 [46464/50000]	Loss: 0.1551	LR: 0.012500
Training Epoch: 100 [46592/50000]	Loss: 0.2130	LR: 0.012500
Training Epoch: 100 [46720/50000]	Loss: 0.2416	LR: 0.012500
Training Epoch: 100 [46848/50000]	Loss: 0.2274	LR: 0.012500
Training Epoch: 100 [46976/50000]	Loss: 0.1805	LR: 0.012500
Training Epoch: 100 [47104/50000]	Loss: 0.2102	LR: 0.012500
Training Epoch: 100 [47232/50000]	Loss: 0.2868	LR: 0.012500
Training Epoch: 100 [47360/50000]	Loss: 0.2321	LR: 0.012500
Training Epoch: 100 [47488/50000]	Loss: 0.2089	LR: 0.012500
Training Epoch: 100 [47616/50000]	Loss: 0.2247	LR: 0.012500
Training Epoch: 100 [47744/50000]	Loss: 0.1661	LR: 0.012500
Training Epoch: 100 [47872/50000]	Loss: 0.2903	LR: 0.012500
Training Epoch: 100 [48000/50000]	Loss: 0.2241	LR: 0.012500
Training Epoch: 100 [48128/50000]	Loss: 0.2133	LR: 0.012500
Training Epoch: 100 [48256/50000]	Loss: 0.2846	LR: 0.012500
Training Epoch: 100 [48384/50000]	Loss: 0.1747	LR: 0.012500
Training Epoch: 100 [48512/50000]	Loss: 0.1851	LR: 0.012500
Training Epoch: 100 [48640/50000]	Loss: 0.2240	LR: 0.012500
Training Epoch: 100 [48768/50000]	Loss: 0.2898	LR: 0.012500
Training Epoch: 100 [48896/50000]	Loss: 0.2462	LR: 0.012500
Training Epoch: 100 [49024/50000]	Loss: 0.2081	LR: 0.012500
Training Epoch: 100 [49152/50000]	Loss: 0.2265	LR: 0.012500
Training Epoch: 100 [49280/50000]	Loss: 0.2626	LR: 0.012500
Training Epoch: 100 [49408/50000]	Loss: 0.2151	LR: 0.012500
Training Epoch: 100 [49536/50000]	Loss: 0.1989	LR: 0.012500
Training Epoch: 100 [49664/50000]	Loss: 0.2571	LR: 0.012500
Training Epoch: 100 [49792/50000]	Loss: 0.2567	LR: 0.012500
Training Epoch: 100 [49920/50000]	Loss: 0.3390	LR: 0.012500
Training Epoch: 100 [50000/50000]	Loss: 0.2122	LR: 0.012500
epoch 100 training time consumed: 54.07s
GPU INFO.....
|===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  134190 KB |    1045 MB |  367005 GB |  367005 GB |
|       from large pool |  123392 KB |    1034 MB |  366644 GB |  366644 GB |
|       from small pool |   10798 KB |      13 MB |     361 GB |     361 GB |
|---------------------------------------------------------------------------|
| Active memory         |  134190 KB |    1045 MB |  367005 GB |  367005 GB |
|       from large pool |  123392 KB |    1034 MB |  366644 GB |  366644 GB |
|       from small pool |   10798 KB |      13 MB |     361 GB |     361 GB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |    1842 MB |    1842 MB |    1842 MB |       0 B  |
|       from large pool |    1826 MB |    1826 MB |    1826 MB |       0 B  |
|       from small pool |      16 MB |      16 MB |      16 MB |       0 B  |
|---------------------------------------------------------------------------|
| Non-releasable memory |  156625 KB |  434719 KB |  161503 GB |  161503 GB |
|       from large pool |  155136 KB |  433088 KB |  161104 GB |  161103 GB |
|       from small pool |    1489 KB |    3494 KB |     399 GB |     399 GB |
|---------------------------------------------------------------------------|
| Allocations           |     256    |     336    |   14161 K  |   14161 K  |
|       from large pool |      24    |      65    |    7392 K  |    7392 K  |
|       from small pool |     232    |     275    |    6769 K  |    6769 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     256    |     336    |   14161 K  |   14161 K  |
|       from large pool |      24    |      65    |    7392 K  |    7392 K  |
|       from small pool |     232    |     275    |    6769 K  |    6769 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      20    |      20    |      20    |       0    |
|       from large pool |      12    |      12    |      12    |       0    |
|       from small pool |       8    |       8    |       8    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      21    |      27    |    7040 K  |    7040 K  |
|       from large pool |       9    |      14    |    3577 K  |    3577 K  |
|       from small pool |      12    |      17    |    3462 K  |    3462 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
Evaluating Network.....
Test set: Epoch: 100, Average loss: 0.0107, Accuracy: 0.6834, Time consumed:3.47s

saving weights file to checkpoint/resnet18/Saturday_27_May_2023_16h_28m_13s_resnet18_cutout_8_cifar100/resnet18-100-regular.pth
